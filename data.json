[
 {
  "repo": "cstack/db_tutorial",
  "language": "C",
  "readme_contents": "# Let's Build a Simple Database\n\n[View rendered tutorial](https://cstack.github.io/db_tutorial/) (with more details on what this is.)\n\n## Notes to myself\n\nRun site locally:\n```\nbundle exec jekyll serve\n```"
 },
 {
  "repo": "rui314/chibicc",
  "language": "C",
  "readme_contents": "# chibicc: A Small C Compiler\n\n(The old master has moved to\n[historical/old](https://github.com/rui314/chibicc/tree/historical/old)\nbranch. This is a new one uploaded in September 2020.)\n\nchibicc is yet another small C compiler that implements most C11\nfeatures. Even though it still probably falls into the \"toy compilers\"\ncategory just like other small compilers do, chibicc can compile\nseveral real-world programs, including [Git](https://git-scm.com/),\n[SQLite](https://sqlite.org),\n[libpng](http://www.libpng.org/pub/png/libpng.html) and chibicc\nitself, without making modifications to the compiled programs.\nGenerated executables of these programs pass their corresponding test\nsuites. So, chibicc actually supports a wide variety of C11 features\nand is able to compile hundreds of thousands of lines of real-world C\ncode correctly.\n\nchibicc is developed as the reference implementation for a book I'm\ncurrently writing about the C compiler and the low-level programming.\nThe book covers the vast topic with an incremental approach; in the first\nchapter, readers will implement a \"compiler\" that accepts just a single\nnumber as a \"language\", which will then gain one feature at a time in each\nsection of the book until the language that the compiler accepts matches\nwhat the C11 spec specifies. I took this incremental approach from [the\npaper](http://scheme2006.cs.uchicago.edu/11-ghuloum.pdf) by Abdulaziz\nGhuloum.\n\nEach commit of this project corresponds to a section of the book. For this\npurpose, not only the final state of the project but each commit was\ncarefully written with readability in mind. Readers should be able to learn\nhow a C language feature can be implemented just by reading one or a few\ncommits of this project. For example, this is how\n[while](https://github.com/rui314/chibicc/commit/773115ab2a9c4b96f804311b95b20e9771f0190a),\n[[]](https://github.com/rui314/chibicc/commit/75fbd3dd6efde12eac8225d8b5723093836170a5),\n[?:](https://github.com/rui314/chibicc/commit/1d0e942fd567a35d296d0f10b7693e98b3dd037c),\nand [thread-local\nvariable](https://github.com/rui314/chibicc/commit/79644e54cc1805e54428cde68b20d6d493b76d34)\nare implemented. If you have plenty of spare time, it might be fun to read\nit from the [first\ncommit](https://github.com/rui314/chibicc/commit/0522e2d77e3ab82d3b80a5be8dbbdc8d4180561c).\n\nIf you like this project, please consider purchasing a copy of the book\nwhen it becomes available! \ud83d\ude00 I publish the source code here to give people\nearly access to it, because I was planing to do that anyway with a\npermissive open-source license after publishing the book. If I don't charge\nfor the source code, it doesn't make much sense to me to keep it private. I\nhope to publish the book in 2021.\nYou can sign up [here](https://forms.gle/sgrMWHGeGjeeEJcX7) to receive a\nnotification when a free chapter is available online or the book is published.\n\nI pronounce chibicc as _chee bee cee cee_. \"chibi\" means \"mini\" or\n\"small\" in Japanese. \"cc\" stands for C compiler.\n\n## Status\n\nchibicc supports almost all mandatory features and most optional\nfeatures of C11 as well as a few GCC language extensions.\n\nFeatures that are often missing in a small compiler but supported by\nchibicc include (but not limited to):\n\n- Preprocessor\n- float, double and long double (x87 80-bit floating point numbers)\n- Bit-fields\n- alloca()\n- Variable-length arrays\n- Compound literals\n- Thread-local variables\n- Atomic variables\n- Common symbols\n- Designated initializers\n- L, u, U and u8 string literals\n- Functions that take or return structs as values, as specified by the\n  x86-64 SystemV ABI\n\nchibicc does not support complex numbers, K&R-style function prototypes\nand GCC-style inline assembly. Digraphs and trigraphs are intentionally\nleft out.\n\nchibicc outputs a simple but nice error message when it finds an error in\nsource code.\n\nThere's no optimization pass. chibicc emits terrible code which is probably\ntwice or more slower than GCC's output. I have a plan to add an\noptimization pass once the frontend is done.\n\nI'm using Ubuntu 20.04 for x86-64 as a development platform. I made a\nfew small changes so that chibicc works on Ubuntu 18.04, Fedora 32 and\nGentoo 2.6, but portability is not my goal at this moment. It may or\nmay not work on systems other than Ubuntu 20.04.\n\n## Internals\n\nchibicc consists of the following stages:\n\n- Tokenize: A tokenizer takes a string as an input, breaks it into a list\n  of tokens and returns them.\n\n- Preprocess: A preprocessor takes as an input a list of tokens and output\n  a new list of macro-expanded tokens. It interprets preprocessor\n  directives while expanding macros.\n\n- Parse: A recursive descendent parser constructs abstract syntax trees\n  from the output of the preprocessor. It also adds a type to each AST\n  node.\n\n- Codegen: A code generator emits an assembly text for given AST nodes.\n\n## Contributing\n\nWhen I find a bug in this compiler, I go back to the original commit that\nintroduced the bug and rewrite the commit history as if there were no such\nbug from the beginning. This is an unusual way of fixing bugs, but as a\npart of a book, it is important to keep every commit bug-free.\n\nThus, I do not take pull requests in this repo. You can send me a pull\nrequest if you find a bug, but it is very likely that I will read your\npatch and then apply that to my previous commits by rewriting history. I'll\ncredit your name somewhere, but your changes will be rewritten by me before\nsubmitted to this repository.\n\nAlso, please assume that I will occasionally force-push my local repository\nto this public one to rewrite history. If you clone this project and make\nlocal commits on top of it, your changes will have to be rebased by hand\nwhen I force-push new commits.\n\n## Design principles\n\nchibicc's core value is its simplicity and the reability of its source\ncode. To achieve this goal, I was careful not to be too clever when\nwriting code. Let me explain what that means.\n\nOftentimes, as you get used to the code base, you are tempted to\n_improve_ the code using more abstractions and clever tricks.\nBut that kind of _improvements_ don't always improve readability for\nfirst-time readers and can actually hurts it. I tried to avoid the\npitfall as much as possible. I wrote this code not for me but for\nfirst-time readers.\n\nIf you take a look at the source code, you'll find a couple of\ndumb-looking pieces of code. These are written intentionally that way\n(but at some places I might be actually missing something,\nthough). Here is a few notable examples:\n\n- The recursive descendent parser contains many similar-looking functions\n  for similar-looking generative grammar rules. You might be tempted\n  to _improve_ it to reduce the duplication using higher-order functions\n  or macros, but I thought that that's too complicated. It's better to\n  allow small duplications instead.\n\n- chibicc doesn't try too hard to save memory. An entire input source\n  file is read to memory first before the tokenizer kicks in, for example.\n\n- Slow algorithms are fine if we know that n isn't too big.\n  For example, we use a linked list as a set in the preprocessor, so\n  the membership check takes O(n) where n is the size of the set.  But\n  that's fine because we know n is usually very small.\n  And even if n can be very big, I stick with a simple slow algorithm\n  until it is proved by benchmarks that that's a bottleneck.\n\n- Each AST node type uses only a few members of the `Node` struct members.\n  Other unused `Node` members are just a waste of memory at runtime.\n  We could save memory using unions, but I decided to simply put everything\n  in the same struct instead. I believe the inefficiency is negligible.\n  Even if it matters, we can always change the code to use unions\n  at any time. I wanted to avoid premature optimization.\n\n- chibicc always allocates heap memory using `calloc`, which is a\n  variant of `malloc` that clears memory with zero. `calloc` is\n  slightly slower than `malloc`, but that should be neligible.\n\n- Last but not least, chibicc allocates memory using `calloc` but never\n  calls `free`. Allocated heap memory is not freed until the process exits.\n  I'm sure that this memory management policy (or lack thereof) looks\n  very odd, but it makes sense for short-lived programs such as compilers.\n  DMD, a compiler for the D programming language, uses the same memory\n  management scheme for the same reason, for example [1].\n\n## About the Author\n\nI'm Rui Ueyama. I'm the creator of [8cc](https://github.com/rui314/8cc),\nwhich is a hobby C compiler, and also the original creator of the current\nversion of [LLVM lld](https://lld.llvm.org) linker, which is a\nproduction-quality linker used by various operating systems and large-scale\nbuild systems.\n\n## References\n\n- [tcc](https://bellard.org/tcc/): A small C compiler written by Fabrice\n  Bellard. I learned a lot from this compiler, but the design of tcc and\n  chibicc are different. In particular, tcc is a one-pass compiler, while\n  chibicc is a multi-pass one.\n\n- [lcc](https://github.com/drh/lcc): Another small C compiler. The creators\n  wrote a [book](https://sites.google.com/site/lccretargetablecompiler/)\n  about the internals of lcc, which I found a good resource to see how a\n  compiler is implemented.\n\n- [An Incremental Approach to Compiler\n  Construction](http://scheme2006.cs.uchicago.edu/11-ghuloum.pdf)\n\n- [Rob Pike's 5 Rules of Programming](https://users.ece.utexas.edu/~adnan/pike.html)\n\n[1] https://www.drdobbs.com/cpp/increasing-compiler-speed-by-over-75/240158941\n\n> DMD does memory allocation in a bit of a sneaky way. Since compilers\n> are short-lived programs, and speed is of the essence, DMD just\n> mallocs away, and never frees.\n"
 },
 {
  "repo": "nelhage/reptyr",
  "language": "C",
  "readme_contents": "reptyr - A tool for \"re-ptying\" programs.\n=========================================\n\nreptyr is a utility for taking an existing running program and\nattaching it to a new terminal. Started a long-running process over\nssh, but have to leave and don't want to interrupt it? Just start a\nscreen, use reptyr to grab it, and then kill the ssh session and head\non home.\n\nUSAGE\n-----\n\n    reptyr PID\n\n\"reptyr PID\" will grab the process with id PID and attach it to your\ncurrent terminal.\n\nAfter attaching, the process will take input from and write output to\nthe new terminal, including ^C and ^Z. (Unfortunately, if you\nbackground it, you will still have to run \"bg\" or \"fg\" in the old\nterminal. This is likely impossible to fix in a reasonable way without\npatching your shell.)\n\nTypical usage pattern\n---------------------\n\n* Start a long running process, e.g. `top`\n* Background the process with CTRL-Z\n* Resume the process in the background: `bg`\n* Display your running background jobs with `jobs -l`, this should look like this:\n  * `[1]+  4711 Stopped (signal)        top`\n  * (The `-l` in `jobs -l` makes sure you'll get the PID)\n* Disown the jobs from the current parent with `disown top`. After that, `jobs` will not show the job any more, but `ps -a` will.\n* Start your terminal multiplexer of choice, e.g. `tmux`\n* Reattach to the backgrounded process: `reptyr 4711`\n* Detach your terminal multiplexer (e.g. CTRL-A D) and close ssh\n* Reconnect ssh, attach to your multiplexer (e.g. `tmux attach`), rejoice!\n\n\"But wait, isn't this just screenify?\"\n--------------------------------------\n\nThere's a shell script called \"screenify\" that's been going around the\ninternet for nigh on 10 years now that uses gdb to (supposedly)\naccomplish the same thing. The difference is that reptyr works much,\nmuch, better.\n\nIf you attach a \"less\" using screenify, it will still take input from\nthe old terminal. If you attach an ncurses program using screenify,\nand resize the window, your program won't notice. If you attach a\nprocess with screenify, ^C in the new terminal won't work.\n\nreptyr fixes all of these problems, and is the only such tool I know\nof that does so. See below for some more details on how it\naccomplishes this.\n\nPORTABILITY\n-----------\n\nreptyr supports Linux and FreeBSD. Not all functionality is currently\navailable on FreeBSD. (Notably, FreeBSD doesn't support `reptyr -T` at\nthis time.\n\n`reptyr` uses ptrace to attach to the target and control it at the\nsyscall level, so it is highly dependent on details of the syscall\nAPI, available syscalls, and terminal ioctl()s. A port to other\noperating systems may be technically feasible, but requires\nsignificant low-level knowledge of the relevant platform, and may\nentail significant refactors.\n\nreptyr works on i386, x86_64, and ARM. Ports to other architectures should be\nstraightforward, and should in most cases be as simple as adding an arch/ARCH.h\nfile and adding a clause to the ifdef ladder in ptrace.c.\n\nptrace_scope on Ubuntu Maverick and up\n--------------------------------------\n\n`reptyr` depends on the `ptrace` system call to attach to the remote program. On\nUbuntu Maverick and higher, this ability is disabled by default for security\nreasons. You can enable it temporarily by doing\n\n    # echo 0 > /proc/sys/kernel/yama/ptrace_scope\n\nas root, or permanently by editing the file /etc/sysctl.d/10-ptrace.conf, which\nalso contains more information about exactly what this setting accomplishes.\n\nreptyr -l\n---------\n\nAs a bonus feature, if you run \"reptyr -l\", reptyr will create a new\npseudo-terminal pair with nothing attached to the slave end, and print\nits name out.\n\nIf you are debugging a program in gdb, you can pass that name to \"set\ninferior-pty\". Because there is no existing program listening to that\ntty, this will work much better than passing an existing shell's\nterminal.\n\nHow does it work?\n-----------------\n\nThe main thing that reptyr does that no one else does is that it\nactually changes the controlling terminal of the process you are\nattaching. I wrote a\n[blog post](https://blog.nelhage.com/2011/02/changing-ctty/)\nexplaining just what the shenanigans involved are.\n\nPRONUNCIATION\n-------------\n\nI pronounce it like \"repeater\", but since that's easily ambiguous,\n\"re-P-T-Y-er\" is also acceptable.\n\n\nCREDITS\n-------\nreptyr was written by Nelson Elhage <nelhage@nelhage.com>. Contact him\nwith any questions or bug reports.\n\nURL\n---\nhttp://github.com/nelhage/reptyr\n"
 },
 {
  "repo": "EyalAr/lwip",
  "language": "C",
  "readme_contents": "[![Version](http://img.shields.io/npm/v/lwip.svg)](https://www.npmjs.org/package/lwip)\n[![Build Status](https://api.travis-ci.org/EyalAr/lwip.svg?branch=master)](https://travis-ci.org/EyalAr/lwip)\n[![Build status](https://ci.appveyor.com/api/projects/status/46mk5218x995svhw/branch/master?svg=true)](https://ci.appveyor.com/project/EyalAr/lwip/branch/master)\n[![Coverage Status](https://img.shields.io/coveralls/EyalAr/lwip/master.svg)](https://coveralls.io/r/EyalAr/lwip)\n\n# Light-weight image processor for NodeJS\n\n[![Join the chat at https://gitter.im/EyalAr/lwip](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/EyalAr/lwip?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n0. [Overview](#overview)\n  0. [Installation](#installation)\n  0. [Usage](#usage)\n  0. [Supported formats](#supported-formats)\n  0. [Colors specification](#colors-specification)\n  0. [Note on transparent images](#note-on-transparent-images)\n  0. [Note on threading performance](#note-on-threading-performance)\n0. [API](#api)\n  0. [Open an image from file or buffer](#open-an-image)\n  0. [Create a new blank image](#create-a-new-image)\n  0. [Image operations](#image-operations)\n    0. [Resize](#resize)\n    0. [Scale](#scale)\n    0. [Contain](#contain)\n    0. [Cover](#cover)\n    0. [Rotate](#rotate)\n    0. [Crop](#crop)\n    0. [Blur](#blur)\n    0. [Sharpen](#sharpen)\n    0. [Mirror](#mirror)\n    0. [Flip](#flip)\n    0. [Border](#border)\n    0. [Pad](#pad)\n    0. [Adjust saturation](#saturate)\n    0. Adjust lightness: [lighten](#lighten) / [darken](#darken)\n    0. [Adjust hue](#hue)\n    0. [Fade (adjust transparency)](#fade)\n    0. [Opacify](#opacify)\n    0. [Paste](#paste)\n    0. [Set pixel](#set-pixel)\n    0. [Set metadata](#set-metadata)\n  0. [Getters](#getters)\n    0. [Width](#width)\n    0. [Height](#height)\n    0. [Pixel](#get-pixel)\n    0. [Clone](#clone)\n    0. [Extract / Copy](#extract)\n    0. [Get as a Buffer](#get-as-a-buffer)\n      0. [JPEG](#jpeg)\n      0. [PNG](#png)\n      0. [GIF](#gif)\n    0. [Write to file](#write-to-file)\n    0. [Get metadata](#get-metadata)\n  0. [Batch operations](#batch-operations)\n0. [Copyrights](#copyrights)\n\n## Overview\n\nThis module provides comprehensive, fast, and simple image processing and\nmanipulation capabilities.\n\n**There are no external runtime dependencies**, which means you don't have to\ninstall anything else on your system.\n\n**This module is in active development. [New features](https://github.com/EyalAr/lwip/issues?labels=feature+request&page=1&state=open) are being added.**\n\n[Read the background for the development of this module.](http://eyalarubas.com/image-processing-nodejs.html)\n\n### Installation\n\n`npm install lwip`\n\nOr, clone this repo and `cd lwip && npm install`.\n\nYou can run tests with `npm test`.\n\n**Note:** Installation of this module involves compiling native code.\nIf `npm install lwip` failes, you probably need to setup your system.\n[See instructions](https://github.com/TooTallNate/node-gyp#installation).\nBuilding on Windows with Visual Studio requires version 2013 or higher.\n\n### Usage\n\n**Typical workflow:**\n\n0. Open an image and get an image object.\n0. Manipulate it.\n0. Save to disk / Send image buffer over network / etc.\n\n**Example (batch operations):**\n\n```Javascript\n// obtain an image object:\nrequire('lwip').open('image.jpg', function(err, image){\n\n  // check err...\n  // define a batch of manipulations and save to disk as JPEG:\n  image.batch()\n    .scale(0.75)          // scale to 75%\n    .rotate(45, 'white')  // rotate 45degs clockwise (white fill)\n    .crop(200, 200)       // crop a 200X200 square from center\n    .blur(5)              // Gaussian blur with SD=5\n    .writeFile('output.jpg', function(err){\n      // check err...\n      // done.\n    });\n\n});\n```\n\n**Example (non-batch):**\n\n```Javascript\nvar lwip = require('lwip');\n\n// obtain an image object:\nlwip.open('image.jpg', function(err, image){\n\n  // check err...\n  // manipulate image:\n  image.scale(0.5, function(err, image){\n\n    // check err...\n    // manipulate some more:\n    image.rotate(45, 'white', function(err, image){\n\n      // check err...\n      // encode to jpeg and get a buffer object:\n      image.toBuffer('jpg', function(err, buffer){\n\n        // check err...\n        // save buffer to disk / send over network / etc.\n\n      });\n\n    });\n\n  });\n\n});\n```\n\n### Supported formats\n\n**Decoding (reading):**\n\n- JPEG, 1 & 3 channels (grayscale & RGB).\n- PNG, transparency supported.\n- GIF, transparency supported. Animated GIFs can be read, but only the first\n  frame will be retrieved.\n\n**Encoding (writing):**\n\n- JPEG, 3 channels (RGB).\n- PNG (lossless), 3 channels (RGB) or 4 channels (RGBA).\n- GIF (no animations)\n\nOther formats may also be supported in the future, but are probably less urgent.\nCheck the issues to see [which formats are planned to be supported](https://github.com/EyalAr/lwip/issues?labels=format+request&page=1&state=open).\nOpen an issue if you need support for a format which is not already listed.\n\n### Colors specification\n\nIn LWIP colors are coded as RGBA values (red, green, blue and an alpha channel).\n\nColors are specified in one of three ways:\n\n- As a string. possible values:\n\n  ```Javascript\n  \"black\"    // {r: 0, g: 0, b: 0, a: 100}\n  \"white\"    // {r: 255, g: 255, b: 255, a: 100}\n  \"gray\"     // {r: 128, g: 128, b: 128, a: 100}\n  \"red\"      // {r: 255, g: 0, b: 0, a: 100}\n  \"green\"    // {r: 0, g: 255, b: 0, a: 100}\n  \"blue\"     // {r: 0, g: 0, b: 255, a: 100}\n  \"yellow\"   // {r: 255, g: 255, b: 0, a: 100}\n  \"cyan\"     // {r: 0, g: 255, b: 255, a: 100}\n  \"magenta\"  // {r: 255, g: 0, b: 255, a: 100}\n  ```\n\n- As an array `[R, G, B, A]` where `R`, `G` and `B` are integers between 0 and\n  255 and `A` is an integer between 0 and 100.\n- As an object `{r: R, g: G, b: B, a: A}` where `R`, `G` and `B` are integers\n  between 0 and 255 and `A` is an integer between 0 and 100.\n\n**Note**: The `A` value (alpha channel) is always optional and defaults to\n100 (completely opaque).\n\n### Note on transparent images\n\n0. Transparency is supported through an alpha channel which ranges between 0\n   and 100. 0 is completely transparent and 100 is completely opaque.\n0. Not all formats support transparency. If an image with an alpha channel is\n   encoded with a format which does not support transparency, the alpha channel\n   will be ignored (effectively setting it to 100% for all pixels).\n\n### Note on threading performance\n\nAll operations are asynchronous, and processing takes place in a thread pool\nmanaged by libuv which is part of NodeJS. This thread pool is separate from the event\nloop used to process HTTP requests, so use of lwip should not significantly affect the\nhandling of HTTP requests by a web application. The thread pool is however shared with\nother threaded native modules such as those providing database and filesystem IO.\n\nThe default thread pool size of 4 will be appropriate for most applications. However\nif your application regularly processes many images concurrently and and you wish\nto take full advantage of a multicore system or prevent heavy image processing work\nfrom delaying database or filesystem IO, you may want to increase the size of the\nthread pool by setting the UV_THREADPOOL_SIZE environmental variable to the NodeJS\nprocess, e.g.:\n\n```\nUV_THREADPOOL_SIZE=8 node your_script.js\n```\n\n## API\n\nAll operations are done on an `image` object. An `image` object can be obtained\nby:\n\n0. Openning an existing image file or buffer with the [`open`](#open-an-image)\n   method.\n0. Creating a new image object with the [`create`](#create-a-new-image) method.\n0. Cloning an existing image object with the [`image.clone`](#clone) method.\n0. Extracting a sub-image from an existing image object with the\n   [`image.extract`](#extract) method.\n\n### Open an image\n\n`lwip.open(source, type, callback)`\n\n0. `source {String/Buffer}`: The path to the image on disk or an image buffer.\n0. `type {String/Object}`: **Optional** type of the image. If omitted, the type\n   will be inferred from the file extension. If `source` is a buffer, `type`\n   must be specified. If `source` is an encoded image buffer, `type` must be\n   a string of the image type (i.e. `\"jpg\"`). If `source` is a raw pixels buffer\n   `type` must be an object with `type.width` and `type.height` properties.\n0. `callback {Function(err, image)}`\n\n**Note about raw pixels buffers:** `source` may be a buffer of raw pixels. The\nbuffer may contain pixels of 1-4 channels, where:\n\n0. 1 channel is a grayscale image.\n0. 2 channels is a grayscale image with an alpha channel.\n0. 3 channels is an RGB image.\n0. 4 channels is an RGBA image (with an alpha channel).\n\nIn other words, if the image in the buffer has width `W` and height `H`, the\nsize of the buffer can be `W*H`, `2*W*H`, `3*W*H` or `4*W*H`.\n\nThe channel values in the buffer must be stored sequentially. I.e. first all the\nRed values, then all the Green values, etc.\n\n#### Open file example\n\n```Javascript\nvar lwip = require('lwip');\nlwip.open('path/to/image.jpg', function(err, image){\n    // check 'err'. use 'image'.\n    // image.resize(...), etc.\n});\n```\n\n#### Open buffer example\n\n```Javascript\nvar fs = require('fs'),\n    lwip = require('lwip');\n\nfs.readFile('path/to/image.png', function(err, buffer){\n  // check err\n  lwip.open(buffer, 'png', function(err, image){\n      // check 'err'. use 'image'.\n      // image.resize(...), etc.\n  });\n});\n```\n\n### Create a new image\n\n`lwip.create(width, height, color, callback)`\n\n0. `width {Integer>0}`: The width of the new image.\n0. `height {Integer>0}`: The height of the new image.\n0. `color {String / Array / Object}`: **Optional** Color of the canvas. See\n   [colors specification](#colors-specification). Defaults to a transparent\n   canvas `{r:0, g:0, b:0, a:0}`.\n0. `callback {Function(err, image)}`\n\n**Example**:\n\n```Javascript\nvar lwip = require('lwip');\n\nlwip.create(500, 500, 'yellow', function(err, image){\n  // check err\n  // 'image' is a 500X500 solid yellow canvas.\n});\n```\n\n### Image operations\n\n#### Resize\n\n`image.resize(width, height, inter, callback)`\n\n0. `width {Integer}`: Width in pixels.\n0. `height {Integer}`: **Optional** height in pixels. If omitted, `width` will\n   be used.\n0. `inter {String}`: **Optional** interpolation method. Defaults to `\"lanczos\"`.\n   Possible values:\n   - `\"nearest-neighbor\"`\n   - `\"moving-average\"`\n   - `\"linear\"`\n   - `\"grid\"`\n   - `\"cubic\"`\n   - `\"lanczos\"`\n0. `callback {Function(err, image)}`\n\n#### Scale\n\n`image.scale(wRatio, hRatio, inter, callback)`\n\n0. `wRatio {Float}`: Width scale ratio.\n0. `hRatio {Float}`: **Optional** height scale ratio. If omitted, `wRatio` will\n   be used.\n0. `inter {String}`: **Optional** interpolation method. Defaults to `\"lanczos\"`.\n   Possible values:\n   - `\"nearest-neighbor\"`\n   - `\"moving-average\"`\n   - `\"linear\"`\n   - `\"grid\"`\n   - `\"cubic\"`\n   - `\"lanczos\"`\n0. `callback {Function(err, image)}`\n\n#### Contain\n\nContain the image in a colored canvas. The image will be resized to the largest\npossible size such that it's fully contained inside the canvas.\n\n`image.contain(width, height, color, inter, callback)`\n\n0. `width {Integer}`: Canvas' width in pixels.\n0. `height {Integer}`: Canvas' height in pixels.\n0. `color {String / Array / Object}`: **Optional** Color of the canvas. See\n   [colors specification](#colors-specification).\n0. `inter {String}`: **Optional** interpolation method. Defaults to `\"lanczos\"`.\n   Possible values:\n   - `\"nearest-neighbor\"`\n   - `\"moving-average\"`\n   - `\"linear\"`\n   - `\"grid\"`\n   - `\"cubic\"`\n   - `\"lanczos\"`\n0. `callback {Function(err, image)}`\n\n#### Cover\n\nCover a canvas with the image. The image will be resized to the smallest\npossible size such that both its dimensions are bigger than the canvas's\ndimensions. Margins of the image exceeding the canvas will be discarded.\n\n`image.cover(width, height, inter, callback)`\n\n0. `width {Integer}`: Canvas' width in pixels.\n0. `height {Integer}`: Canvas' height in pixels.\n0. `inter {String}`: **Optional** interpolation method. Defaults to `\"lanczos\"`.\n   Possible values:\n   - `\"nearest-neighbor\"`\n   - `\"moving-average\"`\n   - `\"linear\"`\n   - `\"grid\"`\n   - `\"cubic\"`\n   - `\"lanczos\"`\n0. `callback {Function(err, image)}`\n\n#### Rotate\n\n`image.rotate(degs, color, callback)`\n\n0. `degs {Float}`: Clockwise rotation degrees.\n0. `color {String / Array / Object}`: **Optional** Color of the canvas. See\n   [colors specification](#colors-specification).\n0. `callback {Function(err, image)}`\n\n#### Crop\n\n#### Crop with rectangle coordinates\n\n`image.crop(left, top, right, bottom, callback)`\n\n0. `left, top, right, bottom {Integer}`: Coordinates of the crop rectangle.\n0. `callback {Function(err, image)}`\n\n#### Crop a rectangle from center\n\n`image.crop(width, height, callback)`\n\n0. `width, height {Integer}`: Width and height of the rectangle to crop from the\n   center of the image.\n0. `callback {Function(err, image)}`\n\n#### Blur\n\nGaussian blur.\n\n`image.blur(sigma, callback)`\n\n0. `sigma {Float>=0}`: Standard deviation of the Gaussian filter.\n0. `callback {Function(err, image)}`\n\n#### Sharpen\n\nInverse diffusion shapren.\n\n`image.sharpen(amplitude, callback)`\n\n0. `amplitude {Float}`: Sharpening amplitude.\n0. `callback {Function(err, image)}`\n\n#### Mirror\n\nMirror an image along the 'x' axis, 'y' axis or both.\n\n`image.mirror(axes, callback)`\n\n0. `axes {String}`: `'x'`, `'y'` or `'xy'` (case sensitive).\n0. `callback {Function(err, image)}`\n\n#### Flip\n\nAlias of [`mirror`](#mirror).\n\n#### Border\n\nAdd a colored border to the image.\n\n`image.border(width, color, callback)`\n\n0. `width {Integer}`: Border width in pixels.\n0. `color {String / Array / Object}`: **Optional** Color of the border. See\n   [colors specification](#colors-specification).\n0. `callback {Function(err, image)}`\n\n#### Pad\n\nPad image edges with colored pixels.\n\n`image.pad(left, top, right, bottom, color, callback)`\n\n0. `left, top, right, bottom {Integer}`: Number of pixels to add to each edge.\n0. `color {String / Array / Object}`: **Optional** Color of the padding. See\n   [colors specification](#colors-specification).\n0. `callback {Function(err, image)}`\n\n#### Saturate\n\nAdjust image saturation.\n\n`image.saturate(delta, callback)`\n\n0. `delta {Float}`: By how much to increase / decrease the saturation.\n0. `callback {Function(err, image)}`\n\n**Examples**:\n\n0. `image.saturate(0, ...)` will have no effect on the image.\n0. `image.saturate(0.5, ...)` will increase the saturation by 50%.\n0. `image.saturate(-1, ...)` will decrease the saturation by 100%, effectively\n   desaturating the image.\n\n#### Lighten\n\nAdjust image lightness.\n\n`image.lighten(delta, callback)`\n\n0. `delta {Float}`: By how much to increase / decrease the lightness.\n0. `callback {Function(err, image)}`\n\n**Examples**:\n\n0. `image.lighten(0, ...)` will have no effect on the image.\n0. `image.lighten(0.5, ...)` will increase the lightness by 50%.\n0. `image.lighten(-1, ...)` will decrease the lightness by 100%, effectively\n   making the image black.\n\n#### Darken\n\nAdjust image lightness.\n\n`image.darken(delta, callback)`\n\nEquivalent to `image.lighten(-delta, callback)`.\n\n#### Hue\n\nAdjust image hue.\n\n`image.hue(shift, callback)`\n\n0. `shift {Float}`: By how many degrees to shift each pixel's hue.\n0. `callback {Function(err, image)}`\n\n**Examples**:\n\n0. `image.lighten(0, ...)` will have no effect on the image.\n0. `image.lighten(100, ...)` will shift pixels' hue by 100 degrees.\n\n**Note:** The hue is shifted in a circular manner in the range [0,360] for each\npixel individually.\n\n#### Fade\n\nAdjust image transperancy.\n\n`image.fade(delta, callback)`\n\n0. `delta {Float}`: By how much to increase / decrease the transperancy.\n0. `callback {Function(err, image)}`\n\n**Note:** The transparency is adjusted independently for each pixel.\n\n**Examples**:\n\n0. `image.fade(0, ...)` will have no effect on the image.\n0. `image.fade(0.5, ...)` will increase the transparency by 50%.\n0. `image.fade(1, ...)` will make the image completely transparent.\n\n#### Opacify\n\nMake image completely opaque.\n\n`image.opacify(callback)`\n\n0. `callback {Function(err, image)}`\n\n#### Paste\n\nPaste an image on top of this image.\n\n`image.paste(left, top, img, callback)`\n\n0. `left, top {Integer}`: Coordinates of the top-left corner of the pasted\n   image.\n0. `img {Image object}`: The image to paste.\n0. `callback {Function(err, image)}`\n\n**Notes:**\n\n0. If the pasted image exceeds the bounds of the base image, an exception\n   is thrown.\n0. `img` is pasted in the state it was at the time `image.paste( ... )` was\n   called, eventhough `callback` is called asynchronously.\n0. For transparent images, alpha blending is done according to the equations\n   described [here](http://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending).\n0. Extra caution is required when using this method in batch mode, as the images\n   may change by the time this operation is called.\n\n#### Set Pixel\n\nSet the color of a pixel.\n\n`image.setPixel(left, top, color, callback)`\n\n0. `left, top {Integer}`: Coordinates of the pixel from the left-top corner of\n   the image.\n0. `color {String / Array / Object}`: Color of the pixel to set.\n   See [colors specification](#colors-specification).\n0. `callback {Function(err, image)}`\n\n**Notes:**\n\n0. If the coordinates exceed the bounds of the image, an exception is thrown.\n0. Extra caution is required when using this method in batch mode, as the\n  dimensions of the image may change by the time this operation is called.\n\n#### Set metadata\n\nSet the metadata in an image. This is currently only supported for PNG files.\nSets a tEXt chunk with the key `lwip_data` and comment as the given string. If\ncalled with a `null` parameter, removes existing metadata from the image,\nif present.\n\n`image.setMetadata(metadata)`\n\n0. `metadata {String}`: a string of arbitrary length, or null.\n\n### Getters\n\n#### Width\n\n`image.width()` returns the image's width in pixels.\n\n#### Height\n\n`image.height()` returns the image's height in pixels.\n\n#### Get Pixel\n\n`image.getPixel(left, top)` returns the color of the pixel at the `(left, top)`\ncoordinate.\n\n0. `left {Integer>=0}`\n0. `top {Integer>=0}`\n\nColor is returned as an object. See [colors specification](#colors-specification).\n\n#### Clone\n\nClone the image into a new image object.\n\n`image.clone(callback)`\n\n0. `callback {Function(err, newImage)}`\n\n**Example**: See [`examples/clone.js`](examples/clone.js)\n\n**Note**: The image is cloned to the state it was at the time\n`image.clone( ... )` was called, eventhough `callback` is called asynchronously.\n\n```Javascript\nimage.width(); // 500\nimage.clone(function(err, clone){\n    clone.width(); // 500\n});\nimage.resize(100, 100, function(err, image){\n    image.width(); //100\n});\n```\n\n#### Extract\n\nCopy an area of the image into a new image object.\n\n`image.extract(left, top, right, bottom, callback)`\n\n0. `left, top, right, bottom {Integer}`: Coordinates of the area to copy.\n0. `callback {Function(err, newImage)}`\n\n**Example**: See [`examples/extract.js`](examples/extract.js)\n\n**Note**: The sub-image is extracted from the original image in the state it was\nat the time `image.extract( ... )` was called, eventhough `callback` is called\nasynchronously.\n\n#### Get as a Buffer\n\nGet encoded binary image data as a NodeJS\n[Buffer](http://nodejs.org/api/buffer.html).\n\nWhen opening an image, it is decoded and stored in memory as an uncompressed\nimage. All manipulations are done on the uncompressed data in memory. This\nmethod allows to encode the image to one of the specified formats and get the\nencoded data as a NodeJS Buffer object.\n\n`image.toBuffer(format, params, callback)`\n\n0. `format {String}`: Encoding format. Possible values:\n  - `\"jpg\"`\n  - `\"png\"`\n  - `\"gif\"`\n0. `params {Object}`: **Optional** Format-specific parameters (See below).\n0. `callback {Function(err, buffer)}`\n\n**Supported encoding formats:**\n\n##### JPEG\n\nThe `params` object should have the following fields:\n\n- `quality {Integer}`: Defaults to `100`.\n\nNote that when encoding to JPEG the alpha channel is discarded.\n\n##### PNG\n\nThe `params` object should have the following fields:\n\n- `compression {String}`: Defaults to `\"fast\"`. Possible values:\n  - `\"none\"` - No compression. Fastest.\n  - `\"fast\"` - Basic compression. Fast.\n  - `\"high\"` - High compression. Slowest.\n- `interlaced {Boolean}`: Defaults to `false`.\n- `transparency {true/false/'auto'}`: Preserve transparency? Defaults to\n  `'auto'`. Determines if the encoded image will have 3 or 4 channels. If\n  `'auto'`, the image will be encoded with 4 channels if it has transparent\n  components, and 3 channels otherwise.\n\n##### GIF\n\nThe `params` object should have the following fields:\n\n- `colors {Integer}`: Defaults to `256`. Number of colors in the color table\n  (at most). Must be between 2 and 256.\n- `interlaced {Boolean}`: Defaults to `false`.\n- `transparency {true/false/'auto'}`: Preserve transparency? Defaults to\n  `'auto'`. Determines if the encoded image will have 3 or 4 channels. If\n  `'auto'`, the image will be encoded with 4 channels if it has transparent\n  components, and 3 channels otherwise.\n- `threshold {Integer}` - Between 0 and 100. Pixels in a gif image are either\n  fully transparent or fully opaque. This value sets the alpha channel\n  threshold to determine if a pixel is opaque or transparent. If the alpha\n  channel of the pixel is above this threshold, this pixel will be considered\n  as opaque; otherwise it will be transparent.\n\n#### Write to file\n\nWrite encoded binary image data directly to a file.\n\n`image.writeFile(path, format, params, callback)`\n\n0. `path {String}`: Path of file to write.\n0. `format {String}`: **Optional** Encoding format. If omitted, will be inferred\n   from `path` extension. Possible values are specified in\n   [Get as a Buffer](#get-as-a-buffer) section.\n0. `params {Object}`: **Optional** Format-specific parameters.\n0. `callback {Function(err)}`\n\n#### Get Metadata\n\nGet the textual metadata from an image. This is currently only supported for\ntEXt chunks in PNG images, and will get the first tEXt chunk found with the key\n`lwip_data`. If none is found, returns null.\n\n`image.getMetadata()`\n\n### Batch operations\n\nEach of the [image operations](#image-operations) above can be done as part of\na batch of operations. Operations can be queued, and executed as a batch at any\ntime.\n\nEach one of the [image operations](#image-operations) has a batch equivalent\nwhich takes the same arguments, except the callback, which is not needed.\n\nWhen all batch operations had been queued, they can be executed in one of\nseveral methods, as explained below.\n\n#### Obtaining a batch object\n\nIn order to start queueing operations, a batch object first needs to be obtained\nfrom the image.\n\n```Javascript\n// obtain a batch object from the image:\nvar batch = image.batch();\n```\n\n#### Using a batch object\n\nUse the batch object to queue [image operations](#image-operations). Each of the\noperations above has a batch equivalent. Operations can be chained.\n\n**Remember, the batch manipulation methods do not take a callback.**\n\n**Example:**\n\n```Javascript\nbatch.rotate(45, 'white').scale(0.5).blur(5);\n```\n\n#### Executing a batch\n\nThere are several methods which start the execution of a batch. Once a batch\nfinishes an execution, it becomes empty and can be resued to queue additional\noperations.\n\n##### Execute batch and obtain the manipulated image object\n\nWhen all desired operations had been queued, execute the batch with the `exec()`\nmethod. `exec` takes a `callback` argument; `callback` is a function which\nreceives an error object and the manipulated image object:\n\n`batch.exec(callback)`\n\n  - `callback {Function(err, image)}`:\n    - `err`: An error object or `null` when no error.\n    - `image`: An image object of the manipulated image.\n\n```Javascript\nbatch.exec(function(err, image){\n  // check err, use image\n});\n```\n\n##### Execute batch and obtain a Buffer object\n\nBatch objects have a `toBuffer` convenience method.\n\n`batch.toBuffer(format, params, callback)`\n\nSee parameters of [`image.toBuffer()`](#get-as-a-buffer).\n\n##### Execute batch and write to file\n\nBatch objects have a `writeFile` convenience method.\n\n`batch.writeFile(path, format, params, callback)`\n\nSee parameters of [`image.writeFile()`](#write-to-file).\n\n#### Notes on batch operations\n\nAn image can have more than one batch object, but all batch objects modify the\nsame underlying image. This means the order of execution matters.\n\n```Javascript\nvar batch1 = image.batch().rotate('45', 'black');\nvar batch2 = image.batch().border(15, 'black');\n```\n\nThis will rotate the image 45degs and then add a black border:\n\n```Javascript\nbatch1.exec(function(err, image){\n    batch2.exec(function(err, image){\n        // ...\n    });\n});\n```\n\nWhile this will add a black border and then rotate the image 45degs:\n\n```Javascript\nbatch2.exec(function(err, image){\n    batch1.exec(function(err, image){\n        // ...\n    });\n});\n```\n\n## Copyrights\n\nThe native part of this module is compiled from source which uses the following:\n\n- Independent JPEG Group's free JPEG software:\n  - [Website](http://www.ijg.org/)\n  - [Readme](https://github.com/EyalAr/lwip/blob/master/src/lib/jpeg/README)\n- libpng:\n  - [Website](http://www.libpng.org/)\n  - [Readme](https://github.com/EyalAr/lwip/blob/master/src/lib/png/README)\n- zlib:\n  - [Website](http://www.zlib.net/)\n  - [Readme](https://github.com/EyalAr/lwip/blob/master/src/lib/zlib/README)\n- The CImg Library\n  - [Website](http://cimg.sourceforge.net/)\n  - [Readme](https://github.com/EyalAr/lwip/blob/master/src/lib/cimg/README.txt)\n- giflib\n  - [Website](http://giflib.sourceforge.net/)\n  - [Readme](https://github.com/EyalAr/lwip/blob/master/src/lib/gif/README)\n"
 },
 {
  "repo": "ibireme/yyjson",
  "language": "C",
  "readme_contents": "\n# Introduction\n\n[![Build](https://flat.badgen.net/github/status/ibireme/yyjson/master/)](https://github.com/ibireme/yyjson/actions)\n[![Codecov](https://flat.badgen.net/codecov/c/github/ibireme/yyjson)](https://codecov.io/gh/ibireme/yyjson)\n[![License](https://flat.badgen.net/github/license/ibireme/yyjson)](https://github.com/ibireme/yyjson/blob/master/LICENSE)\n[![Version](https://flat.badgen.net/github/release/ibireme/yyjson?color=orange)](https://github.com/ibireme/yyjson/releases)\n\nA high performance JSON library written in ANSI C.\n\n# Features\n- **Fast**: can read or write gigabytes per second JSON data on modern CPU.\n- **Portable**: compliance with ANSI C (C89).\n- **Standard**: strict compliance with [RFC 8259](https://tools.ietf.org/html/rfc8259) standard.\n- **Safe**: complete JSON form, number format and UTF-8 validation.\n- **Accuracy**: can read and write `int64`, `uint64` and `double` numbers accurately.\n- **Less Restriction**: support unlimited JSON level, `\\u0000` and non null-terminated string.\n- **Extendable**: options to allow comments, trailing commas, nan/inf, custom memory allocator.\n- **Developer Friendly**: only one `h` and one `c` file, easy to integrate.\n\n# Limitations\n- An array or object is stored as some [data structure](https://ibireme.github.io/yyjson/doc/doxygen/html/md_doc__data_structure.html) like linked list, access elements with index or key is slower than iterator.\n- Duplicate keys are allowed in an object, and the order of the keys is preserved.\n- JSON parsing result is immutable, a `mutable copy` is required for modification.\n\n# Performance\nBenchmark project and dataset: [yyjson_benchmark](https://github.com/ibireme/yyjson_benchmark)\n\nThe simdjson's new `On Demand` API is faster if most JSON fields are known at compile-time.\nThis benchmark project only checks the DOM API, a new benchmark will be added later.\n\n#### AWS EC2 (AMD EPYC 7R32, gcc 9.3)\n![ec2_chart](doc/images/perf_reader_ec2.svg)\n\n|twitter.json|parse (GB/s)|stringify (GB/s)|\n|---|---|---|\n|yyjson(insitu)|1.80|1.51|\n|yyjson|1.72|1.42|\n|simdjson|1.52|0.61|\n|sajson|1.16|   |\n|rapidjson(insitu)|0.77|   |\n|rapidjson(utf8)|0.26|0.39|\n|cjson|0.32|0.17|\n|jansson|0.05|0.11|\n\n\n#### iPhone (Apple A14, clang 12)\n![a14_chart](doc/images/perf_reader_a14.svg)\n\n|twitter.json|parse (GB/s)|stringify (GB/s)|\n|---|---|---|\n|yyjson(insitu)|3.51|2.41|\n|yyjson|2.39|2.01|\n|simdjson|2.19|0.80|\n|sajson|1.74||\n|rapidjson(insitu)|0.75| |\n|rapidjson(utf8)|0.30|0.58|\n|cjson|0.48|0.33|\n|jansson|0.09|0.24|\n\nMore benchmark reports with interactive charts (update 2020-12-12)\n\n|Platform|CPU|Compiler|OS|Report|\n|---|---|---|---|---|\n|Intel NUC 8i5|Core i5-8259U|msvc 2019|Windows 10 2004|[Charts](https://ibireme.github.io/yyjson_benchmark/reports/Intel_NUC_8i5_msvc_2019.html)|\n|Intel NUC 8i5|Core i5-8259U|clang 10.0|Ubuntu 20.04|[Charts](https://ibireme.github.io/yyjson_benchmark/reports/Intel_NUC_8i5_clang_10.html)|\n|Intel NUC 8i5|Core i5-8259U|gcc 9.3|Ubuntu 20.04|[Charts](https://ibireme.github.io/yyjson_benchmark/reports/Intel_NUC_8i5_gcc_9.html)|\n|AWS EC2 c5a.large|AMD EPYC 7R32|gcc 9.3|Ubuntu 20.04|[Charts](https://ibireme.github.io/yyjson_benchmark/reports/EC2_c5a.large_gcc_9.html)|\n|AWS EC2 t4g.medium|Graviton2 (ARM64)|gcc 9.3|Ubuntu 20.04|[Charts](https://ibireme.github.io/yyjson_benchmark/reports/EC2_t4g.medium_gcc_9.html)|\n|Apple iPhone 12 Pro|A14 (ARM64)|clang 12.0|iOS 14|[Charts](https://ibireme.github.io/yyjson_benchmark/reports/Apple_A14_clang_12.html)|\n\n### For better performance, yyjson prefers:\n* A modern processor with:\n    * high instruction level parallelism\n    * excellent branch predictor\n    * low penalty for misaligned memory access\n* A modern compiler with good optimizer.\n\n\n# Sample Code\n\n### Read JSON string\n```c\nconst char *json = \"{\\\"name\\\":\\\"Mash\\\",\\\"star\\\":4,\\\"hits\\\":[2,2,1,3]}\";\n\n// Read JSON and get root\nyyjson_doc *doc = yyjson_read(json, strlen(json), 0);\nyyjson_val *root = yyjson_doc_get_root(doc);\n\n// Get root[\"name\"]\nyyjson_val *name = yyjson_obj_get(root, \"name\");\nprintf(\"name: %s\\n\", yyjson_get_str(name));\nprintf(\"name length:%d\\n\", (int)yyjson_get_len(name));\n\n// Get root[\"star\"]\nyyjson_val *star = yyjson_obj_get(root, \"star\");\nprintf(\"star: %d\\n\", (int)yyjson_get_int(star));\n\n// Get root[\"hits\"], iterate over the array\nyyjson_val *hits = yyjson_obj_get(root, \"hits\");\nsize_t idx, max;\nyyjson_val *hit;\nyyjson_arr_foreach(hits, idx, max, hit) {\n    printf(\"hit%d: %d\\n\", (int)idx, (int)yyjson_get_int(hit));\n}\n\n// Free the doc\nyyjson_doc_free(doc);\n\n// All functions accept NULL input, and return NULL on error.\n```\n\n### Write JSON string\n```c\n// Create a mutable doc\nyyjson_mut_doc *doc = yyjson_mut_doc_new(NULL);\nyyjson_mut_val *root = yyjson_mut_obj(doc);\nyyjson_mut_doc_set_root(doc, root);\n\n// Set root[\"name\"] and root[\"star\"]\nyyjson_mut_obj_add_str(doc, root, \"name\", \"Mash\");\nyyjson_mut_obj_add_int(doc, root, \"star\", 4);\n\n// Set root[\"hits\"] with an array\nint hits_arr[] = {2, 2, 1, 3};\nyyjson_mut_val *hits = yyjson_mut_arr_with_sint32(doc, hits_arr, 4);\nyyjson_mut_obj_add_val(doc, root, \"hits\", hits);\n\n// To string, minified\nconst char *json = yyjson_mut_write(doc, 0, NULL);\nif (json) {\n    printf(\"json: %s\\n\", json); // {\"name\":\"Mash\",\"star\":4,\"hits\":[2,2,1,3]}\n    free((void *)json);\n}\n\n// Free the doc\nyyjson_mut_doc_free(doc);\n```\n\n### Read JSON file with options\n```c\n// Read JSON file, allowing comments and trailing commas\nyyjson_read_flag flg = YYJSON_READ_ALLOW_COMMENTS | YYJSON_READ_ALLOW_TRAILING_COMMAS;\nyyjson_read_err err;\nyyjson_doc *doc = yyjson_read_file(\"/tmp/config.json\", flg, NULL, &err);\n\n// Iterate over the root object\nif (doc) {\n    yyjson_val *obj = yyjson_doc_get_root(doc);\n    yyjson_obj_iter iter;\n    yyjson_obj_iter_init(obj, &iter);\n    yyjson_val *key, *val;\n    while ((key = yyjson_obj_iter_next(&iter))) {\n        val = yyjson_obj_iter_get_val(key);\n        printf(\"%s: %s\\n\", yyjson_get_str(key), yyjson_get_type_desc(val));\n    }\n} else {\n    printf(\"read error (%u): %s at position: %ld\\n\", err.code, err.msg, err.pos);\n}\n\n// Free the doc\nyyjson_doc_free(doc);\n```\n\n### Write JSON file with options\n```c\n// Read the JSON file as a mutable doc\nyyjson_doc *idoc = yyjson_read_file(\"/tmp/config.json\", 0, NULL, NULL);\nyyjson_mut_doc *doc = yyjson_doc_mut_copy(idoc, NULL);\nyyjson_mut_val *obj = yyjson_mut_doc_get_root(doc);\n\n// Remove null values in root object\nyyjson_mut_obj_iter iter;\nyyjson_mut_obj_iter_init(obj, &iter);\nyyjson_mut_val *key, *val;\nwhile ((key = yyjson_mut_obj_iter_next(&iter))) {\n    val = yyjson_mut_obj_iter_get_val(key);\n    if (yyjson_mut_is_null(val)) {\n        yyjson_mut_obj_iter_remove(&iter);\n    }\n}\n\n// Write the json pretty, escape unicode\nyyjson_write_flag flg = YYJSON_WRITE_PRETTY | YYJSON_WRITE_ESCAPE_UNICODE;\nyyjson_write_err err;\nyyjson_mut_write_file(\"/tmp/config.json\", doc, flg, NULL, &err);\nif (err.code) {\n    printf(\"write error (%u): %s\\n\", err.code, err.msg);\n}\n\n// Free the doc\nyyjson_doc_free(idoc);\nyyjson_mut_doc_free(doc);\n```\n\n# Documentation\n* [Home Page](https://ibireme.github.io/yyjson/doc/doxygen/html/)\n    * [Build and test](https://ibireme.github.io/yyjson/doc/doxygen/html/md_doc__build_and_test.html)\n    * [API and sample code](https://ibireme.github.io/yyjson/doc/doxygen/html/md_doc__a_p_i.html)\n    * [Data structure](https://ibireme.github.io/yyjson/doc/doxygen/html/md_doc__data_structure.html)\n    * [Changelog](https://ibireme.github.io/yyjson/doc/doxygen/html/md__c_h_a_n_g_e_l_o_g.html)\n\n# TODO\n* [x] Add documentation page.\n* [x] Add GitHub workflow for CI and codecov.\n* [x] Add more tests: valgrind, sanitizer.\n* [x] Support JSON Pointer to query value from document.\n* [x] Add fuzzer.\n* [x] Add `RAW` type for JSON reader and writer.\n* [ ] Add streaming API for JSON reader and writer.\n* [ ] Add documentation about performance.\n* [ ] Optimize performance for 32-bit processor.\n\n# License\nThis project is released under the MIT license.\n"
 },
 {
  "repo": "ejoy/ejoy2d",
  "language": "C",
  "readme_contents": "EJOY 2D\n=======\n\nMake\n====\n\nFor Windows and msvc\n\n* msvc\\make.bat\n* ej2d examples/ex01.lua to test\n\nFor Windows and mingw32\n\n* Install glew 1.9\n* make or make mingw\n* ej2d examples/ex01.lua to test\n\nFor Linux ,\n\n* Install glew 1.9\n* Install freetype 2\n* make or make linux\n* ./ej2d examples/ex01.lua to test\n\nFor Mac OS ,\n\n* Install glfw3\n* Install freetype 2\n* make or make macosx\n* ./ej2d examples/ex01.lua to test\n\nAPI\n====\n\nhttps://github.com/cloudwu/ejoy2d/blob/master/doc/apicn.md  (work in process , in Chinese)\n\nQuestion?\n=======\n\nPlease read http://blog.codingnow.com/2013/12/ejoy2d.html first (In Chinese) \n\nChinese API document \n\nPut your questions in [Issues](https://github.com/cloudwu/ejoy2d/issues) .\n"
 },
 {
  "repo": "jech/polipo",
  "language": "C",
  "readme_contents": "Polipo is no longer maintained\n==============================\n\nPolipo is no longer maintained.  Sorry.\n\n\nOriginal Polipo README\n======================\n\nPolipo is single-threaded, non blocking caching web proxy that has\nvery modest resource needs.  See the file INSTALL for installation\ninstructions.  See the texinfo manual (available as HTML after\ninstallation) for more information.\n\nCurrent information about Polipo can be found on the Polipo web page,\n\n    http://www.pps.univ-paris-diderot.fr/~jch/software/polipo/\n\nFurther inquiries should be sent to the Polipo-users mailing list:\n\n    <polipo-users@lists.sourceforge.net>\n\nPlease see the Polipo web page for subscription information.\n\n\nJuliusz Chroboczek\n<jch@pps.univ-paris-diderot.fr>\n"
 },
 {
  "repo": "begeekmyfriend/bplustree",
  "language": "C",
  "readme_contents": "# B+Tree\nA minimal B+Tree implementation for millions (even billions) of key-value storage based on Posix.\n\n## Branch\n[in-memory](https://github.com/begeekmyfriend/bplustree/tree/in-memory) for learning and debugging.\n\n## Demo\n```shell\n./demo_build.sh\n```\n\n## Code Coverage Test\n\n**Note:** You need to `rm /tmp/coverage.index*` for this testing every time because the configuration (i.e block size and order etc.) in those index files is immutable!\n\n```shell\n./coverage_build.sh\n```\n"
 },
 {
  "repo": "ThirteenAG/WidescreenFixesPack",
  "language": "C",
  "readme_contents": "<div align=\"center\">\n\n[![wfp](https://thirteenag.github.io/img/logo-stroke.svg)](https://thirteenag.github.io/wfp)\n\n[![GitHub Workflow Status](https://img.shields.io/github/workflow/status/ThirteenAG/WidescreenFixesPack/GitHub%20Actions%20Build?label=GitHub%20Actions%20Build&logo=GitHub)](https://github.com/ThirteenAG/WidescreenFixesPack/actions/workflows/main.yml)\n[![AppVeyor](https://img.shields.io/appveyor/build/ThirteenAG/WidescreenFixesPack?label=AppVeyor%20Build&logo=Appveyor&logoColor=white)](https://ci.appveyor.com/project/ThirteenAG/widescreenfixespack)\n[![Azure DevOps builds](https://img.shields.io/azure-devops/build/ThirteenAG/f555b128-da05-4bad-a972-90d529123a2e/3?label=Azure%20Pipelines%20Build&logo=Azure%20Pipelines)](https://dev.azure.com/thirteenag/WidescreenFixesPack/_build/latest?definitionId=3&branchName=master)\n[![CircleCI](https://img.shields.io/circleci/build/github/ThirteenAG/WidescreenFixesPack?label=CircleCI%20Build&logo=circleci)](https://circleci.com/gh/ThirteenAG/WidescreenFixesPack/tree/master)\n\n[![GitHub license](https://img.shields.io/github/license/ThirteenAG/WidescreenFixesPack?color=blue)](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/license)\n[![GitHub stars](https://img.shields.io/github/stars/ThirteenAG/WidescreenFixesPack)](https://github.com/ThirteenAG/WidescreenFixesPack/stargazers)\n[![Commits](https://img.shields.io/github/commit-activity/m/ThirteenAG/WidescreenFixesPack?label=commits)](https://github.com/ThirteenAG/WidescreenFixesPack/commits)\n[![GitHub last commit](https://img.shields.io/github/last-commit/ThirteenAG/WidescreenFixesPack?color=blue)](https://github.com/ThirteenAG/WidescreenFixesPack/commits)\n\n[![Platforms](https://img.shields.io/badge/platforms:-grey)](https://github.com/ThirteenAG/WidescreenFixesPack/releases)\n[![WINDOWS](https://img.shields.io/badge/WINDOWS-0078D4)](https://github.com/ThirteenAG/WidescreenFixesPack/releases?q=NOT+CXBXR+NOT+PCSX2F+NOT+PPSSPP&expanded=true)\n[![PCSX2F](https://img.shields.io/badge/PCSX2F-0271A6)](https://github.com/ThirteenAG/WidescreenFixesPack/releases?q=PCSX2F&expanded=true)\n[![PPSSPP](https://img.shields.io/badge/PPSSPP-33b5e5)](https://github.com/ThirteenAG/WidescreenFixesPack/releases?q=PPSSPP&expanded=true)\n[![CXBXR](https://img.shields.io/badge/CXBXR-96CE49)](https://github.com/ThirteenAG/WidescreenFixesPack/releases?q=CXBXR&expanded=true)\n[![DOLPHIN](https://img.shields.io/badge/DOLPHIN-30b5ff)](https://github.com/ThirteenAG/WidescreenFixesPack/releases?q=DOLPHIN&expanded=true)\n\n[![Downloads](https://img.shields.io/github/downloads/ThirteenAG/WidescreenFixesPack/total?color=red)](https://github.com/ThirteenAG/WidescreenFixesPack/releases/)\n\nhttps://thirteenag.github.io/wfp\n\nPlugins to make or improve widescreen resolutions support in PC games, add more features and fix bugs.\n\n</div>\n\n# Building and Installing\n\nRequirements:\n\n- [Premake 5](https://premake.github.io/) _(pre-built executable available in this repository root)_\n- [Visual Studio](http://www.visualstudio.com/downloads)\n\nRun the following command in the root of this directory to generate the project files (or simply launch **premake5.bat**):\n\n    premake5 vs2022\n\nThe usage is as simple as inserting the files into game's root directory. Uninstalling is as easy as that too, delete the files and you are done.\n\n# Important notes\n\n- Not compatible with Windows XP (without recompiling with `_xp` toolset and `/Zc:threadSafeInit-`).\n- For using with WINE, follow [this guide](https://cookieplmonster.github.io/setup-instructions/#proton-wine).\n\n# Screenshots\n\n| [![mp](https://i.imgur.com/lDPTxGo.jpg)](https://i.imgur.com/lDPTxGo.jpg)       | [![farcry](https://i.imgur.com/5CMg1fG.jpg)](https://i.imgur.com/5CMg1fG.jpg)   |\n|:-------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------:|\n| [![scdaxbox](https://i.imgur.com/IgPADzY.jpg)](https://i.imgur.com/IgPADzY.jpg) | [![burnout3](https://i.imgur.com/a1Ou0JH.jpg)](https://i.imgur.com/a1Ou0JH.jpg) |\n\n# List of releases\n\n<details>\n<summary>Click to expand</summary>\n\n## GTA SA The Definitive Edition Fusion Mod\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adds an ability to quicksave anywhere by pressing F5.\n\n[Download](http://thirteenag.github.io/wfp#gtasade)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTASADE.FusionMod/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTASADE.FusionMod/Gameface/Binaries/Win64/scripts/GTASADE.FusionMod.ini)\n\n## GTA VC The Definitive Edition Fusion Mod\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adds an ability to quicksave anywhere by pressing F5.\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adds an ability to disable first person aim mode for rifles.\n\n[Download](http://thirteenag.github.io/wfp#gtavcde)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTAVCDE.FusionMod/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTAVCDE.FusionMod/Gameface/Binaries/Win64/scripts/GTAVCDE.FusionMod.ini)\n\n## GTA III The Definitive Edition Fusion Mod\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adds an ability to quicksave anywhere by pressing F5.\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adds an ability to disable first person aim mode for rifles.\n\n[Download](http://thirteenag.github.io/wfp#gta3de)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTA3DE.FusionMod/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTA3DE.FusionMod/Gameface/Binaries/Win64/scripts/GTA3DE.FusionMod.ini)\n\n## Bully: Scholarship Edition Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View (Ultra-Wide)\n\n![](https://habrastorage.org/webt/31/qm/gv/31qmgv6q0kj8zie1itat5ygfsuq.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Unlocked all resolutions supported by the OS\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Fixed an issue when last selected resolution is reset upon startup\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n[Download](http://thirteenag.github.io/wfp#bully)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Bully.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Bully.WidescreenFix/plugins/Bully.WidescreenFix.ini)\n\n## Burnout 3: Takedown Widescreen Fix [PCSX2F]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#burnout3)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Burnout3.PCSX2.WidescreenFix/dllmain.cpp)\n\n## Call of Cthulhu Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Issues unfixed: [#420](https://github.com/ThirteenAG/WidescreenFixesPack/issues/420)\n\n[Download](http://thirteenag.github.io/wfp#callofcthulhu)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/CallOfCthulhu.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/CallOfCthulhu.WidescreenFix/Engine/scripts/CallOfCthulhu.WidescreenFix.ini)\n\n## Cold Fear Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable cutscene borders\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n[Download](http://thirteenag.github.io/wfp#coldfear)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/ColdFear.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/ColdFear.WidescreenFix/scripts/ColdFear.WidescreenFix.ini)\n\n## Condemned: Criminal Origins Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution (Config.exe)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD (Ultra-Wide)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix stretched menu\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix low framerate\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix issue when controls aren't saved\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make windowed mode borderless\n\n[Download](http://thirteenag.github.io/wfp#condemned)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Condemned.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Condemned.WidescreenFix/scripts/Condemned.WidescreenFix.ini)\n\n## Deer Avenger 4 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Some rendering issues are visible in widescreen.\n\n[Download](http://thirteenag.github.io/wfp#da4)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/DeerAvenger4.WidescreenFix/dllmain.cpp)\n\n## DRIV3R Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase draw distance\n\n[Download](http://thirteenag.github.io/wfp#driv3r)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Driv3r.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Driv3r.WidescreenFix/scripts/Driv3r.WidescreenFix.ini)\n\n## Driver Parallel Lines Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n[Download](http://thirteenag.github.io/wfp#driverpl)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/DriverParallelLines.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/DriverParallelLines.WidescreenFix/scripts/DriverParallelLines.WidescreenFix.ini)\n\n## Enter the Matrix Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#enterthematrix)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/EnterTheMatrix.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/EnterTheMatrix.WidescreenFix/scripts/EnterTheMatrix.WidescreenFix.ini)\n\n## Far Cry Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n[Download](http://thirteenag.github.io/wfp#farcry)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/FarCry.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/FarCry.WidescreenFix/Bin32/FarCry.WidescreenFix.ini)\n\n## FlatOut Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to skip intro\n\n[Download](http://thirteenag.github.io/wfp#flatout)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Flatout.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Flatout.WidescreenFix/scripts/Flatout.WidescreenFix.ini)\n\n## FlatOut 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to skip intro\n\n[Download](http://thirteenag.github.io/wfp#flatout2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Flatout2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Flatout2.WidescreenFix/scripts/Flatout2.WidescreenFix.ini)\n\n## FlatOut Ultimate Carnage Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Only supports different aspect ratios like 21:9\n\n[Download](http://thirteenag.github.io/wfp#flatoutuc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/FlatoutUltimateCarnage.WidescreenFix/dllmain.cpp)\n\n## Grand Theft Auto (1) Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n[Download](http://thirteenag.github.io/wfp#gta1)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTA1.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTA1.WidescreenFix/WINO/scripts/GTA1.WidescreenFix.ini)\n\n## Grand Theft Auto 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to quicksave via customizable hotkey\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) If you have the issue with broken graphics when driving at high speeds, try setting the ini parameter `CameraZoomFactor = auto` to a hardcoded value. It's currently calculated this way: [((AspectRatio) / (4.0 / 3.0)) \\* 2.5](<https://www.google.com/search?q=((2560/1080)+/+(4.0+/+3.0))+*+2.5>). Find out what that value is for your resolution using this [link](<https://www.google.com/search?q=((2560/1080)+/+(4.0+/+3.0))+*+2.5>) (e.g. for `2560x1080`, `CameraZoomFactor = 4.44`) and set `CameraZoomFactor` to anything less than that (e.g. for `2560x1080`, `CameraZoomFactor = 3.8`).\n\n[Download](http://thirteenag.github.io/wfp#gta2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTA2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTA2.WidescreenFix/scripts/GTA2.WidescreenFix.ini)\n\n## Grand Theft Auto III Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Many different options available in the ini file\n\n[Download](http://thirteenag.github.io/wfp#gta3)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTA3.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTA3.WidescreenFix/scripts/GTA3.WidescreenFix.ini)\n\n## Grand Theft Auto Vice City Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Many different options available in the ini file\n\n[Download](http://thirteenag.github.io/wfp#gtavc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTAVC.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTAVC.WidescreenFix/scripts/GTAVC.WidescreenFix.ini)\n\n## Grand Theft Auto San Andreas Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Many different options available in the ini file\n\n[Download](http://thirteenag.github.io/wfp#gtasa)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTASA.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTASA.WidescreenFix/scripts/GTASA.WidescreenFix.ini)\n\n## Grand Theft Auto Liberty City Stories Widescreen Fix [PCSX2F]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/31/qm/gv/31qmgv6q0kj8zie1itat5ygfsuq.png) Fixed HUD (Ultra-Wide)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Plugin automatically skips intro\n\n[Download](http://thirteenag.github.io/wfp#gtalcs)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTALCS.PCSX2.WidescreenFix/dllmain.cpp)\n\n## Grand Theft Auto Vice City Stories Widescreen Fix [PCSX2F]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD (Ultra-Wide)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Plugin automatically skips intro\n\n[Download](http://thirteenag.github.io/wfp#gtavcs)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTAVCS.PCSX2.WidescreenFix/dllmain.cpp)\n\n## Grand Theft Auto Liberty City Stories Widescreen Fix [PPSSPP]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to resize HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to enable dual analog patch\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to swap L-button and Square (in vehicle)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to swap R-button and Cross (in vehicle)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to enable 60 fps\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Plugin automatically skips intro\n\n[Download](http://thirteenag.github.io/wfp#gtalcspsp)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTALCS.PPSSPP.WidescreenFix/main.c)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTALCS.PPSSPP.WidescreenFix/memstick/PSP/PLUGINS/GTALCS.PPSSPP.WidescreenFix/GTALCS.PPSSPP.WidescreenFix.ini)\n\n## Grand Theft Auto Vice City Stories Widescreen Fix [PPSSPP]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to resize HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to enable dual analog patch\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to swap L-button and Square (in vehicle)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to swap R-button and Cross (in vehicle)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to enable 60 fps\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Plugin automatically skips intro\n\n[Download](http://thirteenag.github.io/wfp#gtavcspsp)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/GTAVCS.PPSSPP.WidescreenFix/main.c)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/GTAVCS.PPSSPP.WidescreenFix/memstick/PSP/PLUGINS/GTAVCS.PPSSPP.WidescreenFix/GTAVCS.PPSSPP.WidescreenFix.ini)\n\n## Gun Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#gun)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Gun.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Gun.WidescreenFix/scripts/Gun.WidescreenFix.ini)\n\n## Hidden and Dangerous 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Damage overlay is disabled (not scaled properly)\n\n[Download](http://thirteenag.github.io/wfp#hd2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/HiddenandDangerous2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/HiddenandDangerous2.WidescreenFix/scripts/HiddenandDangerous2.WidescreenFix.ini)\n\n## Just Cause Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#justcause)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/JustCause.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/JustCause.WidescreenFix/scripts/JustCause.WidescreenFix.ini)\n\n## King Kong Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase mouse sensitivity beyond the game's limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to hide untextured objects (**experimental, not recommended to use**)\n\n[Download](http://thirteenag.github.io/wfp#kingkong)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/KingKong.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/KingKong.WidescreenFix/scripts/KingKong.WidescreenFix.ini)\n\n## Knight Rider: The Game Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#kr)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/KnightRider.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/KnightRider.WidescreenFix/scripts/KnightRider.WidescreenFix.ini)\n\n## Knight Rider: The Game 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#kr2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/KnightRider2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/KnightRider2.WidescreenFix/scripts/KnightRider2.WidescreenFix.ini)\n\n## L.A. Rush Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#larush)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/LARush.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/LARush.WidescreenFix/plugins/LARush.WidescreenFix.ini)\n\n## Mafia: The City of Lost Heaven Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase draw distance\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix issue with settings\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make windowed mode borderless\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to set deadzone for left stick and right stick on gamepad (otherwise camera just spins on its own)\n\n[Download](http://thirteenag.github.io/wfp#mafia)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Mafia.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Mafia.WidescreenFix/scripts/Mafia.WidescreenFix.ini)\n\n## Manhunt Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n[Download](http://thirteenag.github.io/wfp#manhunt)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Manhunt.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Manhunt.WidescreenFix/scripts/Manhunt.WidescreenFix.ini)\n\n## Mass Effect Trilogy FOV Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#masseffect)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/MassEffectTrilogy.FOVFix/dllmain.cpp)\n\n## Max Payne Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make graphic novels fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable cutscene borders\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to load save game automatically on startup\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to keep the game running on minimize\n\n[Download](http://thirteenag.github.io/wfp#mp1)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/MaxPayne.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/MaxPayne.WidescreenFix/scripts/MaxPayne.WidescreenFix.ini)\n\n## Max Payne 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make graphic novels fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable cutscene borders\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to load save game automatically on startup\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to keep the game running on minimize\n\n[Download](http://thirteenag.github.io/wfp#mp2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/MaxPayne2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/MaxPayne2.WidescreenFix/scripts/MaxPayne2.WidescreenFix.ini)\n\n## Need for Speed Underground Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed black magazine covers\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to switch between different 3D scaling\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to enable windowed mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to improve gamepad support (gamepad icons, fixed bindings etc)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase deadzone for left stick\n\n[Download](http://thirteenag.github.io/wfp#nfsu)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/NFSUnderground.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/NFSUnderground.WidescreenFix/scripts/NFSUnderground.WidescreenFix.ini)\n\n## Need for Speed Underground 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to switch between different 3D scaling\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to enable windowed mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable cutscene borders\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to write registry settings to a file\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to improve gamepad support (gamepad icons, fixed bindings etc)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase in-game cutscenes framerate\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix optical drive bug\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase deadzone for left stick\n\n[Download](http://thirteenag.github.io/wfp#nfsu2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/NFSUnderground2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/NFSUnderground2.WidescreenFix/scripts/NFSUnderground2.WidescreenFix.ini)\n\n## Need for Speed Most Wanted (2005) Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to switch between different 3D scaling\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to enable windowed mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase shadow resolution\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix dynamic shadows\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to enable rearview mirror for all camera views\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to write registry settings to a file\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to improve gamepad support (gamepad icons, fixed bindings etc)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable motion blur\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to force 44100Hz sample rate audio\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase deadzone for left stick\n\n[Download](http://thirteenag.github.io/wfp#nfsmw)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/NFSMostWanted.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/NFSMostWanted.WidescreenFix/scripts/NFSMostWanted.WidescreenFix.ini)\n\n## Need for Speed Carbon Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to switch between different 3D scaling\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to enable windowed mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix crash after loading a profile\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to write registry settings to a file\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to improve gamepad support (gamepad icons, fixed bindings etc)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable motion blur without losing other effects\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to expand controller configuration\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase deadzone for left stick\n\n[Download](http://thirteenag.github.io/wfp#nfsc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/NFSCarbon.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/NFSCarbon.WidescreenFix/scripts/NFSCarbon.WidescreenFix.ini)\n\n## Need for Speed ProStreet Generic Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs (Ultra-Wide)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix resolution detection\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix a game breaking bug after completing a race\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to uncap framerate\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix various crashes\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase shadow resolution\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to skip intro\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to enable windowed mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to write registry settings to a file\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to improve gamepad support (gamepad icons, fixed bindings etc)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase deadzone for left stick\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable motion blur\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix brake lights\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix default gamma setting\n\n[Download](http://thirteenag.github.io/wfp#nfsps)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/NFSProStreet.GenericFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/NFSProStreet.GenericFix/scripts/NFSProStreet.GenericFix.ini)\n\n## Need for Speed Undercover Generic Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD (Ultra-Wide)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View (Ultra-Wide)\n\n![](https://habrastorage.org/webt/31/qm/gv/31qmgv6q0kj8zie1itat5ygfsuq.png) Fixed FMVs (Ultra-Wide)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix resolution detection\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to skip intro\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to enable windowed mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase shadow resolution\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change the scale of the shadow map\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change save game directory\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to write registry settings to a file\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to improve gamepad support (gamepad icons, fixed bindings etc)\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase deadzone for left stick\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to set bloom intensity\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable motion blur\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to improve scenery LOD\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n[Download](http://thirteenag.github.io/wfp#nfsuc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/NFSUndercover.GenericFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/NFSUndercover.GenericFix/scripts/NFSUndercover.GenericFix.ini)\n\n## Onimusha 3 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#onimusha3)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Onimusha3.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Onimusha3.WidescreenFix/scripts/Onimusha3.WidescreenFix.ini)\n\n## Paradise Cracked Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n[Download](http://thirteenag.github.io/wfp#paradisecracked)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/ParadiseCracked.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/ParadiseCracked.WidescreenFix/scripts/ParadiseCracked.WidescreenFix.ini)\n\n## Psi-Ops: The Mindgate Conspiracy Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/31/qm/gv/31qmgv6q0kj8zie1itat5ygfsuq.png) Fixed FMVs [(#272)](https://github.com/ThirteenAG/WidescreenFixesPack/issues/272)\n\n[Download](http://thirteenag.github.io/wfp#psiops)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/PsiOpsTheMindgateConspiracy.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/PsiOpsTheMindgateConspiracy.WidescreenFix/scripts/PsiOpsTheMindgateConspiracy.WidescreenFix.ini)\n\n## Psychonauts Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#psychonauts)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/Psychonauts.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/Psychonauts.WidescreenFix/scripts/Psychonauts.WidescreenFix.ini)\n\n## Resident Evil 2 Fusion Mod [Dolphin]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to skip doors animation\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to unthrottle emulator during door skip (speeds up loading) \n\n[Download](http://thirteenag.github.io/wfp#re2gc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/ResidentEvil2.Dolphin.FusionMod/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/ResidentEvil2.Dolphin.FusionMod/scripts/ResidentEvil2.Dolphin.FusionMod.ini)\n\n## Resident Evil 3: Nemesis Fusion Mod [Dolphin]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to skip doors animation\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to unthrottle emulator during door skip (speeds up loading) \n\n[Website](http://thirteenag.github.io/wfp#re3gc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/ResidentEvil3.Dolphin.FusionMod/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/ResidentEvil3.Dolphin.FusionMod/scripts/ResidentEvil3.Dolphin.FusionMod.ini)\n\n## Second Sight Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#secondsight)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SecondSight.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SecondSight.WidescreenFix/scripts/SecondSight.WidescreenFix.ini)\n\n## Silent Hill 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable cutscene borders\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to set core affinity\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable safe mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make fast transitions\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to set fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change camera speed to match PS2 version\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix gamepad bindings\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix certain lighting issues\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase noise effect resolution\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make background images fullscreen\n\n[Download](http://thirteenag.github.io/wfp#sh2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SilentHill2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SilentHill2.WidescreenFix/scripts/SilentHill2.WidescreenFix.ini)\n\n## Silent Hill 3 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to make FMVs fullscreen\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable cutscene borders\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable safe mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added options to increase resolution of dynamic shadows and other effects\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase fog complexity\n\n[Download](http://thirteenag.github.io/wfp#sh3)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SilentHill3.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SilentHill3.WidescreenFix/scripts/SilentHill3.WidescreenFix.ini)\n\n## Silent Hill 4 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable cutscene borders\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase pause background resolution\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to fix cutscene framerate\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable registry dependency\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable safe mode\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to skip intro\n\n[Download](http://thirteenag.github.io/wfp#sh4)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SilentHill4.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SilentHill4.WidescreenFix/scripts/SilentHill4.WidescreenFix.ini)\n\n## Sniper Elite Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#sniperelite)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SniperElite.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SniperElite.WidescreenFix/scripts/SniperElite.WidescreenFix.ini)\n\n## Sonic Heroes Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#sonicheroes)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SonicHeroes.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SonicHeroes.WidescreenFix/scripts/SonicHeroes.WidescreenFix.ini)\n\n## Splinter Cell Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#sc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SplinterCell.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SplinterCell.WidescreenFix/system/scripts/SplinterCell.WidescreenFix.ini)\n\n## Splinter Cell Pandora Tomorrow Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to increase night vision resolution\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change goggles' light color\n\n[Download](http://thirteenag.github.io/wfp#scpt)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SplinterCellPandoraTomorrow.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SplinterCellPandoraTomorrow.WidescreenFix/offline/system/scripts/SplinterCellPandoraTomorrow.WidescreenFix.ini)\n\n## Splinter Cell Chaos Theory Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change goggles' light color\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to disable Alt+Tab fix\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to adjust the offsets depending on game language\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to adjust shadow map size and shadow filtering\n\n[Download](http://thirteenag.github.io/wfp#scct)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SplinterCellChaosTheory.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SplinterCellChaosTheory.WidescreenFix/system/scripts/SplinterCellChaosTheory.WidescreenFix.ini)\n\n## Splinter Cell Double Agent Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change goggles' light color\n\n[Download](http://thirteenag.github.io/wfp#scda)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SplinterCellDoubleAgent.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SplinterCellDoubleAgent.WidescreenFix/SCDA-Offline/System/scripts/SplinterCellDoubleAgent.WidescreenFix.ini)\n\n## Splinter Cell Double Agent V2 Widescreen Fix [PCSX2F]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/31/qm/gv/31qmgv6q0kj8zie1itat5ygfsuq.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#scdaps2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SplinterCellDoubleAgentPS2.WidescreenFix/dllmain.cpp)\n\n## Splinter Cell Double Agent V2 Widescreen Fix [CXBXR]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#scdaxbox)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SplinterCellDoubleAgent.CXBXR.WidescreenFix/dllmain.cpp)\n\n## Splinter Cell Essentials Dual Analog Patch [PPSSPP]\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to enable dual analog patch\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Added an option to enable 60 fps\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to skip intro\n\n[Download](http://thirteenag.github.io/wfp#sce)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/SplinterCellEssentials.PPSSPP.DualAnalogPatch/main.c)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/SplinterCellEssentials.PPSSPP.DualAnalogPatch/memstick/PSP/PLUGINS/SplinterCellEssentials.PPSSPP.DualAnalogPatch/SplinterCellEssentials.PPSSPP.DualAnalogPatch.ini)\n\n## Street Racing Syndicate Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#srs)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/StreetRacingSyndicate.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/StreetRacingSyndicate.WidescreenFix/Bin/scripts/StreetRacingSyndicate.WidescreenFix.ini)\n\n## Stubbs the Zombie Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Fixed OpenGL crash on startup\n\n[Download](http://thirteenag.github.io/wfp#stubbsthezombie)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/StubbstheZombie.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/StubbstheZombie.WidescreenFix/scripts/StubbstheZombie.WidescreenFix.ini)\n\n## The Punisher Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to skip intro\n\n[Download](http://thirteenag.github.io/wfp#punisher)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/ThePunisher.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/ThePunisher.WidescreenFix/scripts/ThePunisher.WidescreenFix.ini)\n\n## The Godfather Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#thegodfather)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TheGodfather.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TheGodfather.WidescreenFix/scripts/TheGodfather.WidescreenFix.ini)\n\n## The Matrix: Path of Neo Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#thematrixpathofneo)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TheMatrixPathOfNeo.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TheMatrixPathOfNeo.WidescreenFix/scripts/TheMatrixPathOfNeo.WidescreenFix.ini)\n\n## The Suffering Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n[Download](http://thirteenag.github.io/wfp#thesuffering)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TheSuffering.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TheSuffering.WidescreenFix/scripts/TheSuffering.WidescreenFix.ini)\n\n## Tony Hawk's American Wasteland Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to randomize songs order\n\n[Download](http://thirteenag.github.io/wfp#thaw)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TonyHawksAmericanWasteland.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TonyHawksAmericanWasteland.WidescreenFix/Game/scripts/TonyHawksAmericanWasteland.WidescreenFix.ini)\n\n## Tony Hawk's Underground 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to randomize songs order\n\n[Download](http://thirteenag.github.io/wfp#thug2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TonyHawksUnderground2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TonyHawksUnderground2.WidescreenFix/Game/scripts/TonyHawksUnderground2.WidescreenFix.ini)\n\n## Tony Hawk's Underground Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to randomize songs order\n\n[Download](http://thirteenag.github.io/wfp#thug)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TonyHawksUnderground.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TonyHawksUnderground.WidescreenFix/Game/scripts/TonyHawksUnderground.WidescreenFix.ini)\n\n## Tony Hawk's Pro Skater 4 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to randomize songs order\n\n[Download](http://thirteenag.github.io/wfp#thps4)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TonyHawksProSkater4.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TonyHawksProSkater4.WidescreenFix/Game/scripts/TonyHawksProSkater4.WidescreenFix.ini)\n\n## Tony Hawk's Pro Skater 3 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to randomize songs order\n\n[Download](http://thirteenag.github.io/wfp#thps3)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TonyHawksProSkater3.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TonyHawksProSkater3.WidescreenFix/scripts/TonyHawksProSkater3.WidescreenFix.ini)\n\n## Tony Hawk's Pro Skater 2 Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#thps2)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TonyHawksProSkater2.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TonyHawksProSkater2.WidescreenFix/scripts/TonyHawksProSkater2.WidescreenFix.ini)\n\n## Total Overdose Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD (Except menu)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs (Ver-)\n\n[Download](http://thirteenag.github.io/wfp#tod)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TotalOverdose.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TotalOverdose.WidescreenFix/scripts/TotalOverdose.WidescreenFix.ini)\n\n## True Crime Streets of LA Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/31/qm/gv/31qmgv6q0kj8zie1itat5ygfsuq.png) Fixed Field of View\n\n[Download](http://thirteenag.github.io/wfp#truecrimesola)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TrueCrimeStreetsofLA.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TrueCrimeStreetsofLA.WidescreenFix/scripts/TrueCrimeStreetsofLA.WidescreenFix.ini)\n\n## True Crime New York City Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Resolution\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n[Download](http://thirteenag.github.io/wfp#truecrimenyc)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/TrueCrimeNewYorkCity.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/TrueCrimeNewYorkCity.WidescreenFix/scripts/TrueCrimeNewYorkCity.WidescreenFix.ini)\n\n## Ultimate Spider-Man Widescreen Fix\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Aspect Ratio\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed HUD (partially, radar map has to be removed)\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed Field of View\n\n![](https://habrastorage.org/webt/ow/yy/mg/owyymgpibfqzfbwyf_iqoiqrede.png) Fixed FMVs\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Added an option to change fps limit\n\n![](https://habrastorage.org/webt/d_/eg/ym/d_egymd6w_tem2erocab-e9ikna.png) Adjustable FOV via ini\n\n[Download](http://thirteenag.github.io/wfp#usm)\n|\n[Source](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/source/UltimateSpiderMan.WidescreenFix/dllmain.cpp)\n|\n[Default INI File](https://github.com/ThirteenAG/WidescreenFixesPack/blob/master/data/UltimateSpiderMan.WidescreenFix/scripts/UltimateSpiderMan.WidescreenFix.ini)\n\n</details>\n"
 },
 {
  "repo": "mptre/yank",
  "language": "C",
  "readme_contents": "# yank\n\nYank terminal output to clipboard.\n\n![yank](https://raw.githubusercontent.com/mptre/yank/gh-pages/screencast.gif)\n\n## Description\n\nThe\n[yank(1)][yank]\nutility reads input from `stdin` and display a selection interface that allows a\nfield to be selected and copied to the clipboard.\nFields are either recognized by a regular expression using the `-g` option or by\nsplitting the input on a delimiter sequence using the `-d` option.\n\nUsing the arrow keys will move the selected field.\nThe interface supports several Emacs and Vi like key bindings,\nconsult the man page for further reference.\nPressing the return key will invoke the yank command and write the selected\nfield to its `stdin`.\nThe yank command defaults to\n[xsel(1)][xsel]\nbut could be anything that accepts input on `stdin`.\nWhen invoking yank,\neverything supplied after the `--` option will be used as the yank command,\nsee examples below.\n\n## Motivation\n\nOthers including myself consider it a cache miss when resort to using the mouse.\nCopying output from the terminal is still one of the few cases where I still use\nthe mouse.\nSeveral terminal multiplexers solves this issue,\nhowever I don't want to be required to use a multiplexer but instead use a\nterminal agnostic solution.\n\n## Examples\n\n- Yank an environment variable key or value:\n\n  ```sh\n  $ env | yank -d =\n  ```\n\n- Yank a field from a CSV file:\n\n  ```sh\n  $ yank -d \\\", <file.csv\n  ```\n\n- Yank a whole line using the `-l` option:\n\n  ```sh\n  $ make 2>&1 | yank -l\n  ```\n\n- If `stdout` is not a terminal the selected field will be written to `stdout`\n  and exit without invoking the yank command.\n  Kill the selected PID:\n\n  ```sh\n  $ ps ux | yank -g [0-9]+ | xargs kill\n  ```\n\n- Yank the selected field to the clipboard as opposed of the default primary\n  clipboard:\n\n  ```sh\n  $ yank -- xsel -b\n  ```\n\n## Installation\n\n### Arch Linux\n\n```sh\n$ pacman -S yank\n```\n\n### Debian\n\n```sh\n$ sudo apt-get install yank\n```\n\nThe binary is installed at `/usr/bin/yank-cli` due to a naming conflict.\n\n### Fedora\n\nVersions 24/25/26/Rawhide:\n\n```sh\n$ sudo dnf install yank\n```\n\nThe binary is installed at `/usr/bin/yank-cli` due to a naming conflict.\nMan-pages are available as both `yank` and `yank-cli`.\n\n### Nix/NixOS\n\n```sh\n$ nix-env -i yank\n```\n\n### openSUSE\n\n```\n$ zypper install yank\n```\n\n### macOS via Homebrew\n\n```sh\n$ brew install yank\n```\n\n### macOS via MacPorts\n\n```sh\n$ sudo port install yank\n```\n\n### FreeBSD\n\n```sh\n$ pkg install yank\n```\n\n### OpenBSD\n\n```sh\n$ pkg_add yank\n```\n\n### From source\n\nThe install directory defaults to `/usr/local`:\n\n```sh\n$ make install\n```\n\nChange the install directory using the `PREFIX` variable:\n\n```sh\n$ make PREFIX=DIR install\n```\n\nThe default yank command can be defined using the `YANKCMD` variable.\nFor instance,\nmacOS users would prefer `pbcopy(1)`:\n\n```sh\n$ make YANKCMD=pbcopy\n```\n\n## License\n\nCopyright (c) 2015-2022 Anton Lindqvist.\nDistributed under the MIT license.\n\n[xsel]: http://www.vergenet.net/~conrad/software/xsel/\n[yank]: https://www.basename.se/yank/\n"
 },
 {
  "repo": "utkuozbulak/pytorch-cnn-visualizations",
  "language": "Python",
  "readme_contents": "# Convolutional Neural Network Visualizations \n\nThis repository contains a number of convolutional neural network visualization techniques implemented in PyTorch.\n\n**Note**: I removed cv2 dependencies and moved the repository towards PIL. A few things might be broken (although I tested all methods), I would appreciate if you could create an issue if something does not work.\n\n**Note**: The code in this repository was tested with torch version 0.4.1 and some of the functions may not work as intended in later versions. Although it shouldn't be too much of an effort to make it work, I have no plans at the moment to make the code in this repository compatible with the latest version because I'm still using 0.4.1.\n\n## Implemented Techniques\n\n* [Gradient visualization with vanilla backpropagation](#gradient-visualization)\n* [Gradient visualization with guided backpropagation](#gradient-visualization) [1]\n* [Gradient visualization with saliency maps](#gradient-visualization) [4]\n* [Gradient-weighted class activation mapping](#gradient-visualization) [3] (Generalization of [2]) \n* [Guided, gradient-weighted class activation mapping](#gradient-visualization) [3]\n* [Score-weighted class activation mapping](#gradient-visualization) [15] (Gradient-free generalization of [2])\n* [Element-wise gradient-weighted class activation mapping](#hierarchical-gradient-visualization) [16]\n* [Smooth grad](#smooth-grad) [8]\n* [CNN filter visualization](#convolutional-neural-network-filter-visualization) [9]\n* [Inverted image representations](#inverted-image-representations) [5]\n* [Deep dream](#deep-dream) [10]\n* [Class specific image generation](#class-specific-image-generation) [4] [14]\n* [Grad times image](#grad-times-image) [12]\n* [Integrated gradients](#gradient-visualization) [13]\n* [Layerwise relevance propagation](#gradient-visualization) [17]\n\n## General Information\n\nDepending on the technique, the code uses pretrained **AlexNet** or **VGG** from the model zoo. Some of the code also assumes that the layers in the model are separated into two sections; **features**, which contains the convolutional layers and **classifier**, that contains the fully connected layer (after flatting out convolutions). If you want to port this code to use it on your model that does not have such separation, you just need to do some editing on parts where it calls *model.features* and *model.classifier*.\n\nEvery technique has its own python file (e.g. *gradcam.py*) which I hope will make things easier to understand. *misc_functions.py* contains functions like image processing and image recreation which is shared by the implemented techniques.\n\nAll images are pre-processed with mean and std of the ImageNet dataset before being fed to the model. None of the code uses GPU as these operations are quite fast for a single image (except for deep dream because of the example image that is used for it is huge). You can make use of gpu with very little effort. The example pictures below include numbers in the brackets after the description, like *Mastiff (243)*, this number represents the class id in the ImageNet dataset.\n\nI tried to comment on the code as much as possible, if you have any issues understanding it or porting it, don't hesitate to send an email or create an issue.\n\nBelow, are some sample results for each operation.\n\n## Gradient Visualization\n<table border=0 align=center>\n\t<tbody>\n    <tr>\n\t\t\t<td>  </td>\n\t\t\t<td align=\"center\"> Target class: King Snake (56) </td>\n\t\t\t<td align=\"center\"> Target class: Mastiff (243) </td>\n\t\t\t<td align=\"center\"> Target class: Spider (72)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Original Image </td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/snake.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/cat_dog.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/spider.png\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Colored Vanilla Backpropagation </td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Vanilla_BP_color.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Vanilla_BP_color.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Vanilla_BP_color.jpg\"> </td>\n\t\t</tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Vanilla Backpropagation Saliency </td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Vanilla_BP_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Vanilla_BP_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Vanilla_BP_gray.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Colored Guided Backpropagation <br />  <br />  (GB)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Guided_BP_color.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Guided_BP_color.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Guided_BP_color.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\">Guided Backpropagation Saliency<br />  <br /> (GB)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Guided_BP_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Guided_BP_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Guided_BP_gray.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\">Guided Backpropagation Negative Saliency<br />  <br /> (GB)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_neg_sal.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_neg_sal.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_neg_sal.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\">Guided Backpropagation Positive Saliency<br />  <br /> (GB)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_pos_sal.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_pos_sal.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_pos_sal.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Gradient-weighted Class Activation Map <br />  <br /> (Grad-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Cam_Grayscale.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Cam_Grayscale.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Cam_Grayscale.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Gradient-weighted Class Activation Heatmap <br />  <br /> (Grad-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Cam_Heatmap.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Cam_Heatmap.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Cam_Heatmap.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Gradient-weighted Class Activation Heatmap on Image <br />  <br /> (Grad-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Cam_On_Image.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Cam_On_Image.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Cam_On_Image.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Score-weighted Class Activation Map <br />  <br /> (Score-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_ScoreCAM_Grayscale.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_ScoreCAM_Grayscale.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_ScoreCAM_Grayscale.png\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Score-weighted Class Activation Heatmap <br />  <br /> (Score-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_ScoreCAM_Heatmap.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_ScoreCAM_Heatmap.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_ScoreCAM_Heatmap.png\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Score-weighted Class Activation Heatmap on Image <br />  <br /> (Score-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_ScoreCAM_On_Image.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_ScoreCAM_On_Image.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_ScoreCAM_On_Image.png\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Colored Guided Gradient-weighted Class Activation Map <br />  <br /> (Guided-Grad-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_GGrad_Cam.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_GGrad_Cam.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_GGrad_Cam.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Guided Gradient-weighted Class Activation Map Saliency <br />  <br /> (Guided-Grad-CAM)</td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_GGrad_Cam_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_GGrad_Cam_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_GGrad_Cam_gray.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Integrated Gradients <br /> (without image multiplication)  </td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Integrated_G_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Integrated_G_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Integrated_G_gray.jpg\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Layerwise Relevance <br /> (LRP) - Layer 7  </td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/LRP_out_snake_7.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/LRP_out_dog_7.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/LRP_out_spider_7.png\"> </td>\n\t\t</tr>\n    <tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Layerwise Relevance <br /> (LRP) - Layer 1  </td>\n\t\t\t<td width=\"27%\" > <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/LRP_out_snake.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/LRP_out_dog.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/LRP_out_spider.png\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n## Hierarchical Gradient Visualization\nLayerCAM [16] is a simple modification of Grad-CAM [3], which can generate reliable class activation maps from different layers. For the examples provided below, a pre-trained **VGG16** was used.\n\n<table border=0 align=center>\n\t<tbody> \n    <tr>\n\t\t\t<td>  </td>\n\t\t\t<td align=\"center\"> Class Activation Map </td>\n\t\t\t<td align=\"center\"> Class Activation HeatMap </td>\n\t\t\t<td align=\"center\"> Class Activation HeatMap on Image</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> LayerCAM <br /> (Layer 9)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool2_Grayscale.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool2_Heatmap.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool2_On_Image.png\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> LayerCAM <br /> (Layer 16)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool3_Grayscale.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool3_Heatmap.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool3_On_Image.png\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> LayerCAM <br /> (Layer 23)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool4_Grayscale.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool4_Heatmap.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool4_On_Image.png\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> LayerCAM <br /> (Layer 30)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool5_Grayscale.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool5_Heatmap.png\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"results/hierarchical_gradient_visualization/snake_LayerCam_pool5_On_Image.png\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\n## Grad Times Image\nAnother technique that is proposed is simply multiplying the gradients with the image itself. Results obtained with the usage of multiple gradient techniques are below.\n\n<table border=0  align=center>\n\t<tbody> \n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Vanilla Grad <br /> <i>X</i> <br /> Image</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Vanilla_grad_times_image_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Vanilla_grad_times_image_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Vanilla_grad_times_image_gray.jpg\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Guided Grad <br /> <i>X</i> <br /> Image</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Guided_grad_times_image_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Guided_grad_times_image_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Guided_grad_times_image_gray.jpg\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Integrated Grad <br /> <i>X</i> <br /> Image</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/snake_Integrated_grad_times_image_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/cat_dog_Integrated_grad_times_image_gray.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/gradient_visualizations/spider_Integrated_grad_times_image_gray.jpg\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\n## Smooth Grad\nSmooth grad is adding some Gaussian noise to the original image and calculating gradients multiple times and averaging the results [8]. There are two examples at the bottom which use _vanilla_ and _guided_ backpropagation to calculate the gradients. Number of images (_n_) to average over is selected as 50. _\u03c3_ is shown at the bottom of the images.\n\n<table border=0 align=center>\n\t<tbody> \n    <tr>\t\t<td width=\"27%\" align=\"center\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <strong>Vanilla Backprop</strong> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> </td>\n\t\t</tr>\n<tr>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/vanilla/snake_.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/vanilla/dog_.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/vanilla/spider_.gif\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\n<table border=0 align=center>\n\t<tbody> \n    <tr>\t\t<td width=\"27%\" align=\"center\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <strong>Guided Backprop</strong> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> </td>\n\t\t</tr>\n<tr>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/gbp/snake_.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/gbp/dog_.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/gbp/spider_.gif\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n## Convolutional Neural Network Filter Visualization\nCNN filters can be visualized when we optimize the input image with respect to output of the specific convolution operation. For this example I used a pre-trained **VGG16**. Visualizations of layers start with basic color and direction filters at lower levels. As we approach towards the final layer the complexity of the filters also increase. If you employ external techniques like blurring, gradient clipping etc. you will probably produce better images.\n\n<table border=0 align=center>\n\t<tbody> \n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Layer 2 <br /> (Conv 1-2)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l2_f1.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l2_f21.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l2_f54.jpg\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Layer 10 <br /> (Conv 2-1)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l10_f7.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l10_f10.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l10_f69.jpg\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Layer 17 <br /> (Conv 3-1)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l17_f4.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l17_f8.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l17_f9.jpg\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\"> Layer 24 <br /> (Conv 4-1)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l24_f4.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l24_f17.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l24_f22.jpg\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\nAnother way to visualize CNN layers is to to visualize activations for a specific input on a specific layer and filter. This was done in [1] Figure 3. Below example is obtained from layers/filters of VGG16 for the first image using guided backpropagation. The code for this opeations is in *layer_activation_with_guided_backprop.py*. The method is quite similar to guided backpropagation but instead of guiding the signal from the last layer and a specific target, it guides the signal from a specific layer and filter. \n\n<table border=0 align=center>\n\t<tbody> \n    <tr>\t\t<td width=\"27%\" align=\"center\"> Input Image </td>\n\t\t\t<td width=\"27%\" align=\"center\"> Layer Vis. (Filter=0)</td>\n\t\t\t<td width=\"27%\" align=\"center\"> Filter Vis. (Layer=29)</td>\n\t\t</tr>\n<tr>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/spider.png\"> </td>\n\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/spider_layer_graph.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/spider_filter_graph.gif\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\n## Inverted Image Representations\nI think this technique is the most complex technique in this repository in terms of understanding what the code does. It is mainly because of complex regularization. If you truly want to understand how this is implemented I suggest you read the second and third page of the paper [5], specifically, the regularization part. Here, the aim is to generate original image after nth layer. The further we go into the model, the harder it becomes. The results in the paper are incredibly good (see Figure 6) but here, the result quickly becomes messy as we iterate through the layers. This is because the authors of the paper tuned the parameters for each layer individually. You can tune the parameters just like the to ones that are given in the paper to optimize results for each layer. The inverted examples from several layers of **AlexNet** with the previous *Snake* picture are below.\n\n\n<table border=0 align=center>\n\t<tbody> \n    <tr>\t\t<td width=\"27%\" align=\"center\"> Layer 0: <strong>Conv2d</strong> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> Layer 2: <strong>MaxPool2d</strong> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> Layer 4: <strong>ReLU</strong> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/inverted_images/Layer_0_Inverted.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/inverted_images/Layer_2_Inverted.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/inverted_images/Layer_4_Inverted.jpg\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n<table border=0 align=center>\n\t<tbody> \n    <tr>\t\t<td width=\"27%\" align=\"center\"> Layer 7: <strong>ReLU</strong> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> Layer 9: <strong>ReLU</strong> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> Layer 12: <strong>MaxPool2d</strong> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/inverted_images/Layer_7_Inverted.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/inverted_images/Layer_9_Inverted.jpg\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/inverted_images/Layer_12_Inverted.jpg\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\n\n## Deep Dream\nDeep dream is technically the same operation as layer visualization the only difference is that you don't start with a random image but use a real picture. The samples below were created with **VGG19**, the produced result is entirely up to the filter so it is kind of hit or miss. The more complex models produce mode high level features. If you replace **VGG19** with an **Inception** variant you will get more noticable shapes when you target higher conv layers. Like layer visualization, if you employ additional techniques like gradient clipping, blurring etc. you might get better visualizations.\n\n<table border=0 align=center>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\">Original Image</td>\n\t\t\t<td width=\"70%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/dd_tree.png\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\">VGG19 <br /> Layer: 34  <br /> (Final Conv. Layer) Filter: 94</td>\n\t\t\t<td width=\"70%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/dd_l34_f94_iter250.jpg\"> </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"19%\" align=\"center\">VGG19 <br /> Layer: 34  <br /> (Final Conv. Layer) Filter: 103</td>\n\t\t\t<td width=\"70%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/dd_l34_f103_iter250.jpg\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\n## Class Specific Image Generation\nThis operation produces different outputs based on the model and the applied regularization method. Below, are some samples produced with **VGG19** incorporated with Gaussian blur every other iteration (see [14] for details). The quality of generated images also depend on the model, **AlexNet** generally has green(ish) artifacts but VGGs produce (kind of) better images. Note that these images are generated with regular CNNs with optimizing the input and **not with GANs**.\n\n<table border=0 align=center>\n\t<tbody>\n    <tr>\n\t\t\t<td width=\"27%\" align=\"center\"> Target class: Worm Snake (52) - (VGG19) </td>\n\t\t\t<td width=\"27%\" align=\"center\"> Target class: Spider (72) - (VGG19) </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/snake.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/spider.gif\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\nThe samples below show the produced image with no regularization, l1 and l2 regularizations on target class: **flamingo** (130) to show the differences between regularization methods. These images are generated with a pretrained AlexNet. \n\n<table border=0 align=\"center\">\n\t<tbody> \n    <tr>\t\t<td width=\"27%\" align=\"center\"> No Regularization </td>\n\t\t\t<td width=\"27%\" align=\"center\"> L1 Regularization </td>\n\t\t\t<td width=\"27%\" align=\"center\"> L2 Regularization </td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/flamingo_no_norm.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/flamingo_l1_norm.gif\"> </td>\n\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/flamingo_l2_norm.gif\"> </td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\nProduced samples can further be optimized to resemble the desired target class, some of the operations you can incorporate to improve quality are; blurring, clipping gradients that are below a certain treshold, random color swaps on some parts, random cropping the image, forcing generated image to follow a path to force continuity.\n\nSome of these techniques are implemented in *generate_regularized_class_specific_samples.py* (courtesy of [alexstoken](https://github.com/alexstoken)).\n\n## Requirements:\n```\ntorch == 0.4.1\ntorchvision >= 0.1.9\nnumpy >= 1.13.0\nmatplotlib >= 1.5\nPIL >= 1.1.7\n```\n\n## Citation\n\nIf you find the code in this repository useful for your research consider citing it.\n\n\t@misc{uozbulak_pytorch_vis_2022,\n\t  author = {Utku Ozbulak},\n\t  title = {PyTorch CNN Visualizations},\n\t  year = {2019},\n\t  publisher = {GitHub},\n\t  journal = {GitHub repository},\n\t  howpublished = {\\url{https://github.com/utkuozbulak/pytorch-cnn-visualizations}},\n\t  commit = {b7e60adaf64c9be97b480509285718603d1e9ba4}\n\t}\n\t\n## References:\n\n[1] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. *Striving for Simplicity: The All Convolutional Net*, https://arxiv.org/abs/1412.6806\n\n[2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba. *Learning Deep Features for Discriminative Localization*, https://arxiv.org/abs/1512.04150\n\n[3] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra. *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization*, https://arxiv.org/abs/1610.02391\n\n[4] K. Simonyan, A. Vedaldi, A. Zisserman. *Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps*, https://arxiv.org/abs/1312.6034\n\n[5] A. Mahendran, A. Vedaldi. *Understanding Deep Image Representations by Inverting Them*, https://arxiv.org/abs/1412.0035\n\n[6] H. Noh, S. Hong, B. Han,  *Learning Deconvolution Network for Semantic Segmentation* https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf\n\n[7] A. Nguyen, J. Yosinski, J. Clune.  *Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable  Images* https://arxiv.org/abs/1412.1897\n\n[8] D. Smilkov, N. Thorat, N. Kim, F. Vi\u00e9gas, M. Wattenberg. *SmoothGrad: removing noise by adding noise* https://arxiv.org/abs/1706.03825\n\n[9] D. Erhan, Y. Bengio, A. Courville, P. *Vincent. Visualizing Higher-Layer Features of a Deep Network* https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network\n\n[10] A. Mordvintsev, C. Olah, M. Tyka. *Inceptionism: Going Deeper into Neural Networks* https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\n\n[11] I. J. Goodfellow, J. Shlens, C. Szegedy. *Explaining and Harnessing Adversarial Examples* https://arxiv.org/abs/1412.6572\n\n[12] A. Shrikumar, P. Greenside, A. Shcherbina, A. Kundaje. *Not Just a Black Box: Learning Important Features Through Propagating Activation Differences* https://arxiv.org/abs/1605.01713\n\n[13] M. Sundararajan, A. Taly, Q. Yan. *Axiomatic Attribution for Deep Networks* https://arxiv.org/abs/1703.01365\n\n[14] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, Hod Lipson, *Understanding Neural Networks Through Deep Visualization* https://arxiv.org/abs/1506.06579\n\n[15] H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding, P. Mardziel, X. Hu. *Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks* https://arxiv.org/abs/1910.01279\n\n[16] P. Jiang, C. Zhang, Q. Hou, M. Cheng, Y. Wei. LayerCAM: *Exploring Hierarchical Class Activation Maps for Localization* http://mmcheng.net/mftp/Papers/21TIP_LayerCAM.pdf\n\n[17] G. Montavon1, A. Binder, S. Lapuschkin, W. Samek, and K. Muller. *Layer-Wise Relevance Propagation: An Overview* https://www.researchgate.net/publication/335708351_Layer-Wise_Relevance_Propagation_An_Overview\n\n"
 },
 {
  "repo": "shidenggui/easytrader",
  "language": "Python",
  "readme_contents": "# easytrader\n\n[![Package](https://img.shields.io/pypi/v/easytrader.svg)](https://pypi.python.org/pypi/easytrader)\n[![Travis](https://img.shields.io/travis/shidenggui/easytrader.svg)](https://travis-ci.org/shidenggui/easytrader)\n[![License](https://img.shields.io/github/license/shidenggui/easytrader.svg)](https://github.com/shidenggui/easytrader/blob/master/LICENSE)\n\n* \u8fdb\u884c\u81ea\u52a8\u7684\u7a0b\u5e8f\u5316\u80a1\u7968\u4ea4\u6613\n* \u652f\u6301\u8ddf\u8e2a `joinquant`, `ricequant` \u7684\u6a21\u62df\u4ea4\u6613\n* \u652f\u6301\u8ddf\u8e2a \u96ea\u7403\u7ec4\u5408 \u8c03\u4ed3\n* \u652f\u6301\u901a\u7528\u7684\u540c\u82b1\u987a\u5ba2\u6237\u7aef\u6a21\u62df\u64cd\u4f5c\n* \u5b9e\u73b0\u81ea\u52a8\u767b\u5f55\n* \u652f\u6301\u901a\u8fc7 webserver \u8fdc\u7a0b\u64cd\u4f5c\u5ba2\u6237\u7aef\n* \u652f\u6301\u547d\u4ee4\u884c\u8c03\u7528\uff0c\u65b9\u4fbf\u5176\u4ed6\u8bed\u8a00\u9002\u914d\n* \u57fa\u4e8e Python3.6, Win\u3002\u6ce8: Linux \u4ec5\u652f\u6301\u96ea\u7403\n\n\n### \u5fae\u4fe1\u7fa4\u4ee5\u53ca\u516c\u4f17\u53f7\n\n\u6b22\u8fce\u5927\u5bb6\u626b\u7801\u5173\u6ce8\u516c\u4f17\u53f7\u300c\u98df\u706f\u9b3c\u300d\uff0c\u4e00\u8d77\u4ea4\u6d41\u3002\u8fdb\u7fa4\u53ef\u901a\u8fc7\u83dc\u5355\u52a0\u6211\u597d\u53cb\uff0c\u5907\u6ce8\u91cf\u5316\u3002\n\n![\u516c\u4f17\u53f7\u4e8c\u7ef4\u7801](https://gitee.com/shidenggui/assets/raw/master/uPic/mp-qr.png)\n\n\u82e5\u4e8c\u7ef4\u7801\u56e0 Github \u7f51\u7edc\u65e0\u6cd5\u6253\u5f00\uff0c\u8bf7\u70b9\u51fb[\u516c\u4f17\u53f7\u4e8c\u7ef4\u7801](https://gitee.com/shidenggui/assets/raw/master/uPic/mp-qr.png)\u76f4\u63a5\u6253\u5f00\u56fe\u7247\u3002\n\n### Author\n\n**easytrader** \u00a9 [shidenggui](https://github.com/shidenggui), Released under the [MIT](./LICENSE) License.<br>\n\n> Blog [@shidenggui](https://shidenggui.com) \u00b7 Weibo [@\u98df\u706f\u9b3c](https://www.weibo.com/u/1651274491) \u00b7 Twitter [@shidenggui](https://twitter.com/shidenggui)\n\n### \u76f8\u5173\n\n[\u83b7\u53d6\u65b0\u6d6a\u514d\u8d39\u5b9e\u65f6\u884c\u60c5\u7684\u7c7b\u5e93: easyquotation](https://github.com/shidenggui/easyquotation)\n\n[\u7b80\u5355\u7684\u80a1\u7968\u91cf\u5316\u4ea4\u6613\u6846\u67b6 \u4f7f\u7528 easytrader \u548c easyquotation](https://github.com/shidenggui/easyquant)\n\n\n### \u652f\u6301\u5238\u5546\n\n* \u6d77\u901a\u5ba2\u6237\u7aef(\u6d77\u901a\u7f51\u4e0a\u4ea4\u6613\u7cfb\u7edf\u72ec\u7acb\u59d4\u6258)\n* \u534e\u6cf0\u5ba2\u6237\u7aef(\u7f51\u4e0a\u4ea4\u6613\u7cfb\u7edf\uff08\u4e13\u4e1a\u7248\u2161\uff09)\n* \u56fd\u91d1\u5ba2\u6237\u7aef(\u5168\u80fd\u884c\u8bc1\u5238\u4ea4\u6613\u7ec8\u7aefPC\u7248)\n* \u5176\u4ed6\u5238\u5546\u901a\u7528\u540c\u82b1\u987a\u5ba2\u6237\u7aef(\u9700\u8981\u624b\u52a8\u767b\u9646)\n\n\n### \u6a21\u62df\u4ea4\u6613\n\n* \u96ea\u7403\u7ec4\u5408 by @[haogefeifei](https://github.com/haogefeifei)\uff08[\u8bf4\u660e](doc/xueqiu.md)\uff09\n\n### \u4f7f\u7528\u6587\u6863\n\n[\u4e2d\u6587\u6587\u6863](http://easytrader.readthedocs.io/zh/master/)\n\n\n### \u4f5c\u8005\u5176\u4ed6\u4f5c\u54c1\n* [\u5927\u6570\u636e\u7f51\u7edc\u5c0f\u8bf4\u63a8\u8350\u7cfb\u7edf - \u63a8\u4e66\u541b](https://www.tuishujun.com)\n* [\u4e2d\u6587\u72ec\u7acb\u4e2a\u4eba\u535a\u5ba2\u5bfc\u822a - bloghub.fun](https://bloghub.fun)\n"
 },
 {
  "repo": "deis/deis",
  "language": "Python",
  "readme_contents": "|![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Warning.svg/156px-Warning.svg.png) | This repository (`deis/deis`) is [no longer developed or maintained](https://deis.com/blog/2017/deis-paas-v1-takes-a-bow/). The Deis v1 PaaS based on CoreOS Container Linux and Fleet has been replaced by [Deis Workflow](https://github.com/deis/workflow) which is based on Kubernetes. |\n|---|---|\n\n# Deis v1 PaaS\n\nDeis (pronounced DAY-iss) is an open source PaaS that makes it easy to deploy and manage applications on your own servers. Deis builds upon [Docker](http://docker.io/) and [CoreOS](http://coreos.com) to provide a lightweight PaaS with a [Heroku-inspired](http://heroku.com) workflow.\n\n[![Build Status](https://ci.deis.io/buildStatus/icon?job=test-acceptance)](https://ci.deis.io/job/test-acceptance/)\n[![Current Release](http://img.shields.io/badge/release-v1.13.4-1eb0fc.svg)](https://github.com/deis/deis/releases/tag/v1.13.4)\n[![Latest Docs](http://img.shields.io/badge/docs-latest-fc1e5e.svg)](http://docs.deis.io/en/latest/)\n\nNew to Deis?  Learn more about Deis [Concepts](http://docs.deis.io/en/latest/understanding_deis/concepts/), [Architecture](http://docs.deis.io/en/latest/understanding_deis/architecture/) and how to [Deploy an Application](http://docs.deis.io/en/latest/using_deis/deploy-application/).\n\n# Installing Deis v1\n\nDeis is a set of Docker containers that can be deployed anywhere including public cloud, private cloud, bare metal or your workstation. Decide where you'd like to deploy Deis, then follow the [provider-specific documentation](http://docs.deis.io/en/latest/installing_deis/) for provisioning.\n\nTrying out Deis? Please follow the documentation on [getting set up with Vagrant](http://docs.deis.io/en/latest/installing_deis/vagrant/).\nUpgrading from a previous Deis release? See [Upgrading Deis](http://docs.deis.io/en/latest/managing_deis/upgrading-deis/) for additional information.\n\n## Troubleshooting\n\nSee the [Troubleshooting Deis](http://docs.deis.io/en/latest/troubleshooting_deis/) documentation for\nassistance with common issues.\n\n## Contributing\n\nInterested in contributing to Deis?  Check out our [Open Roadmap](http://docs.deis.io/en/latest/roadmap/roadmap/) and [Planning Process](http://docs.deis.io/en/latest/roadmap/planning/) or jump right into [hacking on Deis](http://docs.deis.io/en/latest/contributing/hacking/) and [testing your Deis cluster](http://docs.deis.io/en/latest/contributing/testing/).\n\n## License\n\nCopyright 2013, 2014 Engine Yard, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at <http://www.apache.org/licenses/LICENSE-2.0>\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n"
 },
 {
  "repo": "isnowfy/snownlp",
  "language": "Python",
  "readme_contents": "# SnowNLP: Simplified Chinese Text Processing\n\nSnowNLP\u662f\u4e00\u4e2apython\u5199\u7684\u7c7b\u5e93\uff0c\u53ef\u4ee5\u65b9\u4fbf\u7684\u5904\u7406\u4e2d\u6587\u6587\u672c\u5185\u5bb9\uff0c\u662f\u53d7\u5230\u4e86[TextBlob](https://github.com/sloria/TextBlob)\u7684\u542f\u53d1\u800c\u5199\u7684\uff0c\u7531\u4e8e\u73b0\u5728\u5927\u90e8\u5206\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e93\u57fa\u672c\u90fd\u662f\u9488\u5bf9\u82f1\u6587\u7684\uff0c\u4e8e\u662f\u5199\u4e86\u4e00\u4e2a\u65b9\u4fbf\u5904\u7406\u4e2d\u6587\u7684\u7c7b\u5e93\uff0c\u5e76\u4e14\u548cTextBlob\u4e0d\u540c\u7684\u662f\uff0c\u8fd9\u91cc\u6ca1\u6709\u7528NLTK\uff0c\u6240\u6709\u7684\u7b97\u6cd5\u90fd\u662f\u81ea\u5df1\u5b9e\u73b0\u7684\uff0c\u5e76\u4e14\u81ea\u5e26\u4e86\u4e00\u4e9b\u8bad\u7ec3\u597d\u7684\u5b57\u5178\u3002\u6ce8\u610f\u672c\u7a0b\u5e8f\u90fd\u662f\u5904\u7406\u7684unicode\u7f16\u7801\uff0c\u6240\u4ee5\u4f7f\u7528\u65f6\u8bf7\u81ea\u884cdecode\u6210unicode\u3002\n\n~~~~{python}\nfrom snownlp import SnowNLP\n\ns = SnowNLP(u'\u8fd9\u4e2a\u4e1c\u897f\u771f\u5fc3\u5f88\u8d5e')\n\ns.words         # [u'\u8fd9\u4e2a', u'\u4e1c\u897f', u'\u771f\u5fc3',\n                #  u'\u5f88', u'\u8d5e']\n\ns.tags          # [(u'\u8fd9\u4e2a', u'r'), (u'\u4e1c\u897f', u'n'),\n                #  (u'\u771f\u5fc3', u'd'), (u'\u5f88', u'd'),\n                #  (u'\u8d5e', u'Vg')]\n\ns.sentiments    # 0.9769663402895832 positive\u7684\u6982\u7387\n\ns.pinyin        # [u'zhe', u'ge', u'dong', u'xi',\n                #  u'zhen', u'xin', u'hen', u'zan']\n\ns = SnowNLP(u'\u300c\u7e41\u9ad4\u5b57\u300d\u300c\u7e41\u9ad4\u4e2d\u6587\u300d\u7684\u53eb\u6cd5\u5728\u81fa\u7063\u4ea6\u5f88\u5e38\u898b\u3002')\n\ns.han           # u'\u300c\u7e41\u4f53\u5b57\u300d\u300c\u7e41\u4f53\u4e2d\u6587\u300d\u7684\u53eb\u6cd5\n                # \u5728\u53f0\u6e7e\u4ea6\u5f88\u5e38\u89c1\u3002'\n\ntext = u'''\n\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u4e0e\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u5411\u3002\n\u5b83\u7814\u7a76\u80fd\u5b9e\u73b0\u4eba\u4e0e\u8ba1\u7b97\u673a\u4e4b\u95f4\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u6709\u6548\u901a\u4fe1\u7684\u5404\u79cd\u7406\u8bba\u548c\u65b9\u6cd5\u3002\n\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u4e00\u95e8\u878d\u8bed\u8a00\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u6570\u5b66\u4e8e\u4e00\u4f53\u7684\u79d1\u5b66\u3002\n\u56e0\u6b64\uff0c\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u5c06\u6d89\u53ca\u81ea\u7136\u8bed\u8a00\uff0c\u5373\u4eba\u4eec\u65e5\u5e38\u4f7f\u7528\u7684\u8bed\u8a00\uff0c\n\u6240\u4ee5\u5b83\u4e0e\u8bed\u8a00\u5b66\u7684\u7814\u7a76\u6709\u7740\u5bc6\u5207\u7684\u8054\u7cfb\uff0c\u4f46\u53c8\u6709\u91cd\u8981\u7684\u533a\u522b\u3002\n\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e76\u4e0d\u662f\u4e00\u822c\u5730\u7814\u7a76\u81ea\u7136\u8bed\u8a00\uff0c\n\u800c\u5728\u4e8e\u7814\u5236\u80fd\u6709\u6548\u5730\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\uff0c\n\u7279\u522b\u662f\u5176\u4e2d\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002\u56e0\u800c\u5b83\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u90e8\u5206\u3002\n'''\n\ns = SnowNLP(text)\n\ns.keywords(3)\t# [u'\u8bed\u8a00', u'\u81ea\u7136', u'\u8ba1\u7b97\u673a']\n\ns.summary(3)\t# [u'\u56e0\u800c\u5b83\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u90e8\u5206',\n                #  u'\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u4e00\u95e8\u878d\u8bed\u8a00\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\n\t\t\t\t#\t \u6570\u5b66\u4e8e\u4e00\u4f53\u7684\u79d1\u5b66',\n\t\t\t\t#  u'\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u4e0e\u4eba\u5de5\u667a\u80fd\n\t\t\t\t#\t \u9886\u57df\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u5411']\ns.sentences\n\ns = SnowNLP([[u'\u8fd9\u7bc7', u'\u6587\u7ae0'],\n             [u'\u90a3\u7bc7', u'\u8bba\u6587'],\n             [u'\u8fd9\u4e2a']])\ns.tf\ns.idf\ns.sim([u'\u6587\u7ae0'])# [0.3756070762985226, 0, 0]\n~~~~\n\n## Features\n\n* \u4e2d\u6587\u5206\u8bcd\uff08[Character-Based Generative Model](http://aclweb.org/anthology//Y/Y09/Y09-2047.pdf)\uff09\n* \u8bcd\u6027\u6807\u6ce8\uff08[TnT](http://aclweb.org/anthology//A/A00/A00-1031.pdf) 3-gram \u9690\u9a6c\uff09\n* \u60c5\u611f\u5206\u6790\uff08\u73b0\u5728\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u662f\u4e70\u5356\u4e1c\u897f\u65f6\u7684\u8bc4\u4ef7\uff0c\u6240\u4ee5\u5bf9\u5176\u4ed6\u7684\u4e00\u4e9b\u53ef\u80fd\u6548\u679c\u4e0d\u662f\u5f88\u597d\uff0c\u5f85\u89e3\u51b3\uff09\n* \u6587\u672c\u5206\u7c7b\uff08Naive Bayes\uff09\n* \u8f6c\u6362\u6210\u62fc\u97f3\uff08Trie\u6811\u5b9e\u73b0\u7684\u6700\u5927\u5339\u914d\uff09\n* \u7e41\u4f53\u8f6c\u7b80\u4f53\uff08Trie\u6811\u5b9e\u73b0\u7684\u6700\u5927\u5339\u914d\uff09\n* \u63d0\u53d6\u6587\u672c\u5173\u952e\u8bcd\uff08[TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\u7b97\u6cd5\uff09\n* \u63d0\u53d6\u6587\u672c\u6458\u8981\uff08[TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\u7b97\u6cd5\uff09\n* tf\uff0cidf\n* Tokenization\uff08\u5206\u5272\u6210\u53e5\u5b50\uff09\n* \u6587\u672c\u76f8\u4f3c\uff08[BM25](http://en.wikipedia.org/wiki/Okapi_BM25)\uff09\n* \u652f\u6301python3\uff08\u611f\u8c22[erning](https://github.com/erning)\uff09\n\n## Get It now\n\n~~~~\n$ pip install snownlp\n~~~~\n\n## \u5173\u4e8e\u8bad\u7ec3\n\n\u73b0\u5728\u63d0\u4f9b\u8bad\u7ec3\u7684\u5305\u62ec\u5206\u8bcd\uff0c\u8bcd\u6027\u6807\u6ce8\uff0c\u60c5\u611f\u5206\u6790\uff0c\u800c\u4e14\u90fd\u63d0\u4f9b\u4e86\u6211\u7528\u6765\u8bad\u7ec3\u7684\u539f\u59cb\u6587\u4ef6\n\u4ee5\u5206\u8bcd\u4e3a\u4f8b\n\u5206\u8bcd\u5728`snownlp/seg`\u76ee\u5f55\u4e0b\n~~~~{python}\nfrom snownlp import seg\nseg.train('data.txt')\nseg.save('seg.marshal')\n# from snownlp import tag\n# tag.train('199801.txt')\n# tag.save('tag.marshal')\n# from snownlp import sentiment\n# sentiment.train('neg.txt', 'pos.txt')\n# sentiment.save('sentiment.marshal')\n~~~~\n\u8fd9\u6837\u8bad\u7ec3\u597d\u7684\u6587\u4ef6\u5c31\u5b58\u50a8\u4e3a`seg.marshal`\u4e86\uff0c\u4e4b\u540e\u4fee\u6539`snownlp/seg/__init__.py`\u91cc\u7684`data_path`\u6307\u5411\u521a\u8bad\u7ec3\u597d\u7684\u6587\u4ef6\u5373\u53ef\n\n## License\n\nMIT licensed.\n"
 },
 {
  "repo": "MechanicalSoup/MechanicalSoup",
  "language": "Python",
  "readme_contents": ".. image:: /assets/mechanical-soup-logo.png\n   :alt: MechanicalSoup. A Python library for automating website interaction.\n\nHome page\n---------\n\nhttps://mechanicalsoup.readthedocs.io/\n\nOverview\n--------\n\nA Python library for automating interaction with websites.\nMechanicalSoup automatically stores and sends cookies, follows\nredirects, and can follow links and submit forms. It doesn't do\nJavaScript.\n\nMechanicalSoup was created by `M Hickford\n<https://github.com/hickford/>`__, who was a fond user of the\n`Mechanize <https://github.com/jjlee/mechanize>`__ library.\nUnfortunately, Mechanize was `incompatible with Python 3 until 2019\n<https://github.com/python-mechanize/mechanize/issues/9>`__ and its development\nstalled for several years. MechanicalSoup provides a similar API, built on Python\ngiants `Requests <http://docs.python-requests.org/en/latest/>`__ (for\nHTTP sessions) and `BeautifulSoup\n<https://www.crummy.com/software/BeautifulSoup/>`__ (for document\nnavigation). Since 2017 it is a project actively maintained by a small\nteam including `@hemberger <https://github.com/hemberger>`__ and `@moy\n<https://github.com/moy/>`__.\n\n|Gitter Chat|\n\nInstallation\n------------\n\n|Latest Version| |Supported Versions|\n\nPyPy3 is also supported (and tested against).\n\nDownload and install the latest released version from `PyPI <https://pypi.python.org/pypi/MechanicalSoup/>`__::\n\n  pip install MechanicalSoup\n\nDownload and install the development version from `GitHub <https://github.com/MechanicalSoup/MechanicalSoup>`__::\n\n  pip install git+https://github.com/MechanicalSoup/MechanicalSoup\n\nInstalling from source (installs the version in the current working directory)::\n\n  python setup.py install\n\n(In all cases, add ``--user`` to the ``install`` command to\ninstall in the current user's home directory.)\n\nDocumentation\n-------------\n\nThe full documentation is available on\nhttps://mechanicalsoup.readthedocs.io/. You may want to jump directly to\nthe `automatically generated API\ndocumentation <https://mechanicalsoup.readthedocs.io/en/stable/mechanicalsoup.html>`__.\n\nExample\n-------\n\nFrom `<examples/expl_qwant.py>`__, code to get the results from\na Qwant search:\n\n.. code:: python\n\n    \"\"\"Example usage of MechanicalSoup to get the results from the Qwant\n    search engine.\n    \"\"\"\n\n    import re\n    import mechanicalsoup\n    import html\n    import urllib.parse\n\n    # Connect to Qwant\n    browser = mechanicalsoup.StatefulBrowser(user_agent='MechanicalSoup')\n    browser.open(\"https://lite.qwant.com/\")\n\n    # Fill-in the search form\n    browser.select_form('#search-form')\n    browser[\"q\"] = \"MechanicalSoup\"\n    browser.submit_selected()\n\n    # Display the results\n    for link in browser.page.select('.result a'):\n        # Qwant shows redirection links, not the actual URL, so extract\n        # the actual URL from the redirect link:\n        href = link.attrs['href']\n        m = re.match(r\"^/redirect/[^/]*/(.*)$\", href)\n        if m:\n            href = urllib.parse.unquote(m.group(1))\n        print(link.text, '->', href)\n\nMore examples are available in `<examples/>`__.\n\nFor an example with a more complex form (checkboxes, radio buttons and\ntextareas), read `<tests/test_browser.py>`__\nand `<tests/test_form.py>`__.\n\nDevelopment\n-----------\n\n|Build Status| |Coverage Status|\n|Requirements Status| |Documentation Status|\n|CII Best Practices|\n|LGTM Alerts|\n|LGTM Grade|\n\nInstructions for building, testing and contributing to MechanicalSoup:\nsee `<CONTRIBUTING.rst>`__.\n\nCommon problems\n---------------\n\nRead the `FAQ\n<https://mechanicalsoup.readthedocs.io/en/stable/faq.html>`__.\n\n.. |Latest Version| image:: https://img.shields.io/pypi/v/MechanicalSoup.svg\n   :target: https://pypi.python.org/pypi/MechanicalSoup/\n.. |Supported Versions| image:: https://img.shields.io/pypi/pyversions/mechanicalsoup.svg\n   :target: https://pypi.python.org/pypi/MechanicalSoup/\n.. |Build Status| image:: https://github.com/MechanicalSoup/MechanicalSoup/actions/workflows/python-package.yml/badge.svg?branch=main\n   :target: https://github.com/MechanicalSoup/MechanicalSoup/actions/workflows/python-package.yml?query=branch%3Amain\n.. |Coverage Status| image:: https://codecov.io/gh/MechanicalSoup/MechanicalSoup/branch/main/graph/badge.svg\n   :target: https://codecov.io/gh/MechanicalSoup/MechanicalSoup\n.. |Requirements Status| image:: https://requires.io/github/MechanicalSoup/MechanicalSoup/requirements.svg?branch=main\n   :target: https://requires.io/github/MechanicalSoup/MechanicalSoup/requirements/?branch=main\n.. |Documentation Status| image:: https://readthedocs.org/projects/mechanicalsoup/badge/?version=latest\n   :target: https://mechanicalsoup.readthedocs.io/en/latest/?badge=latest\n.. |CII Best Practices| image:: https://bestpractices.coreinfrastructure.org/projects/1334/badge\n   :target: https://bestpractices.coreinfrastructure.org/projects/1334\n.. |Gitter Chat| image:: https://badges.gitter.im/MechanicalSoup/MechanicalSoup.svg\n   :target: https://gitter.im/MechanicalSoup/Lobby\n.. |LGTM Alerts| image:: https://img.shields.io/lgtm/alerts/g/MechanicalSoup/MechanicalSoup.svg\n   :target: https://lgtm.com/projects/g/MechanicalSoup/MechanicalSoup/\n.. |LGTM Grade| image:: https://img.shields.io/lgtm/grade/python/g/MechanicalSoup/MechanicalSoup.svg\n   :target: https://lgtm.com/projects/g/MechanicalSoup/MechanicalSoup/\n"
 },
 {
  "repo": "agermanidis/autosub",
  "language": "Python",
  "readme_contents": "# Autosub <a href=\"https://pypi.python.org/pypi/autosub\"><img src=\"https://img.shields.io/pypi/v/autosub.svg\"></img></a>\n  \n### Auto-generated subtitles for any video\n\nAutosub is a utility for automatic speech recognition and subtitle generation. It takes a video or an audio file as input, performs voice activity detection to find speech regions, makes parallel requests to Google Web Speech API to generate transcriptions for those regions, (optionally) translates them to a different language, and finally saves the resulting subtitles to disk. It supports a variety of input and output languages (to see which, run the utility with the argument `--list-languages`) and can currently produce subtitles in either the [SRT format](https://en.wikipedia.org/wiki/SubRip) or simple [JSON](https://en.wikipedia.org/wiki/JSON).\n\n### Installation\n\n1. Install [ffmpeg](https://www.ffmpeg.org/).\n2. Run `pip install autosub`.\n\n### Usage\n\n```\n$ autosub -h\nusage: autosub [-h] [-C CONCURRENCY] [-o OUTPUT] [-F FORMAT] [-S SRC_LANGUAGE]\n               [-D DST_LANGUAGE] [-K API_KEY] [--list-formats]\n               [--list-languages]\n               [source_path]\n\npositional arguments:\n  source_path           Path to the video or audio file to subtitle\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -C CONCURRENCY, --concurrency CONCURRENCY\n                        Number of concurrent API requests to make\n  -o OUTPUT, --output OUTPUT\n                        Output path for subtitles (by default, subtitles are\n                        saved in the same directory and name as the source\n                        path)\n  -F FORMAT, --format FORMAT\n                        Destination subtitle format\n  -S SRC_LANGUAGE, --src-language SRC_LANGUAGE\n                        Language spoken in source file\n  -D DST_LANGUAGE, --dst-language DST_LANGUAGE\n                        Desired language for the subtitles\n  -K API_KEY, --api-key API_KEY\n                        The Google Translate API key to be used. (Required for\n                        subtitle translation)\n  --list-formats        List all available subtitle formats\n  --list-languages      List all available source/destination languages\n```\n\n### License\n\nMIT\n"
 },
 {
  "repo": "PokemonGoF/PokemonGo-Bot",
  "language": "Python",
  "readme_contents": "# PokemonGo-Bot\n[PokemonGo-Bot](https://github.com/PokemonGoF/PokemonGo-Bot) is a project created by the [PokemonGoF](https://github.com/PokemonGoF) team. Since no public API available for now, a patch to use HASH-Server was applied. PokemonGoF is not part of HASH-Server dev team and has no connection with it.\n\n[![Bot Map and Terminal](https://i.imgur.com/EWCEDxe.jpg)](https://i.imgur.com/Mrg9aRw.jpg)\n\n# Donation\n\nBitcoin: 1PJMCx9NNQRasQYaa4MMff9yyNFffhHgLu\n\n## Table of Contents\n- [Installation](https://github.com/PokemonGoF/PokemonGo-Bot/blob/dev/docs/installation.md)\n- [Documentation](https://github.com/PokemonGoF/PokemonGo-Bot/blob/dev/docs/)\n- [Support](#support)\n - [Help](#configuration-issueshelp)\n - [Bugs](#bugs--issues)\n - [Feature Requests](#feature-requests)\n - [Pull Requests](#pull-requests)\n- [Features](#features)\n- [Credits](#credits)\n\nThe project is currently setup in two main branches:\n- `dev` also known as `beta` - This is where the latest features are, but you may also experience some issues with stability/crashes.\n- `master` also known as `stable` - The bot 'should' be stable on this branch, and is generally well tested.\n\n## Discord\n  - [Click here to join discord server](https://discord.gg/n3g5puF)\n\n### [Bugs / Issues](https://github.com/PokemonGoF/PokemonGo-Bot/issues?q=is%3Aissue+sort%3Aupdated-desc)\nIf you discover a bug in the bot, please [search our issue tracker](https://github.com/PokemonGoF/PokemonGo-Bot/issues?q=is%3Aissue+sort%3Aupdated-desc) first. If it hasn't been reported, please [create a new issue](https://github.com/PokemonGoF/PokemonGo-Bot/issues/new) and ensure you follow the template guide so that our team can assist you as quickly as possible.\n\n### [Feature Requests](https://github.com/PokemonGoF/PokemonGo-Bot/labels/Feature%20Request)\nIf you have a great idea to improve the bot, please [search our feature tracker](https://github.com/PokemonGoF/PokemonGo-Bot/labels/Feature%20Request) first to ensure someone else hasn't already come up with the same great idea.  If it hasn't been requested, please [create a new request](https://github.com/PokemonGoF/PokemonGo-Bot/issues/new) and ensure you follow the template guide so that it doesnt get lost with the bug reports.\nWhile you're there vote on other feature requests to let the devs know what is most important to you.\n\n### [Pull Requests](https://github.com/PokemonGoF/PokemonGo-Bot/pulls)\nIf you'd like to make your own changes, make sure you follow the pull request template, and ensure your PR is made against the 'dev' branch.\n\nIf this is your first time making a PR or aren't sure of the standard practice of making a PR, here are some articles to get you started.\n - [GitHub Pull Request Tutorial](https://www.thinkful.com/learn/github-pull-request-tutorial/)\n - [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)\n - [A great example from one of our own contributors](https://github.com/PokemonGoF/PokemonGo-Bot/pull/3912)\n\n## Features\n- [x] Based on Python for botting on any operating system - Windows, macOS and Linux\n- [x] Multi-bot supported \n- [x] Able to edit bot if certain level has reached\n- [x] Allow custom hash service provider, if any\n- [x] GPS Location configuration\n- [x] Search & spin Pokestops / Gyms\n- [x] Diverse options for humanlike behavior from movement to overall game play\n- [x] Ability to add multiple coordinates to select between your favorite botting locations\n- [x] Support self defined path / route\n- [x] Advanced catch, evolve and transfer confuration using our PokemonOptimizer settings\n- [x] Determine which pokeball to use\n- [x] Rules to determine the use of Razz and Pinap Berries\n- [x] Exchange, evolve and catch Pokemon base on pre-configured rules\n- [x] Transfer Pokemon in bulk\n- [x] Auto switch mode (Inventory Checks - switches between catch/farming items)\n- [x] Limit the step to farm specific area for pokestops\n- [x] Limit Spin Pokestops/Catch Pokemons per day\n- [x] IV Functionality filter\n- [x] Mass rename of Pokemon with comprehenive rules\n- [x] Adjust delay between Pokemon capture & Transfer as per configuration\n- [x] Telegram integration - reporting of bot's events\n- [x] Snipe Pokemon within a radius of 30Km, either through telegram command or local map (Example Rocket Map)\n- [x] Issue command through telegream - Activate Lucky egg / Incense, Snipping\n- [x] Support dropping of Lure Module\n- [x] Incubate eggs & Buddy walk\n- [x] Bot is able to identify pokemon in their family\n- [x] Set family ID as VIP and prioritize bot to catch it!\n- [x] Transfer red slashed pokemons\n- [x] Set shiny pokemons as VIP\n- [x] Deploy a pokemon in Gym if there are slots available\n- [x] Docker support\n- [x] Auto heal Pokemons\n- [x] Displaying of Hash expiration date and RPM information\n\n\n## Credits\n- [tejado](https://github.com/tejado) many thanks for the API\n- [pogodevorg](https://github.com/pogodevorg/pgoapi) Without keyphact's coordination, this would not gonna happan again.\n- [Mila432](https://github.com/Mila432/Pokemon_Go_API) for the login secrets\n- [elliottcarlson](https://github.com/elliottcarlson) for the Google Auth PR\n- [AeonLucid](https://github.com/AeonLucid/POGOProtos) for improved protos\n- [AHAAAAAAA](https://github.com/AHAAAAAAA/PokemonGo-Map) for parts of the s2sphere stuff\n- [Breeze ro](https://github.com/BreezeRo) for some of the MQTT/Map stuff\n\n## Contributors\n * eggins [first pull request]\n * crack00r\n * ethervoid\n * Bashin\n * tstumm\n * AdaptCharm\n * Reaver01\n * rarshonsky\n * earthchie\n * haykuro\n * 05-032\n * sinistance\n * CapCap\n * YvesHenri\n * mzupan\n * gnekic(GeXx)\n * Shoh\n * JSchwerberg\n * luizperes\n * brantje\n * VirtualSatai\n * dmateusp\n * jtdroste\n * msoedov\n * Grace\n * Calcyfer\n * asaf400\n * guyz\n * DavidK1m\n * budi-khoirudin\n * riberod07\n * th3w4y\n * Leaklessgfy\n * steffwiz\n * pulgalipe\n * BartKoppelmans\n * phil9l\n * VictorChen\n * AlvaroGzP\n * fierysolid\n * surfaace\n * surceis\n * SpaceWhale\n * klingan\n * reddivision\n * DayBr3ak\n * kbinani\n * mhdasding\n * MFizz\n * NamPNQ\n * z4ppy.bbc\n * matheussampaio\n * Abraxas000\n * lucasfevi\n * pokepal\n * Moonlight-Angel\n * mjmadsen\n * nikofil\n * bigkraig\n * nikhil-pandey\n * thebigjc\n * JaapMoolenaar\n * eevee-github\n * g0vanish\n * cmezh\n * Nivong\n * kestel\n * simonsmh\n * joaodragao\n * extink\n * Quantra\n * pmquan\n * net8q\n * SyncX\n * umbreon222\n * DeXtroTip\n * rawgni\n * Breeze Ro\n * bruno-kenji\n * Gobberwart\n * javajohnHub\n * kolinkorr839\n * lepeli\n * davidakachaos\n * MerlionRock\n * walaoaaa1234\n * pogarek\n * goedzo\n * solderzzc aka BIG BOSS\n * nbq aka Holoshed\n \n\n## Disclaimer\n\u00a92016 Niantic, Inc. \u00a92016 Pok\u00e9mon. \u00a91995\u20132016 Nintendo / Creatures Inc. / GAME FREAK inc. \u00a9 2016 Pok\u00e9mon/Nintendo Pok\u00e9mon and Pok\u00e9mon character names are trademarks of Nintendo. The Google Maps Pin is a trademark of Google Inc. and the trade dress in the product design is a trademark of Google Inc. under license to The Pok\u00e9mon Company. Other trademarks are the property of their respective owners.\n[Privacy Policy](http://www.pokemon.com/us/privacy-policy/)\n\n[PokemonGo-Bot](https://github.com/PokemonGoF/PokemonGo-Bot) is intended for academic purposes and should not be used to play the game *PokemonGo* as it violates the TOS and is unfair to the community. Use the bot **at your own risk**.\n\n[PokemonGoF](https://github.com/PokemonGoF) does not support the use of 3rd party apps or apps that violate the TOS.\n\n\n[![Analytics](https://ga-beacon.appspot.com/UA-81468120-1/welcome-page-master)](https://github.com/igrigorik/ga-beacon)\n"
 },
 {
  "repo": "ab77/netflix-proxy",
  "language": "Python",
  "readme_contents": "[![Build Status](https://travis-ci.org/ab77/netflix-proxy.svg?branch=master)](https://travis-ci.org/ab77/netflix-proxy) [![Docker Pulls](https://img.shields.io/docker/pulls/ab77/sniproxy.svg?maxAge=2592000)](https://hub.docker.com/r/ab77/sniproxy/) [![Docker Stars](https://img.shields.io/docker/stars/ab77/sniproxy.svg?maxAge=2592000)](https://hub.docker.com/r/ab77/sniproxy/)\n\n> `TL;DR`\n\nfind a (recent)[n19](https://github.com/ab77/netflix-proxy#footnotes) Debian or Ubuntu box with root on a clean public IP and run:\n\n    apt-get update\\\n\t  && apt-get -y install vim dnsutils curl sudo\\\n\t  && curl -fsSL https://get.docker.com/ | sh || apt-get -y install docker.io\\\n\t  && mkdir -p ~/netflix-proxy\\\n\t  && cd ~/netflix-proxy\\\n\t  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n\t  && ./build.sh\n\nSee the [**Wiki**](https://github.com/ab77/netflix-proxy/wiki) page(s) for some common troubleshooting ideas.\n\n... or subscribe to [Unzoner](http://unzoner.com) VPN service to un-block:\n\n<a href=\"https://dashboard.unzoner.com/sub\"><img align=\"left\" src=\"https://api.unzoner.com/api/v1.0/countries/available/flags.png\"></a><br><br>\n\n# about\n`netflix-proxy` is a smart DNS proxy to stream `Netflix`, `Hulu`[[n2]](#footnotes), `HBO Now` and others out of region. It is deployed using Docker containers and uses `dnsmasq`[[n18]](#footnotes) and `sniproxy`[[n1]](#footnotes) to provide SmartDNS services. It works for some blocked sites, such as [PornHub](http://www.pornhub.com/) and [YouTube](https://en.wikipedia.org/wiki/Blocking_of_YouTube_videos_in_Germany). [Subscribe](http://eepurl.com/cb4rUv) to the mailing list and be notified of new features, updates, etc.\n\n# supported services\nThe following are supported out of the box, however adding additional services is trivial and is done by updating `dnsmasq.conf` file and running `docker restart dnsmasq`:\n* Netflix\n* Hulu[[n2]](#footnotes)\n* HBO Now\n* Amazon Instant Video\n* Crackle\n* Pandora\n* Vudu\n* blinkbox\n* BBC iPlayer[[n5]](#footnotes)\n* NBC Sports and potentially many [more](https://github.com/ab77/netflix-proxy/blob/master/proxy-domains.txt)\n\n# license\nThis project is **free**, covered by the [MIT License](https://github.com/ab77/netflix-proxy/blob/master/LICENSE.md). It is provided without any warranty and can be used for any purpose, including private and commercial. However, if you are planning to use it for commercial purposes (i.e make money off it), please do not expect free support, as it would be unfair. A commercial support model can always be negotiated, if required. Please [contact](https://www.upwork.com/freelancers/~016da2a2dc195af5ec) me if this is something that interests you.\n\n# instructions\nThe following paragraphs show how to get this solution up and running with a few different Cloud providers I've tried so far. If you prefer a video tutorial, [here](https://www.youtube.com/watch?v=8DrNgnq_cdM) is one prapared by one of the users. Note, OpenVZ **won't work**[[n15]](#footnotes), make sure to get a proper virtual machine using KVM or Xen.\n\n[![](https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%201.svg)](https://www.digitalocean.com/?refcode=937b01397c94&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge)\n\n(Netflix is **blocked**[[n16]](#footnotes)) The following is based on a standard Ubuntu Docker image provided by `DigitalOcean`, but should in theory work on any Linux distribution **with** Docker pre-installed.\n\n1. Head over to [Digital Ocean](https://m.do.co/c/937b01397c94) to get **$10 USD credit**\n2. Create a Droplet in a geographic location of interest using the latest Docker image (find in under `One-click Apps` tab).\n3. SSH to your server and run:\n\n```\nmkdir -p ~/netflix-proxy\\\n  && cd ~/netflix-proxy\\\n  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n  && ./build.sh\n```\n\n4. Make sure to **record the URL and credentials** for the `netflix-proxy` admin site.\n5. Set your DNS server to the IP given at the end of the script, then go to [this](http://ifconfig.co/) site to make sure the same IP is displayed.\n6. Finally, enjoy `Netflix` and others out of region.\n7. Enjoy or try `#netflix-proxy` on [freenode](https://webchat.freenode.net/?channels=netflix-proxy) for help.\n\n### authorising additional IPs\nIf you want to share your system with friends and family, you can authorise their home IP address(s) using the `netflix-proxy` admin site, located at `http://<ipaddr>:8080/`, where `ipaddr` is the public IP address of your VPS. Login using `admin` account with the password you recorded during the build. If you've forgotten your admin credentials, [reset](https://github.com/ab77/netflix-proxy/wiki/Change-admin-password).\n\n[![](https://raw.githubusercontent.com/ab77/netflix-proxy/master/static/admin.png)](https://raw.githubusercontent.com/ab77/netflix-proxy/master/static/admin.png)\n\nThe `admin` account does not restrict the entry or removal of IPs. If you want to restrict the entry of IPs to the current client IP using an automatically populated drop-down, create a standard user account using the `account-creator.sh` script located in the `auth` directory, which will prompt you for the input and create the user account.\n\n#### dynamic IPs\nYou can also use the `netflix-proxy` admin site to update your IP address, should your ISP assign you a new one (e.g. via DHCP). If your IP address does change, all HTTP/HTTPS requests will automatically be redirected to the admin site on port `8080`. All DNS requests will be redirected to `dnsmasq` instance running on port `5353`. You will most likely need to purge your browser and system DNS caches after this. On Windows, run `ipconfig /flushdns`. On OS X, run:\n\n```\nsudo killall -HUP mDNSResponder\\\n && sudo dscacheutil -flushcache`\n```\n\nThen restart the browser (e.g `chrome://restart`) and/or reboot the relevant devices. This mechanism should work on browsers, but will most likely cause errors on other devices, such as Apple TVs and smart TVs. If you Internet stops working all of a sudden, try loading a browser and going to `netflix.com`.\n\n#### scripted authorization of IPs\n* to automatically authorise client IP using a script (where `ipaddr` is the public IP address of your VPS), substitute admin credentials and run:\n\n```\ncurl -L http://<ipaddr>:8080/autoadd?username=<admin-username>&password=<admin-password>\n```\n\n* to manually authorise a specific IP, substitute admin credentials and run:\n\n```\ncurl -L http://<ipaddr>:8080/autoadd?ip=<your-public-ipaddr>&username=<admin-username>&password=<admin-password>\n```\n\n#### automatic IP authorization\n**WARNING**: do not do enable this unless you know what you are doing.\n\nTo enable automatic authorization of every IP that hits your proxy, set `AUTO_AUTH = True` in `auth/settings.py` and run `service netflix-proxy-admin restart`. This setting will effectively authorize any IP hitting your proxy IP with a web browser for the first time, including bots, hackers, spammers, etc. Upon successful authorization, the browser will be redirected to [Google](http://google.com/).\n\nThe DNS service is configured with recursion turned on by [default](https://github.com/ab77/netflix-proxy#security), so after a successful authorization, anyone can use your VPS in DNS amplification attacks, which will probably put you in breach of contract with the VPS provider. You have been **WARNED**.\n\n### security\nThe build script automatically configures the system with **DNS recursion turned on**. This has security implications, since it potentially opens your DNS server to a DNS amplification attack, a kind of a [DDoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack). This should not be a concern however, as long as the `iptables` firewall rules configured automatically by the build script for you remain in place. However if you ever decide to turn the firewall off, please be aware of this.\n\n### command line options\nThe following command line options can be optionaly passed to `build.sh` for additional control:\n\n```\nUsage: ./build.sh [-b 0|1] [-c <ip>]\n        -b      grab docker images from repository (0) or build locally (1) (default: 0)\n        -c      specify client-ip instead of being taken from ssh_connection\n```\n\n### updates\nIn order to update your existing database schema, please run the provided `update.sh` script. Alternatively you can run the schema updates manually (e.g. if you skipped a version).\n\n## other cloud providers\n\n### locale issues\n\nThe build script has been designed to work on Ubuntu and Debian. It will most likely fail on all other distributions. Some pre-requisites require the locale to be set correctly and some provider OS images need extra help. If you get `locale` issues reported by `Python` and/or `pip` during the build, try running the following first:\n\n```\nexport LANGUAGE=en_US.UTF-8\\\n  && export LANG=en_US.UTF-8\\\n  && export LC_ALL=en_US.UTF-8\\\n  && export LC_CTYPE=\"en_US.UTF-8\"\\\n  && locale-gen en_US.UTF-8\\\n  && sudo apt-get -y install language-pack-en-base\\\n  && sudo dpkg-reconfigure locales\n```\n\n[![](https://raw.githubusercontent.com/ab77/netflix-proxy/master/static/vultr.png)](http://www.vultr.com/?ref=6871746)\n\n(Netflix is **blocked**[[n16]](#footnotes)) The following is based on a Debian image provided by `Vultr`, but should in theory work on any Debian distribution.\n\n1. For a limited time, head over to [Vultr](http://www.vultr.com/?ref=6962933-3B) to create and account and get **$20 USD credit**.\n2. Create a compute instance in a geographic location of interest using Debian or Ubuntu image.\n3. SSH to your server and run:\n\n```\napt-get update\\\n  && apt-get -y install vim dnsutils curl sudo\\\n  && curl -fsSL https://get.docker.com/ | sh || apt-get -y install docker.io\\\n  && mkdir -p ~/netflix-proxy\\\n  && cd ~/netflix-proxy\\\n  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n  && ./build.sh\n```\n\n4. Make sure to **record the credentials** for the `netflix-proxy` admin site.\n5. Set your DNS server to the IP given at the end of the script, then go to [this](http://ifconfig.co/) site to make sure the same IP is displayed.\n6. Finally, enjoy `Netflix` and others out of region.\n7. Enjoy or try `#netflix-proxy` on [freenode](https://webchat.freenode.net/?channels=netflix-proxy) for help.\n\n[![](https://raw.githubusercontent.com/ab77/netflix-proxy/master/static/kamatera.png)](https://www.kamatera.com/express/compute/?tcampaign=antonbelodedenko&HT=17)\n\n(Netflix is **blocked**[[n16]](#footnotes)) The following is based on a standard Ubuntu image provided by `Kamatera`.\n\n1. Head over to [Kamatera](https://www.kamatera.com/express/compute/?tcampaign=antonbelodedenko&HT=17) to start your **30 Day Free Trial**.\n2. Create a new server in a geographic location of interest using Ubuntu or Debian image.\n3. SSH to your server and run:\n\n```\napt-get update\\\n  && apt-get -y install vim dnsutils curl sudo\\\n  && curl -fsSL https://get.docker.com/ | sh || apt-get -y install docker.io\\\n  && mkdir -p ~/netflix-proxy\\\n  && cd ~/netflix-proxy\\\n  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n  | tar x --strip-components=1\\\n  && ./build.sh\n```\n\n4. Make sure to **record the URL and credentials** for the `netflix-proxy` admin site.\n5. Set your DNS server to the IP given at the end of the script, then go to [this](http://ifconfig.co/) site to make sure the same IP is displayed.\n6. Finally, enjoy `Netflix` and others out of region.\n7. Enjoy or try `#netflix-proxy` on [freenode](https://webchat.freenode.net/?channels=netflix-proxy) for help.\n\n[![](http://www.ramnode.com/images/banners/affbannerdarknewlogo.png)](https://clientarea.ramnode.com/aff.php?aff=3079)\n\n(Netflix is blocked[[n16]](#footnotes)) The following is based on a Debian or Ubuntu images provided by `RamNode`.\n\n1. Head over to [RamNode](https://clientarea.ramnode.com/aff.php?aff=3079) to create an account and buy a **KVM** VPS in a geographic location of interest (OpenVZ won't work).\n2. Log into the `VPS Control Panel` and (re)install the OS using Ubuntu or Debian image.\n3. SSH to your server and run:\n\n```\napt-get update\\\n  && apt-get -y install vim dnsutils curl sudo\\\n  && curl -fsSL https://get.docker.com/ | sh || apt-get -y install docker.io\\\n  && mkdir -p ~/netflix-proxy\\\n  && cd ~/netflix-proxy\\\n  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n  && ./build.sh\n```\n\n4. Make sure to **record the credentials** for the `netflix-proxy` admin site.\n5. Set your DNS server to the IP given at the end of the script, then go to [this](http://ifconfig.co/) site to make sure the same IP is displayed.\n6. Finally, enjoy `Netflix` and others out of region.\n7. Enjoy or try `#netflix-proxy` on [freenode](https://webchat.freenode.net/?channels=netflix-proxy) for help.\n\n[![](https://www.linode.com/media/images/logos/standard/light/linode-logo_standard_light_small.png)](https://www.linode.com/?r=ceb35af7bad520f1e2f4232b3b4d49136dcfe9d9)\n\n(Netflix is **blocked**[[n16]](#footnotes)) The following is based on a standard Ubuntu image provided by `Linode`, but should work on any Linux distribution **without** Docker installed.\n\n1. Head over to [Linode](https://www.linode.com/?r=ceb35af7bad520f1e2f4232b3b4d49136dcfe9d9) and sign-up for an account.\n2. Create a new `Linode` in a geographic location of interest and deploy an Ubuntu image into it.\n3. SSH to your server and run:\n\n```\napt-get update\\\n  && apt-get -y install vim dnsutils curl sudo\\\n  && curl -fsSL https://get.docker.com/ | sh || apt-get -y install docker.io\\\n  && mkdir -p ~/netflix-proxy\\\n  && cd ~/netflix-proxy\\\n  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n  && ./build.sh\n```\n\n4. Make sure to **record the credentials** for the `netflix-proxy` admin site.\n5. Set your DNS server to the IP given at the end of the script, then go to [this](http://ifconfig.co/) site to make sure the same IP is displayed.\n6. Finally, enjoy `Netflix` and others out of region.\n7. Enjoy or try `#netflix-proxy` on [freenode](https://webchat.freenode.net/?channels=netflix-proxy) for help.\n\n[![](https://raw.githubusercontent.com/ab77/netflix-proxy/master/static/dreamhost.png)](http://www.dreamhost.com/r.cgi?2124700)\n\n**(untested)** The following is based on a standard Ubuntu image provided by `DreamHost`, but should work on any Linux distribution **without** Docker installed and running under **non-root** user (e.g. `Amazon Web Services`[[n13]](#footnotes)).\n\n1. Head over to [DreamHost](http://www.dreamhost.com/r.cgi?2124700) and sign-up for an account.\n2. Find the `DreamCompute` or `Public Cloud Computing` section and launch an Ubuntu instance in a geographic location of interest.\n3. Make sure to add an additional firewall rule to allow DNS: `Ingress - IPv4 - UDP - 53 - 0.0.0.0/0 (CIDR)`\n4. Also add a `Floating IP` to your instance.\n5. SSH to your server and run:\n\n```\nsudo apt-get update\\\n  && sudo apt-get -y install vim dnsutils curl\\\n  && curl -fsSL https://get.docker.com/ | sh || apt-get -y install docker.io\\\n  && sudo usermod -aG docker $(whoami | awk '{print $1}')\\\n  && mkdir -p ~/netflix-proxy\\\n  && cd ~/netflix-proxy\\\n  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n  && ./build.sh\n```\n\n6. Make sure to **record the credentials** for the `netflix-proxy` admin site.\n7. Set your DNS server to the IP given at the end of the script, then go to [this](http://ifconfig.co/) site to make sure the same IP is displayed.\n8. Finally, enjoy `Netflix` and others out of region.\n9. Enjoy or try `#netflix-proxy` on [freenode](https://webchat.freenode.net/?channels=netflix-proxy) for help.\n\n[![](https://raw.githubusercontent.com/ab77/netflix-proxy/master/static/gandi.png)](https://www.gandi.net/hosting/iaas/buy)\n\nThe following is based on Ubuntu image provided by `Gandi` using` root` login with SSH key only (no password). For default non-root `admin` login, adjust step 6 to use `sudo` where necessary.\n\n1. Head over to [Gandi](https://www.gandi.net/hosting/iaas/buy) to create a virtual server in a geographic location of interest.\n2. SSH to your server and run:\n\n```\napt-get update\\\n  && apt-get -y install vim dnsutils curl sudo\\\n  && curl -fsSL https://get.docker.com/ | sh || apt-get -y install docker.io\\\n  && mkdir -p ~/netflix-proxy\\\n  && cd ~/netflix-proxy\\\n  && curl -fsSL https://github.com/ab77/netflix-proxy/archive/latest.tar.gz | gunzip - | tar x --strip-components=1\\\n  && ./build.sh\n```\n\n3. Make sure to **record the credentials** for the `netflix-proxy` admin site.\n4. Set your DNS server to the IP given at the end of the script, then go to [this](http://ifconfig.co/) site to make sure the same IP is displayed.\n5. Finally, enjoy `Netflix` and others out of region.\n6. Enjoy or try `#netflix-proxy` on [freenode](https://webchat.freenode.net/?channels=netflix-proxy) for help.\n\n### Microsoft Azure (advanced)\nThe following **has not been tested** and is based on a standard Ubuntu image provided by `Microsoft Azure` using `cloud-harness` automation tool I wrote a while back and assumes an empty `Microsoft Azure` subscription. Also, because Azure [block ICMP](https://blogs.msdn.microsoft.com/mast/2014/06/22/use-port-pings-instead-of-icmp-to-test-azure-vm-connectivity/) thorough the load-balancer and don't offer native IPv6 support, IPv6 isn't going to work.\n\n1. Head over to [Microsoft Azure](https://azure.microsoft.com/en-gb/) and sign-up for an account.\n2. Get [Python](https://www.python.org/downloads/).\n3. On your workstation, run `git clone https://github.com/ab77/cloud-harness.git ~/cloud-harness`.\n4. Follow `cloud-harness` [Installation and Configuration](https://github.com/ab77/cloud-harness#installation-and-configuration) section to set it up.\n5. [Create](https://github.com/ab77/cloud-harness#create-storage-account-name-must-be-unique-as-it-forms-part-of-the-storage-url-check-with---action-check_storage_account_name_availability) a storage account.\n6. [Create](https://github.com/ab77/cloud-harness#create-a-new-hosted-service-name-must-be-unique-within-cloudappnet-domain-check-with---action-check_storage_account_name_availability) a new hosted service.\n7. [Add](https://github.com/ab77/cloud-harness#add-x509-certificate-containing-rsa-public-key-for-ssh-authentication-to-the-hosted-service) a hosted service certificate for SSH public key authentication\n8. [Create](https://github.com/ab77/cloud-harness#create-a-reserved-ip-address-for-the-hosted-service) a reserved ip address.\n9. [Create](https://github.com/ab77/cloud-harness#create-virtual-network) a virtual network.\n10. [Create](https://github.com/ab77/cloud-harness#create-a-new-linux-virtual-machine-deployment-and-role-with-reserved-ip-ssh-authentication-and-customscript-resource-extensionn3) a Ubuntu virtual machine as follows:\n\n```\n    ./cloud-harness.py azure --action create_virtual_machine_deployment \\\n    --service <your hosted service name> \\\n    --deployment <your hosted service name> \\\n    --name <your virtual machine name> \\\n    --label 'Netflix proxy' \\\n    --account <your storage account name> \\\n    --blob b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-14_04-LTS-amd64-server-20140414-en-us-30GB \\\n    --os Linux \\\n    --network VNet1 \\\n    --subnet Subnet-1 \\\n    --ipaddr <your reserved ipaddr name> \\\n    --size Medium \\\n    --ssh_auth \\\n    --disable_pwd_auth \\\n    --verbose\n```\n\n11. Use the [Azure Management Portal](https://manage.windowsazure.com/) to add `DNS (UDP)`, `HTTP (TCP)` and `HTTPS (TCP)` endpoints and secure them to your home/work/whatever IPs using the Azure `ACL` feature.\n12. SSH to your VM as `azureuser` using custom public TCP port (not `22`) and use any non-root user Ubuntu instructions to build/install `netflix-proxy`.\n\n### automated tests\n\n#### test build\nThis project is linked with `Travis CI` to deploy and test the project automatically. The Python script `testbuild.py` is used to deploy and test `netflix-proxy`. This script deploys a test `Droplet` and then runs a serious of tests to verify (a) that all `Docker` containers start; (b) the `built.sh` script outputs the correct message at the end; (c) all the relevant services survive a reboot; and (d) proxy is able to comunicate with Netflix over SSL.\n\nThe `testbuild.py` script can also be used to programatically deploy `Droplets` from the command line:\n\n```\nusage: testbuild.py digitalocean [-h] --api_token API_TOKEN\n                                 [--client_ip CLIENT_IP]\n                                 [--fingerprint FINGERPRINT [FINGERPRINT ...]]\n                                 [--region REGION] [--branch BRANCH]\n                                 [--create] [--destroy] [--list_regions]\n                                 [--name NAME]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --api_token API_TOKEN\n                        DigitalOcean API v2 secret token\n  --client_ip CLIENT_IP\n                        client IP to secure Droplet\n  --fingerprint FINGERPRINT [FINGERPRINT ...]\n                        SSH key fingerprint\n  --region REGION       region to deploy into; use --list_regions for a list\n  --branch BRANCH       netflix-proxy branch to deploy (default: master)\n  --create              Create droplet\n  --destroy             Destroy droplet\n  --list_regions        list all available regions\n  --name NAME           Droplet name\n```\n\nNote, you will need a working `Python 2.7` environment and the modules listed in `tests/requirements.txt` (run `pip install -r tests/requirements.txt`).\n\n#### test video playback\nVideo playback tests are **currently disabled** due to provider blocking.\n\n##### Netflix\nAfter a successful build deployment, `testvideo.py` is executed to test Netflix video playback. This is done by playing back 60 seconds of a title known to only be available in the US region (e.g. [1,000 Times Good Night](https://www.netflix.com/title/80001898)).\n\n```\nusage: testvideo.py netflix [-h] --email EMAIL --password PASSWORD\n                            [--seconds SECONDS] [--titleid TITLEID]\n                            [--tries TRIES]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --email EMAIL        Netflix username\n  --password PASSWORD  Netflix password\n  --seconds SECONDS    playback time per title in seconds (default: 60)\n  --titleid TITLEID    Netflix title_id to play (default: 80001898)\n  --tries TRIES        Playback restart attempts (default: 4)\n```\n\nA screenshot is saved at the end of the test and uploaded to the `gh-pages` branch.\n\n![Netflix VideoPlaybackTest screenshot](https://raw.githubusercontent.com/ab77/netflix-proxy/gh-pages/artifacts/VideoPlaybackTestNflx.png)\n\n##### Hulu\nSimilarly, `testvideo.py` is executed to test Hulu video playback using one of the free titles (e.g. [South Park S01E01: Cartman Gets an Anal Probe](http://www.hulu.com/watch/249837)). The build is configured not to fail in the event of Hulu test failing. This is because Hulu is almost cetrtainly blocked from Digital Ocean.\n\n![Hulu VideoPlaybackTest screenshot](https://raw.githubusercontent.com/ab77/netflix-proxy/gh-pages/artifacts/waitForPlayer.png)\n\n### IPv6\nThis solution uses IPv6 downstream from the proxy to unblock IPv6 enabled providers, such as Netflix. No IPv6 support on the client is required for this to work, only the VPS must have public IPv6 connectivity. You may also need to turn off IPv6 on your local network (and/or relevant devices).[[n6]](#footnotes)\n\n```\n+----------+                  +-----------+                 +-----------------+\n|          |                  |           |                 |                 |\n|  client  | +--------------> |   proxy   | +-------------> |  Netflix, etc.  |\n|          |      (ipv4)      |           |      (ipv6)     |                 |\n+----------+                  +-----------+                 +-----------------+\n```\n\n### contributing\nIf you have any idea, feel free to fork it and submit your changes back to me.\n\n### donate\nIf you find this useful, please feel free to make a small donation with [PayPal](https://www.paypal.me/belodetech) or Bitcoin.\n\n| Paypal | Bitcoin |\n| ------ | ------- |\n|[![](https://www.paypalobjects.com/en_GB/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=5UUCDR8YXWERQ)|![1GUrKgkaCkdsrCzb4pq3bJwkmjTVv9X7eG](https://raw.githubusercontent.com/ab77/netflix-proxy/master/static/bitcoin_qr.png)1GUrKgkaCkdsrCzb4pq3bJwkmjTVv9X7eG|\n\n#### footnotes\n1. [SNIProxy](https://github.com/dlundquist/sniproxy) by Dustin Lundquist `dustin@null-ptr.net`; this solution will only on devices supporting Server Name Indication (SNI)[[n7]](#footnotes) and only if they use DNS to resolve names.\n2. `Hulu` is heavily geo-restricted from most non-residential IP ranges and doesn't support IPv6.\n3. You can now specify your home/office/etc. IP manually using `-c <ip>` option to `build.sh`.\n4. See, serverfault [post](http://serverfault.com/questions/396958/configure-dns-server-to-return-same-ip-for-all-domains).\n5. See, [this](https://daniel.haxx.se/blog/2016/08/18/http2-connection-coalescing/) and [this](https://github.com/dlundquist/sniproxy/issues/178). The following [four](https://github.com/ab77/netflix-proxy/blob/master/proxy-domains.txt#L66-L69) hosts all need to resolve to different public IPs.\n6. If you have a working IPv6 stack, then your device may be preferring it over IPv4, see this [issue](https://forums.he.net/index.php?topic=3056).\n7. See, [article](https://en.wikipedia.org/wiki/Server_Name_Indication).\n8. See, [post](https://www.reddit.com/r/VPN/comments/48v03v/netflix_begins_geo_checks_on_cdn/).\n9. See, [Using NDP proxying](https://docs.docker.com/engine/userguide/networking/default_network/ipv6/). Both the caching resolver and Docker dual-stack support are disabled by default due to differences in IPv6 configurations provided by various hosting providers (i.e. RamNode).\n10. See, [post](http://www.webhostingtalk.com/showthread.php?t=1262537&p=9157381#post9157381).\n11. See, [https://www.facebook.com/GetflixAU/posts/650132888457824](https://www.facebook.com/GetflixAU/posts/650132888457824), [Netflix Geoblocking - Part 2](http://forums.whirlpool.net.au/forum-replies.cfm?t=2508180#r5) and read [How Netflix is blocking VPNs](http://www.techcentral.co.za/how-netflix-is-blocking-vpns/63882/) and [Wiki](https://github.com/ab77/netflix-proxy/wiki/On-how-Netflix-enforces-geographical-boundaries-in-the-Information-Age..).\n12. [Bypass Netflix Geoblocks with IPv6](https://www.ubermotive.com/?p=344).\n13. See, [IPv6 on Amazon AWS EC2](http://blog.iphoting.com/blog/2012/06/02/ipv6-on-amazon-aws-ec2/).\n14. If Netflix still thinks you are in a wrong country, try a different tunnel server (e.g. in a US location).\n15. See, [article](https://openvz.org/Docker_inside_CT).\n16. Netflix have most definitely blocked this service provider network ranges, so following the process is unlikely to yield an unblocking solution. If you own a compatible device, you could try `black.box` [unzoner](http://unzoner.com).\n17. GFW is probably re-writing DNS responses for certain very sensitive domains (i.e. facebook.com), so unfortunately a simple proxy solution like this won't work. VPN technology is required to bypass, try `black.box` [unzoner](http://unzoner.com).\n18. [dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) by `simon@thekelleys.org.uk`.\n19. Python3 (latest) release won't work on Ubuntu16/Debian9, upgrade to a more recent distribution (Python2 EOL Jan/2020).\n\n<hr>\n<p align=\"center\">&copy; 2016-2019 <a href=\"http://ab77.github.io/\">ab1</a></p>\n<p align=\"center\"><a href=\"http://anton.belodedenko.me/\"><img src=\"https://avatars2.githubusercontent.com/u/2033996?v=3&s=50\"></a></p>\n"
 },
 {
  "repo": "onelivesleft/PrettyErrors",
  "language": "Python",
  "readme_contents": "# pretty-errors\n\nPrettifies Python exception output to make it legible. Install it with\n```bash\npython -m pip install pretty_errors\n```\n\nIf you want `pretty_errors` to be used whenever you run a python script you must add it to your python startup procedure.  You can do so easily by running:\n```bash\npython -m pretty_errors\n```\nThis is the recommended way to use `pretty_errors`; apart from being simpler and universal, using it will mean `SyntaxError` exceptions also get formatted prettily (which doesn't work if you are manually importing `pretty_errors`).\n\n---\n\n![Example](https://raw.githubusercontent.com/onelivesleft/PrettyErrors/master/example.png)\n\n---\n\nIf you have not installed it universally you can use it in your project simply by importing it:\n```python\nimport pretty_errors\n```\nNote you need to be running in a terminal capable of colour output in order to get colour output: in Windows this means powershell, cmder, etc.  If you must use a monochrome terminal then you can call the helper function `pretty_errors.mono()`, which will set some config options in a way that is useful for monochrome output.\n\n![Monochrome](https://raw.githubusercontent.com/onelivesleft/PrettyErrors/master/exampleMono.png)\n\nIf you want to configure the output then use `pretty_errors.configure()`, `pretty_errors.whitelist()`, `pretty_errors.blacklist()`, `pretty_errors.pathed_config()`.  For example:\n```python\nimport pretty_errors\npretty_errors.configure(\n    separator_character = '*',\n    filename_display    = pretty_errors.FILENAME_EXTENDED,\n    line_number_first   = True,\n    display_link        = True,\n    lines_before        = 5,\n    lines_after         = 2,\n    line_color          = pretty_errors.RED + '> ' + pretty_errors.default_config.line_color,\n    code_color          = '  ' + pretty_errors.default_config.line_color,\n    truncate_code       = True,\n    display_locals      = True\n)\npretty_errors.blacklist('c:/python')\n```\n\n![Result](https://raw.githubusercontent.com/onelivesleft/PrettyErrors/master/example2.png)\n\n---\n\n##### Scraping STDERR\n\nSometimes it will be impossible for `pretty_errors` to utilize `sys.excepthook`: for instance, if you are using a framework which installs its own logging (such as `uvicorn`).  In such cases, you can make `pretty_errors` scrape the output to `stderr` instead, replacing it with its own.  To do so simple call:\n```python\npretty_errors.replace_stderr()\n```\nNote that this will lose some functionality, since `pretty_errors` will only have access to what is being output on screen, rather then the entire stack trace.  A good API will generally have a way to interact with the exception stack, which will allow for using `excepthook`: `replace_stderr` should be the last resort.  [See this comment for an example](https://github.com/onelivesleft/PrettyErrors/issues/16#issuecomment-751463605)\n\n---\n\n##### Whitelist / Blacklist:\n\nYou may use the functions `whitelist(path)` and `blacklist(path)` to add paths which will be necessary (`whitelist`) or excluded (`blacklist`).  The top frame of the stack is never excluded.\n\n---\n\n##### Pathed Configurations\n\nYou may set up alternate configurations, which are triggered by the path to the code file of the frame.  For example, if you were not interested in the system frames (those under 'c:/python') but did not want to hide them completely by using the `blacklist` you could do this:\n\n```python\nmeh = pretty_errors.config.copy()\nmeh.line_color = meh.code_color = meh.filename_color = meh.function_color = meh.line_number_color = (\n    pretty_errors.GREY\n)\npretty_errors.pathed_config(meh, 'c:/python')\n```\n\n---\n\n##### Environment Variables\n\n* PYTHON_PRETTY_ERRORS<br>\nYou may disable `pretty_errors` by setting the environment variable `PYTHON_PRETTY_ERRORS` to `0`.  i.e. at a command prompt:\n```bash\nset PYTHON_PRETTY_ERRORS=0\n```\n\nCalling `pretty_errors.activate()` will override this.\n\nIf you wish to selectively utilize `pretty_errors`, then use the above, and in your code perform your calculation to determine whether or not to call `pretty_errors.activate()`.\n\n* PYTHON_PRETTY_ERRORS_ISATTY_ONLY<br>\nIt may be desirable to disable `pretty_errors` when you are redirecting output to a file (to keep error logs, for instance).  If you wish to do so, then setting `PYTHON_PRETTY_ERRORS_ISATTY_ONLY` to non-zero will cause `pretty_errors` to check if it is running in an interactive terminal, and only activate if so.\n\n```bash\nset PYTHON_PRETTY_ERRORS_ISATTY_ONLY=1\n```\n\nSetting this will disable `replace_stderr()` in the same situations, unless you call it with the `force` parameter: `replace_stderr(force=True)`.\n\nCalling `pretty_errors.activate()` will override this.\n\nYou may check `pretty_errors.terminal_is_interactive` to see if the terminal is interactive (`pretty_errors` sets this by checking `sys.stderr.isatty()`).  You can use this to select a different config.  For example:\n\n```python\nif not pretty_errors.terminal_is_interactive:\n    pretty_errors.mono()\n```\n\n\n---\n\n##### Configuration settings:\n\nConfiguration settings are stored in `pretty_errors.config`, though should be set using `pretty_errors.configure()`.  A reference for the default config is stored in `pretty_errors.default_config`.\n\n* `name`<br>\nOptional field to store config name in.\n\n* `line_length`<br>\nOutput will be wrapped at this point.  If set to `0` (which is the default) it will automatically match your console width.\n\n* `full_line_newline`<br>\nInsert a hard newline even if the line is full.  If `line_length` is the same as your console width and this is enabled then you will see double newlines when the line is exactly full, so usually you would only set this if they are different.\n\n* `separator_character`<br>\nCharacter used to create the header line.  Hyphen is used by default.  If set to `None` or `''` then header will be disabled.\n\n* `display_timestamp`<br>\nWhen enabled a timestamp is written in the traceback header.\n\n* `timestamp_function`<br>\nFunction called to generate timestamp.  Default is `time.perf_counter`.\n\n* `exception_above`<br>\nWhen enabled the exception is displayed above the stack trace.\n\n* `exception_below`<br>\nWhen enabled the exception is displayed below the stack trace.\n\n* `stack_depth`<br>\nThe maximum number of entries from the stack trace to display.  When `0` will display the entire stack, which is the default.\n\n* `top_first`<br>\nWhen enabled the stack trace will be reversed, displaying the top of the stack first.\n\n* `always_display_bottom`<br>\nWhen enabled (which is the default) the bottom frame of the stack trace will always be displayed.\n\n* `show_suppressed`<br>\nWhen enabled all suppressed exceptions in the stack trace will be shown (typically they are suppressed because an exception above them has replaced them).  The normal python behaviour is to hide them.\n\n* `filename_display`<br>\nHow the filename is displayed: may be `pretty_errors.FILENAME_COMPACT`, `pretty_errors.FILENAME_EXTENDED`, or `pretty_errors.FILENAME_FULL`\n\n* `line_number_first`<br>\nWhen enabled the line number will be displayed first, rather than the filename.\n\n* `display_link`<br>\nWhen enabled a link is written below the error location, which VSCode will allow you to click on.\n\n* `lines_after`, `lines_before`<br>\nHow many lines of code to display for the top frame, before and after the line the exception occurred on.\n\n* `trace_lines_after`, `trace_lines_before`<br>\nHow many lines of code to display for each other frame in the stack trace, before and after the line the exception occurred on.\n\n* `truncate_code`<br>\nWhen enabled each line of code will be truncated to fit the line length.\n\n* `display_locals`<br>\nWhen enabled, local variables appearing in the top stack frame code will be displayed with their values.\n\n* `display_trace_locals`<br>\nWhen enabled, local variables appearing in other stack frame code will be displayed with their values.\n\n* `truncate_locals`<br>\nWhen enabled the values of displayed local variables will be truncated to fit the line length.\n\n* `display_arrow`<br>\nWhen enabled an arrow will be displayed for syntax errors, pointing at the offending token.\n\n* `arrow_head_character`, `arrow_tail_character`<br>\nCharacters used to draw the arrow which points at syntax errors.\n\n* `inner_exception_message`<br>\nMessage displayed when one exception occurs inside another, between the two exceptions.  Default is `None`, which will simply display the exceptions separated by the header.  If you want to emulate the default non-pretty behaviour, use this:\n\n`inner_exception_message = pretty_errors.MAGENTA + \"\\n  During handling of the above exception, another exception occurred:\\n\"`\n\nNote that if you use `top_first` then the order will be reversed, so you should use a message like this instead:\n\n`inner_exception_message = pretty_errors.MAGENTA + \"\\n  The above exception occurred during another exception:\\n\"`\n\n* `inner_exception_separator`<br>\nDefault is `False`.  When set to `True` a header will be written before the `inner_exception_message`.\n\n* `prefix`<br>\nText string which is displayed at the top of the report, just below the header.\n\n* `infix`<br>\nText string which is displayed between each frame of the stack.\n\n* `postfix`<br>\nText string which is displayed at the bottom of the exception report.\n\n* `reset_stdout`<br>\nWhen enabled the reset escape sequence will be written to stdout as well as stderr; turn this on if your console is being left with the wrong color.\n\n---\n\nThese color strings will be output before the relevant part of the exception message.  You may include non-escape sequence strings if you wish; if you do not have a terminal which supports color output, or simply want to include extra demarcation.\n\n* `header_color`<br>\nEscape sequence to set header color.\n\n* `timestamp_color`<br>\nEscape sequence to set timestamp color.\n\n* `exception_color`<br>\nEscape sequence to set exception color.\n\n* `exception_arg_color`<br>\nEscape sequence to set exception arguments color.\n\n* `exception_file_color`<br>\nEscape sequence to set color of filenames in exceptions (for example, in a FileNotFoundError).\n\n* `filename_color`<br>\nEscape sequence to set filename color.\n\n* `line_number_color`<br>\nEscape sequence to set line number color.\n\n* `function_color`<br>\nEscape sequence to set function color.\n\n* `link_color`<br>\nEscape sequence to set link color.\n\n* `line_color`<br>\nEscape sequence to set the color of the line of code which caused the exception.\n\n* `code_color`<br>\nEscape sequence to set the color of other displayed lines of code.\n\n* `arrow_head_color`, `arrow_tail_color`<br>\nEscape sequence to set the color of the arrow which points at syntax errors.\n\n* `syntax_error_color`<br>\nEscape sequence to set the color of the syntax error token.\n\n* `local_name_color`<br>\nEscape sequence to set the color of local variable names.\n\n* `local_value_color`<br>\nEscape sequence to set the color of local variable values.\n\n* `local_len_color`<br>\nEscape sequence to set the color of local value length when local is truncated.\n\n`pretty_errors` has some built in escape sequence constants you can use when setting these colors:\n\n* `BLACK`\n* `GREY`\n* `RED`\n* `GREEN`\n* `YELLOW`\n* `BLUE`\n* `MAGENTA`\n* `CYAN`\n* `WHITE`\n\nFor each color there is a matching `BRIGHT_` variant (i.e. `pretty_errors.BRIGHT_RED`), as well as a `_BACKGROUND` variant to set the background color (i.e. `pretty_errors.RED_BACKGROUND`).\n\nFor example:\n```python\npretty_errors.configure(\n    line_color = pretty_errors.CYAN_BACKGROUND + pretty_errors.BRIGHT_WHITE\n)\n```\n\n---\n\n##### Further customization\n\nFor the most extensive customization (short of forking the package) you may override the default `ExceptionWriter` class, allowing you to tailor the output however you wish.  Typically you will only need to override the `write_` methods.\n\nFor example:\n\n```python\nclass MyExceptionWriter(pretty_errors.ExceptionWriter):\n    def write_header(self):\n        self.output_text('######## ERROR ########')\n\npretty_errors.exception_writer = MyExceptionWriter()\n```\n\nRun `help(pretty_errors.ExceptionWriter)` in the python interpreter for more details.\n"
 },
 {
  "repo": "PacktPublishing/Deep-Reinforcement-Learning-Hands-On",
  "language": "Python",
  "readme_contents": "# Deep Reinforcement Learning Hands-On\n\nCode samples for [Deep Reinforcement Learning Hands-On](https://www.packtpub.com/big-data-and-business-intelligence/practical-deep-reinforcement-learning)\nbook\n\n## Versions and compatibility\n\nThis repository is being maintained by book author [Max Lapan](https://github.com/Shmuma).\nI'm trying to keep all the examples working under the latest versions of [PyTorch](https://pytorch.org/) \nand [gym](https://gym.openai.com/), which is not always simple, as software evolves. For example, OpenAI Universe, \nextensively being used in chapter 13, was discontinued by OpenAI. List of current requirements is present in \n[requirements.txt](requirements.txt) file.\n\nExamples require python 3.6.\n\nAnd, of course, bugs in examples are inevitable, so, exact code might differ from code present in the book text.\n\nToo keep track of major code change, I'm using tags and branches, for example:\n* [tag 01_release](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/01_release) marks code \nstate right after book publication in June 2018\n* [branch master](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On) has the latest \nversion of code updated for the latest stable PyTorch 0.4.1\n* [branch torch_1.0](not_created_yet) keeps the activity of porting examples to PyTorch 1.0 (not yet released)\n\n## Chapters' examples\n\n* [Chapter 2: OpenAI Gym](Chapter02)\n* [Chapter 3: Deep Learning with PyTorch](Chapter03)\n* [Chapter 4: Cross Entropy method](Chapter04)\n* [Chapter 5: Tabular learning and the Bellman equation](Chapter05)\n* [Chapter 6: Deep Q-Networks](Chapter06)\n* [Chapter 7: DQN extensions](Chapter07)\n* [Chapter 8: Stocks trading using RL](Chapter08)\n* [Chapter 9: Policy Gradients: an alternative](Chapter09)\n* [Chapter 10: Actor-Critic method](Chapter10)\n* [Chapter 11: Asynchronous Advantage Actor-Critic](Chapter11)\n* [Chapter 12: Chatbots traning with RL](Chapter12)\n* [Chapter 13: Web navigation](Chapter13)\n* [Chapter 14: Continuous action space](Chapter14)\n* [Chapter 15: Trust regions: TRPO, PPO and ACKTR](Chapter15)\n* [Chapter 16: Black-box optimisation in RL](Chapter16)\n* [Chapter 17: Beyond model-free: imagination](Chapter17)\n* [Chapter 18: AlphaGo Zero](Chapter18)\n\n\n# Deep Reinforcement Learning Hands-On\nThis is the code repository for [Deep Reinforcement Learning Hands-On](https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands?utm_source=github&utm_medium=repository&utm_campaign=9781788834247), published by [Packt](https://www.packtpub.com/?utm_source=github). It contains all the supporting project files necessary to work through the book from start to finish.\n## About the Book\nRecent developments in reinforcement learning (RL), combined with deep learning (DL), have seen unprecedented progress made towards training agents to solve complex problems in a human-like way. Google\u2019s use of algorithms to play and defeat the well-known Atari arcade games has propelled the field to prominence, and researchers are generating new ideas at a rapid pace.\n\nDeep Reinforcement Learning Hands-On is a comprehensive guide to the very latest DL tools and their limitations. You will evaluate methods including Cross-entropy and policy gradients, before applying them to real-world environments. Take on both the Atari set of virtual games and family favorites such as Connect4. The book provides an introduction to the basics of RL, giving you the know-how to code intelligent learning agents to take on a formidable array of practical tasks. Discover how to implement Q-learning on \u2018grid world\u2019 environments, teach your agent to buy and trade stocks, and find out how natural language models are driving the boom in chatbots.\n\n"
 },
 {
  "repo": "facebook/fresco",
  "language": "Java",
  "readme_contents": "# Fresco\n\n<img alt=\"Fresco Logo\" align=\"right\" src=\"docs/static/sample-images/fresco_logo.svg\" width=\"15%\" />\n\n[![Build Status](https://circleci.com/gh/facebook/fresco.svg?style=shield)](https://circleci.com/gh/facebook/fresco)\n[![License](https://img.shields.io/badge/license-MIT-brightgreen)](https://github.com/facebook/fresco/blob/main/LICENSE)\n\nFresco is a powerful system for displaying images in Android applications.\n\nFresco takes care of image loading and display, so you don't have to. It will load images from the network, local storage, or local resources, and display a placeholder until the image has arrived. It has two levels of cache; one in memory and another in internal storage.\n\nIn Android 4.x and lower, Fresco puts images in a special region of Android memory. This lets your application run faster - and suffer the dreaded `OutOfMemoryError` much less often.\n\nFresco also supports:\n\n* streaming of progressive JPEGs\n* display of animated GIFs and WebPs\n* extensive customization of image loading and display\n* and much more!\n\nFind out more at our [website](http://frescolib.org/index.html).\n\n## Requirements\n\nFresco can be included in any Android application.\n\nFresco supports Android 2.3 (Gingerbread) and later.\n\n## Using Fresco in your application\n\nIf you are building with Gradle, simply add the following line to the `dependencies` section of your `build.gradle` file:\n\n```groovy\nimplementation 'com.facebook.fresco:fresco:2.6.0'\n```\n\nFor full details, visit the documentation on our web site, available in English, Chinese, and Korean:\n\n<a href=\"http://frescolib.org/docs/index.html\"><img src=\"http://frescolib.org/static/GetStarted-en.png\" width=\"150\" height=\"42\"/></a>\n\n<a href=\"http://fresco-cn.org/docs/index.html\"><img src=\"http://frescolib.org/static/GetStarted-zh.png\" width=\"104\" height=\"42\"/></a>\n\n<a href=\"http://fresco.recrack.com/docs/index.html\"><img src=\"http://frescolib.org/static/GetStarted-ko.png\" width=\"104\" height=\"42\"/></a>\n\n## Join the Fresco community\n\nPlease use our [issues page](https://github.com/facebook/fresco/issues) to let us know of any problems.\n\nFor pull requests, please see the [CONTRIBUTING](https://github.com/facebook/fresco/blob/main/CONTRIBUTING.md) file for information on how to help out. See our [documentation](http://frescolib.org/docs/building-from-source.html) for information on how to build from source.\n\n\n## License\nFresco is [MIT-licensed](https://github.com/facebook/fresco/blob/main/LICENSE).\n"
 },
 {
  "repo": "zo0r/react-native-push-notification",
  "language": "Java",
  "readme_contents": "# React Native Push Notifications\n\n[![npm version](https://badge.fury.io/js/react-native-push-notification.svg?update=9)](http://badge.fury.io/js/react-native-push-notification)\n[![npm downloads](https://img.shields.io/npm/dm/react-native-push-notification.svg?update=9)](http://badge.fury.io/js/react-native-push-notification)\n\nReact Native Local and Remote Notifications for iOS and Android\n\n## State of the repository\n\nThis repository is not actively maintained. The main reason is time. The second one is probably the complexity of notifications on both iOS and Android.\nSince this project probably need a huge refactor to fix some issue or to implement new features. I think you should probably consider these alternatives: [Notifee](https://notifee.app) free since september or [react-native-notifications](https://github.com/wix/react-native-notifications).\n\nIf you are interested in being a maintainer of this project, feel free to ask in issues.\n\n## \ud83c\udf89 Version 7.x is live ! \ud83c\udf89\n\nCheck out for changes and migration in the CHANGELOG:\n\n[Changelog](https://github.com/zo0r/react-native-push-notification/blob/master/CHANGELOG.md)\n\n# Supporting the project\n\nMaintainers are welcome ! Feel free to contact me :wink:\n\n## Changelog\n\nChangelog is available from version 3.1.3 here: [Changelog](https://github.com/zo0r/react-native-push-notification/blob/master/CHANGELOG.md)\n\n## Installation\n\n### NPM\n\n```\nnpm install --save react-native-push-notification\n```\n\n### Yarn\n\n```\nyarn add react-native-push-notification\n```\n\n**NOTE: If you target iOS you also need to follow the [installation instructions for PushNotificationIOS](https://github.com/react-native-community/react-native-push-notification-ios) since this package depends on it.**\n\n**NOTE: For Android, you will still have to manually update the AndroidManifest.xml (as below) in order to use Scheduled Notifications.**\n\n## Issues\n\nHaving a problem? Read the [troubleshooting](./trouble-shooting.md) guide before raising an issue.\n\n## Pull Requests\n\n[Please read...](./submitting-a-pull-request.md)\n\n## iOS manual Installation\n\nThe component uses PushNotificationIOS for the iOS part. You should follow their [installation instructions](https://github.com/react-native-community/react-native-push-notification-ios).\n\n## Android manual Installation\n\n**NOTE: `firebase-messaging`, prior to version 15 requires to have the same version number in order to work correctly at build time and at run time. To use a specific version:**\n\nIn your `android/build.gradle`\n\n```gradle\next {\n    googlePlayServicesVersion = \"<Your play services version>\" // default: \"+\"\n    firebaseMessagingVersion = \"<Your Firebase version>\" // default: \"21.1.0\"\n\n    // Other settings\n    compileSdkVersion = <Your compile SDK version> // default: 23\n    buildToolsVersion = \"<Your build tools version>\" // default: \"23.0.1\"\n    targetSdkVersion = <Your target SDK version> // default: 23\n    supportLibVersion = \"<Your support lib version>\" // default: 23.1.1\n}\n```\n\n**NOTE: localNotification() works without changes in the application part, while localNotificationSchedule() only works with these changes:**\n\nIn your `android/app/src/main/AndroidManifest.xml`\n\n```xml\n    .....\n    <uses-permission android:name=\"android.permission.VIBRATE\" />\n    <uses-permission android:name=\"android.permission.RECEIVE_BOOT_COMPLETED\"/>\n\n    <application ....>\n        <!-- Change the value to true to enable pop-up for in foreground on receiving remote notifications (for prevent duplicating while showing local notifications set this to false) -->\n        <meta-data  android:name=\"com.dieam.reactnativepushnotification.notification_foreground\"\n                    android:value=\"false\"/>\n        <!-- Change the resource name to your App's accent color - or any other color you want -->\n        <meta-data  android:name=\"com.dieam.reactnativepushnotification.notification_color\"\n                    android:resource=\"@color/white\"/> <!-- or @android:color/{name} to use a standard color -->\n\n        <receiver android:name=\"com.dieam.reactnativepushnotification.modules.RNPushNotificationActions\" />\n        <receiver android:name=\"com.dieam.reactnativepushnotification.modules.RNPushNotificationPublisher\" />\n        <receiver android:name=\"com.dieam.reactnativepushnotification.modules.RNPushNotificationBootEventReceiver\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.BOOT_COMPLETED\" />\n                <action android:name=\"android.intent.action.QUICKBOOT_POWERON\" />\n                <action android:name=\"com.htc.intent.action.QUICKBOOT_POWERON\"/>\n            </intent-filter>\n        </receiver>\n\n        <service\n            android:name=\"com.dieam.reactnativepushnotification.modules.RNPushNotificationListenerService\"\n            android:exported=\"false\" >\n            <intent-filter>\n                <action android:name=\"com.google.firebase.MESSAGING_EVENT\" />\n            </intent-filter>\n        </service>\n     .....\n```\n\nIf not using a built in Android color (`@android:color/{name}`) for the `notification_color` `meta-data` item.\nIn `android/app/src/main/res/values/colors.xml` (Create the file if it doesn't exist).\n\n```xml\n<resources>\n    <color name=\"white\">#FFF</color>\n</resources>\n```\n\nIf your app has an @Override on onNewIntent in `MainActivity.java` ensure that function includes a super call on onNewIntent (if your `MainActivity.java` does not have an @Override for onNewIntent skip this):\n\n```java\n    @Override\n    public void onNewIntent(Intent intent) {\n        ...\n        super.onNewIntent(intent);\n        ...\n    }\n```\n\n### If you use remote notifications\n\nMake sure you have installed setup Firebase correctly.\n\nIn `android/build.gradle`\n\n```gradle\n\nbuildscript {\n    ...\n    dependencies {\n        ...\n        classpath('com.google.gms:google-services:4.3.3')\n        ...\n    }\n}\n```\n\nIn `android/app/build.gradle`\n\n```gradle\ndependencies {\n  ...\n  implementation 'com.google.firebase:firebase-analytics:17.3.0'\n  ...\n}\n\napply plugin: 'com.google.gms.google-services'\n\n```\n\nThen put your `google-services.json` in `android/app/`.\n\n**Note: [firebase/release-notes](https://firebase.google.com/support/release-notes/android)**\n\n> The Firebase Android library `firebase-core` is no longer needed. This SDK included the Firebase SDK for Google Analytics.\n>\n> Now, to use Analytics or any Firebase product that recommends the use of Analytics (see table below), you need to explicitly add the Analytics dependency: `com.google.firebase:firebase-analytics:17.3.0`.\n\n### If you don't use autolink\n\nIn `android/settings.gradle`\n\n```gradle\n...\ninclude ':react-native-push-notification'\nproject(':react-native-push-notification').projectDir = file('../node_modules/react-native-push-notification/android')\n```\n\nIn your `android/app/build.gradle`\n\n```gradle\n dependencies {\n    ...\n    implementation project(':react-native-push-notification')\n    ...\n }\n```\n\nManually register module in `MainApplication.java` (if you did not use `react-native link`):\n\n```java\nimport com.dieam.reactnativepushnotification.ReactNativePushNotificationPackage;  // <--- Import Package\n\npublic class MainApplication extends Application implements ReactApplication {\n\n  private final ReactNativeHost mReactNativeHost = new ReactNativeHost(this) {\n      @Override\n      protected boolean getUseDeveloperSupport() {\n        return BuildConfig.DEBUG;\n      }\n\n      @Override\n      protected List<ReactPackage> getPackages() {\n\n          return Arrays.<ReactPackage>asList(\n              new MainReactPackage(),\n              new ReactNativePushNotificationPackage() // <---- Add the Package\n          );\n    }\n  };\n\n  ....\n}\n```\n\n## Usage\n\n**DO NOT USE `.configure()` INSIDE A COMPONENT, EVEN `App`**\n> If you do, notification handlers will not fire, because they are not loaded. Instead, use `.configure()` in the app's first file, usually `index.js`.\n\n\n```javascript\nimport PushNotificationIOS from \"@react-native-community/push-notification-ios\";\nimport PushNotification from \"react-native-push-notification\";\n\n// Must be outside of any component LifeCycle (such as `componentDidMount`).\nPushNotification.configure({\n  // (optional) Called when Token is generated (iOS and Android)\n  onRegister: function (token) {\n    console.log(\"TOKEN:\", token);\n  },\n\n  // (required) Called when a remote is received or opened, or local notification is opened\n  onNotification: function (notification) {\n    console.log(\"NOTIFICATION:\", notification);\n\n    // process the notification\n\n    // (required) Called when a remote is received or opened, or local notification is opened\n    notification.finish(PushNotificationIOS.FetchResult.NoData);\n  },\n\n  // (optional) Called when Registered Action is pressed and invokeApp is false, if true onNotification will be called (Android)\n  onAction: function (notification) {\n    console.log(\"ACTION:\", notification.action);\n    console.log(\"NOTIFICATION:\", notification);\n\n    // process the action\n  },\n\n  // (optional) Called when the user fails to register for remote notifications. Typically occurs when APNS is having issues, or the device is a simulator. (iOS)\n  onRegistrationError: function(err) {\n    console.error(err.message, err);\n  },\n\n  // IOS ONLY (optional): default: all - Permissions to register.\n  permissions: {\n    alert: true,\n    badge: true,\n    sound: true,\n  },\n\n  // Should the initial notification be popped automatically\n  // default: true\n  popInitialNotification: true,\n\n  /**\n   * (optional) default: true\n   * - Specified if permissions (ios) and token (android and ios) will requested or not,\n   * - if not, you must call PushNotificationsHandler.requestPermissions() later\n   * - if you are not using remote notification or do not have Firebase installed, use this:\n   *     requestPermissions: Platform.OS === 'ios'\n   */\n  requestPermissions: true,\n});\n```\n\n## Example app\n\nExample folder contains an example app to demonstrate how to use this package. The notification Handling is done in `NotifService.js`.\n\nPlease test your PRs with this example app before submitting them. It'll help maintaining this repo.\n\n## Handling Notifications\n\nWhen any notification is opened or received the callback `onNotification` is called passing an object with the notification data.\n\nNotification object example:\n\n```javascript\n{\n    foreground: false, // BOOLEAN: If the notification was received in foreground or not\n    userInteraction: false, // BOOLEAN: If the notification was opened by the user from the notification area or not\n    message: 'My Notification Message', // STRING: The notification message\n    data: {}, // OBJECT: The push data or the defined userInfo in local notifications\n}\n```\n\n## Local Notifications\n\n```js\nPushNotification.localNotification(details: Object)\n```\n\nEXAMPLE:\n\n```javascript\nPushNotification.localNotification({\n  /* Android Only Properties */\n  channelId: \"your-channel-id\", // (required) channelId, if the channel doesn't exist, notification will not trigger.\n  ticker: \"My Notification Ticker\", // (optional)\n  showWhen: true, // (optional) default: true\n  autoCancel: true, // (optional) default: true\n  largeIcon: \"ic_launcher\", // (optional) default: \"ic_launcher\". Use \"\" for no large icon.\n  largeIconUrl: \"https://www.example.tld/picture.jpg\", // (optional) default: undefined\n  smallIcon: \"ic_notification\", // (optional) default: \"ic_notification\" with fallback for \"ic_launcher\". Use \"\" for default small icon.\n  bigText: \"My big text that will be shown when notification is expanded. Styling can be done using HTML tags(see android docs for details)\", // (optional) default: \"message\" prop\n  subText: \"This is a subText\", // (optional) default: none\n  bigPictureUrl: \"https://www.example.tld/picture.jpg\", // (optional) default: undefined\n  bigLargeIcon: \"ic_launcher\", // (optional) default: undefined\n  bigLargeIconUrl: \"https://www.example.tld/bigicon.jpg\", // (optional) default: undefined\n  color: \"red\", // (optional) default: system default\n  vibrate: true, // (optional) default: true\n  vibration: 300, // vibration length in milliseconds, ignored if vibrate=false, default: 1000\n  tag: \"some_tag\", // (optional) add tag to message\n  group: \"group\", // (optional) add group to message\n  groupSummary: false, // (optional) set this notification to be the group summary for a group of notifications, default: false\n  ongoing: false, // (optional) set whether this is an \"ongoing\" notification\n  priority: \"high\", // (optional) set notification priority, default: high\n  visibility: \"private\", // (optional) set notification visibility, default: private\n  ignoreInForeground: false, // (optional) if true, the notification will not be visible when the app is in the foreground (useful for parity with how iOS notifications appear). should be used in combine with `com.dieam.reactnativepushnotification.notification_foreground` setting\n  shortcutId: \"shortcut-id\", // (optional) If this notification is duplicative of a Launcher shortcut, sets the id of the shortcut, in case the Launcher wants to hide the shortcut, default undefined\n  onlyAlertOnce: false, // (optional) alert will open only once with sound and notify, default: false\n  \n  when: null, // (optional) Add a timestamp (Unix timestamp value in milliseconds) pertaining to the notification (usually the time the event occurred). For apps targeting Build.VERSION_CODES.N and above, this time is not shown anymore by default and must be opted into by using `showWhen`, default: null.\n  usesChronometer: false, // (optional) Show the `when` field as a stopwatch. Instead of presenting `when` as a timestamp, the notification will show an automatically updating display of the minutes and seconds since when. Useful when showing an elapsed time (like an ongoing phone call), default: false.\n  timeoutAfter: null, // (optional) Specifies a duration in milliseconds after which this notification should be canceled, if it is not already canceled, default: null\n\n  messageId: \"google:message_id\", // (optional) added as `message_id` to intent extras so opening push notification can find data stored by @react-native-firebase/messaging module. \n\n  actions: [\"Yes\", \"No\"], // (Android only) See the doc for notification actions to know more\n  invokeApp: true, // (optional) This enable click on actions to bring back the application to foreground or stay in background, default: true\n\n  /* iOS only properties */\n  category: \"\", // (optional) default: empty string\n  subtitle: \"My Notification Subtitle\", // (optional) smaller title below notification title\n\n  /* iOS and Android properties */\n  id: 0, // (optional) Valid unique 32 bit integer specified as string. default: Autogenerated Unique ID\n  title: \"My Notification Title\", // (optional)\n  message: \"My Notification Message\", // (required)\n  picture: \"https://www.example.tld/picture.jpg\", // (optional) Display an picture with the notification, alias of `bigPictureUrl` for Android. default: undefined\n  userInfo: {}, // (optional) default: {} (using null throws a JSON value '<null>' error)\n  playSound: false, // (optional) default: true\n  soundName: \"default\", // (optional) Sound to play when the notification is shown. Value of 'default' plays the default sound. It can be set to a custom sound such as 'android.resource://com.xyz/raw/my_sound'. It will look for the 'my_sound' audio file in 'res/raw' directory and play it. default: 'default' (default sound is played)\n  number: 10, // (optional) Valid 32 bit integer specified as string. default: none (Cannot be zero)\n  repeatType: \"day\", // (optional) Repeating interval. Check 'Repeating Notifications' section for more info.\n});\n```\n\n## Scheduled Notifications\n\n```js\nPushNotification.localNotificationSchedule(details: Object)\n```\n\nEXAMPLE:\n\n```javascript\nPushNotification.localNotificationSchedule({\n  //... You can use all the options from localNotifications\n  message: \"My Notification Message\", // (required)\n  date: new Date(Date.now() + 60 * 1000), // in 60 secs\n  allowWhileIdle: false, // (optional) set notification to work while on doze, default: false\n\n  /* Android Only Properties */\n  repeatTime: 1, // (optional) Increment of configured repeatType. Check 'Repeating Notifications' section for more info.\n});\n```\n\n## Get the initial notification\n\n```js\nPushNotification.popInitialNotification(callback)\n```\n\nEXAMPLE:\n\n```javascript\nPushNotification.popInitialNotification((notification) => {\n  console.log('Initial Notification', notification);\n});\n```\n\n## Custom sounds\n\nIn android, add your custom sound file to `[project_root]/android/app/src/main/res/raw`\n\nIn iOS, add your custom sound file to the project `Resources` in xCode.\n\nIn the location notification json specify the full file name:\n\n    soundName: 'my_sound.mp3'\n\n## Channel Management (Android)\n\nTo use channels, create them at startup and pass the matching `channelId` through to `PushNotification.localNotification` or `PushNotification.localNotificationSchedule`.\n\n```javascript\nimport PushNotification, {Importance} from 'react-native-push-notification';\n...\n  PushNotification.createChannel(\n    {\n      channelId: \"channel-id\", // (required)\n      channelName: \"My channel\", // (required)\n      channelDescription: \"A channel to categorise your notifications\", // (optional) default: undefined.\n      playSound: false, // (optional) default: true\n      soundName: \"default\", // (optional) See `soundName` parameter of `localNotification` function\n      importance: Importance.HIGH, // (optional) default: Importance.HIGH. Int value of the Android notification importance\n      vibrate: true, // (optional) default: true. Creates the default vibration pattern if true.\n    },\n    (created) => console.log(`createChannel returned '${created}'`) // (optional) callback returns whether the channel was created, false means it already existed.\n  );\n```\n\n**NOTE: Without channel, notifications don't work**\n\nIn the notifications options, you must provide a channel id with `channelId: \"your-channel-id\"`, if the channel doesn't exist the notification might not be triggered. Once the channel is created, the channel cannot be updated. Make sure your `channelId` is different if you change these options. If you have created a channel in another way, it will apply options of the channel.\n\nIf you want to use a different default channel for remote notification, refer to the documentation of Firebase:\n\n[Set up a Firebase Cloud Messaging client app on Android](https://firebase.google.com/docs/cloud-messaging/android/client?hl=fr)\n\n```xml\n  <meta-data\n      android:name=\"com.google.firebase.messaging.default_notification_channel_id\"\n      android:value=\"@string/default_notification_channel_id\" />\n```\n\nFor local notifications, the same kind of option is available:\n\n- you can use:\n  ```xml\n    <meta-data\n        android:name=\"com.dieam.reactnativepushnotification.default_notification_channel_id\"\n        android:value=\"@string/default_notification_channel_id\" />\n  ```\n- If not defined, fallback to the Firebase value defined in the `AndroidManifest`:\n  ```xml\n    <meta-data\n        android:name=\"com.google.firebase.messaging.default_notification_channel_id\"\n        android:value=\"...\" />\n  ```\n- If not defined, fallback to the default Firebase channel id `fcm_fallback_notification_channel`\n\n### List channels\n\nYou can list available channels with:\n\n```js\nPushNotification.getChannels(function (channel_ids) {\n  console.log(channel_ids); // ['channel_id_1']\n});\n```\n\n### Channel exists\n\nYou can check if a channel exists with:\n\n```js\nPushNotification.channelExists(channel_id, function (exists) {\n  console.log(exists); // true/false\n});\n```\n\n### Channel blocked\n\nYou can check if a channel blocked with:\n\n```js\nPushNotification.channelBlocked(channel_id, function (blocked) {\n  console.log(blocked); // true/false\n});\n```\n\n### Delete channel\n\nYou can delete a channel with:\n\n```js\nPushNotification.deleteChannel(channel_id);\n```\n\n## Cancelling notifications\n\n### 1) cancelLocalNotification\n\nThe `id` parameter for `PushNotification.localNotification` is required for this operation. The id supplied will then be used for the cancel operation.\n\n```javascript\nPushNotification.localNotification({\n    ...\n    id: '123'\n    ...\n});\nPushNotification.cancelLocalNotification('123');\n```\n\n### 2) cancelAllLocalNotifications\n\n```javascript\nPushNotification.cancelAllLocalNotifications()\n```\n\nCancels all scheduled notifications AND clears the notifications alerts that are in the notification centre.\n\n### 3) removeAllDeliveredNotifications\n\n```javascript\nPushNotification.removeAllDeliveredNotifications();\n```\n\nRemove all delivered notifications from Notification Center\n\n### 4) getDeliveredNotifications\n\n```javascript\nPushNotification.getDeliveredNotifications(callback);\n```\n\nProvides you with a list of the app\u2019s notifications that are still displayed in Notification Center\n\n**Parameters:**\n\n| Name     | Type     | Required | Description                                                 |\n| -------- | -------- | -------- | ----------------------------------------------------------- |\n| callback | function | Yes      | Function which receive an array of delivered notifications. |\n\nA delivered notification is an object containing:\n\n- `identifier` : The identifier of this notification.\n- `title` : The title of this notification.\n- `body` : The body of this notification.\n- `category` : The category of this notification (optional).\n- `userInfo` : An object containing additional notification data (optional).\n- `thread-id` : The thread identifier of this notification, if has one.\n\n### 5) removeDeliveredNotifications\n\n```javascript\nPushNotification.removeDeliveredNotifications(identifiers);\n```\n\nRemoves the specified notifications from Notification Center\n\n**Parameters:**\n\n| Name        | Type  | Required | Description                        |\n| ----------- | ----- | -------- | ---------------------------------- |\n| identifiers | array | Yes      | Array of notification identifiers. |\n\n### 6) getScheduledLocalNotifications\n\n```javascript\nPushNotification.getScheduledLocalNotifications(callback);\n```\n\nProvides you with a list of the app\u2019s scheduled local notifications that are yet to be displayed\n\n**Parameters:**\n\n| Name     | Type     | Required | Description                                                 |\n| -------- | -------- | -------- | ----------------------------------------------------------- |\n| callback | function | Yes      | Function which receive an array of delivered notifications. |\n\nReturns an array of local scheduled notification objects containing:\n\n| Name           | Type   | Description                                              |\n| -------------- | ------ | -------------------------------------------------------- |\n| id             | number | The identifier of this notification.                     |\n| date           | Date   | The fire date of this notification.                      |\n| title          | string | The title of this notification.                          |\n| message        | string | The message body of this notification.                   |\n| soundName      | string | The sound name of this notification.                     |\n| repeatInterval | number | (Android only) The repeat interval of this notification. |\n| number         | number | App notification badge count number.                     |\n| data           | any    | The user info of this notification.                      |\n\n## Abandon Permissions\n\n```js\nPushNotification.abandonPermissions()\n```\nRevokes the current token and unregister for all remote notifications received via APNS or FCM.\n\n## Notification priority\n\n(optional) Specify `priority` to set priority of notification. Default value: \"high\"\n\nAvailable options:\n\n```\n\"max\" = NotficationCompat.PRIORITY_MAX\\\n\"high\" = NotficationCompat.PRIORITY_HIGH\\\n\"low\" = NotficationCompat.PRIORITY_LOW\\\n\"min\" = NotficationCompat.PRIORITY_MIN\\\n\"default\" = NotficationCompat.PRIORITY_DEFAULT\n```\n\nMore information: https://developer.android.com/reference/android/app/Notification.html#PRIORITY_DEFAULT\n\n## Notification visibility\n\n(optional) Specify `visibility` to set visibility of notification. Default value: \"private\"\n\nAvailable options:\n\n```\n\"private\" = NotficationCompat.VISIBILITY_PRIVATE\\\n\"public\" = NotficationCompat.VISIBILITY_PUBLIC\\\n\"secret\" = NotficationCompat.VISIBILITY_SECRET \n```\nMore information: https://developer.android.com/reference/android/app/Notification.html#VISIBILITY_PRIVATE\n\n## Notification importance\n\n(optional) Specify `importance` to set importance of notification. Default value: Importance.HIGH  \nConstants available on the `Importance` object. `import PushNotification, {Importance} from 'react-native-push-notification';`\n\nAvailable options:\n```\nImportance.DEFAULT = NotificationManager.IMPORTANCE_DEFAULT\\\nImportance.HIGH = NotificationManager.IMPORTANCE_HIGH\\\nImportance.LOW = NotificationManager.IMPORTANCE_LOW\\\nImportance.MIN = NotificationManager.IMPORTANCE_MIN\\\nImportance.NONE= NotificationManager.IMPORTANCE_NONE\\\nImportance.UNSPECIFIED = NotificationManager.IMPORTANCE_UNSPECIFIED\n```\n\nMore information: https://developer.android.com/reference/android/app/NotificationManager#IMPORTANCE_DEFAULT\n\n## Show notifications while the app is in foreground\n\nIf you want a consistent results in Android & iOS with the most flexibility, it is best to handle it manually by prompting a local notification when `onNotification` is triggered by a remote push notification on foreground (check `notification.foreground` prop).\n\nWatch out for an infinite loop triggering `onNotification` - remote & local notification will trigger it. You can overcome this by marking local notifications' data.\n\n## Notification while idle\n\n(optional) Specify `allowWhileIdle` to set if the notification should be allowed to execute even when the system is on low-power idle modes.\n\nOn Android 6.0 (API level 23) and forward, the Doze was introduced to reduce battery consumption when the device is unused for long periods of time. But while on Doze the AlarmManager alarms (used to show scheduled notifications) are deferred to the next maintenance window. This may cause the notification to be delayed while on Doze.\n\nThis can significantly impact the power use of the device when idle. So it must only be used when the notification is required to go off on a exact time, for example on a calendar notification.\n\nMore information:\nhttps://developer.android.com/training/monitoring-device-state/doze-standby\n\n## Repeating Notifications\n\n(optional) Specify `repeatType` and optionally `repeatTime` (Android-only) while scheduling the local notification. Check the local notification example above.\n\n### iOS\nProperty `repeatType` can only be `month`, `week`, `day`, `hour`, `minute`.\n\nNOTE: `repeatTime` do not work with iOS.\n\n### Android\nProperty `repeatType` could be one of `month`, `week`, `day`, `hour`, `minute`, `time`. \n\nThe interval used can be configured to a different interval using `repeatTime`. If `repeatType` is `time`, `repeatTime` must be specified as the number of milliseconds between each interval.\nFor example, to configure a notification every other day\n\n```javascript\nPushNotification.localNotificationSchedule({\n    ...\n    repeatType: 'day',\n    repeatTime: 2,\n    ...\n});\n```\n\n## Notification Actions\n\n(Android Only)\n\nThis is done by specifying an `actions` parameters while configuring the local notification. This is an array of strings where each string is a notification action that will be presented with the notification.\n\nFor e.g. `actions: ['Accept', 'Reject']`\n\nWhen you handle actions in background (`invokeApp: false`), you can open the application and pass the initial notification by using use `PushNotification.invokeApp(notification)`.\n\nMake sure you have the receiver in `AndroidManifest.xml`:\n\n```xml\n  <receiver android:name=\"com.dieam.reactnativepushnotification.modules.RNPushNotificationActions\" />\n```\n\nNotifications with inline reply: \n\nYou must register an action as \"ReplyInput\", this will show in the notifications an input to write in. \n\nEXAMPLE:\n```javascript\nPushNotification.localNotificationSchedule({\n  message: \"My Notification Message\", // (required)\n  date: new Date(Date.now() + (60 * 1000)), // in 60 secs\n  actions: [\"ReplyInput\"],\n  reply_placeholder_text: \"Write your response...\", // (required)\n  reply_button_text: \"Reply\" // (required)\n});\n```\n\nTo get the text from the notification: \n\n```javascript\n...\nif(notification.action === \"ReplyInput\"){\n  console.log(\"texto\", notification.reply_text)// this will contain the inline reply text. \n}\n...\n```\n\nFor iOS, you can use:\n\n```javascript\nPushNotification.setNotificationCategories(categories);\n```\n\nAnd use the `category` field in the notification.\n\nDocumentation [here](https://github.com/react-native-push-notification-ios/push-notification-ios#how-to-perform-different-action-based-on-user-selected-action) to add notification actions.\n\n## Set application badge icon\n\n```js\nPushNotification.setApplicationIconBadgeNumber(number: number)\n```\n\nWorks natively in iOS.\n\nUses the [ShortcutBadger](https://github.com/leolin310148/ShortcutBadger) on Android, and as such will not work on all Android devices.\n\n## Android Only Methods\n\n```js\nPushNotification.subscribeToTopic(topic: string)\n```\nSubscribe to a topic (works only with Firebase)\n\n```js\nPushNotification.unsubscribeFromTopic(topic: string)\n```\nUnsubscribe from a topic (works only with Firebase)\n\n## Android Custom Notification Handling\n\nUnlike iOS, Android apps handle the creation of their own notifications. React Native Push Notifications does a \"best guess\" to create and handle incoming notifications. However, when using 3rd party notification platforms and tools, the initial notification creation process may need to be customized.\n\n### Customizing Notification Creation\n\nIf your notification service uses a custom data payload format, React Native Push Notifications will not be able to parse the data correctly to create an initial notification.\n\nFor these cases, you should:\n\n1. Remove the intent handler configuration for React Native Push Notifications from your `android/app/src/main/AndroidManifest.xml`.\n2. Implement initial notification creation as per the instructions from your Provider.\n\n### Handling Custom Payloads\n\nData payloads of notifications from 3rd party services may not match the format expected by React Native Push Notification. When tapped, these notifications will not pass the details and data to the `onNotification()` event handler. Custom `IntentHandlers` allow you to fix this so that correct `notification` objects are sent to your `onNotification()` method.\n\nCustom handlers are added in Application init or `MainActivity.onCreate()` methods:\n\n```java\nRNPushNotification.IntentHandlers.add(new RNPushNotification.RNIntentHandler() {\n  @Override\n  public void onNewIntent(Intent intent) {\n    // If your provider requires some parsing on the intent before the data can be\n    // used, add that code here. Otherwise leave empty.\n  }\n\n  @Nullable\n  @Override\n  public Bundle getBundleFromIntent(Intent intent) {\n    // This should return the bundle data that will be serialized to the `notification.data`\n    // property sent to the `onNotification()` handler. Return `null` if there is no data\n    // or this is not an intent from your provider.\n    \n    // Example:\n    if (intent.hasExtra(\"MY_NOTIFICATION_PROVIDER_DATA_KEY\")) {\n      return intent.getBundleExtra(\"MY_NOTIFICATION_PROVIDER_DATA_KEY\");\n    }\n    return null;\n  }\n});\n```\n\n## Checking Notification Permissions\n\n```js\nPushNotification.checkPermissions(callback: Function) //Check permissions\n```\n\n`callback` will be invoked with a `permissions` object:\n\n- `alert`: boolean\n- `badge`: boolean\n- `sound`: boolean\n\n## iOS Only Methods\n\n```js\nPushNotification.getApplicationIconBadgeNumber(callback: Function) //Get badge number\n```\n\n"
 },
 {
  "repo": "liyifeng1994/ssm",
  "language": "Java",
  "readme_contents": "# \u5173\u4e8e\n\u6574\u5408SSM\u6846\u67b6\uff08SpringMVC + Spring + MyBatis\uff09\uff0c\u9002\u5408\u521a\u63a5\u89e6spring\u7684\u7ae5\u978b\uff0c\u9700\u8981\u6709servlet\u548cjsp\u57fa\u7840\u3002\u5b66\u4e60\u5b8cSSM\u6574\u5408\u540e\u53ef\u4ee5\u5e94\u4ed8\u5f00\u53d1\u5de5\u4f5c\uff0c\u4f46\u662f\u5efa\u8bae\u7ee7\u7eed\u6df1\u7a76\uff0c\u5b66\u4e60spring boot\uff0cspring cloud\u7b49\u6280\u672f\uff0c\u8ba9\u5f00\u53d1\u6548\u7387\u66f4\u4e0a\u4e00\u5c42\u697c\u3002\n\n# \u535a\u5ba2\nhttp://blog.csdn.net/qq598535550/article/details/51703190\n\n# \u73af\u5883\n- jdk 1.8\n- tomcat 8.5\n\n![](https://img-ask.csdn.net/upload/201806/05/1528204838_152827.png)\n"
 },
 {
  "repo": "aporter/coursera-android",
  "language": "Java",
  "readme_contents": "coursera-android\n================\n\nSource Code for Android Course Example Applications\n"
 },
 {
  "repo": "citerus/dddsample-core",
  "language": "Java",
  "readme_contents": "# DDDSample\n[![Build Status](https://travis-ci.org/citerus/dddsample-core.svg?branch=master)](https://travis-ci.org/citerus/dddsample-core)\n\nThis is the new home of the original DDD Sample app hosted at SourceForge. \n\nOur intention is to move everything from SourceForge to GitHub in due time while starting upgrading both the technical aspects as well as the DDD aspects of the DDD Sample.\n\nThis project is a joint effort by Eric Evans' company [Domain Language](https://www.domainlanguage.com/) and the [Swedish software consulting company Citerus](https://www.citerus.se/).\n\nThe application uses Spring Boot. To start it go to the root directory and type `mvn spring-boot:run` or run the `main` method of the `Application` class from your IDE.  \nThen open http://localhost:8080/dddsample in your browser (and make sure that no firewall is blocking the communication and that Javascript for localhost is not blocked).\n\nDiscussion group: https://groups.google.com/forum/#!forum/dddsample\n\nDevelopment blog: https://citerus.github.io/dddsample-core/\n\nTrello board: https://trello.com/b/PTDFRyxd\n\n## Using the HandlingReportService (\"regapp\") web service\n\nUsing your favorite SOAP client, send an HTTP POST message to http://localhost:8080/dddsample/ws/RegisterEvent with the following body:\n\n```xml\n<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:ws=\"http://ws.handling.interfaces.dddsample.citerus.se/\">\n    <soapenv:Header/>\n    <soapenv:Body>\n        <ws:submitReport>\n            <!--Optional:-->\n            <arg0>\n                <completionTime>2022-01-01T00:00:00</completionTime>\n                <!--1 or more repetitions:-->\n                <trackingIds>2</trackingIds>\n                <type>LOAD</type>\n                <unLocode>AA234</unLocode>\n                <!--Optional:-->\n                <voyageNumber>5</voyageNumber>\n            </arg0>\n        </ws:submitReport>\n    </soapenv:Body>\n</soapenv:Envelope>\n```\n\nYou can also use cURL to send the request using an xml file for the body:\n\n    curl --data-binary \"@/path/to/project/src/test/resources/sampleSoapBody.xml\" -H 'Content-Type: text/xml;charset=UTF-8' http://localhost:8080/dddsample/ws/RegisterEvent\n"
 },
 {
  "repo": "lisawray/groupie",
  "language": "Java",
  "readme_contents": "# groupie\n\nGroupie is a simple, flexible library for complex RecyclerView layouts.  \n\nGroupie lets you treat your content as logical groups and handles change notifications for you -- think sections with headers and footers, expandable groups, blocks of vertical columns, and much more.  It makes it easy to handle asynchronous content updates and insertions and user-driven content changes.  At the item level, it abstracts the boilerplate of item view types, item layouts, viewholders, and span sizes.  \n \n<img src=\"http://i.imgur.com/eftOE0v.gif\" width=\"300px\"/>\n\n# Try it out:\n\n```gradle\nimplementation \"com.github.lisawray.groupie:groupie:$groupie_version\"\n```\n\nGroupie also has a support module for Android's [view binding](https://developer.android.com/topic/libraries/view-binding). This module also supports Android [data binding](https://developer.android.com/topic/libraries/data-binding/index.html), so if your project uses both data binding and view binding, you don't have to add the dependency on the data binding support module. [Setup here.](#view-binding)\n\n```gradle\nimplementation \"com.github.lisawray.groupie:groupie:$groupie_version\"\nimplementation \"com.github.lisawray.groupie:groupie-viewbinding:$groupie_version\" \n```\n\n### Note:\nIf using `groupie-viewbinding` in a databinding project is only available when using Android Gradle Plugin 3.6.0 or higher.\n\nIf using an older Gradle Plugin version with databinding the you can use the standalone `groupie-databinding` library to generate view holders. [Setup here.](#data-binding)\n\n```gradle\nimplementation \"com.github.lisawray.groupie:groupie:$groupie_version\"\nimplementation \"com.github.lisawray.groupie:groupie-databinding:$groupie_version\" \n```\n\nYou can also use Groupie with Java and your existing ViewHolders. \n\nWhich one to choose?  It's up to you and what your project already uses. You can even use Kotlin and data binding together.[<sup>*</sup>](#kotlin-and-data-binding) Or all your existing hand-written Java ViewHolders, and one new Kotlin item to try it out. Go crazy!  \n    \n## Get started\n\nUse a `GroupieAdapter` anywhere you would normally use a `RecyclerView.Adapter`, and attach it to your RecyclerView as usual.\n\nKotlin\n```kotlin\nval adapter = GroupieAdapter()\nrecyclerView.adapter = adapter\n```\n\nJava\n```java\nGroupieAdapter adapter = new GroupieAdapter();\nrecyclerView.setAdapter(adapter);\n```\n    \n## Groups\n\nGroups are the building block of Groupie.  An individual `Item` (the unit which an adapter inflates and recycles) is a Group of 1.  You can add Groups and Items interchangeably to the adapter.\n\nKotlin\n```kotlin\ngroupAdapter += HeaderItem()\ngroupAdapter += CommentItem()\n\nval section = Section()\nsection.setHeader(HeaderItem())\nsection.addAll(bodyItems)\ngroupAdapter += section\n```\n\nJava\n```java\ngroupAdapter.add(new HeaderItem());\ngroupAdapter.add(new CommentItem());\n\nSection section = new Section();\nsection.setHeader(new HeaderItem());\nsection.addAll(bodyItems);\ngroupAdapter.add(section);\n```\n    \nModifying the contents of the GroupieAdapter in any way automatically sends change notifications.  Adding an item calls `notifyItemAdded()`; adding a group calls `notifyItemRangeAdded()`, etc.\n\nModifying the contents of a Group automatically notifies its parent.  When notifications reach the GroupieAdapter, it dispatches final change notifications.  There's never a need to manually notify or keep track of indices, no matter how you structure your data.\n\n```java\nsection.removeHeader(); // results in a remove event for 1 item in the adapter, at position 2\n```\n    \nThere are a few simple implementations of Groups within the library:\n- `Section`, a list of body content with an optional header group and footer group.  It supports diffing and animating moves, updates and other changes\n- `ExpandableGroup`, a single parent group with a list of body content that can be toggled hidden or shown.\n    \nGroupie tries not to assume what features your groups require.  Instead, groups are flexible and composable.  They can be combined and nested to arbitrary depth.  \n    \nLife (and mobile design) is complicated, so groups are designed so that making new ones and defining their behavior is easy. You should make many small, simple, custom groups as the need strikes you.\n\nYou can implement the `Group` interface directly if you want.  However, in most cases, you can extend `Section` or the base implementation, `NestedGroup`.  Section supports common RV paradigms like diffing, headers, footers, and placeholders.  NestedGroup provides support for arbitrary nesting of groups, registering/unregistering listeners, and fine-grained change notifications to support animations and updating the adapter.\n    \n## Items\n\nGroupie abstracts away the complexity of multiple item view types.  Each Item declares a view layout id, and gets a callback to `bind` the inflated layout.  That's all you need; you can add your new item directly to a `GroupieAdapter` and call it a day.\n\n### Item with Kotlin-Android-Extensions:\n\n**Note: `kotlin-android-extensions` is deprecated since Kotlin 1.4.20. Therefore, `groupie-kotlin-android-extensions` is also deprecated, and `viewbinding` should be preferred.**\n\nGroupie includes a module for Kotlin and Kotlin Android extensions. [Setup here.](#kotlin)\n\n```gradle\nimplementation \"com.github.lisawray.groupie:groupie:$groupie_version\"\nimplementation \"com.github.lisawray.groupie:groupie-kotlin-android-extensions:$groupie_version\"\n```\n\nThe `Item` class gives you simple callbacks to bind your model object to the generated fields.\n\n```kotlin\nimport com.xwray.groupie.kotlinandroidextensions.Item\nimport com.xwray.groupie.kotlinandroidextensions.GroupieViewHolder\nimport kotlinx.android.synthetic.main.song.*\n\nclass SongItem(private val song: Song) : Item() {\n\n    override fun getLayout() = R.layout.song\n\n    override fun bind(viewHolder: GroupieViewHolder, position: Int) {\n        viewHolder.title.text = song.title\n        viewHolder.artist.text = song.artist\n    }\n}\n```\n\n### Item with data binding:\n\nThe `Item` class gives you simple callbacks to bind your model object to the generated binding.  Because of data binding, there's no need to write a view holder.  \n\n```java\npublic class SongItem extends BindableItem<SongBinding> {\n\n    public SongItem(Song song) {\n        this(song);\n    }    \n\n    @Override public void bind(SongBinding binding, int position) {\n        binding.setSong(song);\n    }\n\n    @Override public int getLayout() {\n        return R.layout.song;\n    }\n}\n```\n\nIf you're converting existing ViewHolders, you can reference any named views (e.g. `R.id.title`) directly from the binding instead. \n```java\n    @Override public void bind(SongBinding binding, int position) {\n        binding.title.setText(song.getTitle());\n    }\n```\n\nYou can also mix and match `BindableItem` and other `Items` in the adapter, so you can leave legacy viewholders as they are by making an `Item<MyExistingViewHolder>`.  \n\n### Legacy item (your own ViewHolder)\nYou can leave legacy viewholders as they are by converting `MyExistingViewHolder` to extend `GroupieViewHolder` rather than `RecyclerView.ViewHolder`. Make sure to change the imports to `com.xwray.groupie.Item` and `com.xwray.groupie.GroupieViewHolder`.\n\nFinally, in your `Item<MyExistingViewHolder>`, override \n\n```java\n    @Override\n    public MyExistingViewHolder createViewHolder(@NonNull View itemView) {\n        return new MyExistingViewHolder(itemView);\n    }\n```\n\n### Note: \n\nItems can also declare their own column span and whether they are draggable or swipeable.  \n\n# Gradle setup\n\n## Kotlin\n\nIn your project level `build.gradle` file, include:\n\n```\nbuildscript {\n    ext.kotlin_version = '1.6.21'\n\n    repositories {\n        mavenCentral()\n    }\n\n    dependencies {\n        classpath \"org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version\"\n    }\n}\n\nallprojects {\n    repositories {\n        google()\n        mavenCentral()\n        maven { url \"https://jitpack.io\" }\n    }\n}\n```\n\nIn new projects, the `settings.gradle` file has a `dependencyResolutionManagement` block, which needs to specify the repository as well:\n\n```\ndependencyResolutionManagement {\n    repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)\n    repositories {\n        google()\n        mavenCentral()\n        maven { url 'https://jitpack.io' }  // <--\n        jcenter() // Warning: this repository is going to shut down soon\n    }\n}\n```\n\nIn your app `build.gradle` file, include:\n\n```\nimplementation 'com.github.lisawray.groupie:groupie:$groupie_version'\n```\n\n### Using with Kotlin-Android-Extensions (deprecated)\n\nIf you are still relying on the deprecated `kotlin-android-extensions` module, then also apply the following:\n\n```\napply plugin: 'kotlin-android'\napply plugin: 'kotlin-android-extensions'\n\nandroid {\n  ....\n  \n   // IMPORTANT!  Enables kotlin synthetic view properties.\n   // See: https://github.com/Kotlin/KEEP/blob/master/proposals/android-extensions-entity-caching.md\n    androidExtensions {\n        experimental = true\n    }\n\t\n}\n\ndependencies {\n    implementation 'com.github.lisawray.groupie:groupie:$groupie_version'\n    implementation 'com.github.lisawray.groupie:groupie-kotlin-android-extensions:$groupie_version'\n}\n```\n\nRemember to include \n```kotlin\nimport kotlinx.android.synthetic.main.my_item_layout.*\n```\nin the corresponding Item class for generated view references.\n\n## View binding\n\nAdd to your app module's `build.gradle`:\n\n```gradle\nandroid {\n    buildFeatures {\n        viewBinding true\n    }\n}\n\ndependencies {\n    implementation \"com.github.lisawray.groupie:groupie:$groupie_version\"\n    implementation \"com.github.lisawray.groupie:groupie-viewbinding:$groupie_version\"\n}\n```\n\nThen:\n\n```kotlin\nclass MyLayoutItem: BindableItem<MyLayoutBinding>() {\n    override fun initializeViewBinding(view: View): MyLayoutBinding {\n        return MyLayoutBinding.bind(view)\n    }\n\n    // Other implementations...\n}\n```\n\n### Note:\n\nIf you use `groupie-databinding` with data binding classes and your layouts have some variables or [observable objects](https://developer.android.com/topic/libraries/data-binding/observability), don't forget to run [`executePendingBindings`](https://developer.android.com/topic/libraries/data-binding/generated-binding#immediate_binding) at the last point in `bind`.\n\n## Data binding\n\nAdd to your app module's build.gradle:\n\n```gradle\nandroid {\n    buildFeatures {\n        dataBinding true\n    }\n}\n\ndependencies {\n    implementation \"com.github.lisawray.groupie:groupie:$groupie_version\"\n    implementation \"com.github.lisawray.groupie:groupie-databinding:$groupie_version\"\n}\n```\n\nThen, just wrap each item layout in `<layout>` tags.  (The `<data>` section is optional.)  \n\n`layout/item_song.xml`\n```xml\n<layout xmlns:android=\"http://schemas.android.com/apk/res/android\" \n    xmlns:tools=\"http://schemas.android.com/tools\">\n    <data>\n        <variable name=\"song\" type=\"com.example.Song\" />\n    </data>\n\n    <FrameLayout \n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\" >\n\n        <TextView\n            android:id=\"@+id/title\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:layout_gravity=\"center\"\n            android:text=\"@{song.title}\"\n            tools:text=\"A Song Title\" />\n\n    </FrameLayout>\n</layout>\n```\n\nBindings are only generated for layouts wrapped with <layout/> tags, so there's no need to convert the rest of your project (unless you want to).\n\nYou can add a `<data>` section to directly bind a model or ViewModel, but you don't have to.  The generated view bindings alone are a huge time saver.\n\n### Kotlin AND data binding / view binding?\nSure, why not?  Follow all the instructions from *both* sections above.  You only need to include the `groupie-databinding` or `groupie-viewbinding` dependency, and omit the references to `android-extensions`.  You'll make `BindableItem`s instead of importing and using Kotlin extensions.\n\n\n# Contributing\nContributions you say?  Yes please!\n\n### Bug report? \n- If at all possible, please attach a *minimal* sample project or code which reproduces the bug. \n- Screenshots are also a huge help if the problem is visual.\n### Send a pull request!\n- If you're fixing a bug, please add a failing test or code that can reproduce the issue.\n\n\nIf you try it out, I'd love to know what you think. Please hit up Lisa at [first][last]@gmail.com or on Twitter at [@lisawrayz](https://twitter.com/lisawrayz).\n"
 },
 {
  "repo": "TangoAgency/material-intro-screen",
  "language": "Java",
  "readme_contents": "# Android Material Intro Screen\n [ ![Download](https://api.bintray.com/packages/tangoagency/maven/material-intro-screen/images/download.svg) ](https://bintray.com/tangoagency/maven/material-intro-screen/_latestVersion)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/753f46972d8740d1984f8beb7d04fb9d)](https://www.codacy.com/app/TangoAgency/material-intro-screen?utm_source=github.com&utm_medium=referral&utm_content=TangoAgency/material-intro-screen&utm_campaign=badger)\n[![Build Status](https://travis-ci.org/TangoAgency/material-intro-screen.svg?branch=master)](https://travis-ci.org/TangoAgency/material-intro-screen)\n[![Android Arsenal Material Intro Screen](https://img.shields.io/badge/Android%20Arsenal-Material--Intro--Screen-green.svg?style=true)](http://android-arsenal.com/details/1/4368)\n\nMaterial intro screen is inspired by [Material Intro] and developed with love from scratch. I decided to rewrite completely almost all features in order to make Android intro screen easy to use for everyone and extensible as possible.\n## Features\n  - [Easily add new slides][Intro Activity]\n  - [Custom slides][Custom Slide]\n  - [Parallax slides][Parallax Slide]\n  - Easy extensible api\n  - Android TV support!\n  - Material design at it's best!!!\n\n| [Simple slide][SimpleSlide] | [Custom slide][Custom Slide] | [Permission slide][PermissionSlide] | [Finish slide][FinishSlide]\n|:-:|:-:|:-:|:-:|\n| ![Simple slide] | ![Customslide] | ![Permission slide] | ![Finish slide] |\n\n## Usage\n### Step 1:\n#### Add gradle dependecy\n```\ndependencies {\n  compile 'agency.tango.android:material-intro-screen:{latest_release}'\n}\n```\n### Step 2:\n#### First, your [intro activity][Intro Activity] class needs to extend MaterialIntroActivity:\n```java\npublic class IntroActivity extends MaterialIntroActivity\n```\n### Step 3:\n#### Add activity to [manifest][Manifest] with defined theme:\n```xml\n        <activity\n            android:name=\".IntroActivity\"\n            android:theme=\"@style/Theme.Intro\" />\n```\n### Step 4: \n#### [Add slides:][Intro Activity]\n```java\n @Override\n    protected void onCreate(@Nullable Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        \n        addSlide(new SlideFragmentBuilder()\n                .backgroundColor(R.color.colorPrimary)\n                .buttonsColor(R.color.colorAccent)\n                .possiblePermissions(new String[]{Manifest.permission.CALL_PHONE, Manifest.permission.READ_SMS})\n                .neededPermissions(new String[]{Manifest.permission.CAMERA, Manifest.permission.ACCESS_FINE_LOCATION, Manifest.permission.ACCESS_COARSE_LOCATION})\n                .image(agency.tango.materialintroscreen.R.drawable.ic_next)\n                .title(\"title 3\")\n                .description(\"Description 3\")\n                .build(),\n                new MessageButtonBehaviour(new View.OnClickListener() {\n                    @Override\n                    public void onClick(View v) {\n                        showMessage(\"We provide solutions to make you love your work\");\n                    }\n                }, \"Work with love\"));\n}\n```\n#### Explanation of SlideFragment usage:\n  - ```possiblePermissions``` &#8702; permissions which are not necessary to be granted\n  - ```neededPersmissions``` &#8702; permission which are needed to be granted to move further from that slide\n  - ```MessageButtonBehaviour``` &#8702; create a new instance only if you want to have a custom action or text on a message button\n\n### Step 5: \n#### Customize Intro Activity:\n  - ```setSkipButtonVisible()``` &#8702; show skip button instead of back button on the left bottom of screen\n  - ```hideBackButton()``` &#8702; hides any button on the left bottom of screen\n  - ```enableLastSlideAlphaExitTransition()``` &#8702; set if the last slide should disapear with alpha hiding effect\n\n#### Customizing view animations: \n\nYou can set enter, default and exit translation for every view in intro activity. To achive this you need to get translation wrapper for chosen view (for example: ```getNextButtonTranslationWrapper()```) and set there new class which will implement ```IViewTranslation```\n```java\ngetBackButtonTranslationWrapper()\n                .setEnterTranslation(new IViewTranslation() {\n                    @Override\n                    public void translate(View view, @FloatRange(from = 0, to = 1.0) float percentage) {\n                        view.setAlpha(percentage);\n                    }\n                });\n```\n#### Available [translation wrappers][TranslationWrapper]:\n- ```getNextButtonTranslationWrapper()```\n- ```getBackButtonTranslationWrapper()```\n- ```getPageIndicatorTranslationWrapper()```\n- ```getViewPagerTranslationWrapper()``` \n- ```getSkipButtonTranslationWrapper()``` \n\n## Custom slides\n#### Of course you are able to implement completely custom slides. You only need to extend SlideFragment and override following functions:\n - ```backgroundColor()```\n - ```buttonsColor()```\n - ```canMoveFurther()``` (only if you want to stop user from being able to move further before he will do some action)\n - ```cantMoveFurtherErrorMessage()``` (as above)\n \n#### If you want to use parallax in a fragment please use one of the below views:\n  - [```ParallaxFrameLayout```][ParallaxFrame]\n  - [```ParallaxLinearLayout```][ParallaxLinear]\n  - [```ParallaxRelativeLayout```][ParallaxRelative]\n\n#### And set there the [app:layout_parallaxFactor][ParallaxFactor] attribute:\n```xml\n<agency.tango.materialintroscreen.parallax.ParallaxLinearLayout\nxmlns:android=\"http://schemas.android.com/apk/res/android\">\n\n    <ImageView\n        android:id=\"@+id/image_slide\"\n        app:layout_parallaxFactor=\"0.6\"/>\n```\n\nAll features which are not available in simple Slide Fragment are shown here: [Custom Slide]\n\n## Things I have used to create this\n - For parallax I have used files from [Material Intro] by [@HeinrichReimer]\n - [InkPageIndicator.java] by [@NickButcher]\n - Images used to create sample app are from [freepik]\n - For over scroll effect on last slide I have partially used [Android-Overscroll-ViewPager]\n \n## Getting Help\n\nTo report a specific problem or feature request, [open a new issue on Github](https://github.com/TangoAgency/material-intro-screen/issues/new).\n\n## Company\n \n[![Facebook](https://github.com/TangoAgency/material-intro-screen/blob/master/images/facebook.png)](https://www.facebook.com/TangoDigitalAgency)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[![Twitter](https://github.com/TangoAgency/material-intro-screen/blob/master/images/twitter.png)](https://twitter.com/Tango_Agency)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[![LinkedIn](https://github.com/TangoAgency/material-intro-screen/blob/master/images/linkedin.png)](https://www.linkedin.com/company/tango-digital-agency)\n\n[Here](https://github.com/TangoAgency/) you can see open source work developed by Tango Agency.\n \nWhether you're searching for a new partner or trusted team for creating your new great product we are always ready to start work with you. \n\nYou can contact us via contact@tango.agency.\nThanks in advance.\n \n[Custom Slide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/app/src/main/java/agency/tango/materialintro/CustomSlide.java>\n[Material Intro]: <https://github.com/HeinrichReimer/material-intro/tree/master/library/src/main/java/com/heinrichreimersoftware/materialintro/view/parallax>\n[@HeinrichReimer]: <https://github.com/HeinrichReimer>\n[InkPageIndicator.java]: <https://github.com/nickbutcher/plaid/blob/master/app/src/main/java/io/plaidapp/ui/widget/InkPageIndicator.java>\n[@NickButcher]: <https://github.com/nickbutcher>\n[freepik]: <http://www.freepik.com/>\n[Simple slide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/images/simple_slide.gif>\n[Customslide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/images/custom_slide.gif>\n[Permission slide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/images/permissions_slide.gif>\n[Finish slide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/images/finish_slide.gif>\n[Intro Activity]: <https://github.com/TangoAgency/material-intro-screen/blob/master/app/src/main/java/agency/tango/materialintro/IntroActivity.java>\n[Parallax Slide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/app/src/main/res/layout/fragment_custom_slide.xml>\n[PermissionSlide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/app/src/main/java/agency/tango/materialintro/IntroActivity.java#L52>\n[FinishSlide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/app/src/main/java/agency/tango/materialintro/IntroActivity.java#L19>\n[SimpleSlide]: <https://github.com/TangoAgency/material-intro-screen/blob/master/app/src/main/java/agency/tango/materialintro/IntroActivity.java#L43>\n[ParallaxFrame]: <https://github.com/TangoAgency/material-intro-screen/blob/master/material-intro-screen/src/main/java/agency/tango/materialintroscreen/parallax/ParallaxFrameLayout.java>\n[ParallaxLinear]: <https://github.com/TangoAgency/material-intro-screen/blob/master/material-intro-screen/src/main/java/agency/tango/materialintroscreen/parallax/ParallaxLinearLayout.java>\n[ParallaxRelative]: <https://github.com/TangoAgency/material-intro-screen/blob/master/material-intro-screen/src/main/java/agency/tango/materialintroscreen/parallax/ParallaxRelativeLayout.java>\n[ParallaxFactor]: <https://github.com/TangoAgency/material-intro-screen/blob/master/material-intro-screen/src/main/res/layout/fragment_slide.xml#L29>\n[Manifest]: <https://github.com/TangoAgency/material-intro-screen/blob/master/app/src/main/AndroidManifest.xml#L28>\n[TranslationWrapper]: <https://github.com/TangoAgency/material-intro-screen/blob/master/material-intro-screen/src/main/java/agency/tango/materialintroscreen/animations/ViewTranslationWrapper.java>\n[Android-Overscroll-ViewPager]: <https://github.com/iamjiex/Android-Overscroll-ViewPager>"
 },
 {
  "repo": "esoxjem/MovieGuide",
  "language": "Java",
  "readme_contents": "# MovieGuide\n### \ud83d\udea7 Refactoring in progress \ud83d\udc77\u200d\u2640\ufe0f\u26cf\ud83d\udc77\ud83d\udd27\ufe0f\ud83d\udc77\ud83d\udd27 \ud83d\udea7\n\n- Comments and new issues are welcome. \ud83d\udc4d \n- Currently not accepting external PRs that touch on the app's architecture and features. \ud83d\uded1 \n\n[![Build Status](https://app.bitrise.io/app/e74daa103a89eb3f/status.svg?token=cNHddSJnkUmE_p7ZA9eruQ&branch=master)](https://app.bitrise.io/app/e74daa103a89eb3f)\n\nAndroid app that lists popular/highest-rated movies, shows trailers and reviews.\n\nThis app showcases the MVP pattern, RxJava, Dagger 2 and Uncle Bob Martin's Clean Architecture approach.\nOptimized for tablets.\n\n### Building the app\n1. Open `local.properties` in the root directory.\n2. Add your [TMDB](https://themoviedb.org) API key as follows\n```\ntmdb_api_key=ADD_YOUR_API_KEY_HERE\n```\n3. Run the app.\n\n### Screenshots\n![Screenshot](http://i.imgur.com/72PypXCm.png) \n![screenshot2](http://imgur.com/I96Eka6m.png)\n![screenshot3](http://imgur.com/4qHZcejm.png)\n![screenshot4](http://imgur.com/m7J8HzUm.png)\n![screenshot5](http://imgur.com/PwtjZHKm.png)\n![screenshot6](http://imgur.com/kNHjCXSm.png)\n"
 },
 {
  "repo": "andpor/react-native-sqlite-storage",
  "language": "Java",
  "readme_contents": "# react-native-sqlite-storage\nSQLite3 Native Plugin for React Native for both Android (Classic and Native), iOS and Windows\n\nFoundation of this library is based on Chris Brody's Cordova SQLite plugin.\n\nFeatures:\n  1. iOS and Android supported via identical JavaScript API.\n  2. Android in pure Java and Native modes\n  3. SQL transactions\n  4. JavaScript interface via plain callbacks or Promises.\n  5. Pre-populated SQLite database import from application bundle and sandbox\n  6. Windows supports callback API, identical to iOS and Android\n\nThere are sample apps provided in test directory that can be used in with the AwesomeProject generated by React Native. All you have to do is to copy one of those files into your AwesomeProject replacing index.ios.js.\n\nPlease let me know your projects that use these SQLite React Native modules. I will list them in the reference section. If there are any features that you think would benefit this library please post them.\n\nThe library has been tested with React 16.2 (and earlier) and XCode 7,8,9 - it works fine out of the box without any need for tweaks or code changes. For XCode 7,8 vs. XCode 6 the only difference is that sqlite ios library name suffix is tbd instead of dylib.\n\nVersion 3.2 is the first version compatible with RN 0.40.\n\n# Installation\n```\n  npm install --save react-native-sqlite-storage\n```\nThen follow the instructions for your platform to link react-native-sqlite-storage into your project\n\n## Promises\nTo enable promises, run \n```javascript\nSQLite.enablePromise(true);\n```\n\n## iOS\n#### Standard Method\n** React Native 0.60 and above **\nRun `cd ios && pod install && cd ..`. Linking is not required in React Native 0.60 and above\n\n** React Native 0.59 and below **\n\n#### Step 1. Install Dependencies\n\n##### With CocoaPods:\n\nAdd this to your Podfile which should be located inside the ios project subdirectory\n```ruby\npod 'React', :path => '../node_modules/react-native'\npod 'react-native-sqlite-storage', :path => '../node_modules/react-native-sqlite-storage'\n```\nOr use the sample Podfile included in the package by copying it over to ios subdirectory and replacing AwesomeProject inside of it with the name of your RN project.\n\nRefresh the Pods installation\n```ruby\npod install\n```\nOR\n```ruby\npod update\n```\n\nDone, skip to Step 2.\n\n##### Without CocoaPods:\n\nThis command should be executed in the root directory of your RN project\n```shell\nreact-native link\n```\n\nrnpm and xcode are dependencies of this project and should get installed with the module but in case there are issue running rnpm link and rnpm/xcode are not already installed you can try to install it globally as follows:\n```shell\nnpm -g install rnpm xcode\n```\nAfter linking project should like this:\n\n![alt tag](instructions/after-rnpm.png)\n\n#### Step 1a. If rnpm link does not work for you you can try manually linking according to the instructions below:\n\n\n##### Drag the SQLite Xcode project as a dependency project into your React Native XCode project\n\n![alt tag](https://raw.github.com/andpor/react-native-sqlite-storage/master/instructions/libs.png)\n\n##### XCode SQLite libraries dependency set up\n\nAdd libSQLite.a (from Workspace location) to the required Libraries and Frameworks. Also add sqlite3.0.tbd (XCode 7) or libsqlite3.0.dylib (XCode 6 and earlier) in the same fashion using Required Libraries view (Do not just add them manually as the build paths will not be properly set)\n\n![alt tag](https://raw.github.com/andpor/react-native-sqlite-storage/master/instructions/addlibs.png)\n\n#### Step 2. Application JavaScript require\n\nAdd var SQLite = require('react-native-sqlite-storage') to your index.ios.js\n\n![alt tag](instructions/require.png)\n\n#### Step 3. Write application JavaScript code using the SQLite plugin\n\nAdd JS application code to use SQLite API in your index.ios.js etc. Here is some sample code. For full working example see test/index.ios.callback.js. Please note that Promise based API is now supported as well with full examples in the working React Native app under test/index.ios.promise.js\n\n```javascript\nerrorCB(err) {\n  console.log(\"SQL Error: \" + err);\n},\n\nsuccessCB() {\n  console.log(\"SQL executed fine\");\n},\n\nopenCB() {\n  console.log(\"Database OPENED\");\n},\n\nvar db = SQLite.openDatabase(\"test.db\", \"1.0\", \"Test Database\", 200000, openCB, errorCB);\ndb.transaction((tx) => {\n  tx.executeSql('SELECT * FROM Employees a, Departments b WHERE a.department = b.department_id', [], (tx, results) => {\n      console.log(\"Query completed\");\n\n      // Get rows with Web SQL Database spec compliance.\n\n      var len = results.rows.length;\n      for (let i = 0; i < len; i++) {\n        let row = results.rows.item(i);\n        console.log(`Employee name: ${row.name}, Dept Name: ${row.deptName}`);\n      }\n\n      // Alternatively, you can use the non-standard raw method.\n\n      /*\n        let rows = results.rows.raw(); // shallow copy of rows Array\n\n        rows.map(row => console.log(`Employee name: ${row.name}, Dept Name: ${row.deptName}`));\n      */\n    });\n});\n```\n\n# How to use (Android):\n\n** React Native 0.60 and above **\nIf you would like to use the devices SQLite there are no extra steps.\nHowever, if you would like to use the SQLite bundled with this library (includes support for FTS5), add the following to your `react-native.config.js`\n\n```js\nmodule.exports = {\n  ...,\n  dependencies: {\n    ...,\n    \"react-native-sqlite-storage\": {\n      platforms: {\n        android: {\n          sourceDir:\n            \"../node_modules/react-native-sqlite-storage/platforms/android-native\",\n          packageImportPath: \"import io.liteglue.SQLitePluginPackage;\",\n          packageInstance: \"new SQLitePluginPackage()\"\n        }\n      }\n    }\n    ...\n  }\n  ...\n};\n```\n\n** React Native 0.59 and below **\n\n#### Step 1 - Update Gradle Settings (located under Gradle Settings in Project Panel)\n\n```gradle\n// file: android/settings.gradle\n...\n\ninclude ':react-native-sqlite-storage'\nproject(':react-native-sqlite-storage').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-sqlite-storage/platforms/android') // react-native-sqlite-storage >= 4.0.0\n// IMPORTANT: if you are working with a version less than 4.0.0 the project directory is '../node_modules/react-native-sqlite-storage/src/android'\n```\n\n#### Step 2 - Update app module Gradle Build script (located under Gradle Settings in Project Panel)\n\n```gradle\n// file: android/app/build.gradle\n...\n\ndependencies {\n    ...\n    implementation project(':react-native-sqlite-storage')\n}\n```\n\n#### Step 3 - Register React Package (this should work on React version but if it does not , try the ReactActivity based approach. Note: for version 3.0.0 and below you would have to pass in the instance of your Activity to the SQLitePluginPackage constructor\n\n```java\n...\nimport org.pgsqlite.SQLitePluginPackage;\n\npublic class MainActivity extends Activity implements DefaultHardwareBackBtnHandler {\n\n    private ReactInstanceManager mReactInstanceManager;\n    private ReactRootView mReactRootView;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        mReactRootView = new ReactRootView(this);\n        mReactInstanceManager = ReactInstanceManager.builder()\n                .setApplication(getApplication())\n                .setBundleAssetName(\"index.android.bundle\")  // this is dependant on how you name you JS files, example assumes index.android.js\n                .setJSMainModuleName(\"index.android\")        // this is dependant on how you name you JS files, example assumes index.android.js\n                .addPackage(new MainReactPackage())\n                .addPackage(new SQLitePluginPackage())       // register SQLite Plugin here\n                .setUseDeveloperSupport(BuildConfig.DEBUG)\n                .setInitialLifecycleState(LifecycleState.RESUMED)\n                .build();\n        mReactRootView.startReactApplication(mReactInstanceManager, \"AwesomeProject\", null); //change \"AwesomeProject\" to name of your app\n        setContentView(mReactRootView);\n    }\n...\n\n```\n\nAlternative approach on newer versions of React Native (0.18+). Note: for version 3.0.0 and below you would have to pass in the instance of your Activity to the SQLitePluginPackage constructor\n\n```java\nimport org.pgsqlite.SQLitePluginPackage;\n\npublic class MainApplication extends Application implements ReactApplication {\n  ......\n\n  /**\n   * A list of packages used by the app. If the app uses additional views\n   * or modules besides the default ones, add more packages here.\n   */\n    @Override\n    protected List<ReactPackage> getPackages() {\n      return Arrays.<ReactPackage>asList(\n        new SQLitePluginPackage(),   // register SQLite Plugin here\n        new MainReactPackage());\n    }\n}\n```\n\n#### Step 4 - Require and use in Javascript - see full examples (callbacks and Promise) in test directory.\n\n```js\n// file: index.android.js\n\nvar React = require('react-native');\nvar SQLite = require('react-native-sqlite-storage')\n...\n```\n\n## Windows\n** RNW 0.63 with Autolinking and above **\n\nNo manual steps required\n\n** React Native 0.62 **\n### Step 1: Update the solution file\n\nAdd the `SQLitePlugin` project to your solution.\n\n1. Open the solution in Visual Studio 2019\n2. Right-click Solution icon in Solution Explorer > Add > Existing Project\n3. Select `node_modules\\react-native-sqlite-storage\\platforms\\windows\\SQLitePlugin\\SQLitePlugin.vcxproj`\n\n### Step 2: Update the .vcxproj file\n\nAdd a reference to `SQLitePlugin` to your main application project. From Visual Studio 2019:\n\n1. Right-click main application project > Add > Reference... \n2. Check `SQLitePlugin` from Solution Projects\n\n\n### Step 3: Update the `pch.h` file\n\nAdd `#include \"winrt/SQLitePlugin.h\"`.\n\n### Step 4: Register the package in `App.cpp`\n\nAdd `PackageProviders().Append(winrt::SQLitePlugin::ReactPackageProvider());` before `InitializeComponent();`.\n\nRefer to this guide for more details: https://microsoft.github.io/react-native-windows/docs/next/native-modules-using\n\n\n## Setting up your project to import a pre-populated SQLite database from application for iOS\n\n#### Step 1 - Create 'www' folder.\n\nCreate a folder called 'www' (yes must be called precisely that else things won't work) in the project folder via Finder\n\n#### Step 2 - Create the database file\n\nCopy/paste your pre-populated database file into the 'www' folder. Give it the same name you are going to use in openDatabase call in your application\n\n#### Step 3 - Add file to project\n\nin XCode, right click on the main folder and select Add Files to 'your project name'\n\n![alt tag](https://raw.github.com/andpor/react-native-sqlite-storage/master/instructions/addFilesToProject.png)\n\n#### Step 4 - Choose files to add\n\nIn the Add Files dialog, navigate to the 'www' directory you created in Step 1, select it, make sure you check the option to Create Folder Reference\n\n![alt tag](https://raw.github.com/andpor/react-native-sqlite-storage/master/instructions/addFilesToProjectSelect.png)\n\n#### Step 5 - Verify project structure\n\nEnsure your project structure after previous steps are executed looks like this\n\n![alt tag](https://raw.github.com/andpor/react-native-sqlite-storage/master/instructions/projectStructureAfter.png)\n\n### Step 6 - Adjust openDatabase call\n\nModify you openDatabase call in your application adding createFromLocation param. If you named your database file in step 2 'testDB' the openDatabase call should look like something like this:\n```js\n\n  ...\n  1.SQLite.openDatabase({name : \"testDB\", createFromLocation : 1}, okCallback,errorCallback);\n  // default - if your folder is called www and data file is named the same as the dbName - testDB in this example\n  2.SQLite.openDatabase({name : \"testDB\", createFromLocation : \"~data/mydbfile.sqlite\"}, okCallback,errorCallback);\n  // if your folder is called data rather than www or your filename does not match the name of the db\n  3.SQLite.openDatabase({name : \"testDB\", createFromLocation : \"/data/mydbfile.sqlite\"}, okCallback,errorCallback);\n  // if your folder is not in app bundle but in app sandbox i.e. downloaded from some remote location.\n  ...\n\n```\nFor Android, the www directory is always relative to the assets directory for the app: src/main/assets\n\nEnjoy!\n\n## Opening a database\n\nOpening a database is slightly different between iOS and Android. Where as on Android the location of the database file is fixed, there are three choices of where the database file can be located on iOS. The 'location' parameter you provide to openDatabase call indicated where you would like the file to be created. This parameter is neglected on Android.\n\nWARNING: the default location on iOS has changed in version 3.0.0 - it is now a no-sync location as mandated by Apple so the release is backward incompatible.\n\n\nTo open a database in default no-sync location (affects iOS *only*)::\n\n```js\nSQLite.openDatabase({name: 'my.db', location: 'default'}, successcb, errorcb);\n```\n\nTo specify a different location (affects iOS *only*):\n\n```js\nSQLite.openDatabase({name: 'my.db', location: 'Library'}, successcb, errorcb);\n```\n\nwhere the `location` option may be set to one of the following choices:\n- `default`: `Library/LocalDatabase` subdirectory - *NOT* visible to iTunes and *NOT* backed up by iCloud\n- `Library`: `Library` subdirectory - backed up by iCloud, *NOT* visible to iTunes\n- `Documents`: `Documents` subdirectory - visible to iTunes and backed up by iCloud\n- `Shared`:  app group's shared container - *see next section*\n\nThe original webSql style openDatabase still works and the location will implicitly default to 'default' option:\n\n```js\nSQLite.openDatabase(\"myDatabase.db\", \"1.0\", \"Demo\", -1);\n```\n\n## Opening a database in an App Group's Shared Container (iOS)\n\nIf you have an iOS app extension which needs to share access to the same DB instance as your main app, you must use the shared container of a registered app group.\n\nAssuming you have already set up an app group and turned on the \"App Groups\" entitlement of both the main app and app extension, setting them to the same app group name, the following extra steps must be taken:\n\n#### Step 1 - supply your app group name in all needed `Info.plist`s\n\nIn both `ios/MY_APP_NAME/Info.plist` and `ios/MY_APP_EXT_NAME/Info.plist` (along with any other app extensions you may have), you simply need to add the `AppGroupName` key to the main dictionary with your app group name as the string value:\n\n```xml\n<plist version=\"1.0\">\n<dict>\n  <!-- ... -->\n  <key>AppGroupName</key>\n  <string>MY_APP_GROUP_NAME</string>\n  <!-- ... -->\n</dict>\n</plist>\n```\n\n#### Step 2 - set shared database location\n\nWhen calling `SQLite.openDatabase` in your React Native code, you need to set the `location` param to `'Shared'`:\n\n```js\nSQLite.openDatabase({name: 'my.db', location: 'Shared'}, successcb, errorcb);\n```\n\n## Importing a pre-populated database.\n\nYou can import an existing - prepopulated database file into your application. Depending on your instructions in openDatabase call, the sqlite-storage will look at different places to locate you pre-populated database file.\n\n\nUse this flavor of openDatabase call, if your folder is called www and data file is named the same as the dbName - testDB in this example\n\n```js\nSQLite.openDatabase({name : \"testDB\", createFromLocation : 1}, okCallback,errorCallback);\n```\n\nUse this flavor of openDatabase call if your folder is called data rather than www or your filename does not match the name of the db. In this case db is named testDB but the file is mydbfile.sqlite which is located in a data subdirectory of www\n\n```js\nSQLite.openDatabase({name : \"testDB\", createFromLocation : \"~data/mydbfile.sqlite\"}, okCallback,errorCallback);\n```\n\nUse this flavor of openDatabase call if your folder is not in application bundle but in app sandbox i.e. downloaded from some remote location. In this case the source file is located in data subdirectory of Documents location (iOS) or FilesDir (Android).\n\n```js\nSQLite.openDatabase({name : \"testDB\", createFromLocation : \"/data/mydbfile.sqlite\"}, okCallback,errorCallback);\n```\n\n## Additional options for pre-populated database file\n\nYou can provide additional instructions to sqlite-storage to tell it how to handle your pre-populated database file. By default, the source file is copied over to the internal location which works in most cases but sometimes this is not really an option particularly when the source db file is large. In such situations you can tell sqlite-storage you do not want to copy the file but rather use it in read-only fashion via direct access. You accomplish this by providing an additional optional readOnly parameter to openDatabase call\n\n```js\nSQLite.openDatabase({name : \"testDB\", readOnly: true, createFromLocation : \"/data/mydbfile.sqlite\"}, okCallback,errorCallback);\n```\n\nNote that in this case, the source db file will be open in read-only mode and no updates will be allowed. You cannot delete a database that was open with readOnly option. For Android, the read only option works with pre-populated db files located in FilesDir directory because all other assets are never physically located on the file system but rather read directly from the app bundle.\n\n## Attaching another database\n\nSqlite3 offers the capability to attach another database to an existing database-instance, i.e. for making cross database JOINs available.\nThis feature allows to SELECT and JOIN tables over multiple databases with only one statement and only one database connection.\nTo archieve this, you need to open both databases and to call the attach()-method of the destination (or master) -database to the other ones.\n\n```js\nlet dbMaster, dbSecond;\n\ndbSecond = SQLite.openDatabase({name: 'second'},\n  (db) => {\n    dbMaster = SQLite.openDatabase({name: 'master'},\n      (db) => {\n        dbMaster.attach( \"second\", \"second\", () => console.log(\"Database attached successfully\"), () => console.log(\"ERROR\"))\n      },\n      (err) => console.log(\"Error on opening database 'master'\", err)\n    );\n  },\n  (err) => console.log(\"Error on opening database 'second'\", err)\n);\n```\n\nThe first argument of attach() is the name of the database, which is used in SQLite.openDatabase(). The second argument is the alias, that is used to query on tables of the attached database.\n\nThe following statement would select data from the master database and include the \"second\"-database within a simple SELECT/JOIN-statement:\n\n```sql\nSELECT * FROM user INNER JOIN second.subscriptions s ON s.user_id = user.id\n```\n\nTo detach a database, just use the detach()-method:\n\n```js\ndbMaster.detach( 'second', successCallback, errorCallback );\n```\n\nFor sure, their is also Promise-support available for attach() and detach(), as shown in the example-application under the\ndirectory \"examples\".\n\n# Original Cordova SQLite Bindings from Chris Brody and Davide Bertola\nhttps://github.com/litehelpers/Cordova-sqlite-storage\n\nThe issues and limitations for the actual SQLite can be found on this site.\n\n## Issues\n\n1. Android binds all numeric SQL input values to double. This is due to the underlying React Native limitation where only a Numeric type is available on the interface point making it ambiguous to distinguish integers from doubles. Once I figure out the proper way to do this I will update the codebase [(Issue #4141)] (https://github.com/facebook/react-native/issues/4141).\n"
 },
 {
  "repo": "isl-org/OpenBot",
  "language": "Java",
  "readme_contents": "<a href=\"https://www.openbot.org/\" target=\"_blank\">\n  <img align=\"center\" alt=\"Banner\" width=\"100%\" src=\"docs/images/banner.jpg\" />\n</a>\n\n<h1 align=\"center\"><a>Turning Smartphones into Robots</a></h1>\n\n<p align=\"center\">\n   <img alt=\"GitHub build\" src=\"https://img.shields.io/github/workflow/status/Intel-ISL/OpenBot/Java%20CI%20with%20Gradle\"></a>\n   <img alt=\"GitHub issues\" src=\"https://img.shields.io/github/issues/intel-isl/OpenBot\"></a>\n   <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/intel-isl/OpenBot\"></a>\n   <img alt=\"GitHub forks\" src=\"https://img.shields.io/github/forks/intel-isl/OpenBot\"></a>\n   <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/intel-isl/OpenBot\"></a>\n   <img alt=\"Github downloads\" src=\"https://img.shields.io/github/downloads/Intel-ISL/OpenBot/total\"></a>\n   <img alt=\"Github size\" src=\"https://img.shields.io/github/repo-size/Intel-ISL/OpenBot\"></a>\n   <img alt=\"Github license\" src=\"https://img.shields.io/github/license/intel-isl/OpenBot\"></a>\n</p>\n\n<p align=\"center\">\n  <span>English</span> |\n  <a href=\"README_CN.md\">\u7b80\u4f53\u4e2d\u6587</a>\n</p>\n\nOpenBot leverages smartphones as brains for low-cost robots. We have designed a small electric vehicle that costs about $50 and serves as a robot body. Our software stack for Android smartphones supports advanced robotics workloads such as person following and real-time autonomous navigation.\n\n## Get started with OpenBot\n- Build your own [Robot Body](body/README.md)\n- Flash the [Arduino Firmware](firmware/README.md)\n- Install the [Android Apps](android/README.md)\n- Drive the robot via a [Controller](controller/README.md)\n- Train your own [Driving Policy](policy/README.md)\n\n## Get the source code\n\n- You can download the repo as a [zip file](https://github.com/intel-isl/OpenBot/archive/master.zip) and extract it into a folder of your choice.\n- You can clone the OpenBot repository from GitHub with the following command:\n    ```bash\n    git clone https://github.com/intel-isl/OpenBot.git\n    ```\n- You can fork the OpenBot repository and then clone your local copy. This is recommended, especially if you want to [contribute](CONTRIBUTING.md).\n\n## Videos\n\n<a href=\"https://www.youtube.com/watch?v=RbzPXywJifA\" >\n  <img align=\"center\" width=\"300\" alt=\"youtube video\" src=\"https://img.youtube.com/vi/RbzPXywJifA/hqdefault.jpg\" />\n</a>\n\n<a href=\"https://www.youtube.com/watch?v=qc8hFLyWDOM\" >\n  <img align=\"center\" width=\"300\" alt=\"youtube video\" src=\"https://img.youtube.com/vi/qc8hFLyWDOM/hqdefault.jpg\" />\n</a>\n\n## Cool projects using OpenBot\n\nThere are a lot of cool projects using OpenBot already. Below is a small selection. Click on the images to be redirected to the respective projects.\n\n<p float=\"left\">\n  <a href=\"https://www.thingiverse.com/thing:4670884\" target=\"_blank\">\n    <img alt=\"Tank OpenBot\" width=\"24%\" src=\"docs/images/openbot_tank.jpg\" />\n  </a>\n  <a href=\"https://diyrobocars.com/2020/12/14/an-improved-version-of-the-intel-openbot\" target=\"_blank\">\n    <img alt=\"2WD OpenBot\" width=\"24%\" src=\"docs/images/openbot_2wd.jpg\" />\n  </a>\n  <a href=\"https://custom-build-robots.com/raspberry-pi-robot-cars/openbot-your-smartphone-controls-a-robot-car-introduction/13860?lang=en\" target=\"_blank\">\n    <img alt=\"Cardboard OpenBot\" width=\"24%\" src=\"docs/images/chassis_cardboard_1.jpg\" />\n  </a>\n  <a href=\"https://www.youtube.com/watch?v=PEj8jWapGt4\" target=\"_blank\">\n    <img alt=\"Baby Yoda OpenBot\" width=\"24%\" src=\"docs/images/openbot_yoda.jpg\" />\n  </a>\n</p>\n\n## Contact\n\n- Join our [Slack](https://join.slack.com/t/openbot-community/shared_invite/zt-jl8ygxqt-WNRNi9yzh7Lu60qui6Nh6w) channel to connect with the OpenBot community.\n- Contact us via [Email](mailto:openbot.team@gmail.com)\n\n## Contribute\n\nPlease read the [contribution guidelines](CONTRIBUTING.md). If you are not sure where to start have a look at the [open issues](https://github.com/intel-isl/OpenBot/issues).\n\n## Citation\n\nPlease cite our [paper](https://arxiv.org/abs/2008.10631) if you use OpenBot.\n\n```bib\n@inproceedings{mueller2021openbot,\n    title     = {OpenBot: Turning Smartphones into Robots},\n    author    = {M{\\\"u}ller, Matthias and Koltun, Vladlen},\n    booktitle = {Proceedings of the International Conference on Robotics and Automation (ICRA)},\n    year = {2021},\n}\n```\n\n<a href=\"https://www.openbot.org//\" target=\"_blank\">\n  <img align=\"center\" alt=\"Footer\" width=\"100%\" src=\"docs/images/footer.gif\" />\n</a>\n"
 },
 {
  "repo": "nanomsg/nng",
  "language": "C",
  "readme_contents": "ifdef::env-github[]\n:note-caption: :information_source:\n:important-caption: :heavy_exclamation_mark:\nendif::[]\n= nng - nanomsg-next-gen\n\n// Note: This README is optimized for display with Asciidoctor, or\n// on the github status page.  An HTML version is in the same directory\n// and may be more pleasantly formatted for human readers (when opened\n// in a browser).\n\n// Note: If you're updating this file, don't forget to re-run asciidoctor\n// to update the aforementioned HTML file!\n\nimage:https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/badges/StandWithUkraine.svg[Stand With Ukraine,link=\"https://stand-with-ukraine.pp.ua\"]\nimage:https://img.shields.io/github/workflow/status/nanomsg/nng/linux/master?logoColor=grey&logo=ubuntu&label=[Linux Status,link=\"https://github.com/nanomsg/nng/actions\"]\nimage:https://img.shields.io/github/workflow/status/nanomsg/nng/windows/master?logoColor=grey&logo=windows&label=[Windows Status,link=\"https://github.com/nanomsg/nng/actions\"]\nimage:https://img.shields.io/github/workflow/status/nanomsg/nng/darwin/master?logoColor=grey&logo=apple&label=[macOS Status,link=\"https://github.com/nanomsg/nng/actions\"]\nimage:https://img.shields.io/codecov/c/github/nanomsg/nng?logo=codecov&logoColor=grey&label=[Coverage,link=\"https://codecov.io/gh/nanomsg/nng\"]\nimage:https://img.shields.io/lgtm/grade/cpp/github/nanomsg/nng?logo=lgtm&logoColor=grey&label=[LGTM,link=\"https://lgtm.com/projects/g/nanomsg/nng/?mode=list\"]\nimage:https://img.shields.io/discord/639573728212156478?label=&logo=discord[Discord,link=\"https://discord.gg/Xnac6b9\"]\nimage:https://img.shields.io/static/v1?label=&message=docs&logo=asciidoctor&logoColor=silver&color=blue[Manual,link=\"https://nng.nanomsg.org/man\"]\nimage:https://img.shields.io/github/license/nanomsg/nng.svg?logoColor=silver&logo=open-source-initiative&label=&color=blue[MIT License,link=\"https://github.com/nanomsg/nng/blob/master/LICENSE.txt\"]\nimage:https://img.shields.io/github/v/tag/nanomsg/nng.svg?logo=github&label=[Latest version,link=\"https://github.com/nanomsg/nng/releases\"]\n\nPlease see <<UKRAINE#,here>> for an important message for the people of Russia.\n\nNOTE: If you are looking for the legacy version of nanomsg, please\nsee the https://github.com/nanomsg/nanomsg[nanomsg] repository.\n\nThis project is a rewrite of the Scalability Protocols\nlibrary known as https://github.com/nanomsg/nanomsg[libnanomsg],\nand adds significant new capabilities, while retaining\ncompatibility with the original.\n\nIt may help to think of this as \"nanomsg-next-generation\".\n\n== NNG: Lightweight Messaging Library\n\nNNG, like its predecessors http://nanomsg.org[nanomsg] (and to some extent\nhttp://zeromq.org/[ZeroMQ]), is a lightweight, broker-less library,\noffering a simple API to solve common recurring messaging problems,\nsuch as publish/subscribe, RPC-style request/reply, or service discovery.\nThe API frees the programmer from worrying about details like connection\nmanagement, retries, and other common considerations, so that they\ncan focus on the application instead of the plumbing.\n\nNNG is implemented in C, requiring only C99 and CMake to build.\nIt can be built as a shared or a static library, and is readily\nembeddable.  It is also designed to be easy to port to new platforms\nif your platform is not already supported.\n\n== License\n\nNNG is licensed under a liberal, and commercial friendly, MIT license.\nThe goal to the license is to minimize friction in adoption, use, and\ncontribution.\n\n== Enhancements (Relative to nanomsg)\n\nHere are areas where this project improves on \"nanomsg\":\n\n[horizontal]\n*Reliability*:: NNG is designed for production use from the beginning.  Every\nerror case is considered, and it is designed to avoid crashing except in cases\nof gross developer error.  (Hopefully we don't have any of these in our own\ncode.)\n\n*Scalability*:: NNG scales out to engage multiple cores using a bespoke\nasynchronous I/O framework, using thread pools to spread load without\nexceeding typical system limits.\n\n*Maintainability*:: NNG's architecture is designed to be modular and\neasily grasped by developers unfamiliar with the code base.  The code\nis also well documented.\n\n*Extensibility*:: Because it avoids ties to file descriptors, and avoids\nconfusing interlocking state machines, it is easier to add new protocols\nand transports to NNG.  This was demonstrated by the addition of the\nTLS and ZeroTier transports.\n\n*Security*:: NNG provides TLS 1.2 and ZeroTier transports, offering\nsupport for robust and industry standard authentication and encryption.\nIn addition, it is hardened to be resilient against malicious attackers,\nwith special consideration given to use in a hostile Internet.\n\n*Usability*:: NNG eschews slavish adherence parts of the more complex and\nless well understood POSIX APIs, while adopting the semantics that are\nfamiliar and useful.  New APIs are intuitive, and the optional support\nfor separating protocol context and state from sockets makes creating\nconcurrent applications vastly simpler than previously possible.\n\n== Compatibility\n\nThis project offers both wire compatibility and API compatibility,\nso most nanomsg users can begin using NNG right away.\n\nExisting nanomsg and https://github.com/nanomsg/mangos[mangos] applications\ncan inter-operate with NNG applications automatically.\n\nThat said, there are some areas where legacy nanomsg still offers\ncapabilities NNG lacks -- specifically enhanced observability with\nstatistics, and tunable prioritization of different destinations\nare missing, but will be added in a future release.\n\nAdditionally, some API capabilities that are useful for foreign\nlanguage bindings are not implemented yet.\n\nSome simple single threaded, synchronous applications may perform better under\nlegacy nanomsg than under NNG.  (We believe that these applications are the\nleast commonly deployed, and least interesting from a performance perspective.\nNNG's internal design is slightly less efficient in such scenarios, but it\ngreatly benefits when concurrency or when multiple sockets or network peers\nare involved.)\n\n== Supported Platforms\n\nNNG supports Linux, macOS, Windows (Vista or better), illumos, Solaris,\nFreeBSD, Android, and iOS.  Most other POSIX platforms should work out of\nthe box but have not been tested.  Very old versions of otherwise supported\nplatforms might not work.\n\n== Requirements\n\nTo build this project, you will need a C99 compatible compiler and\nhttp://www.cmake.org[CMake] version 3.13 or newer.\n\nWe recommend using the https://ninja-build.org[Ninja] build\nsystem (pass \"-G Ninja\" to CMake) when you can.\n(And not just because Ninja sounds like \"NNG\" -- it's also\nblindingly fast and has made our lives as developers measurably better.)\n\nIf you want to build with TLS support you will also need\nhttps://tls.mbed.org[mbedTLS].  See <<docs/BUILD_TLS.adoc#>> for details.\n\n== Quick Start\n\nWith a Linux or UNIX environment:\n\n[source,sh]\n----\n  $ mkdir build\n  $ cd build\n  $ cmake -G Ninja ..\n  $ ninja\n  $ ninja test\n  $ ninja install\n----\n\n== API Documentation\n\nThe API documentation is provided in Asciidoc format in the\n`docs/man` subdirectory, and also\nhttps://nanomsg.github.io/nng[online].\nThe <<docs/man/libnng.3.adoc#,libnng(3)>> page is a good starting point.\n\nYou can also purchase a copy of the \nhttp://staysail.tech/books/nng_reference/index.html[__NNG Reference Manual__].\n(It is published in both electronic and printed formats.)\nPurchases of the book help fund continued development of NNG.\n\n== Example Programs\n\nSome demonstration programs have been created to help serve as examples.\nThese are located in the `demo` directory.\n\n== Legacy Compatibility\n\nA legacy libnanomsg compatible API is available, and while it offers\nless capability than the modern NNG API, it may serve as a transition aid.\nPlease see <<docs/man/nng_compat.3compat.adoc#,nng_compat(3)>> for details.\n\n== Commercial Support\n\nCommercial support for NNG is available.\n\nPlease contact mailto:info@staysail.tech[Staysail Systems, Inc.] to\ninquire further.\n\n== Commercial Sponsors\n\nThe development of NNG has been made possible through the generous\nsponsorship of https://www.capitar.com[Capitar IT Group BV] and\nhttp://staysail.tech[Staysail Systems, Inc.].\n"
 },
 {
  "repo": "ohler55/oj",
  "language": "C",
  "readme_contents": "# [![{}j](http://www.ohler.com/dev/images/oj_comet_64.svg)](http://www.ohler.com/oj) gem\n\n[![Build Status](https://img.shields.io/github/workflow/status/ohler55/oj/CI?logo=github)](https://github.com/ohler55/oj/actions/workflows/CI.yml)\n![Gem](https://img.shields.io/gem/v/oj.svg)\n![Gem](https://img.shields.io/gem/dt/oj.svg)\n[![SemVer compatibility](https://api.dependabot.com/badges/compatibility_score?dependency-name=oj&package-manager=bundler&version-scheme=semver)](https://dependabot.com/compatibility-score.html?dependency-name=oj&package-manager=bundler&version-scheme=semver)\n[![TideLift](https://tidelift.com/badges/github/ohler55/oj)](https://tidelift.com/subscription/pkg/rubygems-oj?utm_source=rubygems-oj&utm_medium=referral&utm_campaign=readme)\n\nA *fast* JSON parser and Object marshaller as a Ruby gem.\n\nVersion 3.13 is out with a much faster parser (`Oj::Parser`) and option isolation.\n\n## Using\n\n```ruby\nrequire 'oj'\n\nh = { 'one' => 1, 'array' => [ true, false ] }\njson = Oj.dump(h)\n\n# json =\n# {\n#   \"one\":1,\n#   \"array\":[\n#     true,\n#     false\n#   ]\n# }\n\nh2 = Oj.load(json)\nputs \"Same? #{h == h2}\"\n# true\n```\n\n## Installation\n```\ngem install oj\n```\n\nor in Bundler:\n\n```\ngem 'oj'\n```\n\n## Rails and json quickstart\n\nSee the Quickstart sections of the [Rails](pages/Rails.md) and [json](pages/JsonGem.md) docs.\n\n## multi_json\n\nCode which uses [multi_json](https://github.com/intridea/multi_json)\nwill automatically prefer Oj if it is installed.\n\n## Support\n\n[Get supported Oj with a Tidelift Subscription.](https://tidelift.com/subscription/pkg/rubygems-oj?utm_source=rubygems-oj&utm_medium=referral&utm_campaign=readme) Security updates are [supported](https://tidelift.com/security).\n\n## Further Reading\n\nFor more details on options, modes, advanced features, and more follow these\nlinks.\n\n - [{file:Options.md}](pages/Options.md) for parse and dump options.\n - [{file:Modes.md}](pages/Modes.md) for details on modes for strict JSON compliance, mimicking the JSON gem, and mimicking Rails and ActiveSupport behavior.\n - [{file:JsonGem.md}](pages/JsonGem.md) includes more details on json gem compatibility and use.\n - [{file:Rails.md}](pages/Rails.md) includes more details on Rails and ActiveSupport compatibility and use.\n - [{file:Custom.md}](pages/Custom.md) includes more details on Custom mode.\n - [{file:Encoding.md}](pages/Encoding.md) describes the :object encoding format.\n - [{file:Compatibility.md}](pages/Compatibility.md) lists current compatibility with Rubys and Rails.\n - [{file:Advanced.md}](pages/Advanced.md) for fast parser and marshalling features.\n - [{file:Security.md}](pages/Security.md) for security considerations.\n\n## Releases\n\nSee [{file:CHANGELOG.md}](CHANGELOG.md) and [{file:RELEASE_NOTES.md}](RELEASE_NOTES.md)\n\n## Links\n\n- *Documentation*: http://www.ohler.com/oj/doc, http://rubydoc.info/gems/oj\n\n- *GitHub* *repo*: https://github.com/ohler55/oj\n\n- *RubyGems* *repo*: https://rubygems.org/gems/oj\n\nFollow [@peterohler on Twitter](http://twitter.com/peterohler) for announcements and news about the Oj gem.\n\n#### Performance Comparisons\n\n - [Oj Strict Mode Performance](http://www.ohler.com/dev/oj_misc/performance_strict.html) compares Oj strict mode parser performance to other JSON parsers.\n\n - [Oj Compat Mode Performance](http://www.ohler.com/dev/oj_misc/performance_compat.html) compares Oj compat mode parser performance to other JSON parsers.\n\n - [Oj Object Mode Performance](http://www.ohler.com/dev/oj_misc/performance_object.html) compares Oj object mode parser performance to other marshallers.\n\n - [Oj Callback Performance](http://www.ohler.com/dev/oj_misc/performance_callback.html) compares Oj callback parser performance to other JSON parsers.\n\n#### Links of Interest\n\n - *Fast XML parser and marshaller on RubyGems*: https://rubygems.org/gems/ox\n\n - *Fast XML parser and marshaller on GitHub*: https://github.com/ohler55/ox\n\n - [Need for Speed](http://www.ohler.com/dev/need_for_speed/need_for_speed.html) for an overview of how Oj::Doc was designed.\n\n - *OjC, a C JSON parser*: https://www.ohler.com/ojc also at https://github.com/ohler55/ojc\n\n - *Agoo, a high performance Ruby web server supporting GraphQL on GitHub*: https://github.com/ohler55/agoo\n\n - *Agoo-C, a high performance C web server supporting GraphQL on GitHub*: https://github.com/ohler55/agoo-c\n\n#### Contributing\n\n+ Provide a Pull Request off the `develop` branch.\n+ Report a bug\n+ Suggest an idea\n+ Code is now formatted with the clang-format tool with the configuration file in the root of the repo.\n"
 },
 {
  "repo": "openai/retro",
  "language": "C",
  "readme_contents": "**Status:** Maintenance (expect bug fixes and minor updates)\n\n# Gym Retro\n\nGym Retro lets you turn classic video games into [Gym](https://gym.openai.com/) environments for reinforcement learning and comes with integrations for ~1000 games.  It uses various emulators that support the [Libretro API](https://www.libretro.com/index.php/api/), making it fairly easy to add new emulators.\n\nSupported platforms:\n\n- Windows 7, 8, 10\n- macOS 10.13 (High Sierra), 10.14 (Mojave)\n- Linux (manylinux1)\n\nCPU with `SSSE3` or better\n\nSupported Pythons:\n\n- 3.6\n- 3.7\n- 3.8\n\nEach game integration has files listing memory locations for in-game variables, reward functions based on those variables, episode end conditions, savestates at the beginning of levels and a file containing hashes of ROMs that work with these files.\n\nPlease note that ROMs are not included and you must obtain them yourself.  Most ROM hashes are sourced from their respective No-Intro SHA-1 sums.\n\n# Documentation\n\nDocumentation is available at https://retro.readthedocs.io/en/latest/\n\nYou should probably start with the [Getting Started Guide](https://retro.readthedocs.io/en/latest/getting_started.html).\n\n# Contributing\n\n[See CONTRIBUTING.md](https://github.com/openai/retro/blob/master/CONTRIBUTING.md)\n\n# Changelog\n\n[See CHANGES.md](https://github.com/openai/retro/blob/master/CHANGES.md)\n\n# Emulated Systems\n\n- Atari\n\t- Atari2600 (via Stella)\n- NEC\n\t- TurboGrafx-16/PC Engine (via Mednafen/Beetle PCE Fast)\n- Nintendo\n\t- Game Boy/Game Boy Color (via gambatte)\n\t- Game Boy Advance (via mGBA)\n\t- Nintendo Entertainment System (via FCEUmm)\n\t- Super Nintendo Entertainment System (via Snes9x)\n- Sega\n\t- GameGear (via Genesis Plus GX)\n\t- Genesis/Mega Drive (via Genesis Plus GX)\n\t- Master System (via Genesis Plus GX)\n\nSee [LICENSES.md](https://github.com/openai/retro/blob/master/LICENSES.md) for information on the licenses of the individual cores.\n\n# Included ROMs\n\nThe following non-commercial ROMs are included with Gym Retro for testing purposes:\n\n- [the 128 sine-dot](http://www.pouet.net/prod.php?which=2762) by Anthrox\n- [Sega Tween](https://pdroms.de/files/gamegear/sega-tween) by Ben Ryves\n- [Happy 10!](http://www.pouet.net/prod.php?which=52716) by Blind IO\n- [512-Colour Test Demo](https://pdroms.de/files/pcengine/512-colour-test-demo) by Chris Covell\n- [Dekadrive](http://www.pouet.net/prod.php?which=67142) by Dekadence\n- [Automaton](https://pdroms.de/files/atari2600/automaton-minigame-compo-2003) by Derek Ledbetter\n- [Fire](http://privat.bahnhof.se/wb800787/gb/demo/64/) by dox\n- [FamiCON intro](http://www.pouet.net/prod.php?which=53497) by dr88\n- [Airstriker](https://pdroms.de/genesis/airstriker-v1-50-genesis-game) by Electrokinesis\n- [Lost Marbles](https://pdroms.de/files/gameboyadvance/lost-marbles) by Vantage\n\n# Citation\n\nPlease cite using the following BibTeX entry:\n\n```\n@article{nichol2018retro,\n  title={Gotta Learn Fast: A New Benchmark for Generalization in RL},\n  author={Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},\n  journal={arXiv preprint arXiv:1804.03720},\n  year={2018}\n}\n```\n"
 },
 {
  "repo": "ThrowTheSwitch/Unity",
  "language": "C",
  "readme_contents": "# Unity Test ![CI][]\n\n__Copyright (c) 2007 - 2021 Unity Project by Mike Karlesky, Mark VanderVoord, and Greg Williams__\n\nWelcome to the Unity Test Project, one of the main projects of ThrowTheSwitch.org.\nUnity Test is a unit testing framework built for C, with a focus on working with embedded toolchains.\n\nThis project is made to test code targetting microcontrollers big and small.\nThe core project is a single C file and a pair of headers, allowing it to the added to your existing build setup without too much headache.\nYou may use any compiler you wish, and may use most existing build systems including Make, CMake, etc.\nIf you'd like to leave the hard work to us, you might be interested in Ceedling, a build tool also by ThrowTheSwitch.org.\n\nIf you're new to Unity, we encourage you to tour the [getting started guide][].\n\n## Getting Started\n\nThe [docs][] folder contains a [getting started guide][] and much more tips about using Unity.\n\n## Unity Assertion Summary\n\nFor the full list, see [UnityAssertionsReference.md][].\n\n### Basic Validity Tests\n\n    TEST_ASSERT_TRUE(condition)\n\nEvaluates whatever code is in condition and fails if it evaluates to false\n\n    TEST_ASSERT_FALSE(condition)\n\nEvaluates whatever code is in condition and fails if it evaluates to true\n\n    TEST_ASSERT(condition)\n\nAnother way of calling `TEST_ASSERT_TRUE`\n\n    TEST_ASSERT_UNLESS(condition)\n\nAnother way of calling `TEST_ASSERT_FALSE`\n\n    TEST_FAIL()\n    TEST_FAIL_MESSAGE(message)\n\nThis test is automatically marked as a failure.\nThe message is output stating why.\n\n### Numerical Assertions: Integers\n\n    TEST_ASSERT_EQUAL_INT(expected, actual)\n    TEST_ASSERT_EQUAL_INT8(expected, actual)\n    TEST_ASSERT_EQUAL_INT16(expected, actual)\n    TEST_ASSERT_EQUAL_INT32(expected, actual)\n    TEST_ASSERT_EQUAL_INT64(expected, actual)\n\nCompare two integers for equality and display errors as signed integers.\nA cast will be performed to your natural integer size so often this can just be used.\nWhen you need to specify the exact size, like when comparing arrays, you can use a specific version:\n\n    TEST_ASSERT_EQUAL_UINT(expected, actual)\n    TEST_ASSERT_EQUAL_UINT8(expected, actual)\n    TEST_ASSERT_EQUAL_UINT16(expected, actual)\n    TEST_ASSERT_EQUAL_UINT32(expected, actual)\n    TEST_ASSERT_EQUAL_UINT64(expected, actual)\n\nCompare two integers for equality and display errors as unsigned integers.\nLike INT, there are variants for different sizes also.\n\n    TEST_ASSERT_EQUAL_HEX(expected, actual)\n    TEST_ASSERT_EQUAL_HEX8(expected, actual)\n    TEST_ASSERT_EQUAL_HEX16(expected, actual)\n    TEST_ASSERT_EQUAL_HEX32(expected, actual)\n    TEST_ASSERT_EQUAL_HEX64(expected, actual)\n\nCompares two integers for equality and display errors as hexadecimal.\nLike the other integer comparisons, you can specify the size... here the size will also effect how many nibbles are shown (for example, `HEX16` will show 4 nibbles).\n\n    TEST_ASSERT_EQUAL(expected, actual)\n\nAnother way of calling TEST_ASSERT_EQUAL_INT\n\n    TEST_ASSERT_INT_WITHIN(delta, expected, actual)\n\nAsserts that the actual value is within plus or minus delta of the expected value.\nThis also comes in size specific variants.\n\n    TEST_ASSERT_GREATER_THAN(threshold, actual)\n\nAsserts that the actual value is greater than the threshold.\nThis also comes in size specific variants.\n\n    TEST_ASSERT_LESS_THAN(threshold, actual)\n\nAsserts that the actual value is less than the threshold.\nThis also comes in size specific variants.\n\n### Arrays\n\n    _ARRAY\n\nYou can append `_ARRAY` to any of these macros to make an array comparison of that type.\nHere you will need to care a bit more about the actual size of the value being checked.\nYou will also specify an additional argument which is the number of elements to compare.\nFor example:\n\n    TEST_ASSERT_EQUAL_HEX8_ARRAY(expected, actual, elements)\n\n    _EACH_EQUAL\n\nAnother array comparison option is to check that EVERY element of an array is equal to a single expected value.\nYou do this by specifying the EACH_EQUAL macro.\nFor example:\n\n    TEST_ASSERT_EACH_EQUAL_INT32(expected, actual, elements)\n\n### Numerical Assertions: Bitwise\n\n    TEST_ASSERT_BITS(mask, expected, actual)\n\nUse an integer mask to specify which bits should be compared between two other integers.\nHigh bits in the mask are compared, low bits ignored.\n\n    TEST_ASSERT_BITS_HIGH(mask, actual)\n\nUse an integer mask to specify which bits should be inspected to determine if they are all set high.\nHigh bits in the mask are compared, low bits ignored.\n\n    TEST_ASSERT_BITS_LOW(mask, actual)\n\nUse an integer mask to specify which bits should be inspected to determine if they are all set low.\nHigh bits in the mask are compared, low bits ignored.\n\n    TEST_ASSERT_BIT_HIGH(bit, actual)\n\nTest a single bit and verify that it is high.\nThe bit is specified 0-31 for a 32-bit integer.\n\n    TEST_ASSERT_BIT_LOW(bit, actual)\n\nTest a single bit and verify that it is low.\nThe bit is specified 0-31 for a 32-bit integer.\n\n### Numerical Assertions: Floats\n\n    TEST_ASSERT_FLOAT_WITHIN(delta, expected, actual)\n    TEST_ASSERT_DOUBLE_WITHIN(delta, expected, actual)\n\nAsserts that the actual value is within plus or minus delta of the expected value.\n\n    TEST_ASSERT_FLOAT_NOT_WITHIN(delta, expected, actual)\n    TEST_ASSERT_DOUBLE_NOT_WITHIN(delta, expected, actual)\n\nAsserts that the actual value is NOT within plus or minus delta of the expected value.\n\n    TEST_ASSERT_EQUAL_FLOAT(expected, actual)\n    TEST_ASSERT_EQUAL_DOUBLE(expected, actual)\n\nAsserts that two floating point values are \"equal\" within a small % delta of the expected value.\n\n    TEST_ASSERT_NOT_EQUAL_FLOAT(expected, actual)\n    TEST_ASSERT_NOT_EQUAL_DOUBLE(expected, actual)\n\nAsserts that two floating point values are NOT \"equal\" within a small % delta of the expected value.\n\n    TEST_ASSERT_LESS_THAN_FLOAT(threshold, actual)\n    TEST_ASSERT_LESS_THAN_DOUBLE(threshold, actual)\n    TEST_ASSERT_GREATER_THAN_FLOAT(threshold, actual)\n    TEST_ASSERT_GREATER_THAN_DOUBLE(threshold, actual)\n\nAsserts that the actual value is less than or greater than the threshold.\n\nThere are also `LESS_OR_EQUAL` and `GREATER_OR_EQUAL` variations.\nThese obey the same rules for equality as do `TEST_ASSERT_EQUAL_FLOAT` and `TEST_ASSERT_EQUAL_DOUBLE`:\nIf the two values are within a small % delta of the expected value, the assertion will pass.\n\n### String Assertions\n\n    TEST_ASSERT_EQUAL_STRING(expected, actual)\n\nCompare two null-terminate strings.\nFail if any character is different or if the lengths are different.\n\n    TEST_ASSERT_EQUAL_STRING_LEN(expected, actual, len)\n\nCompare two strings.\nFail if any character is different, stop comparing after len characters.\n\n    TEST_ASSERT_EQUAL_STRING_MESSAGE(expected, actual, message)\n\nCompare two null-terminate strings.\nFail if any character is different or if the lengths are different.\nOutput a custom message on failure.\n\n    TEST_ASSERT_EQUAL_STRING_LEN_MESSAGE(expected, actual, len, message)\n\nCompare two strings.\nFail if any character is different, stop comparing after len characters.\nOutput a custom message on failure.\n\n### Pointer Assertions\n\nMost pointer operations can be performed by simply using the integer comparisons above.\nHowever, a couple of special cases are added for clarity.\n\n    TEST_ASSERT_NULL(pointer)\n\nFails if the pointer is not equal to NULL\n\n    TEST_ASSERT_NOT_NULL(pointer)\n\nFails if the pointer is equal to NULL\n\n### Memory Assertions\n\n    TEST_ASSERT_EQUAL_MEMORY(expected, actual, len)\n\nCompare two blocks of memory.\nThis is a good generic assertion for types that can't be coerced into acting like standard types... but since it's a memory compare, you have to be careful that your data types are packed.\n\n### \\_MESSAGE\n\nYou can append `\\_MESSAGE` to any of the macros to make them take an additional argument.\nThis argument is a string that will be printed at the end of the failure strings.\nThis is useful for specifying more information about the problem.\n\n[CI]: https://github.com/ThrowTheSwitch/Unity/workflows/CI/badge.svg\n[getting started guide]: docs/UnityGettingStartedGuide.md\n[docs]: docs/\n[UnityAssertionsReference.md]: docs/UnityAssertionsReference.md\n"
 },
 {
  "repo": "akheron/jansson",
  "language": "C",
  "readme_contents": "Jansson README\n==============\n\n.. image:: https://github.com/akheron/jansson/workflows/tests/badge.svg\n  :target: https://github.com/akheron/jansson/actions\n\n.. image:: https://ci.appveyor.com/api/projects/status/lmhkkc4q8cwc65ko\n  :target: https://ci.appveyor.com/project/akheron/jansson\n\n.. image:: https://coveralls.io/repos/akheron/jansson/badge.png?branch=master\n  :target: https://coveralls.io/r/akheron/jansson?branch=master\n\nJansson_ is a C library for encoding, decoding and manipulating JSON\ndata. Its main features and design principles are:\n\n- Simple and intuitive API and data model\n\n- `Comprehensive documentation`_\n\n- No dependencies on other libraries\n\n- Full Unicode support (UTF-8)\n\n- Extensive test suite\n\nJansson is licensed under the `MIT license`_; see LICENSE in the\nsource distribution for details.\n\nCompilation and Installation\n----------------------------\n\nIf you obtained a ``jansson-X.Y.tar.*`` tarball from GitHub Releases, just use\nthe standard autotools commands::\n\n   $ ./configure\n   $ make\n   $ make install\n\nTo run the test suite, invoke::\n\n   $ make check\n\nIf the source has been checked out from a Git repository, the ``configure``\nscript has to be generated first. The easiest way is to use autoreconf::\n\n   $ autoreconf -i\n\n\nDocumentation\n-------------\n\nDocumentation is available at http://jansson.readthedocs.io/en/latest/.\n\nThe documentation source is in the ``doc/`` subdirectory. To generate\nHTML documentation, invoke::\n\n   $ make html\n\nThen, point your browser to ``doc/_build/html/index.html``. Sphinx_\n1.0 or newer is required to generate the documentation.\n\n\nCommunity\n---------\n\n* `Documentation <http://jansson.readthedocs.io/en/latest/>`_\n* `Issue tracker <https://github.com/akheron/jansson/issues>`_\n* `Mailing list <http://groups.google.com/group/jansson-users>`_\n* `Wiki <https://github.com/akheron/jansson/wiki>`_ contains some development documentation\n\n.. _Jansson: http://www.digip.org/jansson/\n.. _`Comprehensive documentation`: http://jansson.readthedocs.io/en/latest/\n.. _`MIT license`: http://www.opensource.org/licenses/mit-license.php\n.. _Sphinx: http://sphinx.pocoo.org/\n"
 },
 {
  "repo": "janet-lang/janet",
  "language": "C",
  "readme_contents": "[![Join the chat](https://badges.gitter.im/janet-language/community.svg)](https://gitter.im/janet-language/community)\n&nbsp;\n[![builds.sr.ht status](https://builds.sr.ht/~bakpakin/janet/commits/master/freebsd.yml.svg)](https://builds.sr.ht/~bakpakin/janet/commits/master/freebsd.yml?)\n[![builds.sr.ht status](https://builds.sr.ht/~bakpakin/janet/commits/master/openbsd.yml.svg)](https://builds.sr.ht/~bakpakin/janet/commits/master/openbsd.yml?)\n[![Actions Status](https://github.com/janet-lang/janet/actions/workflows/test.yml/badge.svg)](https://github.com/janet-lang/janet/actions/workflows/test.yml)\n\n<img src=\"https://raw.githubusercontent.com/janet-lang/janet/master/assets/janet-w200.png\" alt=\"Janet logo\" width=200 align=\"left\">\n\n**Janet** is a functional and imperative programming language and bytecode interpreter. It is a\nlisp-like language, but lists are replaced\nby other data structures (arrays, tables (hash table), struct (immutable hash table), tuples).\nThe language also supports bridging to native code written in C, meta-programming with macros, and bytecode assembly.\n\nThere is a REPL for trying out the language, as well as the ability\nto run script files. This client program is separate from the core runtime, so\nJanet can be embedded in other programs. Try Janet in your browser at\n[https://janet-lang.org](https://janet-lang.org).\n\nIf you'd like to financially support the ongoing development of Janet, consider\n[sponsoring its primary author](https://github.com/sponsors/bakpakin) through GitHub.\n\n<br>\n\n## Use Cases\n\nJanet makes a good system scripting language, or a language to embed in other programs.\nIt's like Lua and Guile in that regard. It has more built-in functionality and a richer core language than\nLua, but smaller than GNU Guile or Python.\n\n## Features\n\n* Configurable at build time - turn features on or off for a smaller or more featureful build\n* Minimal setup - one binary and you are good to go!\n* First-class closures\n* Garbage collection\n* First-class green threads (continuations)\n* Python-style generators (implemented as a plain macro)\n* Mutable and immutable arrays (array/tuple)\n* Mutable and immutable hashtables (table/struct)\n* Mutable and immutable strings (buffer/string)\n* Macros\n* Multithreading\n* Per-thread event loop for efficient evented IO\n* Byte code interpreter with an assembly interface, as well as bytecode verification\n* Tail call Optimization\n* Direct interop with C via abstract types and C functions\n* Dynamically load C libraries\n* Functional and imperative standard library\n* Lexical scoping\n* Imperative programming as well as functional\n* REPL\n* Parsing Expression Grammars built into the core library\n* 400+ functions and macros in the core library\n* Embedding Janet in other programs\n* Interactive environment with detailed stack traces\n\n## Documentation\n\n* For a quick tutorial, see [the introduction](https://janet-lang.org/docs/index.html) for more details.\n* For the full API for all functions in the core library, see [the core API doc](https://janet-lang.org/api/index.html)\n\nDocumentation is also available locally in the REPL.\nUse the `(doc symbol-name)` macro to get API\ndocumentation for symbols in the core library. For example,\n```\n(doc apply)\n```\nShows documentation for the `apply` function.\n\nTo get a list of all bindings in the default\nenvironment, use the `(all-bindings)` function. You\ncan also use the `(doc)` macro with no arguments if you are in the REPL\nto show bound symbols.\n\n## Source\n\nYou can get the source on [GitHub](https://github.com/janet-lang/janet) or\n[SourceHut](https://git.sr.ht/~bakpakin/janet). While the GitHub repo is the official repo,\nthe SourceHut mirror is actively maintained.\n\n## Building\n\n### macOS and Unix-like\n\nThe Makefile is non-portable and requires GNU-flavored make.\n\n```\ncd somewhere/my/projects/janet\nmake\nmake test\nmake repl\nmake install\nmake install-jpm-git\n```\n\nFind out more about the available make targets by running `make help`.\n\n### 32-bit Haiku\n\n32-bit Haiku build instructions are the same as the UNIX-like build instructions,\nbut you need to specify an alternative compiler, such as `gcc-x86`.\n\n```\ncd somewhere/my/projects/janet\nmake CC=gcc-x86\nmake test\nmake repl\nmake install\nmake install-jpm-git\n```\n\n### FreeBSD\n\nFreeBSD build instructions are the same as the UNIX-like build instructions,\nbut you need `gmake` to compile. Alternatively, install directly from\npackages, using `pkg install lang/janet`.\n\n```\ncd somewhere/my/projects/janet\ngmake\ngmake test\ngmake repl\ngmake install\ngmake install-jpm-git\n```\n\n### NetBSD\n\nNetBSD build instructions are the same as the FreeBSD build instructions.\nAlternatively, install directly from packages, using `pkgin install janet`.\n\n### Windows\n\n1. Install [Visual Studio](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community&rel=15#) or [Visual Studio Build Tools](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=15#)\n2. Run a Visual Studio Command Prompt (cl.exe and link.exe need to be on the PATH) and cd to the directory with janet.\n3. Run `build_win` to compile janet.\n4. Run `build_win test` to make sure everything is working.\n\nTo build an `.msi` installer executable, in addition to the above steps, you will have to:\n\n5. Install, or otherwise add to your PATH the [WiX 3.11 Toolset](https://github.com/wixtoolset/wix3/releases)\n6. run `build_win dist`\n\nNow you should have an `.msi`. You can run `build_win install` to install the `.msi`, or execute the file itself.\n\n### Meson\n\nJanet also has a build file for [Meson](https://mesonbuild.com/), a cross-platform build\nsystem. Although Meson has a Python dependency, Meson is a very complete build system that\nis maybe more convenient and flexible for integrating into existing pipelines.\nMeson also provides much better IDE integration than Make or batch files, as well as support\nfor cross-compilation.\n\nFor the impatient, building with Meson is as follows. The options provided to\n`meson setup` below emulate Janet's Makefile.\n\n```sh\ngit clone https://github.com/janet-lang/janet.git\ncd janet\nmeson setup build \\\n          --buildtype release \\\n          --optimization 2 \\\n          --libdir /usr/local/lib \\\n          -Dgit_hash=$(git log --pretty=format:'%h' -n 1)\nninja -C build\n\n# Run the binary\nbuild/janet\n\n# Installation\nninja -C build install\n```\n\n## Development\n\nJanet can be hacked on with pretty much any environment you like, but for IDE\nlovers, [Gnome Builder](https://wiki.gnome.org/Apps/Builder) is probably the\nbest option, as it has excellent meson integration. It also offers code completion\nfor Janet's C API right out of the box, which is very useful for exploring. VSCode, Vim,\nEmacs, and Atom will have syntax packages for the Janet language, though.\n\n## Installation\n\nSee the [Introduction](https://janet-lang.org/docs/index.html) for more details. If you just want\nto try out the language, you don't need to install anything. You can also move the `janet` executable wherever you want on your system and run it.\n\n## Usage\n\nA REPL is launched when the binary is invoked with no arguments. Pass the -h flag\nto display the usage information. Individual scripts can be run with `./janet myscript.janet`\n\nIf you are looking to explore, you can print a list of all available macros, functions, and constants\nby entering the command `(all-bindings)` into the REPL.\n\n```\n$ janet\nJanet 1.7.1-dev-951e10f  Copyright (C) 2017-2020 Calvin Rose\njanet:1:> (+ 1 2 3)\n6\njanet:2:> (print \"Hello, World!\")\nHello, World!\nnil\njanet:3:> (os/exit)\n$ janet -h\nusage: build/janet [options] script args...\nOptions are:\n  -h : Show this help\n  -v : Print the version string\n  -s : Use raw stdin instead of getline like functionality\n  -e code : Execute a string of janet\n  -r : Enter the REPL after running all scripts\n  -p : Keep on executing if there is a top-level error (persistent)\n  -q : Hide prompt, logo, and REPL output (quiet)\n  -k : Compile scripts but do not execute (flycheck)\n  -m syspath : Set system path for loading global modules\n  -c source output : Compile janet source code into an image\n  -n : Disable ANSI color output in the REPL\n  -l path : Execute code in a file before running the main script\n  -- : Stop handling options\n```\n\nIf installed, you can also run `man janet` to get usage information.\n\n## Embedding\n\nJanet can be embedded in a host program very easily. The normal build\nwill create a file `build/janet.c`, which is a single C file\nthat contains all the source to Janet. This file, along with\n`src/include/janet.h` and `src/conf/janetconf.h` can be dragged into any C\nproject and compiled into the project. Janet should be compiled with `-std=c99`\non most compilers, and will need to be linked to the math library, `-lm`, and\nthe dynamic linker, `-ldl`, if one wants to be able to load dynamic modules. If\nthere is no need for dynamic modules, add the define\n`-DJANET_NO_DYNAMIC_MODULES` to the compiler options.\n\nSee the [Embedding Section](https://janet-lang.org/capi/embedding.html) on the website for more information.\n\n## Examples\n\nSee the examples directory for some example janet code.\n\n## Discussion\n\nFeel free to ask questions and join the discussion on the [Janet Gitter Channel](https://gitter.im/janet-language/community).\nGitter provides Matrix and irc bridges as well.\n\n## FAQ\n\n### Where is (favorite feature from other language)?\n\nIt may exist, it may not. If you want to propose major language features, go ahead and open an issue, but\nthey will likely by closed as \"will not implement\". Often, such features make one usecase simpler at the expense\nof 5 others by making the language more complicated.\n\n### Is there a language spec?\n\nThere is not currently a spec besides the documentation at https://janet-lang.org.\n\n### Is this Scheme/Common Lisp? Where are the cons cells?\n\nNope. There are no cons cells here.\n\n### Is this a Clojure port?\n\nNo. It's similar to Clojure superficially because I like Lisps and I like the aesthetics.\nInternally, Janet is not at all like Clojure.\n\n### Are the immutable data structures (tuples and structs) implemented as hash tries?\n\nNo. They are immutable arrays and hash tables. Don't try and use them like Clojure's vectors\nand maps, instead they work well as table keys or other identifiers.\n\n### Can I do Object Oriented programming with Janet?\n\nTo some extent, yes. However, it is not the recommended method of abstraction, and performance may suffer.\nThat said, tables can be used to make mutable objects with inheritance and polymorphism, where object\nmethods are implemeted with keywords.\n\n```\n(def Car @{:honk (fn [self msg] (print \"car \" self \" goes \" msg)) })\n(def my-car (table/setproto @{} Car))\n(:honk my-car \"Beep!\")\n```\n\n### Why can't we add (feature from Clojure) into the core?\n\nUsually, one of a few reasons:\n- Often, it already exists in a different form and the Clojure port would be redundant.\n- Clojure programs often generate a lot of garbage and rely on the JVM to clean it up.\n  Janet does not run on the JVM, and has a more primitive garbage collector.\n- We want to keep the Janet core small. With Lisps, usually a feature can be added as a library\n  without feeling \"bolted on\", especially when compared to ALGOL like languages. Adding features\n  to the core also makes it a bit more difficult to keep Janet maximally portable.\n\n### Why is my terminal spitting out junk when I run the REPL?\n\nMake sure your terminal supports ANSI escape codes. Most modern terminals will\nsupport these, but some older terminals, Windows consoles, or embedded terminals\nwill not. If your terminal does not support ANSI escape codes, run the REPL with\nthe `-n` flag, which disables color output. You can also try the `-s` if further issues\nensue.\n\n## Why is it called \"Janet\"?\n\nJanet is named after the almost omniscient and friendly artificial being in [The Good Place](https://en.wikipedia.org/wiki/The_Good_Place).\n"
 },
 {
  "repo": "gozfree/gear-lib",
  "language": "C",
  "readme_contents": "# Gear-Lib\n\n[English](README.md) | \u7b80\u4f53\u4e2d\u6587\n\n[![Build](https://travis-ci.org/gozfree/gear-lib.svg?branch=master)](https://travis-ci.org/gozfree/gear-lib)\n[![Release](https://img.shields.io/github/release/gozfree/gear-lib.svg)](https://github.com/gozfree/gear-lib/releases)\n[![License](https://img.shields.io/github/license/gozfree/gear-lib.svg)](https://github.com/gozfree/gear-lib/blob/master/LICENSE.MIT)\n\n\u8fd9\u662f\u4e00\u7ec4\u901a\u7528\u7684\uff23\u57fa\u7840\u5e93\n* \u5168\u90e8\u7528POSIX C\u5b9e\u73b0\uff0c\u76ee\u6807\u662f\u4e3a\u4e86\u8de8\u5e73\u53f0\u517c\u5bb9linux, windows, android, ios.\n* \u9002\u7528\u4e8e\u7269\u8054\u7f51\uff0c\u5d4c\u5165\u5f0f\uff0c\u4ee5\u53ca\u7f51\u7edc\u670d\u52a1\u5f00\u53d1\u7b49\u573a\u666f\n\n![struct](./build/gear-lib.png)\n\n## \u6570\u636e\u7ed3\u6784\n|  |  |\n|--|--|\n| libdict: \u54c8\u5e0c\u5b57\u5178 | libhash: linux\u5185\u6838\u539f\u751f\u54c8\u5e0c\u5e93 |\n| libringbuffer: \u5faa\u73af\u7f13\u51b2 | libqueue: \u6570\u636e\u961f\u5217 |\n| librbtree: \u5185\u6838rbtree | libsort: |\n| libvector: \u5bb9\u5668\u5e93 | libdarray: \u52a8\u6001\u6570\u7ec4 |\n\n## \u7f51\u7edc\u5e93\n|  |  |\n|--|--|\n| librtsp: RTSP\u534f\u8bae\uff0c\u9002\u5408IPCamera\u548cNVR\u5f00\u53d1 | librtmpc: RTMP\u534f\u8bae\uff0c\u9002\u5408\u63a8\u6d41\u76f4\u64ad |\n| libsock: Socket\u5c01\u88c5 | librpc: \u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\u5e93 |\n| libipc: \u8fdb\u7a0b\u95f4\u901a\u4fe1 | libp2p: p2p\u7a7f\u900f\u4f20\u8f93 |\n| libmqttc: MQTT\u5ba2\u6237\u7aef\u534f\u8bae | libhomekit: Apple homekit\u534f\u8bae\u5e93 |\n\n## \u5f02\u6b65\n|  |  |\n|--|--|\n| libgevent: \u4e8b\u4ef6\u9a71\u52a8 | libthread: \u7ebf\u7a0b |\n| libworkq: \u5de5\u4f5c\u961f\u5217 | |\n\n## I/O\n|  |  |\n|--|--|\n| libstrex: \u5b57\u7b26\u6269\u5c55 | libconfig: \u914d\u7f6e\u6587\u4ef6\u5e93 |\n| liblog: \u65e5\u5fd7\u5e93 | libfile: \u6587\u4ef6\u64cd\u4f5c\u5e93 |\n| libsubmask: \u7f51\u7edc\u5730\u5740\u7ffb\u8bd1 | |\n\n## \u591a\u5a92\u4f53\n|  |  |\n|--|--|\n| libavcap: \u97f3\u9891\u89c6\u9891\u6355\u83b7\u5e93 | libmp4: MP4\u5f55\u5236\u89e3\u6790\u5e93 |\n| libjpeg-ex: | libmedia-io: \u97f3\u9891\u89c6\u9891\u683c\u5f0f\u5b9a\u4e49 |\n\n## \u7cfb\u7edf\u62bd\u8c61\u5c42\n|  |  |\n|--|--|\n| libposix: windows/rtos/ios\u5e73\u53f0posix\u9002\u914d\u5e93 |\n\n## \u5176\u4ed6\n|  |  |\n|--|--|\n| libdebug: \u8c03\u8bd5\u8f85\u52a9\u5e93 | libhal: \u786c\u4ef6\u62bd\u8c61\u5c42 |\n| libplugin: \u52a8\u6001\u52a0\u8f7d\u5e93 | libtime: \u65f6\u95f4\u5e93 |\n| libfsm: \u6709\u9650\u72b6\u6001\u673a | |\n\n## \u7f16\u8bd1\u65b9\u6cd5\n\u8be6\u7ec6\u8bf7\u53c2\u8003[INSTALL.md](https://github.com/gozfree/gear-lib/blob/master/INSTALL.md)\n\n## License\n\u8be6\u7ec6\u8bf7\u53c2\u8003[LICENSE](https://github.com/gozfree/gear-lib/blob/master/LICENSE.MIT)\n\n## \u8054\u7cfb\u4ea4\u6d41\n* \u90ae\u7bb1: gozfree@163.com\n* QQ \u7fa4: 695515645\n* Github: [gear-lib](https://github.com/gozfree/gear-lib)\n* \u7801\u4e91\u4e3b\u9875: [gear-lib](https://gitee.com/gozfreee/gear-lib)\n"
 },
 {
  "repo": "EasyHook/EasyHook",
  "language": "C",
  "readme_contents": "# Welcome to EasyHook - The reinvention of Windows API Hooking\n\n[![Join the chat at https://gitter.im/EasyHook/EasyHook](https://badges.gitter.im/EasyHook/EasyHook.svg)](https://gitter.im/EasyHook/EasyHook?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nMaster branch: [![Master branch build status](https://ci.appveyor.com/api/projects/status/278ff5njpjnarayd/branch/master?svg=true)](https://ci.appveyor.com/project/spazzarama/easyhook/branch/master)\n\nDevelop branch: [![Develop branch build status](https://ci.appveyor.com/api/projects/status/278ff5njpjnarayd/branch/develop?svg=true)](https://ci.appveyor.com/project/spazzarama/easyhook/branch/develop)\n\nYou can support the EasyHook project over at Bountysource or [raise a bounty for an issue to be fixed](https://www.bountysource.com/teams/easyhook/issues): [![Current bounties](https://api.bountysource.com/badge/team?team_id=104536)](https://www.bountysource.com/teams/easyhook/bounties)\n\nThis project supports extending (hooking) unmanaged code (APIs) with pure managed ones, from within a fully managed environment on 32- or 64-bit Windows Vista x64, Windows Server 2008 x64, Windows 7, Windows 8.1, and Windows 10.\n\nEasyHook currently supports injecting assemblies built for .NET Framework 3.5 and 4.0 and can also inject native DLLs.\n\n## EasyHook homepage\n\nFor more information head to the EasyHook site at https://easyhook.github.io\n\n## NuGet\nhttps://www.nuget.org/packages/EasyHook\n\nFor native C++ apps there is also a native NuGet package available: https://www.nuget.org/packages/EasyHookNativePackage\n\n## Bug reports or questions\nReporting bugs is the only way to get them fixed and help other users of the library! If an issue isn't getting addressed, try [raising a bounty for it](https://www.bountysource.com/teams/easyhook/issues).\n\nReport issues at: https://github.com/EasyHook/EasyHook/issues\n\n## License\n    Copyright (c) 2009 Christoph Husse & Copyright (c) 2012 Justin Stenning\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n    \n    The above copyright notice and this permission notice shall be included in\n    all copies or substantial portions of the Software.\n    \n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n    THE SOFTWARE.\n\n## External libraries\nEasyHook includes the UDIS86 library Copyright (c) 2002-2012, Vivek Thampi <vivek.mt@gmail.com>. See .\\DriverShared\\Disassembler\\udis86-LICENSE.txt for license details. Minor modifications have been made for it to compile with EasyHook.\n\nMore information about UDIS86 can be found at https://github.com/vmt/udis86\n"
 },
 {
  "repo": "aws/amazon-freertos",
  "language": "C",
  "readme_contents": "# FreeRTOS AWS Reference Integrations\n\n## Cloning\nThis repo uses [Git Submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules) to bring in dependent components.\n\nNote: If you download the ZIP file provided by GitHub UI, you will not get the contents of the submodules. (The ZIP file is also not a valid git repository)\n\nIf using Windows, because this repository and its submodules contain symbolic links, set `core.symlinks` to true with the following command:\n```\ngit config --global core.symlinks true\n```\nIn addition to this, either enable [Developer Mode](https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) or, whenever using a git command that writes to the system (e.g. `git pull`, `git clone`, and `git submodule update --init --recursive`), use a console elevated as administrator so that git can properly create symbolic links for this repository. Otherwise, symbolic links will be written as normal files with the symbolic links' paths in them as text. [This](https://blogs.windows.com/windowsdeveloper/2016/12/02/symlinks-windows-10/) gives more explanation.\n\nTo clone using HTTPS:\n```\ngit clone https://github.com/aws/amazon-freertos.git --recurse-submodules\n```\nUsing SSH:\n```\ngit clone git@github.com:aws/amazon-freertos.git --recurse-submodules\n```\n\nIf you have downloaded the repo without using the `--recurse-submodules` argument, you need to run:\n```\ngit submodule update --init --recursive\n```\n\n## Important branches to know\nmaster            --> Development is done continuously on this branch  \nrelease           --> Fully tested released source code  \nrelease-candidate --> Preview of upcoming release  \nfeature/*         --> Alpha/beta of an upcoming feature  \n\n## Getting Started\n\nFor more information on FreeRTOS, refer to the [Getting Started section of FreeRTOS webpage](https://aws.amazon.com/freertos).\n\nTo directly access the **Getting Started Guide** for supported hardware platforms, click the corresponding link in the Supported Hardware section below.\n\nFor detailed documentation on FreeRTOS, refer to the [FreeRTOS User Guide](https://aws.amazon.com/documentation/freertos).\n\n### AWS Collection of Metrics\n\nThe demos that connect to AWS IoT report metrics to AWS about the operating system, and the MQTT client library used by sending a specially formatted string in the username field of the MQTT CONNECT packet. These metrics help AWS IoT improve security and provide better technical support. Providing these metrics is optional for users, and these can be disabled by updating the following configuration macros in the `demos/include/aws_iot_metrics.h` file:\n\n```\n#define AWS_IOT_METRICS_STRING           NULL\n#define AWS_IOT_METRICS_STRING_LENGTH    0U\n```\n\n#### Format\n\nThe format of the username string with metrics is:\n\n```\n<Actual_Username>?SDK=<OS_Name>&Version=<OS_Version>MQTTLib=<MQTT_Library_name>@<MQTT_Library_version>\n```\n\nwhere\n\n* **Actual_Username** is the actual username used for authentication (if a username/password is used for authentication).\n* **OS_Name** is the Operating System the application is running on.\n* **OS_Version** is the version number of the Operating System.\n* **MQTT_Library_name** is the MQTT Client library being used.\n* **MQTT_Library_version** is the version of the MQTT Client library being used.\n\n## FreeRTOS Qualified Boards\n\nFor a complete list of boards that have been qualified for FreeRTOS by AWS Partners, please visit the [AWS Partner Device Catalog](https://devices.amazonaws.com/search?page=1&sv=freertos)\n\nIn addition, AWS supports the following boards with FreeRTOS Build Integration and maintains them with each release:\n\n1. **Texas Instruments** - [CC3220SF-LAUNCHXL](https://devices.amazonaws.com/detail/a3G0L00000AANtaUAH/SimpleLink-Wi-Fi%C2%AE-CC3220SF-Wireless-Microcontroller-LaunchPad-Development-Kit).\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_ti.html)\n    * IDEs: [Code Composer Studio](http://www.ti.com/tools-software/ccs.html), [IAR Embedded Workbench](https://www.iar.com/iar-embedded-workbench/partners/texas-instruments)\n2. **STMicroelectronics** - [STM32L4 Discovery kit IoT node](https://devices.amazonaws.com/detail/a3G0L00000AANsWUAX/STM32L4-Discovery-Kit-IoT-Node).\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_st.html)\n    * IDE: [STM32 System Workbench](http://openstm32.org/HomePage)\n3. **NXP** - [LPC54018 IoT Module](https://devices.amazonaws.com/detail/a3G0L00000AANtAUAX/LPC54018-IoT-Solution), \n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_nxp.html)\n    * IDEs: [IAR Embedded Workbench](https://www.iar.com/iar-embedded-workbench/partners/nxp), [MCUXpresso IDE](https://www.nxp.com/mcuxpresso/ide/download)\n4. **Espressif** - [ESP32-DevKitC](https://devices.amazonaws.com/detail/a3G0L00000AANtjUAH/ESP32-WROOM-32-DevKitC), [ESP-WROVER-KIT](https://devices.amazonaws.com/detail/a3G0L00000AANtlUAH/ESP-WROVER-KIT), [ESP32-WROOM-32SE](https://devices.amazonaws.com/detail/a3G0L00000AANtjUAH/ESP32-WROOM-32-DevKitC), [ESP32-S2-SAOLA-1](https://devices.amazonaws.com/detail/a3G0h00000AkFngEAF/ESP32-S2-Saola-1)\n    * [Getting Started Guide - ESP32-DevKitC, ESP-WROVER-KIT](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_espressif.html)\n    * [Getting Started Guide - ESP32-WROOM-32SE](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_esp32wroom-32se.html)\n5. **Infineon** - [Infineon XMC4800 IoT Connectivity Kit](https://devices.amazonaws.com/detail/a3G0L00000AANsbUAH/XMC4800-IoT-FreeRTOS-Connectivity-Kit-WiFi), [Optiga TrustX](https://devices.amazonaws.com/detail/a3G0h000007712QEAQ/OPTIGA%E2%84%A2-Trust-X-Security-Solution)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_infineon.html)\n    * IDE: [DAVE](https://infineoncommunity.com/dave-download_ID645)\n6. **Xilinx** - [Xilinx Zynq-7000 based MicroZed Industrial IoT Bundle](https://devices.amazonaws.com/detail/a3G0L00000AANtqUAH/MicroZed-IIoT-Bundle-with-FreeRTOS)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_xilinx.html)\n    * IDE: [Xilinx SDK](https://www.xilinx.com/products/design-tools/embedded-software/sdk.html)\n7. **MediaTek** - [MediaTek MT7697Hx Development Kit](https://devices.amazonaws.com/detail/a3G0L00000AAOmPUAX/MT7697Hx-Development-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_mediatek.html)\n    * IDE: [Keil uVision](http://www2.keil.com/mdk5/install/)\n8. **Renesas** - [Renesas Starter Kit+ for RX65N-2MB](https://devices.amazonaws.com/detail/a3G0L00000AAOkeUAH/Renesas-Starter-Kit+-for-RX65N-2MB)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_renesas.html)\n    * IDE: [e2 studio](https://www.renesas.com/us/en/products/software-tools/tools/ide/e2studio.html)\n9. **Cypress CYW54907** - [Cypress CYW954907AEVAL1F Evaluation Kit](https://devices.amazonaws.com/detail/a3G0L00000AAPg5UAH/CYW954907AEVAL1F)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_cypress_54.html)\n    * IDE: [WICED Studio](https://community.cypress.com/community/wiced-wifi)\n10. **Cypress CYW43907** - [Cypress CYW943907AEVAL1F Evaluation Kit](https://devices.amazonaws.com/detail/a3G0L00000AAPg0UAH/CYW943907AEVAL1F)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_cypress_43.html)\n    * IDE: [WICED Studio](https://community.cypress.com/community/wiced-wifi)\n11. **Cypress PSoC 64** - [PSoC 64 Standard Secure AWS Wi-Fi Bluetooth Pioneer Kit](https://devices.amazonaws.com/detail/a3G0h0000088AgXEAU/PSoC%C2%AE-64-Standard-Secure-AWS-Wi-Fi-Bluetooth-Pioneer-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_cypress_psoc64.html)\n    * IDE: [ModusToolbox](https://www.cypress.com/products/modustoolbox-software-environment)\n12. **NXP MW320** - [MW320 AWS IoT Starter Kit](https://devices.amazonaws.com/detail/a3G0h000000OaRnEAK/MW320-AWS-IoT-Starter-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_mw32x.html)\n13. **NXP MW322** - [MW322 AWS IoT Starter Kit](https://devices.amazonaws.com/detail/a3G0h000000OblKEAS/MW322-AWS-IoT-Starter-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_mw32x.html)\n14. **Nordic nRF52840 DK** - [nRF52840 DK Development kit](https://devices.amazonaws.com/detail/a3G0L00000AANtrUAH/nRF52840-Development-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_nordic.html)  \n15. **Nuvoton** - [NuMaker-IoT-M487](https://devices.amazonaws.com/detail/a3G0h000000Tg9cEAC/NuMaker-IoT-M487)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting-started-nuvoton-m487.html)\n16. **Windows Simulator** - To evaluate FreeRTOS without using MCU-based hardware, you can use the Windows Simulator.\n    * Requirements: Microsoft Windows 7 or newer, with at least a dual core and a hard-wired Ethernet connection\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_windows.html)\n    * IDE: [Visual Studio Community Edition](https://www.visualstudio.com/downloads/)\n\n\n## amazon-freeRTOS/projects\nThe ```./projects``` folder contains the IDE test and demo projects for each vendor and their boards. The majority of boards can be built with both IDE and cmake (there are some exceptions!). Please refer to the Getting Started Guides above for board specific instructions.\n\n## Mbed TLS License\nThis repository uses Mbed TLS under Apache 2.0.\n\n## amazon-freerTOS/vendors License\nThe `./vendors` directory contains content that may be subject to different license terms. For vendor licensing information, see the LICENSE files or source header documentation for each vendor directory.\n\n## CBMC\n\nThe `tools/cbmc/proofs` directory contains CBMC proofs.\n\nTo learn more about CBMC and proofs specifically, review the training material [here](https://model-checking.github.io/cbmc-training).\n\nIn order to run these proofs you will need to install CBMC and other tools by following the instructions [here](https://model-checking.github.io/cbmc-training/installation.html).\n\n"
 },
 {
  "repo": "ashima/webgl-noise",
  "language": "C",
  "readme_contents": "The wiki for this repository contains more information.\n\nSimplex noise functions are (C) Ashima Arts and Stefan Gustavson\nClassic noise functions are (C) Stefan Gustavson\nCellular noise functions are (C) Stefan Gustavson\nThe \"psrdnoise\" functions are (C) Stefan Gustavson\n\nSource code for the noise functions is released under the\nconditions of the MIT license. See the file LICENSE for details.\n\nThe simplex noise functions follow Ken Perlin's original idea,\nmore clearly explained in Stefan Gustavson's paper\n\"Simplex noise demystified\"\nhttp://www.itn.liu.se/~stegu/simplexnoise/simplexnoise.pdf\nbut without using any uniform arrays or texture engines.\n\nMany other noise implementations make heavy use of a\ntexture lookup table and are texture bandwidth limited.\nThe noise functions in this library, however, are completely\nself contained with no dependency on external data.\nWhile not quite as fast as texture-based implementations\non typical current desktop GPUs, they are more scalable to\nmassive parallelism and much more convenient to use, and\nthey can make good use of unused ALU resources when run\nconcurrently with a typical texture-intensive rendering.\n\n2016-05-13: Ashima Arts now seems to be defunct as a company\n(their website and email addresses have ceased to function)\nso I cloned this repository to:\n\nhttps://github.com/stegu/webgl-noise/\n\nThis site is heavily linked from all over the place on the\nInternet, so I (Stefan Gustavson) will keep updating both sites\nfor the foreseeable future as the (apparently) sole maintainer.\n"
 },
 {
  "repo": "trekhleb/learn-python",
  "language": "Python",
  "readme_contents": "# Playground y Cheatsheet para aprender Python\n\n[![Build Status](https://travis-ci.org/trekhleb/learn-python.svg?branch=master)](https://travis-ci.org/trekhleb/learn-python)\n\n> Esta es una colecci\u00f3n de scripts de Python divididos en [categor\u00edas](#contenido) que contienen\nejemplos de c\u00f3digo con sus explicaciones, diferentes usos y links a recursos adicionales.\n\n> _Lee esto en:_ [_Ingl\u00e9s_](README.md), [_Portugu\u00e9s_](README.pt-BR.md), _Traditional Chinese_](README.zh-TW.md).\n\nEs un **playground** ya que puedes cambiar o a\u00f1adir cosas al c\u00f3digo para ver\nc\u00f3mo funciona y [probarlo](#probando-el-c\u00f3digo) usando aserciones. Tambi\u00e9n puedes\n[revisar el c\u00f3digo](#revisando-el-c\u00f3digo) que has escrito y averiguar si est\u00e1 acorde con\nla gu\u00eda de estilos de Python. Todo esto, en conjunto, puede hacer que tu proceso de aprendizaje\nsea m\u00e1s interactivo y puede ayudarte a mantener la calidad del c\u00f3digo muy alta desde el principio.\n\nEs un **cheatsheet** porque puedes regresar y revisar los ejemplos de c\u00f3digo para\nfortalecer tus conocimientos sobre las [sentencias y contrucciones est\u00e1ndar de Python](#contenido).\nAdem\u00e1s, ya que el c\u00f3digo tiene muchas aserciones, podr\u00e1s ver el resultado de las funciones/sentencias en el mismo\nc\u00f3digo sin la necesidad de ejecutarlos.\n\n> _Tambi\u00e9n puede interesarte \ud83e\udd16 [Interactive Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments)_\n\n## C\u00f3mo usar este repositorio\n\nCada script de Python en este repositorio sigue la estructura:\n\n```python\n\"\"\"Lists  <--- Nombre del tema\n\n# @see: https://www.learnpython.org/en/Lists  <-- Link a recurso adicional\n\nAqu\u00ed puede haber una explicaci\u00f3n detallada del tema en concreto (ej: informaci\u00f3n general sobre listas).\n\"\"\"\n\n\ndef test_list_type():\n    \"\"\"Explicaci\u00f3n del sub-tema.\n    \n    Cada archivo contiene funciones de prueba que muestran sub-temas (ej: tipos de listas, m\u00e9todos en listas).\n    \"\"\"\n    \n    # Este es un ejemplo de c\u00f3mo construir una lista. <-- Estos comentarios explican el procedimiento\n    squares = [1, 4, 9, 16, 25]\n    \n    # Las listas pueden ser indexadas y cortadas. \n    # Al indexar devuelve el item.\n    assert squares[0] == 1  # <-- Estas aserciones muestran el resultado.\n    # Al cortar devuelve una nueva lista.\n    assert squares[-3:] == [9, 16, 25]  # <-- Estas aserciones muestran el resultado.\n```\n\nNormalmente, querr\u00e1s hacer lo siguiente:\n\n- [Encontrar el tema](#contenido) que quieres aprender o revisar.\n- Leer los comentarios y/o la documentaci\u00f3n que est\u00e1 escrita en cada docstring del script (toma como ejemplo el script de arriba).\n- Ver los ejemplos de c\u00f3digo y las aserciones para conocer diferentes maneras de usar el c\u00f3digo y entender el resultado previsto.\n- Cambiar el c\u00f3digo o a\u00f1adir nuevas aserciones para ver c\u00f3mo funcionan las cosas.\n- [Probar](#probando-el-c\u00f3digo) y [revisar](#revisando-el-c\u00f3digo) el c\u00f3digo para ver si funciona y si est\u00e1 escrito\ncorrectamente.\n\n## Contenido\n\n1. **Empezando**\n    - [\u00bfQu\u00e9 es Python?](src/getting_started/what_is_python.md)\n    - [Sintaxis de Python](src/getting_started/python_syntax.md)\n    - [Variables](src/getting_started/test_variables.py)\n2. **Operadores**\n    - [Operadores aritm\u00e9ticos](src/operators/test_arithmetic.py) (`+`, `-`, `*`, `/`, `//`, `%`, `**`)\n    - [Operadores Bitwise](src/operators/test_bitwise.py) (`&`, `|`, `^`, `>>`, `<<`, `~`)\n    - [Operadores de atribuci\u00f3n](src/operators/test_assigment.py) (`=`, `+=`, `-=`, `/=`, `//=` etc.)\n    - [Operadores de comparaci\u00f3n](src/operators/test_comparison.py) (`==`, `!=`, `>`, `<`, `>=`, `<=`)\n    - [Operadores l\u00f3gicos](src/operators/test_logical.py) (`and`, `or`, `not`)\n    - [Operadores de identidad](src/operators/test_identity.py) (`is`, `is not`)\n    - [Operadores de asociaci\u00f3n](src/operators/test_membership.py) (`in`, `not in`)\n3. **Tipos de datos**\n    - [N\u00fameros](src/data_types/test_numbers.py) (incluyendo booleans)\n    - [Strings](src/data_types/test_strings.py) y sus m\u00e9todos\n    - [Listas](src/data_types/test_lists.py) y sus m\u00e9todos (incluyendo comprensi\u00f3n de listas)\n    - [Tuples](src/data_types/test_tuples.py)\n    - [Sets](src/data_types/test_sets.py) y sus m\u00e9todos\n    - [Diccionarios](src/data_types/test_dictionaries.py)\n    - [Tipo de casting](src/data_types/test_type_casting.py)\n4. **Control de flujo**\n    - [La sentencia `if`](src/control_flow/test_if.py)\n    - [La sentencia `for`](src/control_flow/test_for.py) (y la funci\u00f3n `range()`)\n    - [La sentencia `while`](src/control_flow/test_while.py)\n    - [La sentencia `try`](src/control_flow/test_try.py)\n    - [La sentencia `break`](src/control_flow/test_break.py)\n    - [La sentencia `continue`](src/control_flow/test_continue.py)\n5. **Funciones**\n    - [Definici\u00f3n de funci\u00f3n](src/functions/test_function_definition.py) (sentencias `def` y `return`)\n    - [\u00c1mbito de variables dentro de funciones](src/functions/test_function_scopes.py) (sentencias `global` y `nonlocal`)\n    - [Valores de argumento predeterminados](src/functions/test_function_default_arguments.py)\n    - [Argumentos de palabras clave](src/functions/test_function_keyword_arguments.py)\n    - [Listas de argumento arbitrario](src/functions/test_function_arbitrary_arguments.py)\n    - [Listas de argumentos en funciones](src/functions/test_function_unpacking_arguments.py) (sentencias `*` y `**`)\n    - [Expresiones Lambda](src/functions/test_lambda_expressions.py) (sentencia `lambda`)\n    - [Strings de documentaci\u00f3n](src/functions/test_function_documentation_string.py)\n    - [Anotaciones en funciones](src/functions/test_function_annotations.py)\n    - [Decoradores de funciones](src/functions/test_function_decorators.py)\n6. **Clases**\n    - [Definici\u00f3n de clase](src/classes/test_class_definition.py) (sentencia `class`)\n    - [Objetos de clase](src/classes/test_class_objects.py)\n    - [Objetos de instancia](src/classes/test_instance_objects.py)\n    - [M\u00e9todos de objetos](src/classes/test_method_objects.py)\n    - [Variables de clase y de instancia](src/classes/test_class_and_instance_variables.py)\n    - [Herencia](src/classes/test_inheritance.py)\n    - [Herencia m\u00faltiple](src/classes/test_multiple_inheritance.py)\n7. **M\u00f3dulos**\n    - [M\u00f3dulos](src/modules/test_modules.py) (sentencia `import`)\n    - [Paquetes](src/modules/test_packages.py)\n8. **Errores y excepciones**\n    - [Controlando excepciones](src/exceptions/test_handle_exceptions.py) (sentencia `try`)\n    - [Generando excepciones](src/exceptions/test_raise_exceptions.py) (sentencia `raise`) \n9. **Archivos**\n    - [Leyendo y escribiendo](src/files/test_file_reading.py) (sentencia `with`)\n    - [M\u00e9todos de objetos de archivo](src/files/test_file_methods.py)\n10. **Adicionales**\n    - [La sentencia `pass`](src/additions/test_pass.py)\n    - [Generadores](src/additions/test_generators.py) (sentencia `yield`)\n11. **Peque\u00f1o tour de las librer\u00edas est\u00e1ndar**\n    - [Serializaci\u00f3n](src/standard_libraries/test_json.py) (librer\u00eda `json`)\n    - [Par\u00e1metros en archivos](src/standard_libraries/test_glob.py) (librer\u00eda `glob`)\n    - [Expresiones regulares](src/standard_libraries/test_re.py) (librear\u00eda `re`)\n    - [Matem\u00e1tica](src/standard_libraries/test_math.py) (librer\u00edas `math`, `random` y `statistics`)\n    - [Fechas y horas](src/standard_libraries/test_datetime.py) (librer\u00eda `datetime`)\n    - [Compresi\u00f3n de datos](src/standard_libraries/test_zlib.py) (librear\u00eda `zlib`)\n\n## Pre-requisitos\n\n**Instalando Python**\n\nAseg\u00farate de que tienes [Python3 instalado](https://realpython.com/installing-python/) en tu sistema.\n\nPodr\u00edas utilizar la librer\u00eda est\u00e1ndar [venv](https://docs.python.org/es/3/library/venv.html) para crear\nentornos virtuales y tener Python, pip y todos los paquetes instalados en el directorio de tu\nproyecto local para evitar cometer errores con paquetes del sistema y sus versiones.\n\nDependiendo de la instalaci\u00f3n, tendr\u00e1s acceso a Python3 ejecutando `python` o `python3`. Lo mismo\naplica para el administrador de paquetes pip - puedes tener acceso a \u00e9l ejecutando `pip` o `pip3`.\n\nPuedes ver tu versi\u00f3n actual de Python ejecutando:\n\n```bash\npython --version\n```\n\nTen en cuenta que cuando leas `python` en este repositorio, se asume que es Python **3**.\n\n**Instalando dependencias**\n\nInstala todas las depencias requeridas para el proyecto ejecutando:\n\n```bash\npip install -r requirements.txt\n```\n\n## Probando el c\u00f3digo\n\nLas pruebas son hechas usando el framework [pytest](https://docs.pytest.org/en/latest/).\n\nPuedes a\u00f1adir m\u00e1s pruebas por ti mismo a\u00f1adiendo archivos y funciones con el prefijo `test_`\n(ej: `test_topic.py` con la funci\u00f3n `def test_sub_topic()` adentro).\n\nPara ejecutar todas las pruebas, por favor escribe el siguiente comando desde el directorio\nra\u00edz del proyecto:\n\n```bash\npytest\n```\n\nPara ejecutar diferentes pruebas escribe:\n\n```bash\npytest ./path/to/the/test_file.py\n```\n\n## Revisando el c\u00f3digo\n\nLa revisi\u00f3n del c\u00f3digo est\u00e1 hecha usando las librer\u00edas [pylint](http://pylint.pycqa.org/) y [flake8](http://flake8.pycqa.org/en/latest/).\n\n### PyLint\n\nPara revisar si el c\u00f3digo sigue la gu\u00eda de estilos\n[PEP 8](https://www.python.org/dev/peps/pep-0008/), por favor ejecuta:\n\n```bash\npylint ./src/\n```\n\nEn caso de que linter detecte un error (ej: `missing-docstring`), te recomiendo leer m\u00e1s sobre\nel error concreto ejecutando:\n\n```bash\npylint --help-msg=missing-docstring\n```\n\n[M\u00e1s sobre PyLint](http://pylint.pycqa.org/)\n\n### Flake8\n\nPara revisar si el c\u00f3digo sigue la gu\u00eda de estilos\n[PEP 8](https://www.python.org/dev/peps/pep-0008/), por favor ejecuta:\n\n```bash\nflake8 ./src\n```\n\nO, si quieres ver un output m\u00e1s detallado, ejecuta:\n\n```bash\nflake8 ./src --statistics --show-source --count\n```\n\n[M\u00e1s sobre Flake8](http://flake8.pycqa.org/en/latest/)\n\n## Apoya al proyecto\n\nPuedes apoyar al proyecto a trav\u00e9s de \u2764\ufe0f\ufe0f [GitHub](https://github.com/sponsors/trekhleb) o \u2764\ufe0f\ufe0f [Patreon](https://www.patreon.com/trekhleb).\n"
 },
 {
  "repo": "pyecharts/pyecharts",
  "language": "Python",
  "readme_contents": "<p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/19553554/71825144-2d568180-30d6-11ea-8ee0-63c849cfd934.png\" alt=\"pyecharts logo\" width=200 height=200 />\n</p>\n<h1 align=\"center\">pyecharts</h1>\n<p align=\"center\">\n    <em>Python \u2764\ufe0f ECharts = pyecharts</em>\n</p>\n<p align=\"center\">\n    <a href=\"https://github.com/pyecharts/pyecharts/actions\">\n        <img src=\"https://github.com/pyecharts/pyecharts/actions/workflows/python-app.yml/badge.svg\" alt=\"Github Actions Status\">\n    </a>\n    <a href=\"https://codecov.io/gh/pyecharts/pyecharts\">\n        <img src=\"https://codecov.io/gh/pyecharts/pyecharts/branch/master/graph/badge.svg\" alt=\"Codecov\">\n    </a>\n    <a href=\"https://badge.fury.io/py/pyecharts\">\n        <img src=\"https://badge.fury.io/py/pyecharts.svg\" alt=\"Package version\">\n    </a>\n    <a href=\"https://pypi.org/project/pyecharts/\">\n        <img src=\"https://img.shields.io/pypi/pyversions/pyecharts.svg?colorB=brightgreen\" alt=\"PyPI - Python Version\">\n    </a>\n</p>\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/pyecharts\">\n        <img src=\"https://img.shields.io/pypi/format/pyecharts.svg\" alt=\"PyPI - Format\">\n    </a>\n     <a href=\"https://github.com/pyecharts/pyecharts/pulls\">\n        <img src=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\" alt=\"Contributions welcome\">\n    </a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n        <img src=\"https://img.shields.io/badge/License-MIT-brightgreen.svg\" alt=\"License\">\n    </a>\n</p>\n\n## \ud83d\udce3 Introduction\n\n[Apache ECharts](https://github.com/apache/echarts) is easy-to-use, highly interactive and highly performant javascript visualization library under Apache license. Since its first public release in 2013, it now dominates over 74% of Chinese web front-end market. Yet Python is an expressive language and is loved by data science community. Combining the strength of both technologies, [pyecharts](https://github.com/pyecharts/pyecharts) is born.\n\n## \u2728 Feature highlights\n\n* Simple API, Sleek and method chaining\n* Support 30 + popular charts\n* Suppot data science tools: Jupyter Notebook, JupyterLab, nteract\n* Integrate with Flask\uff0cDjango at ease\n* Easy to use and highly configurable\n* Detailed documentation and examples.\n* More than 400+ geomaps assets for geograpic information processing\n\n## \ud83d\udd30 Installation\n\n**pip install**\n```shell\n$ pip install pyecharts\n```\n\n**Install from source**\n```shell\n$ git clone https://github.com/pyecharts/pyecharts.git\n$ cd pyecharts\n$ pip install -r requirements.txt\n$ python setup.py install\n```\n\n## \ud83d\udcdd Usage\n\n### Local computer\n\n#### HTML\n```python\nfrom pyecharts.charts import Bar\nfrom pyecharts import options as opts\n\nbar = (\n    Bar()\n    .add_xaxis([\"\u886c\u886b\", \"\u6bdb\u8863\", \"\u9886\u5e26\", \"\u88e4\u5b50\", \"\u98ce\u8863\", \"\u9ad8\u8ddf\u978b\", \"\u889c\u5b50\"])\n    .add_yaxis(\"\u5546\u5bb6A\", [114, 55, 27, 101, 125, 27, 105])\n    .add_yaxis(\"\u5546\u5bb6B\", [57, 134, 137, 129, 145, 60, 49])\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"\u67d0\u5546\u573a\u9500\u552e\u60c5\u51b5\"))\n)\nbar.render()\n```\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/19553554/55270272-d6ff1b80-52d7-11e9-820f-30660a068e3e.gif\"  width=\"85%\" />\n</p>\n\n#### image\n```python\nfrom pyecharts.render import make_snapshot\n\n# needs to configure selenium\nmake_snapshot(bar.render(), \"bar.png\")\n```\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/19553554/55270432-7a9cfb80-52d9-11e9-81b5-4ceb4dcd1756.png\"  width=\"85%\" />\n</p>\n\n### Notebook\n\n#### Jupyter Notebook\n\n![](https://user-images.githubusercontent.com/19553554/55270255-b3d46c00-52d7-11e9-8aa5-f7b3819a1e88.png)\n\n#### JupyterLab\n\n![](https://user-images.githubusercontent.com/19553554/55270259-c0f15b00-52d7-11e9-8811-93bfca1cc027.png)\n\n#### Web framework\n\n![](https://user-images.githubusercontent.com/19553554/35081158-3faa7c34-fc4d-11e7-80c9-2de79371374f.gif)\n\n## \ud83d\udd16 Demo\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/19553554/52197440-843a5200-289a-11e9-8601-3ce8d945b04a.gif\" width=\"33%\" alt=\"bar\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52360729-ad640980-2a77-11e9-84e2-feff7e11aea5.gif\" width=\"33%\" alt=\"boxplot\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52535290-4b611800-2d87-11e9-8bf2-b43a54a3bda8.png\" width=\"33%\" alt=\"effectScatter\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52332816-ac5eb800-2a36-11e9-8227-3538976f447d.gif\" width=\"33%\" alt=\"funnel\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52332988-0b243180-2a37-11e9-9db8-eb6b8c86a0de.png\" width=\"33%\" alt=\"gague\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52344575-133f9980-2a56-11e9-93e0-568e484936ce.gif\" width=\"33%\" alt=\"geo\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/35082102-fd8d884a-fc52-11e7-9e40-5f94098d4493.gif\" width=\"33%\" alt=\"geo\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52727805-f7f20280-2ff0-11e9-91ab-cd99848e3127.gif\" width=\"33%\" alt=\"graph\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52345115-6534ef00-2a57-11e9-80cd-9cbfed252139.gif\" width=\"33%\" alt=\"heatmap\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52345490-4a16af00-2a58-11e9-9b43-7bbc86aa05b6.gif\" width=\"33%\" alt=\"kline\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52346064-b7770f80-2a59-11e9-9e03-6dae3a8c637d.gif\" width=\"33%\" alt=\"line\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52347117-248ba480-2a5c-11e9-8402-5a94054dca50.gif\" width=\"33%\" alt=\"liquid\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52347915-0a52c600-2a5e-11e9-8039-41268238576c.gif\" width=\"33%\" alt=\"map\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52535013-e48e2f80-2d83-11e9-8886-ac0d2122d6af.png\" width=\"33%\" alt=\"parallel\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52348202-bb596080-2a5e-11e9-84a7-60732be0743a.gif\" width=\"33%\" alt=\"pie\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/35090457-afc0658e-fc74-11e7-9c58-24c780436287.gif\" width=\"33%\" alt=\"ploar\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52533994-932b7380-2d76-11e9-93b4-0de3132eb941.gif\" width=\"33%\" alt=\"radar\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52348431-420e3d80-2a5f-11e9-8cab-7b415592dc77.gif\" width=\"33%\" alt=\"scatter\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/44004598-5636d74e-9e97-11e8-8a5c-92de6278880d.gif\" width=\"33%\" alt=\"tree\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/35082251-b9e23982-fc53-11e7-8341-e7da1842389f.gif\" width=\"33%\" alt=\"treemap\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52348737-01fb8a80-2a60-11e9-94ac-dacbd7b58811.png\" width=\"33%\" alt=\"wordCloud\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52433989-4f075b80-2b49-11e9-9979-ef32c2d17c96.gif\" width=\"33%\" alt=\"bar3D\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52464826-4baab900-2bb7-11e9-8299-776f5ee43670.gif\" width=\"33%\" alt=\"line3D\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52802261-8d0cfe00-30ba-11e9-8ae7-ae0773770a59.gif\" width=\"33%\" alt=\"sankey\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52464647-aee81b80-2bb6-11e9-864e-c544392e523a.gif\" width=\"33%\" alt=\"scatter3D\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52465183-a55fb300-2bb8-11e9-8c10-4519c4e3f758.gif\" width=\"33%\" alt=\"surface3D\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52798246-7ebae400-30b2-11e9-8489-6c10339c3429.gif\" width=\"33%\" alt=\"themeRiver\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/52349544-c2ce3900-2a61-11e9-82af-28aaaaae0d67.gif\" width=\"33%\" alt=\"overlap\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/35089737-ccc1c01c-fc72-11e7-874d-8ba8b89572eb.png\" width=\"33%\" alt=\"grid\"/>\n<img src=\"https://user-images.githubusercontent.com/19553554/35082279-e111743c-fc53-11e7-9362-580160593715.gif\" width=\"33%\" alt=\"timeline\"/>\n</div>\n\nFor more documentation, please visit\n\n* [Chinese documentation](https://pyecharts.org/#/zh-cn/)\n* [English Documentation](https://pyecharts.org/#/en-us/)\n* [Example Documentation](https://gallery.pyecharts.org/)\n\n## \u26cf Software development\n\n### Unit tests\n\n```shell\n$ pip install -r test/requirements.txt\n$ make\n```\n\n### Team development\n\n[Travis CI](https://travis-ci.org/) and [AppVeyor](https://ci.appveyor.com/) is place for continuous integration.\n\n### Coding styles\n\n[flake8](http://flake8.pycqa.org/en/latest/index.html), [Codecov](https://codecov.io/) and [pylint](https://www.pylint.org/) are used\n\n## \ud83d\ude09 Author\n\npyecharts are co-maintained by:\n\n* [@chenjiandongx](https://github.com/chenjiandongx)\n* [@chfw](https://github.com/chfw)\n* [@kinegratii](https://github.com/kinegratii)\n* [@sunhailin-Leo](https://github.com/sunhailin-Leo)\n\nFor more contributors, please visit [pyecharts/graphs/contributors](https://github.com/pyecharts/pyecharts/graphs/contributors)\n\n## \ud83d\udc8c Donation\n\nTo develop and maintain pyecharts, it took me a lot of overnights. If you think pyecharts has helped you, please consider buying me a coffee:\n\n<img src=\"https://user-images.githubusercontent.com/19553554/35425853-500d6b5c-0299-11e8-80a1-ebb6629b497e.png\" width=\"19.8%\" alt=\"Alipay\">\u3000\u3000\u3000<img src=\"https://user-images.githubusercontent.com/19553554/35425854-504e716a-0299-11e8-81fc-4a511f1c47e8.png\" width=\"20%\" alt=\"Wechat\">\n\n\nPlease also buy the other maintainer a coffee if you think their work helped you too [donation details](http://pyecharts.org/#/zh-cn/donate)\n\n## \ud83d\udcc3 License\n\nMIT [\u00a9chenjiandongx](https://github.com/chenjiandongx)\n"
 },
 {
  "repo": "tgbot-collection/YYeTsBot",
  "language": "Python",
  "readme_contents": "# YYeTsBot\r\n\r\n[![build docker image](https://github.com/tgbot-collection/YYeTsBot/actions/workflows/builder.yaml/badge.svg)](https://github.com/tgbot-collection/YYeTsBot/actions/workflows/builder.yaml)\r\n[![Docker Pulls](https://img.shields.io/docker/pulls/bennythink/yyetsbot)](https://hub.docker.com/r/bennythink/yyetsbot)\r\n\r\n* \u4eba\u4eba\u5f71\u89c6bot\uff0c[\u6233\u6211\u4f7f\u7528](https://t.me/yyets_bot)\r\n\r\n* \u4eba\u4eba\u5f71\u89c6\u5206\u4eab\u7ad9\uff0c[\u6233\u6211\u4f7f\u7528](https://yyets.dmesg.app/)\r\n\r\n\u673a\u5668\u4eba\u548c\u7f51\u7ad9\u7531\u6211\u957f\u671f\u7ef4\u62a4\uff0c\u5982\u679c\u9047\u5230\u95ee\u9898\u53ef\u4ee5\u63d0issue\u3002\r\n\r\n![](assets/index.png)\r\n\r\n\ud83d\udc49 \u524d\u7aef[\u5728\u8fd9\u91cc](https://github.com/tgbot-collection/YYeTsFE) \ud83d\udc48\r\n\r\n# \u4f7f\u7528\u8bf4\u660e\r\n\r\n\u76f4\u63a5\u53d1\u9001\u60f3\u8981\u770b\u7684\u5267\u96c6\u540d\u79f0\u5c31\u53ef\u4ee5\u4e86\uff0c\u53ef\u9009\u5206\u4eab\u7f51\u9875\u6216\u8005\u94fe\u63a5\uff08ed2k\u548c\u78c1\u529b\u94fe\u63a5\uff09\u3002\r\n\r\n\u652f\u6301\u5b57\u5e55\u4fa0\u3001\u4eba\u4eba\u5f71\u89c6\u79bb\u7ebf\u8d44\u6e90\r\n\r\n\u641c\u7d22\u8d44\u6e90\u65f6\uff0c\u4f1a\u6309\u7167\u6211\u9884\u5b9a\u7684\u4f18\u5148\u7ea7\uff08\u4eba\u4eba\u5f71\u89c6\u79bb\u7ebf\u3001\u5b57\u5e55\u4fa0\uff09\u8fdb\u884c\u641c\u7d22\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4\u5f3a\u5236\u67d0\u4e2a\u5b57\u5e55\u7ec4\uff0c\u5982 `/yyets_offline \u9003\u907f\u53ef\u803b`\r\n\r\n**\u7531\u4e8e\u8bd1\u540d\u7684\u4e0d\u540c\uff0c\u5efa\u8bae\u8f93\u5165\u90e8\u5206\u8bd1\u540d\uff0c\u7136\u540e\u4ece\u5217\u8868\u4e2d\u8fdb\u884c\u9009\u62e9\u3002\u6bd4\u5982\u8bf4\u60f3\u770b\u6743\u529b\u7684\u6e38\u620f\u7b2c\u56db\u5b63\uff0c\u90a3\u4e48\u76f4\u63a5\u641c\u7d22\"\u6743\u529b\u7684\u6e38\u620f\"\u5c31\u53ef\u4ee5\u4e86\u3002**\r\n\r\n## \u547d\u4ee4\r\n\r\n```\r\nstart - \u5f00\u59cb\u4f7f\u7528\r\nhelp - \u5e2e\u52a9\r\ncredits - \u81f4\u8c22\r\nping - \u8fd0\u884c\u72b6\u6001\r\nsettings - \u83b7\u53d6\u516c\u544a\r\nzimuxia_online - \u5b57\u5e55\u4fa0\u5728\u7ebf\u6570\u636e  \r\nnewzmz_online - new\u5b57\u5e55\u7ec4\u5728\u7ebf\u6570\u636e \r\nyyets_offline - \u4eba\u4eba\u5f71\u89c6\u79bb\u7ebf\u6570\u636e\r\n```\r\n\r\n# \u622a\u56fe\r\n\r\n## \u5e38\u89c4\u641c\u7d22\r\n\r\n![](assets/1.png)\r\n\r\n## \u8d44\u6e90\u5206\u4eab\u7ad9\u622a\u56fe\r\n\r\n\u672c\u7f51\u7ad9\u6c38\u4e45\u514d\u8d39\uff0c\u5e76\u4e14\u6ca1\u6709\u4efb\u4f55\u9650\u5236\u3002\r\n![](assets/new_resource.png)\r\n\r\n![](assets/2.png)\r\n\r\n\u652f\u6301\u6536\u85cf\u529f\u80fd\uff0c\u4f1a\u8de8\u8bbe\u5907\u540c\u6b65\r\n![](assets/like.png)\r\n\r\n## \u6307\u5b9a\u5b57\u5e55\u7ec4\u641c\u7d22\r\n\r\n\u76ee\u524d\u53ea\u652f\u6301YYeTsOffline\u3001ZimuxiaOnline\u548cNewzmzOnline\r\n\r\n![](assets/3.png)\r\n\r\n# \u5982\u4f55\u4e0b\u8f7d\u78c1\u529b\u548c\u7535\u9a74\u8d44\u6e90\uff1f\u8fc5\u96f7\u63d0\u793a\u8d44\u6e90\u654f\u611f\r\n\r\n## \u7535\u9a74\u8d44\u6e90\r\n\r\n\u8bf7\u4e0b\u8f7d\u4f7f\u7528 [eMule](https://www.emule-project.net/home/perl/general.cgi?l=42) \uff0c\u7136\u540e\u6dfb\u52a0\u5982\u4e0b\u4e24\u4e2aserver list\r\n\r\n* [server.met](http://www.server-met.de/)\r\n* [server list for emule](https://www.emule-security.org/serverlist/)\r\n\r\n![](assets/emule.jpeg)\r\n\u901f\u5ea6\u8fd8\u53ef\u4ee5\u54e6\r\n\r\n## \u78c1\u529b\r\n\r\n\u4f7f\u7528\u767e\u5ea6\u7f51\u76d8\u3001115\u7b49\u79bb\u7ebf\uff0c\u6216\u4f7f\u7528utorrent\u7b49\u5de5\u5177\uff0c\u8bb0\u5f97\u66f4\u65b0\u4e0b [tracker list](https://raw.githubusercontent.com/ngosang/trackerslist/master/trackers_all.txt)\r\n\u54e6\r\n\r\n# \u5c0f\u767d\u4f7f\u7528\r\n\r\n\u60f3\u8981\u81ea\u5df1\u7559\u4e00\u4efd\u8d44\u6e90\uff0c\u4f46\u662f\u53c8\u4e0d\u61c2\u7f16\u7a0b\uff1f \u6ca1\u5173\u7cfb\uff01\u76ee\u524d\u63d0\u4f9b\u4e24\u79cd\u65b9\u5f0f\uff0c\u8bf7\u6839\u636e\u81ea\u5df1\u60c5\u51b5\u9009\u62e9\r\n\r\n## \u4e00\u952e\u5b89\u88c5\u5305\r\n\r\n\u8fd9\u4e2a\u7248\u672c\u662f\u65b0\u7684UI\uff0c\u62e5\u6709\u5168\u90e8\u7684\u6700\u65b0\u529f\u80fd\u3002\r\n[\u53c2\u8003\u6587\u6863](https://github.com/tgbot-collection/YYeTsBot/blob/master/DEVELOPMENT.md#%E4%B8%80%E9%94%AE%E8%84%9A%E6%9C%AC)\r\n\r\n## \u4e00\u952e\u8fd0\u884c\u5305\r\n\r\n\u8fd9\u4e2a\u7248\u672c\u4f7f\u7528\u8d77\u6765\u4e5f\u5f88\u7b80\u5355\uff0c\u4e5f\u540c\u6837\u662f\u6700\u65b0\u7684UI\uff0c\u53ea\u4e0d\u8fc7\u53ea\u6709\u6700\u57fa\u7840\u7684\u641c\u7d22\u529f\u80fd\u3002\u6b65\u9aa4\u5982\u4e0b\r\n\r\n1. \u8bf7\u5230 [GitHub Release](https://github.com/tgbot-collection/YYeTsBot/releases) \u6839\u636e\u81ea\u5df1\u5e73\u53f0\u4e0b\u8f7d\u6700\u65b0\u7684\u4e00\u952e\u8fd0\u884c\u5305\r\n2. windows\uff1a\u53cc\u51fb\u7b2c\u4e00\u6b65\u4e0b\u8f7d\u7684exe\u6587\u4ef6\uff1b macos/Linux\uff0ccd\u5230\u4f60\u7684\u76ee\u5f55, `chmod +x yyetsweb ; ./yyetsweb`\r\n3. \u7a0b\u5e8f\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u6570\u636e\u5e93\u5e76\u542f\u52a8\u3002\u7b49\u5230\u51fa\u73b0\u542f\u52a8banner\u65f6\uff0c \u6253\u5f00\u6d4f\u89c8\u5668 http://127.0.0.1:8888 \u5c31\u53ef\u4ee5\u770b\u5230\u719f\u6089\u7684\u641c\u7d22\u754c\u9762\u5566\uff01\r\n\r\n# \u5f00\u53d1\r\n\r\n## \u7f51\u7ad9\u5f00\u53d1\r\n\r\n\u5982\u4f55\u90e8\u7f72\u3001\u53c2\u4e0e\u5f00\u53d1\u3001\u5177\u4f53API\u63a5\u53e3\uff0c\u53ef\u4ee5 [\u53c2\u8003\u8fd9\u4e2a\u6587\u6863](DEVELOPMENT.md)\r\n\r\n## Python Library\r\n\r\n\u4e5f\u53ef\u4ee5\u4f5c\u4e3aPython Library\u53bb\u8c03\u7528\r\n\r\n`pip3 install yyets`\r\n\r\n```\r\n>>> from yyets import YYeTs\r\n>>> yy=YYeTs(\"\u9003\u907f\")\r\n[2021-09-21 19:22:32 __init__.py:54 I] Fetching \u9003\u907f\u53ef\u803b\u5374\u6709\u7528...https://yyets.dmesg.app/api/resource?id=34812\r\n[2021-09-21 19:22:33 __init__.py:54 I] Fetching \u65e0\u6cd5\u9003\u907f...https://yyets.dmesg.app/api/resource?id=29540\r\n[2021-09-21 19:22:35 __init__.py:54 I] Fetching \u9003\u907f\u8005...https://yyets.dmesg.app/api/resource?id=37089\r\n\r\n>>> yy.result\r\n[<yyets.Resource object at 0x10cc7b130>, <yyets.Resource object at 0x10ca0e880>, <yyets.Resource object at 0x10cc7b040>]\r\n\r\n>>> for y in yy.result:\r\n        print(y)\r\n    \r\n\u9003\u907f\u53ef\u803b\u5374\u6709\u7528 - NIGERUHA HAJIDAGA YAKUNITATSU\r\n\u65e0\u6cd5\u9003\u907f - Inescapable\r\n\u9003\u907f\u8005 - Shirkers\r\n\r\n>>> yy.result[0].cnname\r\n'\u9003\u907f\u53ef\u803b\u5374\u6709\u7528'\r\n\r\n>>> yy.result[0].list\r\n[{'season_num': '101', 'season_cn': '\u5355\u5267', 'items': {'APP': [{'ite\r\n```\r\n\r\n# Credits\r\n\r\n* [\u4eba\u4eba\u5f71\u89c6](http://www.zmz2019.com/)\r\n* [\u8ffd\u65b0\u756a](http://www.fanxinzhui.com/)\r\n* [FIX\u5b57\u5e55\u4fa0](https://www.zimuxia.cn/)\r\n* [new\u5b57\u5e55\u7ec4](https://newzmz.com/)\r\n\r\n# \u652f\u6301\u6211\r\n\r\n\u89c9\u5f97\u672c\u9879\u76ee\u5bf9\u4f60\u6709\u5e2e\u52a9\uff1f\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8868\u8fbe\u4f60\u7684\u611f\u53d7\uff1a\r\n\r\n* \u611f\u8c22\u5b57\u5e55\u7ec4\r\n* \u70b9\u4e00\u4e2astar\ud83c\udf1f\u548cfork\ud83c\udf74\r\n* \u5ba3\u4f20\uff0c\u4f7f\u7528\uff0c\u63d0\u4ea4\u95ee\u9898\u62a5\u544a\r\n* \u6536\u85cf[\u6211\u7684\u535a\u5ba2](https://dmesg.app/)\r\n* [Telegram Channel](https://t.me/mikuri520)\r\n* \u6350\u52a9\u6211\uff0c[\u7ed9\u6211\u4e70\u676f\u5496\u5561\uff1f](https://www.buymeacoffee.com/bennythink)\r\n* \u6350\u52a9\u6211\uff0c[\u7231\u53d1\u7535\uff1f](https://afdian.net/@BennyThink)\r\n\r\n# \u611f\u8c22\r\n\r\n\u611f\u8c22\u6240\u6709[\u652f\u6301\u672c\u9879\u76ee](SPONSOR.md)\u7684\u4eba\uff01\r\n\r\n# License\r\n\r\n[MIT](LICENSE)\r\n"
 },
 {
  "repo": "jindongwang/transferlearning",
  "language": "Python",
  "readme_contents": "[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n\n<h1 align=\"center\">\n  <br>\n  <img src=\"png/logo.jpg\" alt=\"Transfer Leanring\" width=\"500\">\n</h1>\n\n<h4 align=\"center\">Everything about Transfer Learning. \u8fc1\u79fb\u5b66\u4e60.</h4>\n\n<p align=\"center\">\n  <strong><a href=\"#0papers-\u8bba\u6587\">Papers</a></strong> \u2022\n  <strong><a href=\"#1introduction-and-tutorials-\u7b80\u4ecb\u4e0e\u6559\u7a0b\">Tutorials</a></strong> \u2022\n  <a href=\"#2transfer-learning-areas-and-papers-\u7814\u7a76\u9886\u57df\u4e0e\u76f8\u5173\u8bba\u6587\">Research areas</a> \u2022\n  <a href=\"#3theory-and-survey-\u7406\u8bba\u4e0e\u7efc\u8ff0\">Theory</a> \u2022\n  <a href=\"#3theory-and-survey-\u7406\u8bba\u4e0e\u7efc\u8ff0\">Survey</a> \u2022\n  <strong><a href=\"https://github.com/jindongwang/transferlearning/tree/master/code\">Code</a></strong> \u2022\n  <strong><a href=\"#7datasets-and-benchmarks-\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u7ed3\u679c\">Dataset & benchmark</a></strong>\n</p>\n<p align=\"center\">\n  <a href=\"#6transfer-learning-thesis-\u7855\u535a\u58eb\u8bba\u6587\">Thesis</a> \u2022\n  <a href=\"#5transfer-learning-scholars-\u8457\u540d\u5b66\u8005\">Scholars</a> \u2022\n  <a href=\"#8transfer-learning-challenges-\u8fc1\u79fb\u5b66\u4e60\u6bd4\u8d5b\">Contests</a> \u2022\n  <a href=\"#journals-and-conferences\">Journal/conference</a> \u2022\n  <a href=\"#applications-\u8fc1\u79fb\u5b66\u4e60\u5e94\u7528\">Applications</a> \u2022\n  <a href=\"#other-resources-\u5176\u4ed6\u8d44\u6e90\">Others</a> \u2022\n  <a href=\"#contributing-\u6b22\u8fce\u53c2\u4e0e\u8d21\u732e\">Contributing</a>\n</p>\n\n**Widely used by top conferences and journals:** \n- Conferences: [[CVPR'22](https://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Zhang_Segmenting_Across_Places_The_Need_for_Fair_Transfer_Learning_With_CVPRW_2022_paper.html)] [[NeurIPS'21](https://proceedings.neurips.cc/paper/2021/file/731b03008e834f92a03085ef47061c4a-Paper.pdf)] [[IJCAI'21](https://arxiv.org/abs/2103.03097)] [[ESEC/FSE'20](https://dl.acm.org/doi/abs/10.1145/3368089.3409696)] [[IJCNN'20](https://ieeexplore.ieee.org/abstract/document/9207556)] [[ACMMM'18](https://dl.acm.org/doi/abs/10.1145/3240508.3240512)] [[ICME'19](https://ieeexplore.ieee.org/abstract/document/8784776/)]\n- Journals: [[IEEE TKDE](https://ieeexplore.ieee.org/abstract/document/9782500/)] [[ACM TIST](https://dl.acm.org/doi/abs/10.1145/3360309)] [[Information sciences](https://www.sciencedirect.com/science/article/pii/S0020025520308458)] [[Neurocomputing](https://www.sciencedirect.com/science/article/pii/S0925231221007025)] [[IEEE Transactions on Cognitive and Developmental Systems](https://ieeexplore.ieee.org/abstract/document/9659817)]\n\n```\n@Misc{transferlearning.xyz,\nhowpublished = {\\url{http://transferlearning.xyz}},   \ntitle = {Everything about Transfer Learning and Domain Adapation},  \nauthor = {Wang, Jindong and others}  \n}  \n```\n\n[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE) [![996.icu](https://img.shields.io/badge/link-996.icu-red.svg)](https://996.icu) \n\nRelated repos\uff1a[[USB: unified semi-supervised learning benchmark](https://github.com/microsoft/Semi-supervised-learning)] | [[TorchSSL: a unified SSL library](https://github.com/TorchSSL/TorchSSL)] | [[PersonalizedFL: library for personalized federated learning](https://github.com/microsoft/PersonalizedFL)] | [[Activity recognition](https://github.com/jindongwang/activityrecognition)]\uff5c[[Machine learning](https://github.com/jindongwang/MachineLearning)]\n\n- - -\n\n**NOTE:** You can directly open the code in [Gihub Codespaces](https://docs.github.com/en/codespaces/getting-started/quickstart#introduction) on the web to run them without downloading! Also, try [github.dev](https://github.dev/jindongwang/transferlearning).\n\n## 0.Papers (\u8bba\u6587)\n\n[Awesome transfer learning papers (\u8fc1\u79fb\u5b66\u4e60\u6587\u7ae0\u6c47\u603b)](https://github.com/jindongwang/transferlearning/tree/master/doc/awesome_paper.md)\n\n- [Paperweekly](http://www.paperweekly.site/collections/231/papers): A website to recommend and read paper notes\n\n**Latest papers**: \n\n- By topic: [doc/awesome_papers.md](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md)\n- By date: [[2022-09](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-09)] [[2022-08](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-08)] [[2022-07](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-07)] [[2022-06](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-06)] [[2022-05](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-05)] [[2022-04](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-04)] [[2022-03](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-03)] [[2022-02](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-02)] [[2022-01](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2022-01)] [[2021-12](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2021-12)] [[2021-11](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2021-11)] [[2021-10](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2021-10)] [[2021-09](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2021-09)] [[2021-08](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2021-08)] [[2021-07](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper_date.md#2021-07)]\n\n*Updated at 2022-10-17:*\n\n- The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning [[arxiv](https://openreview.net/forum?id=Qs3EfpieOh)]\n  - Evolution of OOD robustness by fine-tuning \n\n- Visual Prompt Tuning for Test-time Domain Adaptation [[arxiv](http://arxiv.org/abs/2210.04831)]\n  - VPT for test-time adaptation \u7528prompt tuning\u8fdb\u884ctest-time DA\n\n*Updated at 2022-10-10:*\n\n- Unsupervised Domain Adaptation for COVID-19 Information Service with Contrastive Adversarial Domain Mixup [[arxiv](https://arxiv.org/abs/2210.03250)]\n  - Domain adaptation for COVID-19 \u7528DA\u8fdb\u884cCOVID-19\u9884\u6d4b\n\n- ICONIP'22 IDPL: Intra-subdomain adaptation adversarial learning segmentation method based on Dynamic Pseudo Labels [[arxiv](https://arxiv.org/abs/2210.03435)]\n  - Intra-domain adaptation for segmentation \u5b50\u9886\u57df\u5bf9\u6297Adaptation\n\n- NeurIPS'22 Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks [[arxiv](https://arxiv.org/abs/2210.03265)]\n  - Parameter-efficient multi-task adaptation \u53c2\u6570\u9ad8\u6548\u7684\u591a\u4efb\u52a1adaptation\n\n- Out-of-Distribution Generalization in Algorithmic Reasoning Through Curriculum Learning [[arxiv](https://arxiv.org/abs/2210.03275)]\n  - OOD in algorithmic reasoning \u7b97\u6cd5reasoning\u8fc7\u7a0b\u4e2d\u7684OOD\n\n- Towards Out-of-Distribution Adversarial Robustness [[arxiv](https://arxiv.org/abs/2210.03150)]\n  - OOD adversarial robustness OOD\u5bf9\u6297\u9c81\u68d2\u6027\n\n*Updated at 2022-10-08:*\n\n- TripleE: Easy Domain Generalization via Episodic Replay [[arxiv](https://arxiv.org/pdf/2210.01807.pdf)]\n  - Easy domain generalization by episodic replay\n\n- Deep Spatial Domain Generalization [[arxiv](https://web7.arxiv.org/pdf/2210.00729.pdf)]\n  - Deep spatial domain generalization\n\n- - -\n\n## 1.Introduction and Tutorials (\u7b80\u4ecb\u4e0e\u6559\u7a0b)\n\nWant to quickly learn transfer learning\uff1f\u60f3\u5c3d\u5feb\u5165\u95e8\u8fc1\u79fb\u5b66\u4e60\uff1f\u770b\u4e0b\u9762\u7684\u6559\u7a0b\u3002\n\n- Books \u4e66\u7c4d\n  - **\u300a\u8fc1\u79fb\u5b66\u4e60\u300b\uff08\u6768\u5f3a\uff09** [[Buy](https://item.jd.com/12930984.html)] [[English version](https://www.cambridge.org/core/books/transfer-learning/CCFFAFE3CDBC245047F1DEC71D9EF3C7)]\n  - **\u300a\u8fc1\u79fb\u5b66\u4e60\u5bfc\u8bba\u300b(\u738b\u664b\u4e1c\u3001\u9648\u76ca\u5f3a\u8457)** [[Homepage](http://jd92.wang/tlbook)] [[Buy](https://item.jd.com/13272157.html)]\n\n- Blogs \u535a\u5ba2\n  - [Zhihu blogs - \u77e5\u4e4e\u4e13\u680f\u300a\u5c0f\u738b\u7231\u8fc1\u79fb\u300b\u7cfb\u5217\u6587\u7ae0](https://zhuanlan.zhihu.com/p/130244395)\n\t\n- Video tutorials \u89c6\u9891\u6559\u7a0b\n  - Transfer learning \u8fc1\u79fb\u5b66\u4e60:\n    - [Recent advance of transfer learning - 2022\u5e74\u6700\u65b0\u8fc1\u79fb\u5b66\u4e60\u53d1\u5c55\u73b0\u72b6\u63a2\u8ba8](https://www.bilibili.com/video/BV1nY411E7Uc/)\n    - [Definitions of transfer learning area - \u8fc1\u79fb\u5b66\u4e60\u9886\u57df\u540d\u8bcd\u89e3\u91ca](https://www.bilibili.com/video/BV1fu411o7BW) [[Article](https://zhuanlan.zhihu.com/p/428097044)]\n    - [Transfer learning by Hung-yi Lee @ NTU - \u53f0\u6e7e\u5927\u5b66\u674e\u5b8f\u6bc5\u7684\u89c6\u9891\u8bb2\u89e3(\u4e2d\u6587\u89c6\u9891)](https://www.youtube.com/watch?v=qD6iD4TFsdQ)\n  - Domain generalization \u9886\u57df\u6cdb\u5316\uff1a\n    - [IJCAI-ECAI'22 tutorial on domain generalization - \u9886\u57df\u6cdb\u5316tutorial](https://dgresearch.github.io/)\n    - [Domain generalization - \u8fc1\u79fb\u5b66\u4e60\u65b0\u5174\u7814\u7a76\u65b9\u5411\u9886\u57df\u6cdb\u5316](https://www.bilibili.com/video/BV1ro4y1S7dd/)\n  - Domain adaptation \u9886\u57df\u81ea\u9002\u5e94\uff1a\n    - [Domain adaptation - \u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5(\u4e2d\u6587)](https://www.bilibili.com/video/BV1T7411R75a/) \n  \n\n- Brief introduction and slides \u7b80\u4ecb\u4e0eppt\u8d44\u6599\n  - [Recent advance of transfer learning](https://jd92.wang/assets/files/l16_aitime.pdf)\n  - [Domain generalization survey](http://jd92.wang/assets/files/DGSurvey-ppt.pdf)\n  - [Brief introduction in Chinese](https://github.com/jindongwang/transferlearning/blob/master/doc/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B.md)\n\t- [PPT (English)](http://jd92.wang/assets/files/l03_transferlearning.pdf) | [PPT (\u4e2d\u6587)](http://jd92.wang/assets/files/l08_tl_zh.pdf)\n  - \u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5 Domain adaptation: [PDF](http://jd92.wang/assets/files/l12_da.pdf) \uff5c [Video on Bilibili](https://www.bilibili.com/video/BV1T7411R75a/) | [Video on Youtube](https://www.youtube.com/watch?v=RbIsHNtluwQ&t=22s)\n  - Tutorial on transfer learning by Qiang Yang: [IJCAI'13](http://ijcai13.org/files/tutorial_slides/td2.pdf) | [2016 version](http://kddchina.org/file/IntroTL2016.pdf)\n\n- Talk is cheap, show me the code \u52a8\u624b\u6559\u7a0b\u3001\u4ee3\u7801\u3001\u6570\u636e \n  - [Pytorch tutorial on transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n\t- [Pytorch finetune](https://github.com/jindongwang/transferlearning/tree/master/code/AlexNet_ResNet)\n\t- [DeepDA: a unified deep domain adaptation toolbox](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDA)\n\t- [DeepDG: a unified deep domain generalization toolbox](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG)\n\t- [\u66f4\u591a More...](https://github.com/jindongwang/transferlearning/tree/master/code)\n\n- [Transfer Learning Scholars and Labs - \u8fc1\u79fb\u5b66\u4e60\u9886\u57df\u7684\u8457\u540d\u5b66\u8005\u3001\u4ee3\u8868\u5de5\u4f5c\u53ca\u5b9e\u9a8c\u5ba4\u4ecb\u7ecd](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)\n- [Negative transfer - \u8d1f\u8fc1\u79fb](https://www.zhihu.com/question/66492194/answer/242870418)\n\n- - -\n\n## 2.Transfer Learning Areas and Papers (\u7814\u7a76\u9886\u57df\u4e0e\u76f8\u5173\u8bba\u6587)\n\n- [Survey](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#survey)\n- [Theory](#theory)\n- [Per-training/Finetuning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#per-trainingfinetuning)\n- [Knowledge distillation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#knowledge-distillation)\n- [Traditional domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#traditional-domain-adaptation)\n- [Deep domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#deep-domain-adaptation)\n- [Domain generalization](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#domain-generalization)\n- [Source-free domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#source-free-domain-adaptation)\n- [Multi-source domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#multi-source-domain-adaptation)\n- [Heterogeneous transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#heterogeneous-transfer-learning)\n- [Online transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#online-transfer-learning)\n- [Zero-shot / few-shot learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#zero-shot--few-shot-learning)\n- [Multi-task learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#multi-task-learning)\n- [Transfer reinforcement learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-reinforcement-learning)\n- [Transfer metric learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-metric-learning)\n- [Federated transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#federated-transfer-learning)\n- [Lifelong transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#lifelong-transfer-learning)\n- [Safe transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#safe-transfer-learning)\n- [Transfer learning applications](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-learning-applications)\n\n- - -\n\n## 3.Theory and Survey (\u7406\u8bba\u4e0e\u7efc\u8ff0)\n\nHere are some articles on transfer learning theory and survey.\n\n**Survey (\u7efc\u8ff0\u6587\u7ae0)\uff1a**\n\n- 2022 [Transfer Learning for Future Wireless Networks: A Comprehensive Survey](https://arxiv.org/abs/2102.07572)\n- 2022 [A Review of Deep Transfer Learning and Recent Advancements](https://arxiv.org/abs/2201.09679)\n- 2022 [Transferability in Deep Learning: A Survey](https://paperswithcode.com/paper/transferability-in-deep-learning-a-survey), from Mingsheng Long in THU.\n- 2021 Domain generalization: IJCAI-21 [Generalizing to Unseen Domains: A Survey on Domain Generalization](https://arxiv.org/abs/2103.03097) | [\u77e5\u4e4e\u6587\u7ae0](https://zhuanlan.zhihu.com/p/354740610) | [\u5fae\u4fe1\u516c\u4f17\u53f7](https://mp.weixin.qq.com/s/DsoVDYqLB1N7gj9X5UnYqw)\n  - First survey on domain generalization\n  - \u7b2c\u4e00\u7bc7\u5bf9Domain generalization (\u9886\u57df\u6cdb\u5316)\u7684\u7efc\u8ff0\n- 2021 Vision-based activity recognition: [A Survey of Vision-Based Transfer Learning in Human Activity Recognition](https://www.mdpi.com/2079-9292/10/19/2412)\n- 2021 ICSAI [A State-of-the-Art Survey of Transfer Learning in Structural Health Monitoring](https://ieeexplore.ieee.org/abstract/document/9664171)\n- 2020 [Transfer learning: survey and classification](https://link.springer.com/chapter/10.1007/978-981-15-5345-5_13), Advances in Intelligent Systems and Computing. \n- 2020 \u8fc1\u79fb\u5b66\u4e60\u6700\u65b0survey\uff0c\u6765\u81ea\u4e2d\u79d1\u9662\u8ba1\u7b97\u6240\u5e84\u798f\u632f\u56e2\u961f\uff0c\u53d1\u8868\u5728Proceedings of the IEEE: [A Comprehensive Survey on Transfer Learning](https://arxiv.org/abs/1911.02685)\n- 2020 \u8d1f\u8fc1\u79fb\u7684\u7efc\u8ff0\uff1a[Overcoming Negative Transfer: A Survey](https://arxiv.org/abs/2009.00909)\n- 2020 \u77e5\u8bc6\u84b8\u998f\u7684\u7efc\u8ff0: [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525)\n- \u7528transfer learning\u8fdb\u884csentiment classification\u7684\u7efc\u8ff0\uff1a[A Survey of Sentiment Analysis Based on Transfer Learning](https://ieeexplore.ieee.org/abstract/document/8746210) \n- 2019 \u4e00\u7bc7\u65b0survey\uff1a[Transfer Adaptation Learning: A Decade Survey](https://arxiv.org/abs/1903.04687)\n- 2018 \u4e00\u7bc7\u8fc1\u79fb\u5ea6\u91cf\u5b66\u4e60\u7684\u7efc\u8ff0: [Transfer Metric Learning: Algorithms, Applications and Outlooks](https://arxiv.org/abs/1810.03944)\n- 2018 \u4e00\u7bc7\u6700\u8fd1\u7684\u975e\u5bf9\u79f0\u60c5\u51b5\u4e0b\u7684\u5f02\u6784\u8fc1\u79fb\u5b66\u4e60\u7efc\u8ff0\uff1a[Asymmetric Heterogeneous Transfer Learning: A Survey](https://arxiv.org/abs/1804.10834)\n- 2018 Neural style transfer\u7684\u4e00\u4e2asurvey\uff1a[Neural Style Transfer: A Review](https://arxiv.org/abs/1705.04058)\n- 2018 \u6df1\u5ea6domain adaptation\u7684\u4e00\u4e2a\u7efc\u8ff0\uff1a[Deep Visual Domain Adaptation: A Survey](https://www.sciencedirect.com/science/article/pii/S0925231218306684)\n- 2017 \u591a\u4efb\u52a1\u5b66\u4e60\u7684\u7efc\u8ff0\uff0c\u6765\u81ea\u9999\u6e2f\u79d1\u6280\u5927\u5b66\u6768\u5f3a\u56e2\u961f\uff1a[A survey on multi-task learning](https://arxiv.org/abs/1707.08114)\n- 2017 \u5f02\u6784\u8fc1\u79fb\u5b66\u4e60\u7684\u7efc\u8ff0\uff1a[A survey on heterogeneous transfer learning](https://link.springer.com/article/10.1186/s40537-017-0089-0)\n- 2017 \u8de8\u9886\u57df\u6570\u636e\u8bc6\u522b\u7684\u7efc\u8ff0\uff1a[Cross-dataset recognition: a survey](https://arxiv.org/abs/1705.04396)\n- 2016 [A survey of transfer learning](https://pan.baidu.com/s/1gfgXLXT)\u3002\u5176\u4e2d\u4ea4\u4ee3\u4e86\u4e00\u4e9b\u6bd4\u8f83\u7ecf\u5178\u7684\u5982\u540c\u6784\u3001\u5f02\u6784\u7b49\u5b66\u4e60\u65b9\u6cd5\u4ee3\u8868\u6027\u6587\u7ae0\u3002\n- 2015 \u4e2d\u6587\u7efc\u8ff0\uff1a[\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u8fdb\u5c55](https://pan.baidu.com/s/1bpautob)\n- 2010 [A survey on transfer learning](http://ieeexplore.ieee.org/abstract/document/5288526/)\n- Survey on applications - \u5e94\u7528\u5bfc\u5411\u7684\u7efc\u8ff0\uff1a\n\t- \u89c6\u89c9domain adaptation\u7efc\u8ff0\uff1a[Visual Domain Adaptation: A Survey of Recent Advances](https://pan.baidu.com/s/1o8BR7Vc)\n\t- \u8fc1\u79fb\u5b66\u4e60\u5e94\u7528\u4e8e\u884c\u4e3a\u8bc6\u522b\u7efc\u8ff0\uff1a[Transfer Learning for Activity Recognition: A Survey](https://pan.baidu.com/s/1kVABOYr)\n\t- \u8fc1\u79fb\u5b66\u4e60\u4e0e\u589e\u5f3a\u5b66\u4e60\uff1a[Transfer Learning for Reinforcement Learning Domains: A Survey](https://pan.baidu.com/s/1slfr0w1)\n\t- \u591a\u4e2a\u6e90\u57df\u8fdb\u884c\u8fc1\u79fb\u7684\u7efc\u8ff0\uff1a[A Survey of Multi-source Domain Adaptation](https://pan.baidu.com/s/1eSGREF4)\u3002\n\n**Theory \uff08\u7406\u8bba\u6587\u7ae0\uff09:**\n\n- ICML-20 [Few-shot domain adaptation by causal mechanism transfer](https://arxiv.org/pdf/2002.03497.pdf)\n\t- The first work on causal transfer learning\n\t- \u65e5\u672c\u7406\u8bba\u7ec4\u5927\u4f6cSugiyama\u7684\u5de5\u4f5c\uff0ccausal transfer learning\n- CVPR-19 [Characterizing and Avoiding Negative Transfer](https://arxiv.org/abs/1811.09751)\n\t- Characterizing and avoid negative transfer\n\t- \u5f62\u5f0f\u5316\u5e76\u63d0\u51fa\u5982\u4f55\u907f\u514d\u8d1f\u8fc1\u79fb\n- ICML-20 [On Learning Language-Invariant Representations for Universal Machine Translation](https://arxiv.org/abs/2008.04510)\n  - Theory for universal machine translation\n  - \u5bf9\u7edf\u4e00\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u8fdb\u884c\u4e86\u7406\u8bba\u8bba\u8bc1\n- NIPS-06 [Analysis of Representations for Domain Adaptation](https://dl.acm.org/citation.cfm?id=2976474)\n- ML-10 [A Theory of Learning from Different Domains](https://link.springer.com/article/10.1007/s10994-009-5152-4)\n- NIPS-08 [Learning Bounds for Domain Adaptation](http://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation)\n- COLT-09 [Domain adaptation: Learning bounds and algorithms](https://arxiv.org/abs/0902.3430)\n- MMD paper\uff1a[A Hilbert Space Embedding for Distributions](https://link.springer.com/chapter/10.1007/978-3-540-75225-7_5) and [A Kernel Two-Sample Test](http://www.jmlr.org/papers/v13/gretton12a.html)\n- Multi-kernel MMD paper: [Optimal kernel choice for large-scale two-sample tests](http://papers.nips.cc/paper/4727-optimal-kernel-choice-for-large-scale-two-sample-tests)\n\n_ _ _\n\n## 4.Code (\u4ee3\u7801)\n\nUnified codebases for:\n- [Deep domain adaptation](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDA)\n- [Deep domain generalization](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG)\n- See all codes here: https://github.com/jindongwang/transferlearning/tree/master/code.\n\nMore: see [HERE](https://github.com/jindongwang/transferlearning/tree/master/code) and [HERE](https://colab.research.google.com/drive/1MVuk95mMg4ecGyUAIG94vedF81HtWQAr?usp=sharing) for an instant run using Google's Colab.\n\n_ _ _\n\n## 5.Transfer Learning Scholars (\u8457\u540d\u5b66\u8005)\n\nHere are some transfer learning scholars and labs.\n\n**\u5168\u90e8\u5217\u8868\u4ee5\u53ca\u4ee3\u8868\u5de5\u4f5c\u6027\u89c1[\u8fd9\u91cc](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)** \n\nPlease note that this list is far not complete. A full list can be seen in [here](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md). Transfer learning is an active field. *If you are aware of some scholars, please add them here.*\n\n_ _ _\n\n## 6.Transfer Learning Thesis (\u7855\u535a\u58eb\u8bba\u6587)\n\nHere are some popular thesis on transfer learning.\n\n[\u8fd9\u91cc](https://pan.baidu.com/share/init?surl=iuzZhHdumrD64-yx_VAybA), \u63d0\u53d6\u7801\uff1atxyz\u3002\n\n- - -\n\n## 7.Datasets and Benchmarks (\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u7ed3\u679c)\n\nPlease see [HERE](https://github.com/jindongwang/transferlearning/blob/master/data) for the popular transfer learning **datasets and benchmark** results.\n\n[\u8fd9\u91cc](https://github.com/jindongwang/transferlearning/blob/master/data)\u6574\u7406\u4e86\u5e38\u7528\u7684\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e00\u4e9b\u5df2\u53d1\u8868\u7684\u6587\u7ae0\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002\n\n- - -\n\n## 8.Transfer Learning Challenges (\u8fc1\u79fb\u5b66\u4e60\u6bd4\u8d5b)\n\n- [Visual Domain Adaptation Challenge (VisDA)](http://ai.bu.edu/visda-2018/)\n\n- - -\n\n## Journals and Conferences\n\nSee [here](https://github.com/jindongwang/transferlearning/blob/master/doc/venues.md) for a full list of related journals and conferences.\n\n- - -\n\n## Applications (\u8fc1\u79fb\u5b66\u4e60\u5e94\u7528)\n\n- [Computer vision](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#computer-vision)\n- [Medical and healthcare](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#medical-and-healthcare)\n- [Natural language processing](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#natural-language-processing)\n- [Time series](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#time-series)\n- [Speech](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#speech)\n- [Multimedia](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#multimedia)\n- [Recommendation](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#recommendation)\n- [Human activity recognition](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#human-activity-recognition)\n- [Autonomous driving](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#autonomous-driving)\n- [Others](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#others)\n\nSee [HERE](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md) for transfer learning applications.\n\n\u8fc1\u79fb\u5b66\u4e60\u5e94\u7528\u8bf7\u89c1[\u8fd9\u91cc](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md)\u3002\n\n- - -\n\n## Other Resources (\u5176\u4ed6\u8d44\u6e90)\n\n- Call for papers:\n  - [Advances in Transfer Learning: Theory, Algorithms, and Applications](https://www.frontiersin.org/research-topics/21133/advances-in-transfer-learning-theory-algorithms-and-applications), DDL: October 2021\n\n- Related projects:\n  - Salad: [A semi-supervised domain adaptation library](https://domainadaptation.org)\n\n- - -\n\n## Contributing (\u6b22\u8fce\u53c2\u4e0e\u8d21\u732e)\n\nIf you are interested in contributing, please refer to [HERE](https://github.com/jindongwang/transferlearning/blob/master/CONTRIBUTING.md) for instructions in contribution.\n\n- - -\n\n### Copyright notice\n\n> ***[Notes]This Github repo can be used by following the corresponding licenses. I want to emphasis that it may contain some PDFs or thesis, which were downloaded by me and can only be used for academic purposes. The copyrights of these materials are owned by corresponding publishers or organizations. All this are for better adademic research. If any of the authors or publishers have concerns, please contact me to delete or replace them.***\n\n[contributors-shield]: https://img.shields.io/github/contributors/jindongwang/transferlearning.svg?style=for-the-badge\n[contributors-url]: https://github.com/jindongwang/transferlearning/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/jindongwang/transferlearning.svg?style=for-the-badge\n[forks-url]: https://github.com/jindongwang/transferlearning/network/members\n[stars-shield]: https://img.shields.io/github/stars/jindongwang/transferlearning.svg?style=for-the-badge\n[stars-url]: https://github.com/jindongwang/transferlearning/stargazers\n[issues-shield]: https://img.shields.io/github/issues/jindongwang/transferlearning.svg?style=for-the-badge\n[issues-url]: https://github.com/jindongwang/transferlearning/issues\n[license-shield]: https://img.shields.io/github/license/jindongwang/transferlearning.svg?style=for-the-badge\n[license-url]: https://github.com/jindongwang/transferlearning/blob/main/LICENSE.txt\n"
 },
 {
  "repo": "pre-commit/pre-commit",
  "language": "Python",
  "readme_contents": "[![Build Status](https://dev.azure.com/asottile/asottile/_apis/build/status/pre-commit.pre-commit?branchName=main)](https://dev.azure.com/asottile/asottile/_build/latest?definitionId=21&branchName=main)\n[![Azure DevOps coverage](https://img.shields.io/azure-devops/coverage/asottile/asottile/21/main.svg)](https://dev.azure.com/asottile/asottile/_build/latest?definitionId=21&branchName=main)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/pre-commit/pre-commit/main.svg)](https://results.pre-commit.ci/latest/github/pre-commit/pre-commit/main)\n\n## pre-commit\n\nA framework for managing and maintaining multi-language pre-commit hooks.\n\nFor more information see: https://pre-commit.com/\n"
 },
 {
  "repo": "facebook/chisel",
  "language": "Python",
  "readme_contents": "<a href=\"https://opensource.facebook.com/support-ukraine\">\n  <img src=\"https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB\" alt=\"Support Ukraine - Help Provide Humanitarian Aid to Ukraine.\" />\n</a>\n\n# Chisel\n`Chisel` is a collection of `LLDB` commands to assist in the debugging of iOS apps.\n\n[[Installation](#installation) &bull; [Commands](#commands) &bull; [Custom Commands](#custom-commands) &bull; [Development Workflow](#development-workflow) [Contributing](#contributing) &bull; [License](#license)]\n\nFor a comprehensive overview of LLDB, and how Chisel complements it, read Ari Grant's [Dancing in the Debugger \u2014 A Waltz with LLDB](http://www.objc.io/issue-19/lldb-debugging.html) in issue 19 of [objc.io](http://www.objc.io/).\n\n## Installation\n\n```shell\nbrew update\nbrew install chisel\n```\n\nif `.lldbinit` file doesn't exist you can create it & open it by tapping on the terminal\n\n ```shell\n touch .lldbinit \n open .lldbinit \n```\n\nThen add the following line to your `~/.lldbinit` file.\n\n```Python\n# ~/.lldbinit\n...\ncommand script import /usr/local/opt/chisel/libexec/fbchisellldb.py\n```\n\n* Note that if you are installing on an M1 Mac, the path above should be `/opt/homebrew/opt/chisel/libexec/fbchisellldb.py` instead.\n\nAlternatively, download chisel and add the following line to your _~/.lldbinit_ file.\n\n```Python\n# ~/.lldbinit\n...\ncommand script import /path/to/fbchisellldb.py\n\n```\n\nThe commands will be available the next time `Xcode` starts.\n\n## Commands\nThere are many commands; here's a few:\n*(Compatibility with iOS/Mac indicated at right)*\n\n|Command          |Description     |iOS    |OS X   |\n|-----------------|----------------|-------|-------|\n|pviews           |Print the recursive view description for the key window.|Yes|Yes|\n|pvc              |Print the recursive view controller description for the key window.|Yes|No|\n|visualize        |Open a `UIImage`, `CGImageRef`, `UIView`, `CALayer`, `NSData` (of an image), `UIColor`, `CIColor`, or `CGColorRef` in Preview.app on your Mac.|Yes|No|\n|fv               |Find a view in the hierarchy whose class name matches the provided regex.|Yes|No|\n|fvc              |Find a view controller in the hierarchy whose class name matches the provided regex.|Yes|No|\n|show/hide        |Show or hide the given view or layer. You don't even have to continue the process to see the changes!|Yes|Yes|\n|mask/unmask      |Overlay a view or layer with a transparent rectangle to visualize where it is.|Yes|No|\n|border/unborder  |Add a border to a view or layer to visualize where it is.|Yes|Yes|\n|caflush          |Flush the render server (equivalent to a \"repaint\" if no animations are in-flight).|Yes|Yes|\n|bmessage         |Set a symbolic breakpoint on the method of a class or the method of an instance without worrying which class in the hierarchy actually implements the method.|Yes|Yes|\n|wivar            |Set a watchpoint on an instance variable of an object.|Yes|Yes|\n|presponder       |Print the responder chain starting from the given object.|Yes|Yes|\n|...              |... and many more!|\n\nTo see the list of **all** of the commands execute the help command in `LLDB` or go to the [Wiki](https://github.com/facebook/chisel/wiki).\n\n```Python\n(lldb) help\nThe following is a list of built-in, permanent debugger commands:\n...\n\nThe following is a list of your current user-defined commands:\n...\n```\n\nThe bottom list contains all the commands sourced from `Chisel`.\n\nYou can also inspect a specific command by passing its name as an argument to the help command (as with all other `LLDB` commands). \n\n```\n(lldb) help border\nDraws a border around <viewOrLayer>. Color and width can be optionally provided.\n\nArguments:\n  <viewOrLayer>; Type: UIView*; The view to border.\n\nOptions:\n  --color/-c <color>; Type: string; A color name such as 'red', 'green', 'magenta', etc.\n  --width/-w <width>; Type: CGFloat; Desired width of border.\n\nSyntax: border [--color=color] [--width=width] <viewOrLayer>\n```\n\nAll of the commands provided by `Chisel` come with verbose help. Be sure to read it when in doubt!\n\n## Custom Commands\nYou can add local, custom commands. Here's a contrived example.\n\n```python\n#!/usr/bin/python\n# Example file with custom commands, located at /magical/commands/example.py\n\nimport lldb\nimport fbchisellldbbase as fb\n\ndef lldbcommands():\n  return [ PrintKeyWindowLevel() ]\n  \nclass PrintKeyWindowLevel(fb.FBCommand):\n  def name(self):\n    return 'pkeywinlevel'\n    \n  def description(self):\n    return 'An incredibly contrived command that prints the window level of the key window.'\n    \n  def run(self, arguments, options):\n    # It's a good habit to explicitly cast the type of all return\n    # values and arguments. LLDB can't always find them on its own.\n    lldb.debugger.HandleCommand('p (CGFloat)[(id)[(id)[UIApplication sharedApplication] keyWindow] windowLevel]')\n```\n\nThen all that's left is to source the commands in lldbinit. `Chisel` has a python function just for this, _loadCommandsInDirectory_ in the _fbobjclldb.py_ module.\n\n```Python\n# ~/.lldbinit\n...\ncommand script import /path/to/fbobjclldb.py\nscript fbobjclldb.loadCommandsInDirectory('/magical/commands/')\n\n```\n\nThere's also builtin support to make it super easy to specify the arguments and options that a command takes. See the _border_ and _pinvocation_ commands for example use.\n\n## Development Workflow\nDeveloping commands, whether for local use or contributing to `Chisel` directly, both follow the same workflow. Create a command as described in the [Custom Commands](#custom-commands) section and then\n\n1. Start `LLDB`\n2. Reach a breakpoint (or simply pause execution via the pause button in `Xcode`'s debug bar or `process interrupt` if attached directly)\n3. Execute `command source ~/.lldbinit` in LLDB to source the commands\n4. Run the command you are working on\n5. Modify the command\n6. Optionally run `script reload(modulename)`\n7. Repeat steps 3-6 until the command becomes a source of happiness\n\n## Contributing\nPlease contribute any generic commands that you make. If it helps you then it will likely help many others! :D See `CONTRIBUTING.md` to learn how to contribute.\n\n## License\n`Chisel` is MIT-licensed. See `LICENSE`.\n"
 },
 {
  "repo": "smicallef/spiderfoot",
  "language": "Python",
  "readme_contents": "<a href=\"https://www.spiderfoot.net/r.php?u=aHR0cHM6Ly93d3cuc3BpZGVyZm9vdC5uZXQv&s=os_gh\"><img src=\"https://www.spiderfoot.net/wp-content/themes/spiderfoot/img/spiderfoot-wide.png\"></a>\n\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/smicallef/spiderfoot/master/LICENSE)\n[![Python Version](https://img.shields.io/badge/python-3.7+-green)](https://www.python.org)\n[![Stable Release](https://img.shields.io/badge/version-4.0-blue.svg)](https://github.com/smicallef/spiderfoot/releases/tag/v4.0)\n[![CI status](https://github.com/smicallef/spiderfoot/workflows/Tests/badge.svg)](https://github.com/smicallef/spiderfoot/actions?query=workflow%3A\"Tests\")\n[![Last Commit](https://img.shields.io/github/last-commit/smicallef/spiderfoot)](https://github.com/smicallef/spiderfoot/commits/master)\n[![Codecov](https://codecov.io/github/smicallef/spiderfoot/coverage.svg)](https://codecov.io/github/smicallef/spiderfoot)\n[![Twitter Follow](https://img.shields.io/twitter/follow/spiderfoot?label=follow&style=social)](https://twitter.com/spiderfoot)\n[![Discord](https://img.shields.io/discord/770524432464216074)](https://discord.gg/vyvztrG)\n\n**SpiderFoot** is an open source intelligence (OSINT) automation tool. It integrates with just about every data source available and utilises a range of methods for data analysis, making that data easy to navigate. \n\nSpiderFoot has an embedded web-server for providing a clean and intuitive web-based interface but can also be used completely via the command-line.  It's written in **Python 3** and **MIT-licensed**.\n\n<img src=\"https://www.spiderfoot.net/wp-content/uploads/2022/04/opensource-screenshot-v4.png\" />\n\n### FEATURES\n\n- Web based UI or CLI\n- Over 200 modules (see below)\n- Python 3.7+\n- YAML-configurable [correlation engine](/correlations/README.md) with [37 pre-defined rules](/correlations)\n- CSV/JSON/GEXF export\n- API key export/import\n- SQLite back-end for custom querying\n- Highly configurable\n- Fully documented\n- Visualisations\n- TOR integration for dark web searching\n- Dockerfile for Docker-based deployments\n- Can call other tools like DNSTwist, Whatweb, Nmap and CMSeeK\n- [Actively developed since 2012!](https://medium.com/@micallst/lessons-learned-from-my-10-year-open-source-project-4a4c8c2b4f64)\n\n### WANT MORE?\n\nNeed more from SpiderFoot? Check out [SpiderFoot HX](https://www.spiderfoot.net/hx) for:\n- 100% Cloud-based and managed for you\n- Attack Surface Monitoring with change notifications by email, REST and Slack\n- Multiple targets per scan\n- Multi-user collaboration\n- Authenticated and 2FA\n- Investigations\n- Customer support\n- Third party tools pre-installed & configured\n- Drive it with a fully RESTful API\n- TOR integration built-in\n- Screenshotting\n- Bring your own Python SpiderFoot modules\n- Feed scan data to Splunk, ElasticSearch and REST endpoints\n\nSee the full set of differences between SpiderFoot HX and the open source version [here](https://www.spiderfoot.net/open-source-vs-hx/).\n\n### USES\n\nSpiderFoot can be used offensively (e.g. in a red team exercise or penetration test) for reconnaissance of your target or defensively to gather information about what you or your organisation might have exposed over the Internet.\n\nYou can target the following entities in a SpiderFoot scan:\n\n - IP address\n - Domain/sub-domain name\n - Hostname\n - Network subnet (CIDR)\n - ASN\n - E-mail address\n - Phone number\n - Username\n - Person's name\n - Bitcoin address\n \nSpiderFoot's 200+ modules feed each other in a publisher/subscriber model to ensure maximum data extraction to do things like:\n\n- [Host/sub-domain/TLD enumeration/extraction](https://asciinema.org/a/295912)\n- [Email address, phone number and human name extraction](https://asciinema.org/a/295947)\n- [Bitcoin and Ethereum address extraction](https://asciinema.org/a/295957)\n- [Check for susceptibility to sub-domain hijacking](https://asciinema.org/a/344377)\n- DNS zone transfers\n- [Threat intelligence and Blacklist queries](https://asciinema.org/a/295949)\n- API integration with [SHODAN](https://asciinema.org/a/127601), [HaveIBeenPwned](https://asciinema.org/a/128731), [GreyNoise](https://asciinema.org/a/295943), AlienVault, SecurityTrails, etc.\n- [Social media account enumeration](https://asciinema.org/a/295923)\n- [S3/Azure/Digitalocean bucket enumeration/scraping](https://asciinema.org/a/295941)\n- IP geo-location\n- Web scraping, web content analysis\n- [Image, document and binary file meta data analysis](https://asciinema.org/a/296274)\n- Dark web searches\n- [Port scanning and banner grabbing](https://asciinema.org/a/295939)\n- [Data breach searches](https://asciinema.org/a/296145)\n- So much more...\n\n### INSTALLING & RUNNING\n\nTo install and run SpiderFoot, you need at least Python 3.7 and a number of Python libraries which you can install with `pip`. We recommend you install a packaged release since master will often have bleeding edge features and modules that aren't fully tested.\n\n#### Stable build (packaged release):\n\n```\n$ wget https://github.com/smicallef/spiderfoot/archive/v4.0.tar.gz\n$ tar zxvf v4.0.tar.gz\n$ cd spiderfoot-4.0\n$ pip3 install -r requirements.txt\n$ python3 ./sf.py -l 127.0.0.1:5001\n```\n\n#### Development build (cloning git master branch):\n\n```\n$ git clone https://github.com/smicallef/spiderfoot.git\n$ cd spiderfoot\n$ pip3 install -r requirements.txt\n$ python3 ./sf.py -l 127.0.0.1:5001\n```\n\nCheck out the [documentation](https://www.spiderfoot.net/documentation) and our [asciinema videos](https://asciinema.org/~spiderfoot) for more tutorials.\n\n### COMMUNITY\n\nWhether you're a contributor, user or just curious about SpiderFoot and OSINT in general, we'd love to have you join our community! SpiderFoot now has a [Discord server](https://discord.gg/vyvztrG) for seeking help from the community, requesting features or just general OSINT chit-chat.\n\n### WRITING CORRELATION RULES\n\nWe have a comprehensive write-up and reference of the correlation rule-set introduced in SpiderFoot 4.0 [here](/correlations/README.md).\n\nAlso take a look at the [template.yaml](/correlations/template.yaml) file for a walk through. The existing [37 rules](/correlations) are also quite readable and good as starting points for additional rules.\n\n### MODULES / INTEGRATIONS\n\nSpiderFoot has over 200 modules, most of which *don't require API keys*, and many of those that do require API keys *have a free tier*.\n\n| Name     | Description | Type   |\n|:---------| :-----------|:-------|\n[AbstractAPI](https://app.abstractapi.com/)|Look up domain, phone and IP address information from AbstractAPI.|Tiered API\n[abuse.ch](https://www.abuse.ch)|Check if a host/domain, IP address or netblock is malicious according to Abuse.ch.|Free API\n[AbuseIPDB](https://www.abuseipdb.com)|Check if an IP address is malicious according to AbuseIPDB.com blacklist.|Tiered API\n[Abusix Mail Intelligence](https://abusix.org/)|Check if a netblock or IP address is in the Abusix Mail Intelligence blacklist.|Tiered API\nAccount Finder|Look for possible associated accounts on over 500 social and other websites such as Instagram, Reddit, etc.|Internal\n[AdBlock Check](https://adblockplus.org/)|Check if linked pages would be blocked by AdBlock Plus.|Tiered API\n[AdGuard DNS](https://adguard.com/)|Check if a host would be blocked by AdGuard DNS.|Free API\n[Ahmia](https://ahmia.fi/)|Search Tor 'Ahmia' search engine for mentions of the target.|Free API\n[AlienVault IP Reputation](https://cybersecurity.att.com/)|Check if an IP or netblock is malicious according to the AlienVault IP Reputation database.|Free API\n[AlienVault OTX](https://otx.alienvault.com/)|Obtain information from AlienVault Open Threat Exchange (OTX)|Tiered API\n[Amazon S3 Bucket Finder](https://aws.amazon.com/s3/)|Search for potential Amazon S3 buckets associated with the target and attempt to list their contents.|Free API\n[Apple iTunes](https://itunes.apple.com/)|Search Apple iTunes for mobile apps.|Free API\n[Archive.org](https://archive.org/)|Identifies historic versions of interesting files/pages from the Wayback Machine.|Free API\n[ARIN](https://www.arin.net/)|Queries ARIN registry for contact information.|Free API\n[Azure Blob Finder](https://azure.microsoft.com/en-in/services/storage/blobs/)|Search for potential Azure blobs associated with the target and attempt to list their contents.|Free API\n[Bad Packets](https://badpackets.net)|Obtain information about any malicious activities involving IP addresses found|Commercial API\nBase64 Decoder|Identify Base64-encoded strings in URLs, often revealing interesting hidden information.|Internal\n[BGPView](https://bgpview.io/)|Obtain network information from BGPView API.|Free API\nBinary String Extractor|Attempt to identify strings in binary content.|Internal\n[BinaryEdge](https://www.binaryedge.io/)|Obtain information from BinaryEdge.io Internet scanning systems, including breaches, vulnerabilities, torrents and passive DNS.|Tiered API\n[Bing (Shared IPs)](https://www.bing.com/)|Search Bing for hosts sharing the same IP.|Tiered API\n[Bing](https://www.bing.com/)|Obtain information from bing to identify sub-domains and links.|Tiered API\nBitcoin Finder|Identify bitcoin addresses in scraped webpages.|Internal\n[Bitcoin Who's Who](https://bitcoinwhoswho.com/)|Check for Bitcoin addresses against the Bitcoin Who's Who database of suspect/malicious addresses.|Tiered API\n[BitcoinAbuse](https://www.bitcoinabuse.com/)|Check Bitcoin addresses against the bitcoinabuse.com database of suspect/malicious addresses.|Free API\n[Blockchain](https://www.blockchain.com/)|Queries blockchain.info to find the balance of identified bitcoin wallet addresses.|Free API\n[blocklist.de](http://www.blocklist.de/en/index.html)|Check if a netblock or IP is malicious according to blocklist.de.|Free API\n[BotScout](https://botscout.com/)|Searches BotScout.com's database of spam-bot IP addresses and e-mail addresses.|Tiered API\n[botvrij.eu](https://botvrij.eu/)|Check if a domain is malicious according to botvrij.eu.|Free API\n[BuiltWith](https://builtwith.com/)|Query BuiltWith.com's Domain API for information about your target's web technology stack, e-mail addresses and more.|Tiered API\n[C99](https://api.c99.nl/)|Queries the C99 API which offers various data (geo location, proxy detection, phone lookup, etc).|Commercial API\n[CallerName](http://callername.com/)|Lookup US phone number location and reputation information.|Free API\n[Censys](https://censys.io/)|Obtain host information from Censys.io.|Tiered API\n[Certificate Transparency](https://crt.sh/)|Gather hostnames from historical certificates in crt.sh.|Free API\n[CertSpotter](https://sslmate.com/certspotter/)|Gather information about SSL certificates from SSLMate CertSpotter API.|Tiered API\n[CINS Army List](https://cinsscore.com/)|Check if a netblock or IP address is malicious according to Collective Intelligence Network Security (CINS) Army list.|Free API\n[CIRCL.LU](https://www.circl.lu/)|Obtain information from CIRCL.LU's Passive DNS and Passive SSL databases.|Free API\n[CleanBrowsing.org](https://cleanbrowsing.org/)|Check if a host would be blocked by CleanBrowsing.org DNS content filters.|Free API\n[CleanTalk Spam List](https://cleantalk.org)|Check if a netblock or IP address is on CleanTalk.org's spam IP list.|Free API\n[Clearbit](https://clearbit.com/)|Check for names, addresses, domains and more based on lookups of e-mail addresses on clearbit.com.|Tiered API\n[CloudFlare DNS](https://www.cloudflare.com/)|Check if a host would be blocked by CloudFlare DNS.|Free API\n[CoinBlocker Lists](https://zerodot1.gitlab.io/CoinBlockerListsWeb/)|Check if a domain appears on CoinBlocker lists.|Free API\n[CommonCrawl](http://commoncrawl.org/)|Searches for URLs found through CommonCrawl.org.|Free API\n[Comodo Secure DNS](https://www.comodo.com/secure-dns/)|Check if a host would be blocked by Comodo Secure DNS.|Tiered API\nCompany Name Extractor|Identify company names in any obtained data.|Internal\nCookie Extractor|Extract Cookies from HTTP headers.|Internal\nCountry Name Extractor|Identify country names in any obtained data.|Internal\nCredit Card Number Extractor|Identify Credit Card Numbers in any data|Internal\n[Crobat API](https://sonar.omnisint.io/)|Search Crobat API for subdomains.|Free API\nCross-Referencer|Identify whether other domains are associated ('Affiliates') of the target by looking for links back to the target site(s).|Internal\n[CRXcavator](https://crxcavator.io/)|Search CRXcavator for Chrome extensions.|Free API\nCustom Threat Feed|Check if a host/domain, netblock, ASN or IP is malicious according to your custom feed.|Internal\n[CyberCrime-Tracker.net](https://cybercrime-tracker.net/)|Check if a host/domain or IP address is malicious according to CyberCrime-Tracker.net.|Free API\n[Debounce](https://debounce.io/)|Check whether an email is disposable|Free API\n[Dehashed](https://www.dehashed.com/)|Gather breach data from Dehashed API.|Commercial API\n[Digital Ocean Space Finder](https://www.digitalocean.com/products/spaces/)|Search for potential Digital Ocean Spaces associated with the target and attempt to list their contents.|Free API\nDNS Brute-forcer|Attempts to identify hostnames through brute-forcing common names and iterations.|Internal\nDNS Common SRV|Attempts to identify hostnames through brute-forcing common DNS SRV records.|Internal\n[DNS for Family](https://dnsforfamily.com/)|Check if a host would be blocked by DNS for Family.|Free API\nDNS Look-aside|Attempt to reverse-resolve the IP addresses next to your target to see if they are related.|Internal\nDNS Raw Records|Retrieves raw DNS records such as MX, TXT and others.|Internal\nDNS Resolver|Resolves hosts and IP addresses identified, also extracted from raw content.|Internal\nDNS Zone Transfer|Attempts to perform a full DNS zone transfer.|Internal\n[DNSDB](https://www.farsightsecurity.com)|Query FarSight's DNSDB for historical and passive DNS data.|Tiered API\n[DNSDumpster](https://dnsdumpster.com/)|Passive subdomain enumeration using HackerTarget's DNSDumpster|Free API\n[DNSGrep](https://opendata.rapid7.com/)|Obtain Passive DNS information from Rapid7 Sonar Project using DNSGrep API.|Free API\n[DroneBL](https://dronebl.org/)|Query the DroneBL database for open relays, open proxies, vulnerable servers, etc.|Free API\n[DuckDuckGo](https://duckduckgo.com/)|Query DuckDuckGo's API for descriptive information about your target.|Free API\nE-Mail Address Extractor|Identify e-mail addresses in any obtained data.|Internal\n[EmailCrawlr](https://emailcrawlr.com/)|Search EmailCrawlr for email addresses and phone numbers associated with a domain.|Tiered API\n[EmailFormat](https://www.email-format.com/)|Look up e-mail addresses on email-format.com.|Free API\n[EmailRep](https://emailrep.io/)|Search EmailRep.io for email address reputation.|Tiered API\n[Emerging Threats](https://rules.emergingthreats.net/)|Check if a netblock or IP address is malicious according to EmergingThreats.net.|Free API\nError String Extractor|Identify common error messages in content like SQL errors, etc.|Internal\nEthereum Address Extractor|Identify ethereum addresses in scraped webpages.|Internal\n[Etherscan](https://etherscan.io)|Queries etherscan.io to find the balance of identified ethereum wallet addresses.|Free API\nFile Metadata Extractor|Extracts meta data from documents and images.|Internal\n[Flickr](https://www.flickr.com/)|Search Flickr for domains, URLs and emails related to the specified domain.|Free API\n[Focsec](https://focsec.com/)|Look up IP address information from Focsec.|Tiered API\n[FortiGuard Antispam](https://www.fortiguard.com/)|Check if an IP address is malicious according to FortiGuard Antispam.|Free API\n[Fraudguard](https://fraudguard.io/)|Obtain threat information from Fraudguard.io|Tiered API\n[F-Secure Riddler.io](https://riddler.io/)|Obtain network information from F-Secure Riddler.io API.|Commercial API\n[FullContact](https://www.fullcontact.com)|Gather domain and e-mail information from FullContact.com API.|Tiered API\n[FullHunt](https://fullhunt.io/)|Identify domain attack surface using FullHunt API.|Tiered API\n[Github](https://github.com/)|Identify associated public code repositories on Github.|Free API\n[GLEIF](https://search.gleif.org/)|Look up company information from Global Legal Entity Identifier Foundation (GLEIF).|Tiered API\n[Google Maps](https://cloud.google.com/maps-platform/)|Identifies potential physical addresses and latitude/longitude coordinates.|Tiered API\n[Google Object Storage Finder](https://cloud.google.com/storage)|Search for potential Google Object Storage buckets associated with the target and attempt to list their contents.|Free API\n[Google SafeBrowsing](https://developers.google.com/safe-browsing/v4/lookup-api)|Check if the URL is included on any of the Safe Browsing lists.|Free API\n[Google](https://developers.google.com/custom-search)|Obtain information from the Google Custom Search API to identify sub-domains and links.|Tiered API\n[Gravatar](https://secure.gravatar.com/)|Retrieve user information from Gravatar API.|Free API\n[Grayhat Warfare](https://buckets.grayhatwarfare.com/)|Find bucket names matching the keyword extracted from a domain from Grayhat API.|Tiered API\n[Greensnow](https://greensnow.co/)|Check if a netblock or IP address is malicious according to greensnow.co.|Free API\n[grep.app](https://grep.app/)|Search grep.app API for links and emails related to the specified domain.|Free API\n[GreyNoise Community](https://greynoise.io/)|Obtain IP enrichment data from GreyNoise Community API|Tiered API\n[GreyNoise](https://greynoise.io/)|Obtain IP enrichment data from GreyNoise|Tiered API\n[HackerOne (Unofficial)](http://www.nobbd.de/)|Check external vulnerability scanning/reporting service h1.nobbd.de to see if the target is listed.|Free API\n[HackerTarget](https://hackertarget.com/)|Search HackerTarget.com for hosts sharing the same IP.|Free API\nHash Extractor|Identify MD5 and SHA hashes in web content, files and more.|Internal\n[HaveIBeenPwned](https://haveibeenpwned.com/)|Check HaveIBeenPwned.com for hacked e-mail addresses identified in breaches.|Commercial API\nHosting Provider Identifier|Find out if any IP addresses identified fall within known 3rd party hosting ranges, e.g. Amazon, Azure, etc.|Internal\n[Host.io](https://host.io)|Obtain information about domain names from host.io.|Tiered API\nHuman Name Extractor|Attempt to identify human names in fetched content.|Internal\n[Hunter.io](https://hunter.io/)|Check for e-mail addresses and names on hunter.io.|Tiered API\n[Hybrid Analysis](https://www.hybrid-analysis.com)|Search Hybrid Analysis for domains and URLs related to the target.|Free API\nIBAN Number Extractor|Identify International Bank Account Numbers (IBANs) in any data.|Internal\n[Iknowwhatyoudownload.com](https://iknowwhatyoudownload.com/en/peer/)|Check iknowwhatyoudownload.com for IP addresses that have been using torrents.|Tiered API\n[Instagram](https://www.instagram.com/)|Gather information from Instagram profiles.|Free API\n[IntelligenceX](https://intelx.io/)|Obtain information from IntelligenceX about identified IP addresses, domains, e-mail addresses and phone numbers.|Tiered API\nInteresting File Finder|Identifies potential files of interest, e.g. office documents, zip files.|Internal\n[Internet Storm Center](https://isc.sans.edu)|Check if an IP address is malicious according to SANS ISC.|Free API\n[ipapi.co](https://ipapi.co/)|Queries ipapi.co to identify geolocation of IP Addresses using ipapi.co API|Tiered API\n[ipapi.com](https://ipapi.com/)|Queries ipapi.com to identify geolocation of IP Addresses using ipapi.com API|Tiered API\n[IPInfo.io](https://ipinfo.io)|Identifies the physical location of IP addresses identified using ipinfo.io.|Tiered API\n[IPQualityScore](https://www.ipqualityscore.com/)|Determine if target is malicious using IPQualityScore API|Tiered API\n[ipregistry](https://ipregistry.co/)|Query the ipregistry.co database for reputation and geo-location.|Tiered API\n[ipstack](https://ipstack.com/)|Identifies the physical location of IP addresses identified using ipstack.com.|Tiered API\n[JsonWHOIS.com](https://jsonwhois.com)|Search JsonWHOIS.com for WHOIS records associated with a domain.|Tiered API\nJunk File Finder|Looks for old/temporary and other similar files.|Internal\n[Keybase](https://keybase.io/)|Obtain additional information about domain names and identified usernames.|Free API\n[Koodous](https://koodous.com/apks/)|Search Koodous for mobile apps.|Tiered API\n[LeakIX](https://leakix.net/)|Search LeakIX for host data leaks, open ports, software and geoip.|Free API\n[Leak-Lookup](https://leak-lookup.com/)|Searches Leak-Lookup.com's database of breaches.|Free API\n[Maltiverse](https://maltiverse.com)|Obtain information about any malicious activities involving IP addresses|Free API\n[MalwarePatrol](https://www.malwarepatrol.net/)|Searches malwarepatrol.net's database of malicious URLs/IPs.|Tiered API\n[MetaDefender](https://metadefender.opswat.com/)|Search MetaDefender API for IP address and domain IP reputation.|Tiered API\n[Mnemonic PassiveDNS](https://www.mnemonic.no)|Obtain Passive DNS information from PassiveDNS.mnemonic.no.|Free API\n[multiproxy.org Open Proxies](https://multiproxy.org/)|Check if an IP address is an open proxy according to multiproxy.org open proxy list.|Free API\n[MySpace](https://myspace.com/)|Gather username and location from MySpace.com profiles.|Free API\n[NameAPI](https://www.nameapi.org/)|Check whether an email is disposable|Tiered API\n[NetworksDB](https://networksdb.io/)|Search NetworksDB.io API for IP address and domain information.|Tiered API\n[NeutrinoAPI](https://www.neutrinoapi.com/)|Search NeutrinoAPI for phone location information, IP address information, and host reputation.|Tiered API\n[numverify](http://numverify.com/)|Lookup phone number location and carrier information from numverify.com.|Tiered API\n[Onion.link](https://onion.link/)|Search Tor 'Onion City' search engine for mentions of the target domain using Google Custom Search.|Free API\n[Onionsearchengine.com](https://as.onionsearchengine.com)|Search Tor onionsearchengine.com for mentions of the target domain.|Free API\n[Onyphe](https://www.onyphe.io)|Check Onyphe data (threat list, geo-location, pastries, vulnerabilities)  about a given IP.|Tiered API\n[Open Bug Bounty](https://www.openbugbounty.org/)|Check external vulnerability scanning/reporting service openbugbounty.org to see if the target is listed.|Free API\n[OpenCorporates](https://opencorporates.com)|Look up company information from OpenCorporates.|Tiered API\n[OpenDNS](https://www.opendns.com/)|Check if a host would be blocked by OpenDNS.|Free API\n[OpenNIC DNS](https://www.opennic.org/)|Resolves host names in the OpenNIC alternative DNS system.|Free API\n[OpenPhish](https://openphish.com/)|Check if a host/domain is malicious according to OpenPhish.com.|Free API\n[OpenStreetMap](https://www.openstreetmap.org/)|Retrieves latitude/longitude coordinates for physical addresses from OpenStreetMap API.|Free API\nPage Information|Obtain information about web pages (do they take passwords, do they contain forms, etc.)|Internal\n[PasteBin](https://pastebin.com/)|PasteBin search (via Google Search API) to identify related content.|Tiered API\nPGP Key Servers|Look up domains and e-mail addresses in PGP public key servers.|Internal\n[PhishStats](https://phishstats.info/)|Check if a netblock or IP address is malicious according to PhishStats.|Free API\n[PhishTank](https://phishtank.com/)|Check if a host/domain is malicious according to PhishTank.|Free API\nPhone Number Extractor|Identify phone numbers in scraped webpages.|Internal\nPort Scanner - TCP|Scans for commonly open TCP ports on Internet-facing systems.|Internal\n[Project Honey Pot](https://www.projecthoneypot.org/)|Query the Project Honey Pot database for IP addresses.|Free API\n[ProjectDiscovery Chaos](https://chaos.projectdiscovery.io)|Search for hosts/subdomains using chaos.projectdiscovery.io|Commercial API\n[Psbdmp](https://psbdmp.cc/)|Check psbdmp.cc (PasteBin Dump) for potentially hacked e-mails and domains.|Free API\n[Pulsedive](https://pulsedive.com/)|Obtain information from Pulsedive's API.|Tiered API\n[PunkSpider](https://punkspider.io/)|Check the QOMPLX punkspider.io service to see if the target is listed as vulnerable.|Free API\n[Quad9](https://quad9.net/)|Check if a host would be blocked by Quad9 DNS.|Free API\n[ReverseWhois](https://www.reversewhois.io/)|Reverse Whois lookups using reversewhois.io.|Free API\n[RIPE](https://www.ripe.net/)|Queries the RIPE registry (includes ARIN data) to identify netblocks and other info.|Free API\n[RiskIQ](https://community.riskiq.com/)|Obtain information from RiskIQ's (formerly PassiveTotal) Passive DNS and Passive SSL databases.|Tiered API\n[Robtex](https://www.robtex.com/)|Search Robtex.com for hosts sharing the same IP.|Free API\n[searchcode](https://searchcode.com/)|Search searchcode for code repositories mentioning the target domain.|Free API\n[SecurityTrails](https://securitytrails.com/)|Obtain Passive DNS and other information from SecurityTrails|Tiered API\n[Seon](https://seon.io/)|Queries seon.io to gather intelligence about IP Addresses, email addresses, and phone numbers|Commercial API\n[SHODAN](https://www.shodan.io/)|Obtain information from SHODAN about identified IP addresses.|Tiered API\nSimilar Domain Finder|Search various sources to identify similar looking domain names, for instance squatted domains.|Internal\n[Skymem](http://www.skymem.info/)|Look up e-mail addresses on Skymem.|Free API\n[SlideShare](https://www.slideshare.net)|Gather name and location from SlideShare profiles.|Free API\n[Snov](https://snov.io/)|Gather available email IDs from identified domains|Tiered API\n[Social Links](https://sociallinks.io/)|Queries SocialLinks.io to gather intelligence from social media platforms and dark web.|Commercial API\n[Social Media Profile Finder](https://developers.google.com/custom-search)|Tries to discover the social media profiles for human names identified.|Tiered API\nSocial Network Identifier|Identify presence on social media networks such as LinkedIn, Twitter and others.|Internal\n[SORBS](http://www.sorbs.net/)|Query the SORBS database for open relays, open proxies, vulnerable servers, etc.|Free API\n[SpamCop](https://www.spamcop.net/)|Check if a netblock or IP address is in the SpamCop database.|Free API\n[Spamhaus Zen](https://www.spamhaus.org/)|Check if a netblock or IP address is in the Spamhaus Zen database.|Free API\n[spur.us](https://spur.us/)|Obtain information about any malicious activities involving IP addresses found|Commercial API\n[SpyOnWeb](http://spyonweb.com/)|Search SpyOnWeb for hosts sharing the same IP address, Google Analytics code, or Google Adsense code.|Tiered API\nSSL Certificate Analyzer|Gather information about SSL certificates used by the target's HTTPS sites.|Internal\n[StackOverflow](https://www.stackexchange.com)|Search StackOverflow for any mentions of a target domain. Returns potentially related information.|Tiered API\n[Steven Black Hosts](https://github.com/StevenBlack/hosts)|Check if a domain is malicious (malware or adware) according to Steven Black Hosts list.|Free API\nStrange Header Identifier|Obtain non-standard HTTP headers returned by web servers.|Internal\nSubdomain Takeover Checker|Check if affiliated subdomains are vulnerable to takeover.|Internal\n[Sublist3r PassiveDNS](https://api.sublist3r.com)|Passive subdomain enumeration using Sublist3r's API|Free API\n[SURBL](http://www.surbl.org/)|Check if a netblock, IP address or domain is in the SURBL blacklist.|Free API\n[Talos Intelligence](https://talosintelligence.com/)|Check if a netblock or IP address is malicious according to TalosIntelligence.|Free API\n[TextMagic](https://www.textmagic.com/)|Obtain phone number type from TextMagic API|Tiered API\n[Threat Jammer](https://threatjammer.com)|Check if an IP address is malicious according to ThreatJammer.com|Tiered API\n[ThreatCrowd](https://www.threatcrowd.org)|Obtain information from ThreatCrowd about identified IP addresses, domains and e-mail addresses.|Free API\n[ThreatFox](https://threatfox.abuse.ch)|Check if an IP address is malicious according to ThreatFox.|Free API\n[ThreatMiner](https://www.threatminer.org/)|Obtain information from ThreatMiner's database for passive DNS and threat intelligence.|Free API\nTLD Searcher|Search all Internet TLDs for domains with the same name as the target (this can be very slow.)|Internal\n[Tool - CMSeeK]([https://github.com/Tuhinshubhra/CMSeeK](https://github.com/Tuhinshubhra/CMSeeK))|Identify what Content Management System (CMS) might be used.|Tool\n[Tool - DNSTwist]([https://github.com/elceef/dnstwist](https://github.com/elceef/dnstwist))|Identify bit-squatting, typo and other similar domains to the target using a local DNSTwist installation.|Tool\n[Tool - nbtscan]([http://www.unixwiz.net/tools/nbtscan.html](http://www.unixwiz.net/tools/nbtscan.html))|Scans for open NETBIOS nameservers on your target's network.|Tool\n[Tool - Nmap]([https://nmap.org/](https://nmap.org/))|Identify what Operating System might be used.|Tool\n[Tool - Nuclei]([https://nuclei.projectdiscovery.io/](https://nuclei.projectdiscovery.io/))|Fast and customisable vulnerability scanner.|Tool\n[Tool - onesixtyone]([https://github.com/trailofbits/onesixtyone](https://github.com/trailofbits/onesixtyone))|Fast scanner to find publicly exposed SNMP services.|Tool\n[Tool - Retire.js]([http://retirejs.github.io/retire.js/](http://retirejs.github.io/retire.js/))|Scanner detecting the use of JavaScript libraries with known vulnerabilities|Tool\n[Tool - snallygaster]([https://github.com/hannob/snallygaster](https://github.com/hannob/snallygaster))|Finds file leaks and other security problems on HTTP servers.|Tool\n[Tool - testssl.sh]([https://testssl.sh](https://testssl.sh))|Identify various TLS/SSL weaknesses, including Heartbleed, CRIME and ROBOT.|Tool\n[Tool - TruffleHog]([https://github.com/trufflesecurity/truffleHog](https://github.com/trufflesecurity/truffleHog))|Searches through git repositories for high entropy strings and secrets, digging deep into commit history.|Tool\n[Tool - WAFW00F]([https://github.com/EnableSecurity/wafw00f](https://github.com/EnableSecurity/wafw00f))|Identify what web application firewall (WAF) is in use on the specified website.|Tool\n[Tool - Wappalyzer]([https://www.wappalyzer.com/](https://www.wappalyzer.com/))|Wappalyzer indentifies technologies on websites.|Tool\n[Tool - WhatWeb]([https://github.com/urbanadventurer/whatweb](https://github.com/urbanadventurer/whatweb))|Identify what software is in use on the specified website.|Tool\n[TOR Exit Nodes](https://metrics.torproject.org/)|Check if an IP adddress or netblock appears on the Tor Metrics exit node list.|Free API\n[TORCH](https://torchsearch.wordpress.com/)|Search Tor 'TORCH' search engine for mentions of the target domain.|Free API\n[Trashpanda](https://got-hacked.wtf)|Queries Trashpanda to gather intelligence about mentions of target in pastesites|Tiered API\n[Trumail](https://trumail.io/)|Check whether an email is disposable|Free API\n[Twilio](https://www.twilio.com/)|Obtain information from Twilio about phone numbers. Ensure you have the Caller Name add-on installed in Twilio.|Tiered API\n[Twitter](https://twitter.com/)|Gather name and location from Twitter profiles.|Free API\n[UCEPROTECT](http://www.uceprotect.net/)|Check if a netblock or IP address is in the UCEPROTECT database.|Free API\n[URLScan.io](https://urlscan.io/)|Search URLScan.io cache for domain information.|Free API\n[Venmo](https://venmo.com/)|Gather user information from Venmo API.|Free API\n[ViewDNS.info](https://viewdns.info/)|Identify co-hosted websites and perform reverse Whois lookups using ViewDNS.info.|Tiered API\n[VirusTotal](https://www.virustotal.com/)|Obtain information from VirusTotal about identified IP addresses.|Tiered API\n[VoIP Blacklist (VoIPBL)](https://voipbl.org/)|Check if an IP address or netblock is malicious according to VoIP Blacklist (VoIPBL).|Free API\n[VXVault.net](http://vxvault.net/)|Check if a domain or IP address is malicious according to VXVault.net.|Free API\nWeb Analytics Extractor|Identify web analytics IDs in scraped webpages and DNS TXT records.|Internal\nWeb Framework Identifier|Identify the usage of popular web frameworks like jQuery, YUI and others.|Internal\nWeb Server Identifier|Obtain web server banners to identify versions of web servers being used.|Internal\nWeb Spider|Spidering of web-pages to extract content for searching.|Internal\n[WhatCMS](https://whatcms.org/)|Check web technology using WhatCMS.org API.|Tiered API\n[Whoisology](https://whoisology.com/)|Reverse Whois lookups using Whoisology.com.|Commercial API\nWhois|Perform a WHOIS look-up on domain names and owned netblocks.|Internal\n[Whoxy](https://www.whoxy.com/)|Reverse Whois lookups using Whoxy.com.|Commercial API\n[WiGLE](https://wigle.net/)|Query WiGLE to identify nearby WiFi access points.|Free API\n[Wikileaks](https://wikileaks.org/)|Search Wikileaks for mentions of domain names and e-mail addresses.|Free API\n[Wikipedia Edits](https://www.wikipedia.org/)|Identify edits to Wikipedia articles made from a given IP address or username.|Free API\n[XForce Exchange](https://exchange.xforce.ibmcloud.com/)|Obtain IP reputation and passive DNS information from IBM X-Force Exchange.|Tiered API\n[Yandex DNS](https://yandex.com/)|Check if a host would be blocked by Yandex DNS.|Free API\n[Zetalytics](https://zetalytics.com/)|Query the Zetalytics database for hosts on your target domain(s).|Tiered API\n[ZoneFile.io](https://zonefiles.io)|Search ZoneFiles.io Domain query API for domain information.|Tiered API\n[Zone-H Defacement Check](https://zone-h.org/)|Check if a hostname/domain appears on the zone-h.org 'special defacements' RSS feed.|Free API\n\n### DOCUMENTATION\n\nRead more at the [project website](https://www.spiderfoot.net/r.php?u=aHR0cHM6Ly93d3cuc3BpZGVyZm9vdC5uZXQv&s=os_gh), including more complete documentation, blog posts with tutorials/guides, plus information about [SpiderFoot HX](https://www.spiderfoot.net/r.php?u=aHR0cHM6Ly93d3cuc3BpZGVyZm9vdC5uZXQvaHgvCg==&s=os_gh).\n\nLatest updates announced on [Twitter](https://twitter.com/spiderfoot).\n"
 },
 {
  "repo": "ReFirmLabs/binwalk",
  "language": "Python",
  "readme_contents": "# Binwalk\n\n[![Build Status](https://travis-ci.org/ReFirmLabs/binwalk.svg?branch=master)](https://travis-ci.org/ReFirmLabs/binwalk)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/ReFirmLabs/binwalk/graphs/commit-activity)\n[![GitHub license](https://img.shields.io/github/license/ReFirmLabs/binwalk.svg)](https://github.com/ReFirmLabs/binwalk/blob/master/LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/badges/shields.svg?style=social&label=Stars)](https://github.com/ReFirmLabs/binwalk/stargazers)\n\nBinwalk is a fast, easy to use tool for analyzing, reverse engineering, and extracting firmware images.\n\n\n### *** Extraction Security Notice ***\n\nPrior to Binwalk v2.3.3, extracted archives could create symlinks which point anywhere on the file system, potentially resulting in a directory traversal attack if subsequent extraction utilties blindly follow these symlinks. More generically, Binwalk makes use of many third-party extraction utilties which may have unpatched security issues; Binwalk v2.3.3 and later allows external extraction tools to be run as an unprivileged user using the `run-as` command line option (this requires Binwalk itself to be run with root privileges). Additionally, Binwalk v2.3.3 and later will refuse to perform extraction as root unless `--run-as=root` is specified.\n\n\n### *** Python 2.7 Deprecation Notice ***\n\nEven though many major Linux distros are still shipping Python 2.7 as the default interpreter in their currently stable release, we are making the difficult decision to move binwalk support exclusively to Python 3. This is likely to make many upset and others rejoice. If you need to install binwalk into a Python 2.7 environment we will be creating a tag `python27` that will be a snapshot of `master` before all of these major changes are made. Thank you for being patient with us through this transition process.\n\n\n### Installation and Usage\n\n* [Installation](./INSTALL.md)\n* [API](./API.md)\n* [Supported Platforms](https://github.com/ReFirmLabs/binwalk/wiki/Supported-Platforms)\n* [Getting Started](https://github.com/ReFirmLabs/binwalk/wiki/Quick-Start-Guide)\n* [Binwalk Command Line Usage](https://github.com/ReFirmLabs/binwalk/wiki/Usage)\n* [Binwalk IDA Plugin Usage](https://github.com/ReFirmLabs/binwalk/wiki/Creating-Custom-Plugins)\n\nMore information on [Wiki](https://github.com/ReFirmLabs/binwalk/wiki)\n\n# Binwalk Professional Edition\n\nAfter years of developing and supporting binwalk as an open source project we have finally sold out to the man and released a cloud-based firmware extraction engine called *Binwalk Enterprise*. After all someone needs to pay devttys0 so he can buy more milling equipment and feed his children (in that order). Please consider subscribing and reap the benefits of getting actual customer support for all your firmware extraction and analysis needs. Please visit https://www.refirmlabs.com/binwalk-enterprise/ for more information. \n"
 },
 {
  "repo": "miguelgrinberg/flasky",
  "language": "Python",
  "readme_contents": "Flasky\n======\n\nThis repository contains the source code examples for the second edition of my O'Reilly book [Flask Web Development](http://www.flaskbook.com).\n\nThe commits and tags in this repository were carefully created to match the sequence in which concepts are presented in the book. Please read the section titled \"How to Work with the Example Code\" in the book's preface for instructions.\n\nFor Readers of the First Edition of the Book\n--------------------------------------------\n\nThe code examples for the first edition of the book were moved to a different repository: [https://github.com/miguelgrinberg/flasky-first-edition](https://github.com/miguelgrinberg/flasky-first-edition).\n"
 },
 {
  "repo": "docopt/docopt",
  "language": "Python",
  "readme_contents": "``docopt`` creates *beautiful* command-line interfaces\n======================================================================\n\n.. image:: https://travis-ci.org/docopt/docopt.svg?branch=master\n    :target: https://travis-ci.org/docopt/docopt\n\n.. image:: https://img.shields.io/pypi/v/docopt.svg\n    :target: https://pypi.python.org/pypi/docopt\n\nVideo introduction to **docopt**: `PyCon UK 2012: Create *beautiful*\ncommand-line interfaces with Python <http://youtu.be/pXhcPJK5cMc>`_\n\n    New in version 0.6.1:\n\n    - Fix issue `#85 <https://github.com/docopt/docopt/issues/85>`_\n      which caused improper handling of ``[options]`` shortcut\n      if it was present several times.\n\n    New in version 0.6.0:\n\n    - New argument ``options_first``, disallows interspersing options\n      and arguments.  If you supply ``options_first=True`` to\n      ``docopt``, it will interpret all arguments as positional\n      arguments after first positional argument.\n\n    - If option with argument could be repeated, its default value\n      will be interpreted as space-separated list. E.g. with\n      ``[default: ./here ./there]`` will be interpreted as\n      ``['./here', './there']``.\n\n    Breaking changes:\n\n    - Meaning of ``[options]`` shortcut slightly changed. Previously\n      it meant *\"any known option\"*. Now it means *\"any option not in\n      usage-pattern\"*.  This avoids the situation when an option is\n      allowed to be repeated unintentionally.\n\n    - ``argv`` is ``None`` by default, not ``sys.argv[1:]``.\n      This allows ``docopt`` to always use the *latest* ``sys.argv``,\n      not ``sys.argv`` during import time.\n\nIsn't it awesome how ``optparse`` and ``argparse`` generate help\nmessages based on your code?!\n\n*Hell no!*  You know what's awesome?  It's when the option parser *is*\ngenerated based on the beautiful help message that you write yourself!\nThis way you don't need to write this stupid repeatable parser-code,\nand instead can write only the help message--*the way you want it*.\n\n**docopt** helps you create most beautiful command-line interfaces\n*easily*:\n\n.. code:: python\n\n    \"\"\"Naval Fate.\n\n    Usage:\n      naval_fate.py ship new <name>...\n      naval_fate.py ship <name> move <x> <y> [--speed=<kn>]\n      naval_fate.py ship shoot <x> <y>\n      naval_fate.py mine (set|remove) <x> <y> [--moored | --drifting]\n      naval_fate.py (-h | --help)\n      naval_fate.py --version\n\n    Options:\n      -h --help     Show this screen.\n      --version     Show version.\n      --speed=<kn>  Speed in knots [default: 10].\n      --moored      Moored (anchored) mine.\n      --drifting    Drifting mine.\n\n    \"\"\"\n    from docopt import docopt\n\n\n    if __name__ == '__main__':\n        arguments = docopt(__doc__, version='Naval Fate 2.0')\n        print(arguments)\n\nBeat that! The option parser is generated based on the docstring above\nthat is passed to ``docopt`` function.  ``docopt`` parses the usage\npattern (``\"Usage: ...\"``) and option descriptions (lines starting\nwith dash \"``-``\") and ensures that the program invocation matches the\nusage pattern; it parses options, arguments and commands based on\nthat. The basic idea is that *a good help message has all necessary\ninformation in it to make a parser*.\n\nAlso, `PEP 257 <http://www.python.org/dev/peps/pep-0257/>`_ recommends\nputting help message in the module docstrings.\n\nInstallation\n======================================================================\n\nUse `pip <http://pip-installer.org>`_ or easy_install::\n\n    pip install docopt==0.6.2\n\nAlternatively, you can just drop ``docopt.py`` file into your\nproject--it is self-contained.\n\n**docopt** is tested with Python 2.7, 3.4, 3.5, and 3.6.\n\nTesting\n======================================================================\n\nYou can run unit tests using the command:\n\n    python setup.py test\n\nAPI\n======================================================================\n\n.. code:: python\n\n    from docopt import docopt\n\n.. code:: python\n\n    docopt(doc, argv=None, help=True, version=None, options_first=False)\n\n``docopt`` takes 1 required and 4 optional arguments:\n\n- ``doc`` could be a module docstring (``__doc__``) or some other\n  string that contains a **help message** that will be parsed to\n  create the option parser.  The simple rules of how to write such a\n  help message are given in next sections.  Here is a quick example of\n  such a string:\n\n.. code:: python\n\n    \"\"\"Usage: my_program.py [-hso FILE] [--quiet | --verbose] [INPUT ...]\n\n    -h --help    show this\n    -s --sorted  sorted output\n    -o FILE      specify output file [default: ./test.txt]\n    --quiet      print less text\n    --verbose    print more text\n\n    \"\"\"\n\n- ``argv`` is an optional argument vector; by default ``docopt`` uses\n  the argument vector passed to your program (``sys.argv[1:]``).\n  Alternatively you can supply a list of strings like ``['--verbose',\n  '-o', 'hai.txt']``.\n\n- ``help``, by default ``True``, specifies whether the parser should\n  automatically print the help message (supplied as ``doc``) and\n  terminate, in case ``-h`` or ``--help`` option is encountered\n  (options should exist in usage pattern, more on that below). If you\n  want to handle ``-h`` or ``--help`` options manually (as other\n  options), set ``help=False``.\n\n- ``version``, by default ``None``, is an optional argument that\n  specifies the version of your program. If supplied, then, (assuming\n  ``--version`` option is mentioned in usage pattern) when parser\n  encounters the ``--version`` option, it will print the supplied\n  version and terminate.  ``version`` could be any printable object,\n  but most likely a string, e.g. ``\"2.1.0rc1\"``.\n\n    Note, when ``docopt`` is set to automatically handle ``-h``,\n    ``--help`` and ``--version`` options, you still need to mention\n    them in usage pattern for this to work. Also, for your users to\n    know about them.\n\n- ``options_first``, by default ``False``.  If set to ``True`` will\n  disallow mixing options and positional argument.  I.e. after first\n  positional argument, all arguments will be interpreted as positional\n  even if the look like options.  This can be used for strict\n  compatibility with POSIX, or if you want to dispatch your arguments\n  to other programs.\n\nThe **return** value is a simple dictionary with options, arguments\nand commands as keys, spelled exactly like in your help message.  Long\nversions of options are given priority. For example, if you invoke the\ntop example as::\n\n    naval_fate.py ship Guardian move 100 150 --speed=15\n\nthe return dictionary will be:\n\n.. code:: python\n\n    {'--drifting': False,    'mine': False,\n     '--help': False,        'move': True,\n     '--moored': False,      'new': False,\n     '--speed': '15',        'remove': False,\n     '--version': False,     'set': False,\n     '<name>': ['Guardian'], 'ship': True,\n     '<x>': '100',           'shoot': False,\n     '<y>': '150'}\n\nHelp message format\n======================================================================\n\nHelp message consists of 2 parts:\n\n- Usage pattern, e.g.::\n\n    Usage: my_program.py [-hso FILE] [--quiet | --verbose] [INPUT ...]\n\n- Option descriptions, e.g.::\n\n    -h --help    show this\n    -s --sorted  sorted output\n    -o FILE      specify output file [default: ./test.txt]\n    --quiet      print less text\n    --verbose    print more text\n\nTheir format is described below; other text is ignored.\n\nUsage pattern format\n----------------------------------------------------------------------\n\n**Usage pattern** is a substring of ``doc`` that starts with\n``usage:`` (case *insensitive*) and ends with a *visibly* empty line.\nMinimum example:\n\n.. code:: python\n\n    \"\"\"Usage: my_program.py\n\n    \"\"\"\n\nThe first word after ``usage:`` is interpreted as your program's name.\nYou can specify your program's name several times to signify several\nexclusive patterns:\n\n.. code:: python\n\n    \"\"\"Usage: my_program.py FILE\n              my_program.py COUNT FILE\n\n    \"\"\"\n\nEach pattern can consist of the following elements:\n\n- **<arguments>**, **ARGUMENTS**. Arguments are specified as either\n  upper-case words, e.g. ``my_program.py CONTENT-PATH`` or words\n  surrounded by angular brackets: ``my_program.py <content-path>``.\n- **--options**.  Options are words started with dash (``-``), e.g.\n  ``--output``, ``-o``.  You can \"stack\" several of one-letter\n  options, e.g. ``-oiv`` which will be the same as ``-o -i -v``. The\n  options can have arguments, e.g.  ``--input=FILE`` or ``-i FILE`` or\n  even ``-iFILE``. However it is important that you specify option\n  descriptions if you want your option to have an argument, a default\n  value, or specify synonymous short/long versions of the option (see\n  next section on option descriptions).\n- **commands** are words that do *not* follow the described above\n  conventions of ``--options`` or ``<arguments>`` or ``ARGUMENTS``,\n  plus two special commands: dash \"``-``\" and double dash \"``--``\"\n  (see below).\n\nUse the following constructs to specify patterns:\n\n- **[ ]** (brackets) **optional** elements.  e.g.: ``my_program.py\n  [-hvqo FILE]``\n- **( )** (parens) **required** elements.  All elements that are *not*\n  put in **[ ]** are also required, e.g.: ``my_program.py\n  --path=<path> <file>...`` is the same as ``my_program.py\n  (--path=<path> <file>...)``.  (Note, \"required options\" might be not\n  a good idea for your users).\n- **|** (pipe) **mutually exclusive** elements. Group them using **(\n  )** if one of the mutually exclusive elements is required:\n  ``my_program.py (--clockwise | --counter-clockwise) TIME``. Group\n  them using **[ ]** if none of the mutually-exclusive elements are\n  required: ``my_program.py [--left | --right]``.\n- **...** (ellipsis) **one or more** elements. To specify that\n  arbitrary number of repeating elements could be accepted, use\n  ellipsis (``...``), e.g.  ``my_program.py FILE ...`` means one or\n  more ``FILE``-s are accepted.  If you want to accept zero or more\n  elements, use brackets, e.g.: ``my_program.py [FILE ...]``. Ellipsis\n  works as a unary operator on the expression to the left.\n- **[options]** (case sensitive) shortcut for any options.  You can\n  use it if you want to specify that the usage pattern could be\n  provided with any options defined below in the option-descriptions\n  and do not want to enumerate them all in usage-pattern.\n- \"``[--]``\". Double dash \"``--``\" is used by convention to separate\n  positional arguments that can be mistaken for options. In order to\n  support this convention add \"``[--]``\" to your usage patterns.\n- \"``[-]``\". Single dash \"``-``\" is used by convention to signify that\n  ``stdin`` is used instead of a file. To support this add \"``[-]``\"\n  to your usage patterns. \"``-``\" acts as a normal command.\n\nIf your pattern allows to match argument-less option (a flag) several\ntimes::\n\n    Usage: my_program.py [-v | -vv | -vvv]\n\nthen number of occurrences of the option will be counted. I.e.\n``args['-v']`` will be ``2`` if program was invoked as ``my_program\n-vv``. Same works for commands.\n\nIf your usage patterns allows to match same-named option with argument\nor positional argument several times, the matched arguments will be\ncollected into a list::\n\n    Usage: my_program.py <file> <file> --path=<path>...\n\nI.e. invoked with ``my_program.py file1 file2 --path=./here\n--path=./there`` the returned dict will contain ``args['<file>'] ==\n['file1', 'file2']`` and ``args['--path'] == ['./here', './there']``.\n\n\nOption descriptions format\n----------------------------------------------------------------------\n\n**Option descriptions** consist of a list of options that you put\nbelow your usage patterns.\n\nIt is necessary to list option descriptions in order to specify:\n\n- synonymous short and long options,\n- if an option has an argument,\n- if option's argument has a default value.\n\nThe rules are as follows:\n\n- Every line in ``doc`` that starts with ``-`` or ``--`` (not counting\n  spaces) is treated as an option description, e.g.::\n\n    Options:\n      --verbose   # GOOD\n      -o FILE     # GOOD\n    Other: --bad  # BAD, line does not start with dash \"-\"\n\n- To specify that option has an argument, put a word describing that\n  argument after space (or equals \"``=``\" sign) as shown below. Follow\n  either <angular-brackets> or UPPER-CASE convention for options'\n  arguments.  You can use comma if you want to separate options. In\n  the example below, both lines are valid, however you are recommended\n  to stick to a single style.::\n\n    -o FILE --output=FILE       # without comma, with \"=\" sign\n    -i <file>, --input <file>   # with comma, without \"=\" sign\n\n- Use two spaces to separate options with their informal description::\n\n    --verbose More text.   # BAD, will be treated as if verbose option had\n                           # an argument \"More\", so use 2 spaces instead\n    -q        Quit.        # GOOD\n    -o FILE   Output file. # GOOD\n    --stdout  Use stdout.  # GOOD, 2 spaces\n\n- If you want to set a default value for an option with an argument,\n  put it into the option-description, in form ``[default:\n  <my-default-value>]``::\n\n    --coefficient=K  The K coefficient [default: 2.95]\n    --output=FILE    Output file [default: test.txt]\n    --directory=DIR  Some directory [default: ./]\n\n- If the option is not repeatable, the value inside ``[default: ...]``\n  will be interpreted as string.  If it *is* repeatable, it will be\n  splited into a list on whitespace::\n\n    Usage: my_program.py [--repeatable=<arg> --repeatable=<arg>]\n                         [--another-repeatable=<arg>]...\n                         [--not-repeatable=<arg>]\n\n    # will be ['./here', './there']\n    --repeatable=<arg>          [default: ./here ./there]\n\n    # will be ['./here']\n    --another-repeatable=<arg>  [default: ./here]\n\n    # will be './here ./there', because it is not repeatable\n    --not-repeatable=<arg>      [default: ./here ./there]\n\nExamples\n----------------------------------------------------------------------\n\nWe have an extensive list of `examples\n<https://github.com/docopt/docopt/tree/master/examples>`_ which cover\nevery aspect of functionality of **docopt**.  Try them out, read the\nsource if in doubt.\n\nSubparsers, multi-level help and *huge* applications (like git)\n----------------------------------------------------------------------\n\nIf you want to split your usage-pattern into several, implement\nmulti-level help (with separate help-screen for each subcommand),\nwant to interface with existing scripts that don't use **docopt**, or\nyou're building the next \"git\", you will need the new ``options_first``\nparameter (described in API section above). To get you started quickly\nwe implemented a subset of git command-line interface as an example:\n`examples/git\n<https://github.com/docopt/docopt/tree/master/examples/git>`_\n\n\nData validation\n----------------------------------------------------------------------\n\n**docopt** does one thing and does it well: it implements your\ncommand-line interface.  However it does not validate the input data.\nOn the other hand there are libraries like `python schema\n<https://github.com/halst/schema>`_ which make validating data a\nbreeze.  Take a look at `validation_example.py\n<https://github.com/docopt/docopt/tree/master/examples/validation_example.py>`_\nwhich uses **schema** to validate data and report an error to the\nuser.\n\nUsing docopt with config-files\n----------------------------------------------------------------------\n\nOften configuration files are used to provide default values which\ncould be overriden by command-line arguments.  Since **docopt**\nreturns a simple dictionary it is very easy to integrate with\nconfig-files written in JSON, YAML or INI formats.\n`config_file_example.py <examples/config_file_example.py>`_ provides\nand example of how to use **docopt** with JSON or INI config-file.\n\n\nDevelopment\n======================================================================\n\nWe would *love* to hear what you think about **docopt** on our `issues\npage <http://github.com/docopt/docopt/issues>`_\n\nMake pull requests, report bugs, suggest ideas and discuss\n**docopt**. You can also drop a line directly to\n<vladimir@keleshev.com>.\n\nPorting ``docopt`` to other languages\n======================================================================\n\nWe think **docopt** is so good, we want to share it beyond the Python\ncommunity! All official docopt ports to other languages can be found\nunder the `docopt organization page <http://github.com/docopt>`_\non GitHub.\n\nIf your favourite language isn't among then, you can always create a\nport for it! You are encouraged to use the Python version as a\nreference implementation.  A Language-agnostic test suite is bundled\nwith `Python implementation <http://github.com/docopt/docopt>`_.\n\nPorting discussion is on `issues page\n<http://github.com/docopt/docopt/issues>`_.\n\nChangelog\n======================================================================\n\n**docopt** follows `semantic versioning <http://semver.org>`_.  The\nfirst release with stable API will be 1.0.0 (soon).  Until then, you\nare encouraged to specify explicitly the version in your dependency\ntools, e.g.::\n\n    pip install docopt==0.6.2\n\n- 0.6.2 Bugfix release.\n- 0.6.1 Bugfix release.\n- 0.6.0 ``options_first`` parameter.\n  **Breaking changes**: Corrected ``[options]`` meaning.\n  ``argv`` defaults to ``None``.\n- 0.5.0 Repeated options/commands are counted or accumulated into a\n  list.\n- 0.4.2 Bugfix release.\n- 0.4.0 Option descriptions become optional,\n  support for \"``--``\" and \"``-``\" commands.\n- 0.3.0 Support for (sub)commands like `git remote add`.\n  Introduce ``[options]`` shortcut for any options.\n  **Breaking changes**: ``docopt`` returns dictionary.\n- 0.2.0 Usage pattern matching. Positional arguments parsing based on\n  usage patterns.\n  **Breaking changes**: ``docopt`` returns namespace (for arguments),\n  not list. Usage pattern is formalized.\n- 0.1.0 Initial release. Options-parsing only (based on options\n  description).\n"
 },
 {
  "repo": "GitLqr/LQRWeChat",
  "language": "Java",
  "readme_contents": "\u9ad8\u4eff\u5fae\u4fe16.5.7\uff08\u878d\u4e91\u7248\uff09\n============\n\n## \u76ee\u5f55\n* [\u4e00\u3001\u7b80\u8ff0](#\u4e00\u7b80\u8ff0)\n* [\u4e8c\u3001\u529f\u80fd](#\u4e8c\u529f\u80fd)\n* [\u4e09\u3001\u6548\u679c\u56fe](#\u4e09\u6548\u679c\u56fe)\n* [\u56db\u3001\u5176\u4ed6\u76f8\u5173](#\u56db\u5176\u4ed6\u76f8\u5173)\n* [\u4e94\u3001\u6253\u8d4f\u652f\u6301](#\u4e94\u6253\u8d4f\u652f\u6301)\n\n# \u4e00\u3001\u7b80\u8ff0\n\n>\u672c\u9879\u76ee\u7531 CSDN_LQR \u4e2a\u4eba\u72ec\u7acb\u5f00\u53d1\u3002\n>\n>\u9879\u76ee\u535a\u5ba2\u5730\u5740\uff1a[\u9ad8\u4eff\u5fae\u4fe16.5.7\uff08\u878d\u4e91\u7248\uff09](http://www.jianshu.com/p/f119810520e4)\n>\n>\u9879\u76ee\u6e90\u7801\u5730\u5740\uff1a\n>\n>[GitHub\uff1ahttps://github.com/GitLqr/LQRWeChat](https://github.com/GitLqr/LQRWeChat)\n>\n>[\u7801\u4e91\uff1ahttps://git.oschina.net/CSDNLQR/lqrwechatrongcloud](https://git.oschina.net/CSDNLQR/lqrwechatrongcloud)\n>\n>\u9879\u76eeDemoApp\u4e0b\u8f7d\uff1a[Demo](app-debug.apk)\n\n**\u6ce8\uff1a\u65e7\u7248\u672c\u5df2\u8fc1\u81f3[NimVersion\u5206\u652f](https://github.com/GitLqr/LQRWeChat/tree/NimVersion)**\n\t\n## 1\u3001\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b\uff1a\n\u8fd9\u4e2a\u9879\u76ee\u662f\u672c\u4eba\u72ec\u7acb\u5f00\u53d1\u7684\u7b2c\u4e8c\u4e2a\u9ad8\u4eff\u5fae\u4fe1\u9879\u76ee\uff0c\u4eff\u6700\u65b0\u7248\u5fae\u4fe16.5.7\uff08\u9664\u56fe\u7247\u9009\u62e9\u5668\u5916\uff09\u3002\u672c\u9879\u76ee\u57fa\u4e8e\u878d\u4e91SDK\uff0c\u4f7f\u7528\u76ee\u524d\u8f83\u706b\u7684 Rxjava+Retrofit+MVP+Glide \u6280\u672f\u5f00\u53d1\u3002\u76f8\u6bd4\u4e0a\u4e2a\u7248\u672c\uff0c\u52a0\u5165\u53d1\u9001\u4f4d\u7f6e\u6d88\u606f\uff0c\u7ea2\u5305\u6d88\u606f\u7b49\u529f\u80fd\u3002\u6b22\u8fcestart\u548cfork~~\n\n## 2\u3001\u5236\u4f5c\u8be5\u5f00\u6e90\u9879\u76ee\u7684\u539f\u56e0\u6709\uff1a\n\n1. \u719f\u7ec3\u4f7f\u7528 Rxjava+Retrofit+MVP+lambda \u7b49\u65b0\u5b89\u5353\u6280\u672f\u3002\n2. \u719f\u6089\u878d\u4e91\u7b49SDK\u7684\u4f7f\u7528\u3002\n3. \u5411\u9ad8\u624b\u8fdb\u9636\u8fc7\u6e21\u3002\n\n## 3\u3001\u7edf\u4e00\u56de\u590d\u4e0b\u7f51\u53cb\u7684\u95ee\u9898\uff1a\n\u6709\u7f51\u53cb\u8bf4\u770b\u6211\u4e0a\u4e00\u4e2a\u9879\u76ee\u6709\u522b\u4eba\u63d0\u51fa\u7684\u5f88\u591a\u95ee\u9898\uff0c\u800c\u4e14\u6211\u90fd\u6ca1\u6709\u56de\u590d\u5e76\u89e3\u51b3\uff0c\u5b9e\u9645\u662f\u6709\u7684\uff0c\u53ea\u4e0d\u8fc7\u90a3\u65f6\u5df2\u7ecf\u5728\u7740\u624b\u51c6\u5907\u5f00\u53d1\u8fd9\u4e2a\u65b0\u7684\u9ad8\u4eff\u5fae\u4fe1\uff0c\u800c\u4e14\u56e0\u4e3a\u4e0a\u4e00\u4e2a\u7248\u672c\u4f7f\u7528\u7684\u662f\u7f51\u6613\u4e91SDK\uff0c\u5f00\u53d1\u4e0a\u6bd4\u8f83\u7b80\u5355\uff0c\u540c\u65f6\u8be5SDK\u7684\u5c01\u88c5\u5b9e\u5728\u662f\u592a\u597d\u4e86\uff0c\u6240\u4ee5\u6ca1\u5730\u65b9\u53ef\u4ee5\u65bd\u5c55Retrofit\uff0c\u8fbe\u4e0d\u5230\u6211\u9884\u8ba1\u7684\u63d0\u5347\u8981\u6c42\uff0c\u4e8e\u662f\u4fbf\u9009\u7528\u4e86\u878d\u4e91SDK\u5e72\u8106\u505a\u4e86\u4e00\u4e2a\u65b0\u7684\uff0c\u4e0a\u7248\u4e2d\u5b58\u5728\u7684\u4e00\u4e9b\u95ee\u9898\u5df2\u7ecf\u5728\u8fd9\u4e2a\u7248\u672c\u4e2d\u57fa\u672c\u89e3\u51b3\uff0c\u540c\u65f6\u5236\u4f5c\u5e76\u66f4\u65b0\u4e86\u51e0\u4e2a\u81ea\u5df1\u7684\u5e93\uff08\u5982\uff1a\u8868\u60c5\u5e93\u548c\u8bed\u97f3\u5e93\u7b49\uff09\u3002\n\n# \u4e8c\u3001\u529f\u80fd\n\n## 1\u3001\u597d\u53cb\n\n1. \u67e5\u8be2\u597d\u53cb\n1. \u53d1\u8d77\u6dfb\u52a0\u597d\u53cb\u8bf7\u6c42\n1. \u67e5\u770b\u597d\u53cb\u4e2a\u4eba\u4fe1\u606f\n1. \u8bbe\u7f6e\u5907\u6ce8\n1. \u5220\u9664\u597d\u53cb\n1. \u626b\u7801\u52a0\u597d\u53cb\n1. \u67e5\u770b\u65b0\u52a0\u670b\u53cb\n\n## 2\u3001\u7fa4\u7ec4\n\n1. \u62c9\u4eba\u8fdb\u7fa4\n1. \u8e22\u4eba\u53bb\u7fa4\n1. \u4fee\u6539\u7fa4\u6635\u79f0\n1. \u67e5\u770b\u7fa4\u4e8c\u7ea7\u7801\n1. \u626b\u7801\u52a0\u5165\u7fa4\u7ec4\n1. \u89e3\u6563\u7fa4\uff08\u7fa4\u4e3b\uff09\n1. \u9000\u51fa\u7fa4\uff08\u7fa4\u6210\u5458\uff09\n\n## 3\u3001\u4e2a\u4eba\n\n1. \u67e5\u770b\u5934\u50cf\n1. \u4e0a\u4f20\u66f4\u65b0\u5934\u50cf\n1. \u4fee\u6539\u4e2a\u4eba\u6635\u79f0\n1. \u67e5\u770b\u4e2a\u4eba\u4e8c\u7ef4\u7801\n\n## 4\u3001\u4f1a\u8bdd\n\n1. \u4f1a\u8bdd\u7f6e\u9876\n1. \u53d6\u6d88\u7f6e\u9876\n1. \u5220\u9664\u4f1a\u8bdd\n1. \u64a4\u56de\u6d88\u606f\n1. \u53d1\u9001\u6587\u672c\u6d88\u606f\n1. \u53d1\u9001\u56fe\u7247\u6d88\u606f\n1. \u53d1\u9001\u89c6\u9891\u6d88\u606f\n1. \u53d1\u9001\u8bed\u97f3\u6d88\u606f\n1. \u53d1\u9001\u8d34\u56fe\u6d88\u606f\n1. \u53d1\u9001\u4f4d\u7f6e\u6d88\u606f\n1. \u53d1\u9001\u7ea2\u5305\u6d88\u606f\n\n## 5\u3001\u7cfb\u7edf\n\n1. \u767b\u5f55\n1. \u6ce8\u518c\n1. \u9000\u51fa\u5f53\u524d\u8d26\u53f7\n1. \u9000\u51faAPP\n\n## 6\u3001\u5c1a\u672a\u5b8c\u6210\n\n1. \u6d88\u606f\u901a\u77e5\n1. @\u529f\u80fd\n1. \u5bf9\u65b9\u8f93\u5165\u72b6\u6001\u63d0\u793a\n\n# \u4e09\u3001\u6548\u679c\u56fe\n\n![\u4e3b\u754c\u9762](screenshots/1.gif)\n![\u4f1a\u8bdd\u63a7\u5236](screenshots/2.gif)\n![\u5f55\u5236\u3001\u53d1\u9001\u8bed\u97f3](screenshots/3.gif)\n![\u53d1\u9001\u8868\u60c5\u6587\u5b57](screenshots/4.gif)\n![\u53d1\u9001\u7ea2\u5305](screenshots/5.gif)\n![\u62a2\u7ea2\u5305](screenshots/6.gif)\n![\u53d1\u9001\u4f4d\u7f6e](screenshots/7.gif)\n![\u5f55\u5236\u3001\u53d1\u9001\u5c0f\u89c6\u9891](screenshots/8.gif)\n![\u9009\u62e9\u3001\u53d1\u9001\u56fe\u7247](screenshots/9.gif)\n![\u67e5\u770b\u3001\u64a4\u56de\u6d88\u606f](screenshots/10.gif)\n![\u62c9\u4eba\u5165\u7fa4](screenshots/11.gif)\n![\u8e22\u4eba\u51fa\u7fa4](screenshots/12.gif)\n![\u4fee\u6539\u7fa4\u6635\u79f0](screenshots/13.gif)\n![\u53d1\u8d77\u7fa4\u804a](screenshots/14.gif)\n\n\n# \u56db\u3001\u5176\u4ed6\u76f8\u5173\n\n## 1\u3001\u8be5\u9879\u76ee\u4f7f\u7528\u5230\u7684\u6280\u672f\u6709\uff1a\n\n1. Rxjava 2.0\n1. Retrofit 2.0\n1. MVP \n1. Glide\n1. lambda\n1. ...\n\n## 2\u3001\u7528\u5230\u7684\u4e3b\u8981\u5e93\u6709\uff1a\n\n### \u4e3b\u8981\u7684\u5927\u795e\u5e93\uff1a\n\n1. [\u9e3f\u795e\u7684AutoLayout](https://github.com/hongyangAndroid/AndroidAutoLayout)\n1. [\u90ed\u795e\u7684LitePal](https://github.com/LitePalFramework/LitePal)\n1. [bingoogolapple\u7684\u4e07\u80fd\u5237\u65b0\u63a7\u4ef6](https://github.com/bingoogolapple/BGARefreshLayout-Android)\n1. [bingoogolapple\u7684\u4e8c\u7ef4\u7801\u63a7\u4ef6\u626b\u63cf\u5e93](https://github.com/bingoogolapple/BGAQRCode-Android)\n1. [CJT2325\u7684\u4eff\u5fae\u4fe1\u62cd\u7167Android\u63a7\u4ef6](https://github.com/CJT2325/CameraView)\n1. ...\n\n### \u81ea\u5df1(CSDN_LQR)\u505a\u7684\u5e93\uff1a\n\n1. [\u4e07\u80fd\u9002\u914d\u5668](https://github.com/GitLqr/LQRAdapterLibrary)\n1. [\u5305\u88c5\u8fc7\u7684RecyclerView](https://github.com/GitLqr/LQRRecyclerViewLibrary)\n1. [\u9ad8\u4eff\u5fae\u4fe1\u8868\u60c5\u5e93](https://github.com/GitLqr/LQREmojiLibrary)\n1. [\u9ad8\u4eff\u5fae\u4fe1\u4e3b\u610f\u5e93](https://github.com/GitLqr/LQRAudioRecord)\n1. [\u9ad8\u4eff\u5fae\u4fe1\u56fe\u7247\u9009\u62e9\u5668](https://github.com/GitLqr/LQRImagePicker)\n1. [\u9ad8\u4eff\u5fae\u4fe1\u4e5d\u5bab\u683c\u63a7\u4ef6](https://github.com/GitLqr/LQRNineGridImageView)\n1. [\u5e38\u7528\u9009\u9879\u6761\u76ee\u5e93](https://github.com/GitLqr/LQROptionItemView)\n\n## 3\u3001\u8bf4\u660e\u4e0e\u9e23\u8c22\uff1a\n\n\u4e0d\u63d0\u4f9b\u6d4b\u8bd5\u53f7\uff0c\u8bf7\u4f7f\u7528\u81ea\u5df1\u624b\u673a\u6ce8\u518c\u540e\u767b\u5f55\uff0c\u56e0\u4e3a\u672c\u4eba\u624b\u673a\u53f7\u6709\u9650\uff0c\u6d4b\u8bd5\u4e0a\u5f88\u6709\u5c40\u9650\uff0c\u53ef\u80fd\u5b58\u5728\u4e00\u4e9b\u6211\u4e0d\u77e5\u9053\u7684bug\uff0c\u8bf7\u591a\u5305\u6db5\uff0c\u53ef\u5728\u9879\u76ee\u4e2d\u63d0\u51faissue\u3002\u672c\u4eba\u505a\u8fd9\u4e2a\u9879\u76ee\u53ea\u4e3a\u63d0\u5347\u4e2a\u4eba\u5b89\u5353\u5f00\u53d1\u80fd\u529b\uff0c\u6545\u4f9d\u8d56\u878d\u4e91\u5b98\u65b9\u7ed9\u51fa\u7684server\u7aef\u505a\u4e3a\u672c\u9879\u76ee\u7684\u540e\u53f0\u670d\u52a1\uff0c\u8be5server\u6e90\u7801\u4f7f\u7528Node.js\u5f00\u53d1\uff0c\u76ee\u524d\u672c\u4eba\u53ea\u4f1a\u7528java\u5f00\u53d1\u540e\u7aef\uff0c\u6240\u4ee5\u5982\u679c\u8981\u641e\u70b9\u522b\u7684\u529f\u80fd\u7684\u8bdd\uff0c\u76ee\u524d\u662f\u4e0d\u53ef\u80fd\u5566\uff0c\u6709\u5174\u8da3\u7684\u540c\u5b66\u53ef\u4ee5\u770b\u770b\u8fd9\u4e2a[\u55e8\u8c79 IM \u5e94\u7528\u670d\u52a1\u5668](https://github.com/sealtalk/sealtalk-server)\uff0c\u5f53\u7136\u878d\u4e91\u4e5f\u6709\u5b83\u7684\u5751\uff0c\u7279\u522b\u662f\u7ea2\u5305module\uff0c\u6211\u5e72\u8106\u4e0d\u7528\u5b83\u7684\u4e86\uff0c\u5e0c\u671b\u8be5\u9879\u76ee\u53ef\u4ee5\u5e2e\u5230\u90a3\u4e9b\u6b63\u5728\u8e29\u5751\u7684\u4eba\uff08\u81f3\u5c11\u6211\u5df2\u7ecf\u8e29\u4e86\u4e00\u6b21\u4e86\uff0c\u563f\u563f\uff09\uff0c\u6b64\u5916\uff0c\u5f88\u611f\u8c22\u5f88\u591a\u7f51\u53cb\u5bf9\u6211\u7684\u652f\u6301\uff0c\u8fd8\u6709\u4e13\u95e8\u8dd1\u5230CSDN\u8ddf\u6211\u79c1\u4fe1\u7ed9\u6211\u9f13\u52b1\u7684\uff0c\u771f\u7684\u5f88\u611f\u52a8\uff0c\u8c22\u8c22\u3002\n\n# \u4e94\u3001\u6253\u8d4f\u652f\u6301\n\n\u6700\u540e\uff0c\u5982\u679c\u89c9\u5f97\u672c\u9879\u76ee\u5bf9\u60a8\u6709\u7528\uff0c\u8bf7\u968f\u610f\u6253\u8d4f\uff0c\u9f13\u52b1\u6211\u7ee7\u7eed\u521b\u4f5c\uff0c\u8c22\u8c22\u5566\u3002\n\n![wechat](screenshots/wechat_pay.png)\n![alipay](screenshots/alipay.png)\n\n\n"
 },
 {
  "repo": "evrencoskun/TableView",
  "language": "Java",
  "readme_contents": "<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/evrencoskun/TableViewSample/master/art/Logo-5.png\" >\n    <h2>TableView for Android</h2>\n    <p align=\"center\">\n        <p>TableView is a powerful Android library for displaying complex data structures and rendering tabular data composed of rows, columns and cells. \n           TableView relies on a separate model object to hold and represent the data it displays. This repository also contains a sample app that is\n           designed to show you how to create your own TableView in your application.</p>        \n        <a href=\"https://youtu.be/1DWFIqrqrPk\">\n            <b> Full video \u00bb</b>\n        </a>\n    </p>\n\n</div>\n\n<p align=\"center\">\n    <a href=\"https://youtu.be/1DWFIqrqrPk\">\n      <img src=\"https://raw.githubusercontent.com/evrencoskun/TableViewSample/master/art/TableView-0_8_5_1_2.gif\">\n    </a>\n</p>\n\n## Features\n\n  - [x] Each column width value can be calculated automatically considering the largest one.\n  - [x] Setting your own model class to be displayed in a table view easily.\n  - [x] `TableView` has an action listener interface to listen user touch interaction for each cell.\n  - [x] `TableView` columns can be sorted in ascending or descending order.\n  - [x] Hiding & showing the rows and columns is pretty easy.\n  - [x] Filtering by more than one data.\n  - [x] Pagination functionality.\n\n## What's new\n\nYou can check new implementations of `TableView` on the [release page](https://github.com/evrencoskun/TableView/releases).\n  \n## Table of Contents\n\n  - [Installation](#installation)\n  - [Documentation](#documentation)\n  - [Sample Apps](#sample-apps)\n  - [Donations](#donations)\n  - [Contributors](#contributors)\n  - [License](#license)\n\n## Installation\n\nTo use this library in your Android project, just add the following dependency into your module's `build.gradle`:\n\n***Use Jitpack implementation***\n\n1. Check Jitpack use : \n```\n\tallprojects {\n\t\trepositories {\n\t\t\t...\n\t\t\tmaven { url 'https://jitpack.io' }\n\t\t}\n\t}\n```\n\n2. Add implementation in project build :\n```\nimplementation 'com.github.evrencoskun:TableView:v0.8.9.4'\n```\n\n## Documentation \n\nPlease check out the [project's wiki](https://github.com/evrencoskun/TableView/wiki).\n\n## Sample Apps\n\n- This repository has a [sample application](https://github.com/evrencoskun/TableView/tree/master/app) of `TableView`.\n- [TableViewSample 2](https://github.com/evrencoskun/TableViewSample2)\n- [Xently-UI](https://github.com/ajharry69/Xently-UI)\n- [Price List Lite](https://pricelistlite.isolpro.in)\n- [Database Client for MySQL and PostgreSQL](https://play.google.com/store/apps/details?id=dev.dhruv.databaseclient)\n- ([Submit a Pull Request](https://github.com/evrencoskun/TableView/compare) to mention your app on this page)\n\n## Donations\n\n**This project needs you!** If you would like to support this project's further development, the creator of this project or the continuous maintenance of this project, **feel free to donate**. Your donation is highly appreciated (and I love food, coffee and beer). Thank you!\n\n**PayPal**\n\n- [**Donate 5 $**](https://www.paypal.me/evrencoshkun): Thank's for creating this project, here's a coffee (or some beer) for you!\n- [**Donate 10 $**](https://www.paypal.me/evrencoshkun): Wow, I am stunned. Let me take you to the movies!\n- [**Donate 15 $**](https://www.paypal.me/evrencoshkun): I really appreciate your work, let's grab some lunch!\n- [**Donate 25 $**](https://www.paypal.me/evrencoshkun): That's some awesome stuff you did right there, dinner is on me!\n- Or you can also [**choose what you want to donate**](https://www.paypal.me/evrencoshkun), all donations are awesome!\n\n## Contributors\n\nContributions of any kind are welcome! I would like to thank all the [contributors](https://github.com/evrencoskun/TableView/graphs/contributors) for sharing code and\nmaking `TableView` a better product.\n\nIf you wish to contribute to this project, please refer to our [contributing guide](.github/CONTRIBUTING.md).\n\n## License\n\n```\nMIT License\n\nCopyright (c) 2021 Evren Co\u015fkun\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n"
 },
 {
  "repo": "dmytrodanylyk/android-process-button",
  "language": "Java",
  "readme_contents": "### Description [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.dmytrodanylyk.android-process-button/library/badge.png?style=flat)](http://goo.gl/GydnKe) [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Android%20Process%20Button-brightgreen.svg?style=flat)](https://android-arsenal.com/details/1/367)\n\nAndroid Buttons With Built-in Progress Meters.\n\n![](screenshots/sample1_small1.gif)\n![](screenshots/sample1_small2.gif)\n\n### Wiki\n\n- [Home]\n- [Screenshots]\n- [User Guide]\n\n### Integration\n\nThe lib is available on Maven Central, you can find it with [Gradle, please]\n\n```\ndependencies {\n    compile 'com.github.dmytrodanylyk.android-process-button:library:1.0.4'\n}\n```\n\n### Sample\n\n<a href=\"https://play.google.com/store/apps/details?id=com.dd.sample.processbutton\">\n  <img alt=\"Android app on Google Play\"\n       src=\"https://developer.android.com/images/brand/en_app_rgb_wo_45.png\" />\n</a>\n<a href=\"https://play.google.com/store/apps/details?id=com.inappsquared.devappsdirect\">\n  <img alt=\"DevAppsDirect\"\n       src=\"http://www.inappsquared.com/img/icons/devappsdirect_icon.png\" width=\"48\" height=\"48\" />\n</a>\n\n### License\n\n```\nThe MIT License (MIT)\n\nCopyright (c) 2014 Danylyk Dmytro\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n[SmoothProgressBar]:https://github.com/castorflex/SmoothProgressBar\n[Home]:https://github.com/dmytrodanylyk/android-process-buton/wiki\n[Screenshots]:https://github.com/dmytrodanylyk/android-process-buton/wiki/Screenshots\n[User Guide]:https://github.com/dmytrodanylyk/android-process-buton/wiki/User-Guide\n[Gradle, Please]:http://gradleplease.appspot.com/\n\n[![Analytics](https://ga-beacon.appspot.com/UA-44382495-2/android-process-buton/readme)](https://github.com/igrigorik/ga-beacon)\n"
 },
 {
  "repo": "szerhusenBC/jwt-spring-security-demo",
  "language": "Java",
  "readme_contents": "# JWT Spring Security Demo\n\n![Screenshot from running application](etc/screenshot-jwt-spring-security-demo.png?raw=true \"Screenshot JWT Spring Security Demo\")\n\n## About\nThis is a demo for using **[JWT (JSON Web Token)](https://jwt.io)** with **[Spring Security](https://spring.io/projects/spring-security)** and\n**[Spring Boot](https://spring.io/projects/spring-boot)**. I completely rewrote my first version. Now this solution is based on the code base\nfrom the [JHipster Project](https://www.jhipster.tech/). I tried to extract the minimal configuration and classes that are needed \nfor JWT-Authentication and did some changes.\n\n[![Build Status](https://travis-ci.org/szerhusenBC/jwt-spring-security-demo.svg?branch=master)](https://travis-ci.org/szerhusenBC/jwt-spring-security-demo)\n\n## Requirements\nThis demo is build with with Maven 3.6.x and Java 11.\n\n## Usage\nJust start the application with the Spring Boot maven plugin (`mvn spring-boot:run`). The application is\nrunning at [http://localhost:8080](http://localhost:8080).\n\nYou can use the **H2-Console** for exploring the database under [http://localhost:8080/h2-console](http://localhost:8080/h2-console):\n\n![Screenshot from h2-console login](etc/screenshot-h2-console-login.png?raw=true \"Screenshot H2-Console login\")\n\n## Backend\nThere are three user accounts present to demonstrate the different levels of access to the endpoints in\nthe API and the different authorization exceptions:\n```\nAdmin - admin:admin\nUser - user:password\nDisabled - disabled:password (this user is deactivated)\n```\n\nThere are four endpoints that are reasonable for the demo:\n```\n/api/authenticate - authentication endpoint with unrestricted access\n/api/user - returns detail information for an authenticated user (a valid JWT token must be present in the request header)\n/api/persons - an example endpoint that is restricted to authorized users with the authority 'ROLE_USER' (a valid JWT token must be present in the request header)\n/api/hiddenmessage - an example endpoint that is restricted to authorized users with the authority 'ROLE_ADMIN' (a valid JWT token must be present in the request header)\n```\n\n## Frontend\nI've written a small Javascript client and put some comments in the code that hopefully makes this demo understandable.\nYou can find it at [/src/main/resources/static/js/client.js](/src/main/resources/static/js/client.js).\n\n### Generating password hashes for new users\n\nI'm using [bcrypt](https://en.wikipedia.org/wiki/Bcrypt) to encode passwords. Your can generate your hashes with this simple \ntool: [Bcrypt Generator](https://www.bcrypt-generator.com)\n\n### Using another database\n\nActually this demo is using an embedded H2 database that is automatically configured by Spring Boot. If you want to connect \nto another database you have to specify the connection in the *application.yml* in the resource directory. Here is an example for a MySQL DB:\n\n```\nspring:\n  jpa:\n    hibernate:\n      # possible values: validate | update | create | create-drop\n      ddl-auto: create-drop\n  datasource:\n    url: jdbc:mysql://localhost/myDatabase\n    username: myUser\n    password: myPassword\n    driver-class-name: com.mysql.jdbc.Driver\n```\n\n*Hint: For other databases like MySQL sequences don't work for ID generation. So you have to change the GenerationType in the entity beans to 'AUTO' or 'IDENTITY'.*\n\nYou can find a reference of all application properties [here](http://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html).\n\n### Using Flyway\n\nhttps://github.com/szerhusenBC/jwt-spring-security-demo/issues/81\n\n## Docker\nThis project has a docker image. You can find it at [https://hub.docker.com/r/hubae/jwt-spring-security-demo/](https://hub.docker.com/r/hubae/jwt-spring-security-demo/).\n\n## Questions\nIf you have project related questions please take a look at the [past questions](https://github.com/szerhusenBC/jwt-spring-security-demo/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%2Cclosed%20label%3Aquestion%20) or create a new ticket with your question.\n\n*If you have questions that are not directly related to this project (e.g. common questions to the Spring Framework or Spring Security etc.) please search the web or look at [Stackoverflow](http://www.stackoverflow.com).*\n\nSorry for that but I'm very busy right now and don't have much time.\n\n## Interesting projects\n\n* [spring-security-pac4j](https://github.com/pac4j/spring-security-pac4j) a Spring Boot integration for Pac4j (a Java security engine that covers JWT beside others)\n* For more complex microservice environments take a look here: [Using JWT with Spring Security OAuth](http://www.baeldung.com/spring-security-oauth-jwt)\n\n## Author\n\n**Stephan Zerhusen**\n\n* https://twitter.com/stzerhus\n* https://github.com/szerhusenBC\n\n## Copyright and license\n\nThe code is released under the [MIT license](LICENSE?raw=true).\n\n---------------------------------------\n\nPlease feel free to send me some feedback or questions!\n"
 },
 {
  "repo": "race604/FlyRefresh",
  "language": "Java",
  "readme_contents": "# FlyRefresh\r\nThe Android implementation of [Replace](https://dribbble.com/shots/2067564-Replace), designed by [Zee Youn](https://dribbble.com/zeeyoung).\r\nI implement this as a **FlyRefresh** layout. The content of the layout can be any `NestedScrollingChild`, such as a RecyclerView, NestedScrollView, VerticalGridView, etc.\r\nThis library can also work with `NestedScrollingParent` as parent, such as CoordinatorLayout.\r\n\r\n# How it looks\r\n![flyrefresh](./images/flyrefresh.gif)\r\n\r\n# Features\r\n* Work with all [NestedScrollingParent](https://developer.android.com/reference/android/support/v4/view/NestedScrollingParent.html) and [NestedScrollingChild](https://developer.android.com/reference/android/support/v4/view/NestedScrollingChild.html)\r\n* Default minimize configuration for [Replace](https://dribbble.com/shots/2067564-Replace) animation\r\n* Expendable/Shrinkable header\r\n* Support custom header view\r\n* Support custom refresh animation\r\n\r\n# How to use\r\n\r\nAdd Gradle dependency:\r\n\r\n```gradle\r\ndependencies {\r\n   compile 'com.race604.flyrefresh:library:2.0.0'\r\n}\r\n```\r\n\r\nAn example of basic usage in `layout.xml`:\r\n\r\n```xml\r\n<com.race604.flyrefresh.FlyRefreshLayout\r\n  android:id=\"@+id/fly_layout\"\r\n  android:layout_width=\"match_parent\"\r\n  android:layout_height=\"match_parent\">\r\n\r\n    <android.support.v7.widget.RecyclerView\r\n      android:id=\"@+id/list\"\r\n      android:layout_width=\"match_parent\"\r\n      android:layout_height=\"match_parent\"\r\n      android:paddingTop=\"24dp\"\r\n      android:background=\"#FFFFFF\"/>\r\n</com.race604.flyrefresh.FlyRefreshLayout>\r\n```\r\n\r\nOr you can use `PullHeaderLayout` for more configurations, you can set custom attributes as shown below:\r\n\r\n```xml\r\n<declare-styleable name=\"PullHeaderLayout\">\r\n    <!-- hader size -->\r\n    <attr name=\"phl_header_height\" format=\"dimension\" />\r\n    <attr name=\"phl_header_expand_height\" format=\"dimension\" />\r\n    <attr name=\"phl_header_shrink_height\" format=\"dimension\" />\r\n    <!-- header view id -->\r\n    <attr name=\"phl_header\" format=\"reference\" />\r\n    <!-- content view id -->\r\n    <attr name=\"phl_content\" format=\"reference\" />\r\n    <!-- Float action button icon -->\r\n    <attr name=\"phl_action\" format=\"reference\" />\r\n</declare-styleable>\r\n```\r\nFor more, please turn to the source code.\r\n\r\n# License\r\n`FlyRefresh` is available under the MIT license.\r\n"
 },
 {
  "repo": "500px/500px-android-blur",
  "language": "Java",
  "readme_contents": "## [Currently unmaintained]\n\n# 500px Android Blurring View\n\nFor more information, please see [our blog post](http://developers.500px.com/2015/03/17/a-blurring-view-for-android.html).\n\n## Download\n\nDefine via Gradle:\n\n``` groovy\nrepositories {\n    maven { url 'https://github.com/500px/500px-android-blur/raw/master/releases/' }\n}\n\ndependencies {\n    compile 'com.fivehundredpx:blurringview:1.0.0'\n}\n```\n\nEnable renderscript in your module's `defaultConfig`:\n```groovy\nandroid {\n    defaultConfig {\n\n        renderscriptTargetApi 21\n        renderscriptSupportModeEnabled true\n        ...\n    }\n}\n\n```\n\n\n## Usage\n\nFirst, give the blurring view a reference to the view to be blurred:\n\n``` java\nblurringView.setBlurredView(blurredView);\n```\n\nand then whenever the blurred view changes, invalidate the blurring view:\n\n``` java\nblurringView.invalidate();\n```\n\n## Demo\n\n![500px Blurring View Demo](blurdemo.gif \"500px Blurring View Demo\")\n\n## License\n\nThis project is licensed under the terms of [the MIT license](LICENSE.txt).\n"
 },
 {
  "repo": "Todd-Davies/ProgressWheel",
  "language": "Java",
  "readme_contents": "![Project frozen](https://img.shields.io/badge/status-frozen-blue.png) ![Project unmaintained](https://img.shields.io/badge/project-unmaintained-red.svg)\n\n\nDeprecation warning\n===================\n\nThis project is no-longer maintained, and has not been maintained for a few years now. If you're looking for an alternative library, consider the below options:\n\n- https://github.com/pnikosis/materialish-progress\n- https://github.com/ybq/Android-SpinKit\n- https://github.com/zekapp/Android-ProgressViews\n\nIf, on the other hand, you'd like to take over maintinance of the project or modernise it, feel free to do so; just send me pull requests or an email (at todd434@gmail.com).\n\nProgress Wheel\n=============\n\nThis is a custom component for Android intended for use instead of a progress bar.\n\n![Sample Image](https://github.com/Todd-Davies/ProgressWheel/raw/master/sample_image.png \"An example implementation\")\n![Sample Image 3](https://github.com/Todd-Davies/ProgressWheel/raw/master/sample_image_3.png \"Another example implementation\")\n![Sample Image 4](https://github.com/Todd-Davies/ProgressWheel/raw/master/sample_image_4.png \"Another example implementation\")\n\nCompare it side by side with the Android 2x progress wheel:\n\n![Sample Image 5](https://github.com/Todd-Davies/ProgressWheel/raw/master/sample_image5.png \"Side by side comparison\")\n\nA complete walkthrough of how to use this component in your app\n-------------\n\n**XML:**   \nTo implement the view in your xml layout do the following:\n\n1. Add the following to your attrs.xml file (in res/values):\n``` xml\n<declare-styleable name=\"ProgressWheel\">\n        <attr name=\"pwText\" format=\"string\" />\n        <attr name=\"pwTextColor\" format=\"color\" />\n        <attr name=\"pwTextSize\" format=\"dimension\" />\n        <attr name=\"pwBarColor\" format=\"color\" />\n        <attr name=\"pwRimColor\" format=\"color\" />\n        <attr name=\"pwRimWidth\" format=\"dimension\" />\n        <attr name=\"pwSpinSpeed\" format=\"dimension\" />\n        <attr name=\"pwDelayMillis\" format=\"integer\" />\n        <attr name=\"pwCircleColor\" format=\"color\" />\n        <attr name=\"pwRadius\" format=\"dimension\" />\n        <attr name=\"pwBarWidth\" format=\"dimension\" />\n        <attr name=\"pwBarLength\" format=\"dimension\" />\n        <attr name=\"pwContourColor\" format=\"color\"/>\n        <attr name=\"pwContourSize\" format=\"dimension\"/>\n</declare-styleable>\n```\n\n2. Add the following code to the root view of your layout:\n`xmlns:ProgressWheel=\"http://schemas.android.com/apk/res/com.visualdenim.schooltraq\"`\n\n3. Add the widget code in the appropriate place in your xml file. Here's a sample implementation:\n``` xml\n<com.todddavies.components.progressbar.ProgressWheel   \n    android:id=\"@+id/pw_spinner\"     \n    android:layout_width=\"200dp\"    \n    android:layout_height=\"200dp\"   \n    android:layout_centerInParent=\"true\"   \n    ProgressWheel:pwText=\"Authenticating...\"    \n    ProgressWheel:pwTextColor=\"#222\"   \n    ProgressWheel:pwTextSize=\"14sp\"   \n    ProgressWheel:pwRimColor=\"#330097D6\"   \n    ProgressWheel:pwBarLength=\"60dp\"    \n    ProgressWheel:pwBarColor=\"#0097D6\"   \n    ProgressWheel:pwBarWidth=\"5dp\"   \n    ProgressWheel:pwRimWidth=\"2dp\" /> \n```\n\t\n**Java:**   \nFirst you need to either get a ProgressWheel from a layout file, or initalise one. Do this by:\n\n-  `ProgressWheel pw = new ProgressWheel(myContext, myAttributes);`\n-  `ProgressWheel pw = (ProgressWheel) findViewById(R.id.pw_spinner);`\n\nTo spin the progress wheel, you just call .`startSpinning()` and to stop it spinning, you call `.stopSpinning()`\n\nIncrementing the progress wheel is slightly more tricky, you call `.incrementProgress()`. However, this is out of 360,  \n(because a circle has 360 degrees), and will automatically reset once you get past 360. A percentage display is   \nautomatically displayed.\n\nUsing as a dependency\n--------------------------\n\nAdd this to your build.gradle:\n\n```gradle\n\trepositories {\n\t    maven { url \"https://jitpack.io\" }\n\t}\n\t\n\tdependencies {\n\t    compile 'com.github.Todd-Davies:ProgressWheel:1.2'\n\t}\n```\n\nUsing as a library project\n--------------------------\n\nTo use it as a library in Android Studio, please edit build.gradle.\n\nModify:\n\n    apply plugin: 'android'\n\nInto:\n\n    apply plugin: 'android-library'\n\nSince Android SDK Tools revision 17 (released March 2012), this component can\nbe used as a library project. In this case, you do *not* need to copy anything\ninto your project's attrs.xml, and you must use the following namespace URI,\ninstead of the above:\n\n`xmlns:ProgressWheel=\"http://schemas.android.com/apk/res-auto\"`\n\nOtherwise, usage should be the same.\n\n\n[Todd Davies](http://todddavies.co.uk) - [@Todd__Davies](http://twitter.com/todd__davies) - 2012\n"
 },
 {
  "repo": "VerbalExpressions/JavaVerbalExpressions",
  "language": "Java",
  "readme_contents": "JavaVerbalExpressions\n=====================\n[![release](http://github-release-version.herokuapp.com/github/VerbalExpressions/JavaVerbalExpressions/release.svg?style=flat)](https://github.com/VerbalExpressions/JavaVerbalExpressions/releases/latest) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/ru.lanwen.verbalregex/java-verbal-expressions/badge.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/ru.lanwen.verbalregex/java-verbal-expressions)\n[<img src=\"http://img.shields.io/badge/ported%20from-%20JSVerbalExpressions-orange.svg?style=flat\">](https://github.com/VerbalExpressions/JSVerbalExpressions)\n[![Coverage Status](https://coveralls.io/repos/VerbalExpressions/JavaVerbalExpressions/badge.svg)](https://coveralls.io/r/VerbalExpressions/JavaVerbalExpressions)\n\nVerbalExpressions is a Java library that helps to construct difficult regular expressions.\n\n\n\n## Getting Started\n\nMaven Dependency:\n\n```xml\n<dependency>\n  <groupId>ru.lanwen.verbalregex</groupId>\n  <artifactId>java-verbal-expressions</artifactId>\n  <version>1.8</version>\n</dependency>\n```\n\nYou can use *SNAPSHOT* dependency with adding to `pom.xml`:\n```xml\n<repositories>\n  <repository>\n    <id>ossrh</id>\n    <url>https://oss.sonatype.org/content/repositories/snapshots</url>\n  </repository>\n</repositories>\n```\n\n## Examples\n```java\nVerbalExpression testRegex = VerbalExpression.regex()\n                                                .startOfLine().then(\"http\").maybe(\"s\")\n\t           \t\t\t\t.then(\"://\")\n\t           \t\t\t\t.maybe(\"www.\").anythingBut(\" \")\n\t           \t\t\t\t.endOfLine()\n\t           \t\t\t\t.build();\n\n// Create an example URL\nString url = \"https://www.google.com\";\n\n// Use VerbalExpression's testExact() method to test if the entire string matches the regex\ntestRegex.testExact(url); //True\n\ntestRegex.toString(); // Outputs the regex used:\n                      // ^(?:http)(?:s)?(?:\\:\\/\\/)(?:www\\.)?(?:[^\\ ]*)$\n\n```\n\n```java\nVerbalExpression testRegex = VerbalExpression.regex()\n                                                .startOfLine().then(\"abc\").or(\"def\")\n                                                .build();\n\nString testString = \"defzzz\";\n\n//Use VerbalExpression's test() method to test if parts if the string match the regex\ntestRegex.test(testString);       // true\ntestRegex.testExact(testString);  // false\ntestRegex.getText(testString);    // returns: def\n```\n\nBuilder can be cloned:\n```java\nVerbalExpression regex = regex(regex().anything().addModifier('i')).endOfLine().build();\n``` \n\nOr can be used in another regex: \n```java\nVerbalExpression.Builder digits = regex().capt().digit().oneOrMore().endCapt().tab();\nVerbalExpression regex2 = regex().add(digits).add(digits).build();\n``` \n\nFeel free to use any predefined char groups: \n```java\nregex().wordChar().nonWordChar()\n   .space().nonSpace()\n   .digit().nonDigit()\n```\n\nDefine captures:\n```java \nString text = \"aaabcd\";\nVerbalExpression regex = regex()\n                .find(\"a\")\n                .capture().find(\"b\").anything().endCapture().then(\"cd\").build();\n\nregex.getText(text)     // returns \"abcd\"\nregex.getText(text, 1)  // returns \"b\"\n``` \n\n## More complex examples \n* [Parse long strings example](https://github.com/VerbalExpressions/JavaVerbalExpressions/wiki/Parse-long-strings-example)\n\n## Other implementations  \nYou can view all implementations on [VerbalExpressions.github.io](http://VerbalExpressions.github.io) \n\n[\n[Javascript](https://github.com/VerbalExpressions/JSVerbalExpressions) - \n[PHP](https://github.com/VerbalExpressions/PHPVerbalExpressions) - \n[Python](https://github.com/VerbalExpressions/PythonVerbalExpressions) - \n[C#](https://github.com/VerbalExpressions/CSharpVerbalExpressions) - \n[Objective-C](https://github.com/VerbalExpressions/ObjectiveCVerbalExpressions) - \n[Ruby](https://github.com/ryan-endacott/verbal_expressions) - \n[Groovy](https://github.com/VerbalExpressions/GroovyVerbalExpressions) - \n[Haskell](https://github.com/VerbalExpressions/HaskellVerbalExpressions) - \n[C++](https://github.com/VerbalExpressions/CppVerbalExpressions) - ... ([moarr](https://github.com/VerbalExpressions)) ] \n\n## Project released with travis\n\nWith help of this tutorial:\nhttps://dracoblue.net/dev/uploading-snapshots-and-releases-to-maven-central-with-travis/\n"
 },
 {
  "repo": "Ramotion/paper-onboarding-android",
  "language": "Java",
  "readme_contents": "<a href=\"https://www.ramotion.com/agency/app-development/?utm_source=gthb&utm_medium=repo&utm_campaign=paper-onboarding-android\"><img src=\"https://github.com/Ramotion/folding-cell/blob/master/header.png\"></a>\n\n<a href=\"https://github.com/Ramotion/paper-onboarding-android\">\n<img align=\"left\" src=\"https://github.com/Ramotion/paper-onboarding-android/blob/master/onboarding_preview.gif\" width=\"480\" height=\"360\" /></a>\n\n<p><h1 align=\"left\">PAPER ONBOARDING</h1></p>\n\n<h4>Android library Paper Onboarding is a material design UI slider written on Java</h4>\n\n\n___\n\n\n<p><h6>We specialize in the designing and coding of custom UI for Mobile Apps and Websites.</h6>\n<a href=\"https://www.ramotion.com/agency/app-development/?utm_source=gthb&utm_medium=repo&utm_campaign=paper-onboarding-android\">\n<img src=\"https://github.com/ramotion/gliding-collection/raw/master/contact_our_team@2x.png\" width=\"187\" height=\"34\"></a>\n</p>\n<p><h6>Stay tuned for the latest updates:</h6>\n<a href=\"https://goo.gl/rPFpid\" >\n<img src=\"https://i.imgur.com/ziSqeSo.png/\" width=\"156\" height=\"28\"></a></p>\n\n</br>\n\n[![CircleCI](https://circleci.com/gh/Ramotion/paper-onboarding-android.svg?style=svg)](https://circleci.com/gh/Ramotion/paper-onboarding-android)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/ed1eb5c89dfc45eabb80e93c6a124012)](https://www.codacy.com/app/Ramotion/paper-onboarding-android?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=ramotion/paper-onboarding-android&amp;utm_campaign=Badge_Grade)\n[![Twitter](https://img.shields.io/badge/Twitter-@Ramotion-blue.svg?style=flat)](http://twitter.com/Ramotion)\n[![Donate](https://img.shields.io/badge/Donate-PayPal-blue.svg)](https://paypal.me/Ramotion)\n\n## Requirements\n\u200b\n- Android 4.0.3 IceCreamSandwich (API lvl 15) or greater\n- Your favorite IDE\n\n## Installation\n\u200b\nJust download the package from [here](http://central.maven.org/maven2/com/ramotion/paperonboarding/paper-onboarding/1.1.3/paper-onboarding-1.1.3.aar) and add it to your project classpath, or just use the maven repo:\n\u200b\nGradle:\n```groovy\n'com.ramotion.paperonboarding:paper-onboarding:1.1.3'\n```\nSBT:\n```scala\nlibraryDependencies += \"com.ramotion.paperonboarding\" % \"paper-onboarding\" % \"1.1.3\"\n```\nMaven:\n```xml\n<dependency>\n    <groupId>com.ramotion.paperonboarding</groupId>\n    <artifactId>paper-onboarding</artifactId>\n    <version>1.1.3</version>\n    <type>aar</type>\n</dependency>\n```\n\n## Basic usage\n\nPaper Onboarding is a simple and easy to use onboarding slider for your app. You just need to provide content for each slider page - a main icon, text, and small round icon for the bottom.\n\n1 Use `PaperOnboardingPage` to prepare your data for slider:\n```java\nPaperOnboardingPage scr1 = new PaperOnboardingPage(\"Hotels\",\n\t\"All hotels and hostels are sorted by hospitality rating\",\n        Color.parseColor(\"#678FB4\"), R.drawable.hotels, R.drawable.key);\nPaperOnboardingPage scr2 = new PaperOnboardingPage(\"Banks\",\n\t\"We carefully verify all banks before add them into the app\",\n        Color.parseColor(\"#65B0B4\"), R.drawable.banks, R.drawable.wallet);\nPaperOnboardingPage scr3 = new PaperOnboardingPage(\"Stores\",\n\t\"All local stores are categorized for your convenience\",\n        Color.parseColor(\"#9B90BC\"), R.drawable.stores, R.drawable.shopping_cart);\n\nArrayList<PaperOnboardingPage> elements = new ArrayList<>();\nelements.add(scr1);\nelements.add(scr2);\nelements.add(scr3);\n```\n\n\n2 Create a fragment from `PaperOnboardingFragment` and provide your data.\n```java\nPaperOnboardingFragment onBoardingFragment = PaperOnboardingFragment.newInstance(elements);\n```\n\n3 Done! Now you can use this fragment as you want in your activity, for example :\n\n```java\nFragmentTransaction fragmentTransaction = fragmentManager.beginTransaction();\nfragmentTransaction.add(R.id.fragment_container, onBoardingFragment);\nfragmentTransaction.commit();\n```\n\n4 Extra step : You can add event listeners to fragments with your logic, like replacing this fragment to another when the user swipes next from the last screen:\n\n```java\nonBoardingFragment.setOnRightOutListener(new PaperOnboardingOnRightOutListener() {\n    @Override\n    public void onRightOut() {\n        FragmentTransaction fragmentTransaction = fragmentManager.beginTransaction();\n        Fragment bf = new BlankFragment();\n        fragmentTransaction.replace(R.id.fragment_container, bf);\n        fragmentTransaction.commit();\n    }\n});\n```\nCurrently, there are three listeners that cover all events - onRightOut, onLeftOut and onChange; see code examples and usage in the repo.\n\n<br>\n\n## \ud83d\uddc2 Check this library on other language:\n<a href=\"https://github.com/Ramotion/paper-onboarding\"> \n<img src=\"https://github.com/ramotion/navigation-stack/raw/master/Swift@2x.png\" width=\"178\" height=\"81\"></a>\n\n\n## \ud83d\udcc4 License\n\nPaper Onboarding Android is released under the MIT license.\nSee [LICENSE](./LICENSE) for details.\n\nThis library is a part of a <a href=\"https://github.com/Ramotion/android-ui-animation-components-and-libraries\"><b>selection of our best UI open-source projects</b></a>\n\nIf you use the open-source library in your project, please make sure to credit and backlink to www.ramotion.com\n\n## \ud83d\udcf1 Get the Showroom App for Android to give it a try\nTry this UI component and more like this in our Android app. Contact us if interested.\n\n<a href=\"https://play.google.com/store/apps/details?id=com.ramotion.showroom\" >\n<img src=\"https://raw.githubusercontent.com/Ramotion/react-native-circle-menu/master/google_play@2x.png\" width=\"104\" height=\"34\"></a>\n\n<a href=\"https://www.ramotion.com/agency/app-development/?utm_source=gthb&utm_medium=repo&utm_campaign=paper-onboarding-android\">\n<img src=\"https://github.com/ramotion/gliding-collection/raw/master/contact_our_team@2x.png\" width=\"187\" height=\"34\"></a>\n"
 },
 {
  "repo": "ozodrukh/CircularReveal",
  "language": "Java",
  "readme_contents": "[![](https://www.jitpack.io/v/Ozodrukh/CircularReveal.svg)](https://www.jitpack.io/#Ozodrukh/CircularReveal)\n\nCircularReveal\n==============\n\nLollipop ViewAnimationUtils.createCircularReveal for everyone 14+\n\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=tPjpF75-BWA\n\" target=\"_blank\">Yotube Video <br /> <img src=\"http://img.youtube.com/vi/tPjpF75-BWA/0.jpg\"\nalt=\"Circular Reveal\" width=\"320\" height=\"240\" border=\"10\" /></a>\n\n#### [Checout demo application ](https://github.com/ozodrukh/CircularReveal/releases)\n\n\nHow to use:\n======\n\nUse regular `RevealFrameLayout` & `RevealLinearLayout` don't worry, only target will be clipped :)\n\n```xml\n<io.codetail.widget.RevealFrameLayout\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <!-- Put more views here if you want, it's stock frame layout  -->\n\n    <android.support.v7.widget.CardView\n        xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n        android:id=\"@+id/awesome_card\"\n        style=\"@style/CardView\"\n        app:cardBackgroundColor=\"@color/material_deep_teal_500\"\n        app:cardElevation=\"2dp\"\n        app:cardPreventCornerOverlap=\"false\"\n        app:cardUseCompatPadding=\"true\"\n        android:layout_marginLeft=\"8dp\"\n        android:layout_marginRight=\"8dp\"\n        android:layout_marginTop=\"8dp\"\n        android:layout_width=\"300dp\"\n        android:layout_height=\"300dp\"\n        android:layout_gravity=\"center_horizontal\"\n        />\n\n</io.codetail.widget.RevealFrameLayout>\n```\n\n```java\n\n    View myView = findView(R.id.awesome_card);\n\n    // get the center for the clipping circle\n    int cx = (myView.getLeft() + myView.getRight()) / 2;\n    int cy = (myView.getTop() + myView.getBottom()) / 2;\n\n    // get the final radius for the clipping circle\n    int dx = Math.max(cx, myView.getWidth() - cx);\n    int dy = Math.max(cy, myView.getHeight() - cy);\n    float finalRadius = (float) Math.hypot(dx, dy);\n\n    // Android native animator\n    Animator animator =\n            ViewAnimationUtils.createCircularReveal(myView, cx, cy, 0, finalRadius);\n    animator.setInterpolator(new AccelerateDecelerateInterpolator());\n    animator.setDuration(1500);\n    animator.start();\n\n```\n\nHow to add dependency\n=====================\n\nThis library is not released in Maven Central, but instead you can use [JitPack](https://www.jitpack.io/#ozodrukh/CircularReveal)\n\nadd remote maven url\n\n```groovy\n\trepositories {\n\t    maven {\n\t        url \"https://jitpack.io\"\n\t    }\n\t}\n```\n\nthen add a library dependency\n\n```groovy\n\tdependencies {\n\t    implementation ('com.github.ozodrukh:CircularReveal:2.0.1@aar') {\n\t        transitive = true;\n\t    }\n\t}\n```\n\n\nLicense\n--------\n\n    The MIT License (MIT)\n\n    Copyright (c) 2016 Abdullaev Ozodrukh\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in\n    all copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n    THE SOFTWARE.\n"
 },
 {
  "repo": "zodiacon/WindowsInternals",
  "language": "C",
  "readme_contents": "# Windows Internals Book 7th Edition Tools\nThe Windows Internals book, 7th edition Part 1, uses many tools to demonstrate various features of the Windows operating system. Most are from Sysinternals (http://www.sysinternals.com) and built-in tools. But some tools were written by Alex Ionescu and myself and used in the book; these are be published here with full source code.\n\nPlease note that these tools were NOT written by or endorsed by Microsoft. They are provided \"as is\" without any warranties or guarantees. For all I know, they might format your hard drive or even the entire World Wide Web. :) Use at your own risk!\n\n"
 },
 {
  "repo": "boazsegev/facil.io",
  "language": "C",
  "readme_contents": "<p align=\"center\"><img src=\"https://s33.postimg.cc/5d7j1nacf/Readme_1.jpg\"></p>\n\n[![GitHub](https://img.shields.io/badge/Open%20Source-MIT-blue.svg)](https://github.com/boazsegev/facil.io)\n[![Build Status](https://travis-ci.org/boazsegev/facil.io.svg?branch=master)](https://travis-ci.org/boazsegev/facil.io)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/2abeba588afb444ca6d92e68ccfbe36b)](https://www.codacy.com/app/boazsegev/facil.io?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=boazsegev/facil.io&amp;utm_campaign=Badge_Grade)\n[![codecov](https://codecov.io/gh/boazsegev/facil.io/branch/master/graph/badge.svg)](https://codecov.io/gh/boazsegev/facil.io)\n\n[facil.io](http://facil.io) is a C micro-framework for web applications. facil.io includes:\n\n* A fast HTTP/1.1 and Websocket static file + application server.\n* Support for custom network protocols for both server and client connections.\n* Dynamic types designed with web applications in mind (Strings, Hashes, Arrays etc').\n* Performant JSON parsing and formatting for easy network communication.\n* A pub/sub process cluster engine for local and Websocket pub/sub.\n* Optional connectivity with Redis.\n\n[facil.io](http://facil.io) provides high performance TCP/IP network services to Linux / BSD (and macOS) by using an evented design (as well as thread pool and forking support) and provides an easy solution to [the C10K problem](http://www.kegel.com/c10k.html).\n\nYou can read more about [facil.io](http://facil.io) on the [facil.io](http://facil.io) website.\n\n### Important to Note\n\nThe master branch on the `git` repo is the development branch and is likely to be broken at any given time (especially when working on major revisions, as I am at the moment).\n\nPlease select a release version for any production needs.\n\n### Who's running on `facil.io`\n\n* [Iodine, a Ruby HTTP/Websockets Ruby application server](https://github.com/boazsegev/iodine) is powered by `facil.io` - so everyone using the iodine server is running on facil.io.\n\n* Are you using `facil.io`? Let me know!\n\n### An HTTP example\n\n```c\n#include \"http.h\" /* the HTTP facil.io extension */\n\n// We'll use this callback in `http_listen`, to handles HTTP requests\nvoid on_request(http_s *request);\n\n// These will contain pre-allocated values that we will use often\nFIOBJ HTTP_X_DATA;\n\n// Listen to HTTP requests and start facil.io\nint main(int argc, char const **argv) {\n  // allocating values we use often\n  HTTP_X_DATA = fiobj_str_new(\"X-Data\", 6);\n  // listen on port 3000 and any available network binding (NULL == 0.0.0.0)\n  http_listen(\"3000\", NULL, .on_request = on_request, .log = 1);\n  // start the server\n  facil_start(.threads = 1);\n  // deallocating the common values\n  fiobj_free(HTTP_X_DATA);\n}\n\n// Easy HTTP handling\nvoid on_request(http_s *request) {\n  http_set_cookie(request, .name = \"my_cookie\", .name_len = 9, .value = \"data\",\n                  .value_len = 4);\n  http_set_header(request, HTTP_HEADER_CONTENT_TYPE,\n                  http_mimetype_find(\"txt\", 3));\n  http_set_header(request, HTTP_X_DATA, fiobj_str_new(\"my data\", 7));\n  http_send_body(request, \"Hello World!\\r\\n\", 14);\n}\n```\n\n## Using `facil.io` in your project\n\nIt's possible to either start a new project with `facil.io` or simply add it to an existing one. GNU `make` is the default build system and CMake is also supported.\n\n`facil.io` should be C99 compatible.\n\n### Starting a new project with `facil.io`\n\nTo start a new project using the `facil.io` framework, run the following command in the terminal (change `appname` to whatever you want):\n\n     $ bash <(curl -s https://raw.githubusercontent.com/boazsegev/facil.io/master/scripts/new/app) appname\n\nYou can [review the script here](scripts/new/app). In short, it will create a new folder, download a copy of the stable branch, add some demo boiler plate code and run `make clean` (which is required to build the `tmp` folder structure).\n\nNext, edit the `makefile` to remove any generic features you don't need, such as the `DUMP_LIB` feature, the `DEBUG` flag or the `DISAMS` disassembler and start development.\n\nCredit to @benjcal for suggesting the script.\n\n**Notice: The *master* branch is the development branch. Please select the latest release tag for the latest stable release version.**\n\n### Adding facil.io to an existing project\n\n[facil.io](http://facil.io) is a source code library, so it's easy to copy the source code into an existing project and start using the library right away.\n\nThe `make libdump` command will dump all the relevant files in a single folder called `libdump`, and you can copy them all or divide them into header ands source files.\n\nIt's also possible to compile the facil.io library separately using the `make lib` command.\n\n### Using `facil.io` as a CMake submodule\n\n[facil.io](http://facil.io) also supports both `git` and CMake submodules. Credit to @OwenDelahoy (PR#8).\n\nFirst, add the repository as a submodule using `git`:\n\n    git submodule add https://github.com/boazsegev/facil.io.git\n\nThen add the following line the project's `CMakeLists.txt`\n\n    add_subdirectory(facil.io)\n\n### Using `facil.io` with Meson\n\n[facil.io](http://facil.io) is available at [Meson Wrap DB](https://wrapdb.mesonbuild.com/facil).\n\nFirst, install the wrap file:\n\n    meson wrap install facil\n\nThen add the following line to your project's `meson.build`:\n\n    facil_dep = subproject('facil').get_variable('facil_dep')\n\n## More Examples\n\nThe examples folder includes code examples for a [telnet echo protocol](examples/raw-echo.c), a [Simple Hello World server](examples/raw-http.c), an example for [Websocket pub/sub with (optional) Redis](examples/http-chat.c), etc'.\n\nYou can find more information on the [facil.io](http://facil.io) website\n\n---\n\n## Forking, Contributing and all that Jazz\n\n[The contribution guide can be found here](CONTRIBUTING.md).\n\nSure, why not. If you can add Solaris or Windows support to `evio` and `sock`, that could mean `facil` would become available for use on these platforms as well.\n\nIf you encounter any issues, open an issue (or, even better, a pull request with a fix) - that would be great :-)\n\nHit me up if you want to:\n\n* Write tests... I always need more tests...\n\n* Help me write HPACK / HTTP2 protocol support.\n\n* Help me design / write a generic HTTP routing helper library for the `http_s` struct.\n\n* If you want to help me write a new SSL/TLS library or have an SSL/TLS solution we can fit into `facil` (as source code)... Note: SSL/TLS solutions should fit both client and server modes.\n\n* If you want to help promote the library, that would be great as well. Perhaps publish [benchmarks](https://github.com/TechEmpower/FrameworkBenchmarks) or share your story.\n\n* Writing documentation into the `facil.io` website would be great. I keep the source code documentation fairly updated, but the documentation should be copied to the `docs` folder to get the documentation website up and running.\n\n<p align=\"center\"><img src=\"https://s33.postimg.cc/seo47ha0v/Readme_2.jpg\"></p>\n"
 },
 {
  "repo": "wysaid/android-gpuimage-plus",
  "language": "C",
  "readme_contents": "# Android-GPUImage-Plus\n\nImage, Camera And Video Filters Based On OpenGL\n\n>To get pure lib without ffmpeg(No feature of video recording), please checkout the branch [min](https://github.com/wysaid/android-gpuimage-plus/tree/min). The whole jni module will be less than 600KB.\n\n## New Feature\n\nSee the `image deform demo`.\n\n![](screenshots/6.gif) ![](screenshots/5.gif)\n\n## Gradle dependency\n\n```gradle\nrepositories {\n    jcenter()\n}\n\n//Choose only one of them\ndependencies {\n    //All arch: armeabi, armeabi-v7a, arm64-v8a, x86\n    compile 'org.wysaid:gpuimage-plus:2.6.3'\n\n    //Pure graphics lib without ffmpeg. (all arch for branch 'min')\n    compile 'org.wysaid:gpuimage-plus:2.6.3-min'\n}\n```\n\n> Use other ffmpeg-library, see: <https://github.com/wysaid/FFmpeg-Android.git>\n\n## Abstract\n\n* This repo is an Android Studio Project, comprising \"cgeDemo\", \"library\" two sub-modules. Hundreds of built-in filters are available in the demo. \ud83d\ude0bIf you'd like to add your own filter, please take a look at the manual page. Or you can follow the demo code. The new custom filter should be written in C++.\n\n* Demo and Library will be updated as needed. Welcome for your questions or PR.\n\n* To build with the jni part, pleasae try:\n\n```shell\nexport NDK=path/of/your/ndk\ncd folder/of/jni (android-gpuimage-plus/library/src/main/jni)\n\n#This will make all arch: armeabi, armeabi-v7a arm64-v8a, x86, mips\n./buildJNI\n#Or use \"sh buildJNI\"\n\n#Try this if you failed to run the shell above\nexport CGE_USE_VIDEO_MODULE=1\n$NDK/ndk-build\n\n#If you don't want anything except the image filter,\n#Do as below to build with only cge module\n#No ffmpeg, opencv or faceTracker.\n#And remove the loading part of ffmpeg&facetracker\n$NDK/ndk-build\n\n#For Window user, you should include the `.cmd` extension to `ndk-build` like this:\ncd your_path_to\\android-gpuimage-plus-master\\library\\src\\main\\jni\nyour_path_to_ndk\\ndk-bundle\\ndk-build.cmd\n\n#Also remember to comment out these line in NativeLibraryLoader\n//System.loadLibrary(\"ffmpeg\");\n//CGEFFmpegNativeLibrary.avRegisterAll();\n```\n\n> You can find precompiled libs here: [android-gpuimage-plus-libs](https://github.com/wysaid/android-gpuimage-plus-libs) (The precompiled '.so' files are generated with NDK-r23b)\n\nNote that the generated file \"libFaceTracker.so\" is not necessary. So just remove this file if you don't want any feature of it.\n\n* iOS version: [https://github.com/wysaid/ios-gpuimage-plus](https://github.com/wysaid/ios-gpuimage-plus \"http://wysaid.org\")\n\n## Manual\n\n### 1. Usage\n\n___Sample Code for doing a filter with Bitmap___\n\n```java\n//Simply apply a filter to a Bitmap.\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_main);\n\n    Bitmap srcImage = ...;\n\n    //HSL Adjust (hue: 0.02, saturation: -0.31, luminance: -0.17)\n    //Please see the manual for more details.\n    String ruleString = \"@adjust hsl 0.02 -0.31 -0.17\";\n\n    Bitmap dstImage = CGENativeLibrary.filterImage_MultipleEffects(src, ruleString, 1.0f);\n\n    //Then the dstImage is applied with the filter.\n    //It's so convenient, isn't it?\n\n    //Save the result image to /sdcard/libCGE/rec_???.jpg.\n    ImageUtil.saveBitmap(dstImage);\n}\n```\n\n### 2. Custom Shader Filter\n\n#### 2.1 Write your own filter\n\n>Your filter must inherit [CGEImageFilterInterfaceAbstract](https://github.com/wysaid/android-gpuimage-plus/blob/master/library/src/main/jni/include/cgeImageFilter.h#L42) or its child class. Most of the filters are inherited from [CGEImageFilterInterface](https://github.com/wysaid/android-gpuimage-plus/blob/master/library/src/main/jni/include/cgeImageFilter.h#L57) because it has many useful functions.\n\n```cpp\n// A simple customized filter to do a color reversal.\nclass MyCustomFilter : public CGE::CGEImageFilterInterface\n{\npublic:\n    \n    bool init()\n    {\n        CGEConstString fragmentShaderString = CGE_SHADER_STRING_PRECISION_H\n        (\n        varying vec2 textureCoordinate;  //defined in 'g_vshDefaultWithoutTexCoord'\n        uniform sampler2D inputImageTexture; // the same to above.\n\n        void main()\n        {\n            vec4 src = texture2D(inputImageTexture, textureCoordinate);\n            src.rgb = 1.0 - src.rgb;  //Simply reverse all channels.\n            gl_FragColor = src;\n        }\n        );\n\n        //m_program is defined in 'CGEImageFilterInterface'\n        return m_program.initWithShaderStrings(g_vshDefaultWithoutTexCoord, s_fsh);\n    }\n\n    //void render2Texture(CGE::CGEImageHandlerInterface* handler, GLuint srcTexture, GLuint vertexBufferID)\n    //{\n    //  //Your own render functions here.\n    //  //Do not override this function to use the CGEImageFilterInterface's.\n    //}\n};\n```\n\n>Note: To add your own shader filter with c++. [Please see the demo for further details](https://github.com/wysaid/android-gpuimage-plus/blob/master/library/src/main/jni/source/customFilter_N.cpp).\n\n#### 2.2 Run your own filter\n\n__In C++, you can use a CGEImageHandler to do that:__\n\n```cpp\n//Assume the gl context already exists:\n//JNIEnv* env = ...;\n//jobject bitmap = ...;\nCGEImageHandlerAndroid handler;\nCustomFilterType* customFilter = new CustomFilterType();\n\n//You should handle the return value (false is returned when failed.)\ncustomFilter->init();\nhandler.initWithBitmap(env, bitmap);\n\n//The customFilter will be released when the handler' destructor is called.\n//So you don't have to call 'delete customFilter' if you add it into the handler.\nhandler.addImageFilter(customFilter);\n\nhandler.processingFilters(); //Run the filters.\n\njobject resultBitmap = handler.getResultBitmap(env);\n```\n\n>If no gl context exists, the class [CGESharedGLContext](https://github.com/wysaid/android-gpuimage-plus/blob/master/library/src/main/jni/interface/cgeSharedGLContext.h#L22) may be helpful.\n\n__In Java, you can simply follow the sample:__\n\nSee: [CGENativeLibrary.cgeFilterImageWithCustomFilter](https://github.com/wysaid/android-gpuimage-plus/blob/master/cgeDemo/src/main/java/org/wysaid/cgeDemo/TestCaseActivity.java#L123)\n\n__Or to do with a [CGEImageHandler](https://github.com/wysaid/android-gpuimage-plus/blob/master/library/src/main/java/org/wysaid/nativePort/CGEImageHandler.java#L93)__\n\n### 3. Filter Rule String ###\n\nDoc: <https://github.com/wysaid/android-gpuimage-plus/wiki>\n\nEn: [https://github.com/wysaid/android-gpuimage-plus/wiki/Parsing-String-Rule-(EN)](https://github.com/wysaid/android-gpuimage-plus/wiki/Parsing-String-Rule-(EN) \"http://wysaid.org\")\n\nCh: [https://github.com/wysaid/android-gpuimage-plus/wiki/Parsing-String-Rule-(ZH)](https://github.com/wysaid/android-gpuimage-plus/wiki/Parsing-String-Rule-(ZH) \"http://wysaid.org\")\n\n## Tool\n\nSome utils are available for creating filters: [https://github.com/wysaid/cge-tools](https://github.com/wysaid/cge-tools \"http://wysaid.org\")\n\n[![Tool](https://raw.githubusercontent.com/wysaid/cge-tools/master/screenshots/0.jpg \"cge-tool\")](https://github.com/wysaid/cge-tools)\n\n## License\n\n[MIT License](https://github.com/wysaid/android-gpuimage-plus/blob/master/LICENSE)\n\n## Donate\n\nAlipay:\n\n![Alipay](https://raw.githubusercontent.com/wysaid/android-gpuimage-plus/master/screenshots/alipay.jpg \"alipay\")\n\nPaypal:\n\n[![Paypal](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif \"Paypal\")](http://blog.wysaid.org/p/donate.html)\n"
 },
 {
  "repo": "mgerdes/Open-Golf",
  "language": "C",
  "readme_contents": "# Open Golf\nA cross-platform minigolf game written in C. \n\n## Platforms\n- HTML: https://mgerdes.github.io/minigolf.html (Works best in Chrome)\n- iOS: https://apps.apple.com/us/app/open-golf/id1615224465\n- Android: https://play.google.com/store/apps/details?id=me.mgerdes.open_golf\n- Windows\n- Linux\n\n## Info\n![Image](https://i.imgur.com/TBlXedl.gif)\n- Used the [Sokol](https://github.com/floooh/sokol) libraries to create a cross platform application with 3D graphics and audio.\n- Wrote the Physics code to handle collision detection and collision response for the golf ball.\n- Used [ImGui](https://github.com/ocornut/imgui) to create in games tools for fast iteration. Also created an in game-editor that can be used to modify the terrain of a hole and then quickly play to get fast feedback. The game-editor can also run scripts to generate the points and faces of more interesting models.\n![Image](https://i.imgur.com/fCoKT2e.gif)\n- Used the library [Lightmapper](https://github.com/ands/lightmapper) to generate lightmaps for the terrain and also [xatlas](https://github.com/jpcy/xatlas) to generate lightmap UVs. These lightmaps are then baked into the files for the courses. It can also interpolate between multiple samples to create lightmaps for some moving objects.\n![Image](https://i.imgur.com/ADw5kCw.gif)\n![Image](https://i.imgur.com/tUJyHRk.gif)\n\n## Building\n### Windows\n- To compile run `build\\build-win64.bat`\n\n- To start the game run `out\\win64\\golf.exe`\n\n- This also creates `out\\win64\\golf.sln` which can be opened in Visual Studio to compile / run everything\n\n### Linux\n- To compile run `./build/build-linux.sh`\n\n- To start the game run `out/linux/golf`\n\n## OSX\n- To compile run `./build/build-osx.sh`\n\n- To start the game run `out/osx/golf`\n\n## 3rd Party Libraries\n- [cembed](https://github.com/rxi/cembed)\n- [cimgui](https://github.com/cimgui/cimgui)\n- [fast_obj](https://github.com/thisistherk/fast_obj)\n- [glfw](https://github.com/glfw/glfw)\n- [glslcc](https://github.com/septag/glslcc)\n- [imgui](https://github.com/ocornut/imgui)\n- [Kenney Art Assets](https://kenney.nl/assets)\n- [lightmapper](https://github.com/ands/lightmapper)\n- [mattiasgustavsson/libs](https://github.com/mattiasgustavsson/libs)\n- [miniz](https://github.com/richgel999/miniz)\n- [parson](https://github.com/kgabis/parson)\n- [sokol](https://github.com/floooh/sokol)\n- [stb](https://github.com/nothings/stb)\n- [xatlas](https://github.com/jpcy/xatlas)\n"
 },
 {
  "repo": "haampie/libtree",
  "language": "C",
  "readme_contents": "# libtree\n\nA tool that:\n- :deciduous_tree: turns `ldd` into a tree\n- :point_up: explains how shared libraries are found or why they cannot be located\n\n![Screenshot of libtree](doc/screenshot.png)\n\n\n## Output\n\nBy default, certain standard dependencies are not shown. For more verbose output use\n\n-  `libtree -v`             Show libraries skipped by default\n-  `libtree -vv`            Show dependencies of libraries skipped by default\n-  `libtree -vvv`           Show dependencies of already encountered libraries\n\nUse the `--path` or `-p` flags to show paths rather than sonames:\n\n- `libtree -p $(which tar)`\n\nUse `--max-depth` to limit the recursion depth.\n\n\n## Install\n\n- [Prebuilt binaries for **v3.1.1**](https://github.com/haampie/libtree/releases/tag/v3.1.1)\n  | arch    | sha256sum |\n  |---------|-----------|\n  | [aarch64 (linux)](https://github.com/haampie/libtree/releases/download/v3.1.1/libtree_aarch64) | `c5d4fbcd4e3fb46f02c028532f60fcf1c92f7c6aad5b07a991c67550c2554862` |\n  | [armv6l (linux)](https://github.com/haampie/libtree/releases/download/v3.1.1/libtree_armv6l) | `16f5a7503a095bd88ebc5e21ec4ba8337c5d9712cac355bf89399c9e6beef661` |\n  | [armv7l (linux)](https://github.com/haampie/libtree/releases/download/v3.1.1/libtree_armv7l) | `17f493621e7cc651e2bddef207c1554a64a114e1c907dbe5b79ff0e97180b29e` |\n  | [i686 (linux)](https://github.com/haampie/libtree/releases/download/v3.1.1/libtree_i686) | `230a163c20f4a88a983d8647a9aa793317be6556e2c6a79e8a6295389e651ef5` |\n  | [x86_64 (linux)](https://github.com/haampie/libtree/releases/download/v3.1.1/libtree_x86_64) | `49218482f89648972ea4ef38cf986e85268efd1ce8f27fe14b23124bca009e6f` |\n- Fedora / RHEL / CentOS\n  ```console\n  $ dnf install epel-release # For RHEL and derivatives enable EPEL first \n  $ dnf install libtree-ldd\n  ```\n- Ubuntu 22.04+\n  ```console\n  apt-get install libtree\n  ```\n\n- [GNU Guix](https://guix.gnu.org/)\n  ```console\n  guix install libtree\n  ```\n\n- [Older release **v2.0.0**](https://github.com/haampie/libtree/releases/tag/v2.0.0)\n\n\n## Building from sources\n\n`libtree` requires a C compiler that understands c99\n\n```\ngit clone https://github.com/haampie/libtree.git\ncd libtree\nmake # recommended: LDFLAGS=-static\n```\n\n<details>\n<summary>Or use the following unsafe quick install instructions</summary>\n\n```\ncurl -Lfs https://raw.githubusercontent.com/haampie/libtree/master/libtree.c | ${CC:-cc} -o libtree -x c - -std=c99 -D_FILE_OFFSET_BITS=64\n```\n</details>\n"
 },
 {
  "repo": "armink/EasyFlash",
  "language": "C",
  "readme_contents": "# EasyFlash \n\n[![GitHub release](https://img.shields.io/github/release/armink/EasyFlash.svg)](https://github.com/armink/EasyFlash/releases/latest) [![GitHub commits](https://img.shields.io/github/commits-since/armink/EasyFlash/4.1.0.svg)](https://github.com/armink/EasyFlash/compare/4.1.0...master) [![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/armink/EasyFlash/master/LICENSE)\n\n## 1\u3001\u4ecb\u7ecd\uff08[English](#1-introduction)\uff09\n\n> **\u63d0\u793a** \uff1a\u4ece EasyFlash V4.1 \u540e\uff0c\u57fa\u4e8e EasyFlash \u5168\u65b0\u8bbe\u8ba1\u5f00\u53d1\u7684 [FlashDB](https://github.com/armink/FlashDB) \u5f00\u6e90\u9879\u76ee\u6b63\u5f0f\u4e0a\u7ebf\uff0c\u65b0\u96c6\u6210\u4e86\u65f6\u5e8f\u6570\u636e\u5e93\u3001\u591a\u5206\u533a\u7ba1\u7406\uff0c\u591a\u6570\u636e\u5e93\u5b9e\u4f8b\u7b49\u529f\u80fd\uff0c\u4e5f\u4ece\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u6b22\u8fce\u5173\u6ce8\uff1ahttps://github.com/armink/FlashDB  \u3002\u540c\u65f6\uff0c\u73b0\u6709\u7684 EasyFlash \u4e5f\u4f1a\u7ee7\u7eed\u7ef4\u62a4\u3002\n\n[EasyFlash](https://github.com/armink/EasyFlash)\u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u8f7b\u91cf\u7ea7\u5d4c\u5165\u5f0fFlash\u5b58\u50a8\u5668\u5e93\uff0c\u65b9\u4fbf\u5f00\u53d1\u8005\u66f4\u52a0\u8f7b\u677e\u7684\u5b9e\u73b0\u57fa\u4e8eFlash\u5b58\u50a8\u5668\u7684\u5e38\u89c1\u5e94\u7528\u5f00\u53d1\u3002\u975e\u5e38\u9002\u5408\u667a\u80fd\u5bb6\u5c45\u3001\u53ef\u7a7f\u6234\u3001\u5de5\u63a7\u3001\u533b\u7597\u3001\u7269\u8054\u7f51\u7b49\u9700\u8981\u65ad\u7535\u5b58\u50a8\u529f\u80fd\u7684\u4ea7\u54c1\uff0c\u8d44\u6e90\u5360\u7528\u6781\u4f4e\uff0c\u652f\u6301\u5404\u79cd MCU \u7247\u4e0a\u5b58\u50a8\u5668\u3002\u8be5\u5e93\u4e3b\u8981\u5305\u62ec **\u4e09\u5927\u5b9e\u7528\u529f\u80fd** \uff1a\n\n- **ENV** \u5feb\u901f\u4fdd\u5b58\u4ea7\u54c1\u53c2\u6570\uff0c\u652f\u6301 **\u5199\u5e73\u8861\uff08\u78e8\u635f\u5e73\u8861\uff09** \u53ca **\u6389\u7535\u4fdd\u62a4** \u529f\u80fd\n\nEasyFlash\u4e0d\u4ec5\u80fd\u591f\u5b9e\u73b0\u5bf9\u4ea7\u54c1\u7684 **\u8bbe\u5b9a\u53c2\u6570** \u6216 **\u8fd0\u884c\u65e5\u5fd7** \u7b49\u4fe1\u606f\u7684\u6389\u7535\u4fdd\u5b58\u529f\u80fd\uff0c\u8fd8\u5c01\u88c5\u4e86\u7b80\u6d01\u7684 **\u589e\u52a0\u3001\u5220\u9664\u3001\u4fee\u6539\u53ca\u67e5\u8be2** \u65b9\u6cd5\uff0c \u964d\u4f4e\u4e86\u5f00\u53d1\u8005\u5bf9\u4ea7\u54c1\u53c2\u6570\u7684\u5904\u7406\u96be\u5ea6\uff0c\u4e5f\u4fdd\u8bc1\u4e86\u4ea7\u54c1\u5728\u540e\u671f\u5347\u7ea7\u65f6\u62e5\u6709\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002\u8ba9Flash\u53d8\u4e3aNoSQL\uff08\u975e\u5173\u7cfb\u578b\u6570\u636e\u5e93\uff09\u6a21\u578b\u7684\u5c0f\u578b\u952e\u503c\uff08Key-Value\uff09\u5b58\u50a8\u6570\u636e\u5e93\u3002\n\n- **IAP** \u5728\u7ebf\u5347\u7ea7\u518d\u4e5f\u4e0d\u662f\u96be\u4e8b\u513f\n\n\u8be5\u5e93\u5c01\u88c5\u4e86IAP(In-Application Programming)\u529f\u80fd\u5e38\u7528\u7684\u63a5\u53e3\uff0c\u652f\u6301CRC32\u6821\u9a8c\uff0c\u540c\u65f6\u652f\u6301Bootloader\u53caApplication\u7684\u5347\u7ea7\u3002\n\n- **Log** \u65e0\u9700\u6587\u4ef6\u7cfb\u7edf\uff0c\u65e5\u5fd7\u53ef\u76f4\u63a5\u5b58\u50a8\u5728Flash\u4e0a\n\n\u975e\u5e38\u9002\u5408\u5e94\u7528\u5728\u5c0f\u578b\u7684\u4e0d\u5e26\u6587\u4ef6\u7cfb\u7edf\u7684\u4ea7\u54c1\u4e2d\uff0c\u65b9\u4fbf\u5f00\u53d1\u4eba\u5458\u5feb\u901f\u5b9a\u4f4d\u3001\u67e5\u627e\u7cfb\u7edf\u53d1\u751f\u5d29\u6e83\u6216\u6b7b\u673a\u7684\u539f\u56e0\u3002\u540c\u65f6\u914d\u5408[EasyLogger](https://github.com/armink/EasyLogger)(\u6211\u5f00\u6e90\u7684\u8d85\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6027\u80fdC\u65e5\u5fd7\u5e93\uff0c\u5b83\u63d0\u4f9b\u4e0eEasyFlash\u7684\u65e0\u7f1d\u63a5\u53e3)\u4e00\u8d77\u4f7f\u7528\uff0c\u8f7b\u677e\u5b9e\u73b0C\u65e5\u5fd7\u7684Flash\u5b58\u50a8\u529f\u80fd\u3002\n\n### 1.1 \u4e24\u79cd ENV \u6a21\u5f0f\n\n\u76ee\u524d ENV \u529f\u80fd\u6709\u4e24\u79cd\u4e3b\u8981\u6a21\u5f0f\uff0c\u4e00\u79cd\u4e3a V4.0 \u5e26\u6765\u7684 **NG** \u6a21\u5f0f\uff0c\u8fd8\u6709\u4e00\u79cd\u4e3a\u5ef6\u7eed V3.0 \u7684 **legacy** \u6a21\u5f0f\n\n#### 1.1.1\u3001V4.0 \u5f15\u5165\u7684 NG \u6a21\u5f0f\n\n>  \u5bf9\u5e94\u6e90\u7801\u6587\u4ef6\u4e3a\uff1a `ef_env.c`\n\n\u81ea 2019 \u5e74\u6625\u8282\u540e\uff0cEasyFlash \u7ecf\u8fc7 4 \u5e74\u591a\u7684\u8fed\u4ee3\uff0c\u7ed3\u5408\u4f17\u591a\u5f00\u53d1\u8005\u7684\u9700\u6c42\u53ca\u5efa\u8bae\uff0c\u7ec8\u4e8e\u53d1\u5e03\u4e86 V4.0 \u7248\u672c\uff0c\u8be5\u7248\u672c\u4e2d\u7684 ENV \u529f\u80fd\u88ab\u547d\u540d\u4e3a **NG** (Next Generation) \u6a21\u5f0f\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b8c\u5168\u91cd\u6784\u7684\u65b0\u7248\u672c\uff0c\u5177\u6709\u4ee5\u4e0b\u65b0\u7279\u6027\uff1a\n\n- \u66f4\u5c0f\u7684\u8d44\u6e90\u5360\u7528\uff0c\u5185\u5b58\u5360\u7528 **\u51e0\u4e4e\u4e3a 0** \uff1b\uff08V4.0 \u4ee5\u524d\u7248\u672c\u4f1a\u4f7f\u7528\u989d\u5916\u7684 RAM \u7a7a\u95f4\u8fdb\u884c\u7f13\u5b58\uff09\n- ENV \u7684\u503c\u7c7b\u578b\u652f\u6301 **\u4efb\u610f\u7c7b\u578b** \u3001\u4efb\u610f\u957f\u5ea6\uff0c\u76f8\u5f53\u4e8e\u76f4\u63a5 memcpy \u53d8\u91cf\u81f3 flash \uff1b\uff08V4.0 \u4e4b\u524d\u53ea\u652f\u6301\u5b58\u50a8\u5b57\u7b26\u4e32\uff09\n- ENV \u64cd\u4f5c\u6548\u7387\u6bd4\u4ee5\u524d\u7684\u6a21\u5f0f\u9ad8\uff0c\u5145\u5206\u5229\u7528\u5269\u4f59\u7a7a\u95f2\u533a\u57df\uff0c\u64e6\u9664\u6b21\u6570\u53ca\u64cd\u4f5c\u65f6\u95f4\u663e\u8457\u964d\u4f4e\uff1b\n- **\u539f\u751f\u652f\u6301** \u78e8\u635f\u5e73\u8861\u3001\u6389\u7535\u4fdd\u62a4\u529f\u80fd \uff08V4.0 \u4e4b\u524d\u9700\u8981\u5360\u7528\u989d\u5916\u7684 Flash \u6247\u533a\uff09\uff1b\n- ENV \u652f\u6301 **\u589e\u91cf\u5347\u7ea7** \uff0c\u56fa\u4ef6\u5347\u7ea7\u540e ENV \u4e5f\u652f\u6301\u5347\u7ea7\uff1b\n- \u652f\u6301\u5927\u6570\u636e\u5b58\u50a8\u6a21\u5f0f\uff0c**\u957f\u5ea6\u65e0\u9650\u5236**\uff0c\u6570\u636e\u53ef\u5728\u591a\u4e2a Flash \u6247\u533a\u4e0a\u987a\u5e8f\u5b58\u50a8\u3002\u50cf\u811a\u672c\u7a0b\u5e8f\u3001\u97f3\u9891\u7b49\u5360\u7528 Flash \u8d85\u8fc7 1 \u4e2a\u6247\u533a\u7684\u8d44\u6e90\u4e5f\u90fd\u53ef\u4ee5\u5b58\u5165 ENV\uff08\u5373\u5c06\u5728 V4.2 \u652f\u6301\uff09\uff1b\n- \u652f\u6301 **\u6570\u636e\u52a0\u5bc6** \uff0c\u63d0\u5347\u5b58\u50a8\u7684\u5b89\u5168\u6027\uff0c\u7269\u8054\u7f51\u65f6\u4ee3\u7684\u5fc5\u5907\u529f\u80fd\uff08\u5373\u5c06\u5728 V4.3 \u652f\u6301\uff09\uff1b\n- \u652f\u6301 **\u6570\u636e\u538b\u7f29** \uff0c\u51cf\u4f4e Flash \u5360\u7528\uff08\u5373\u5c06\u5728 V4.4 \u652f\u6301\uff09\uff1b\n\nV4.0 \u8bbe\u8ba1\u53ca\u5185\u90e8\u539f\u7406\uff0cV4.0 \u8fc1\u79fb\u6307\u5357\u7b49\u66f4\u591a\u5185\u5bb9\u8bf7\u7ee7\u7eed\u9605\u8bfb\u4e0b\u9762\u7684 [\u6587\u6863\u7ae0\u8282](#3\u6587\u6863) \n\n> **\u6ce8\u610f** \uff1a\u4e2a\u522b Flash \u5b58\u5728\u65e0\u6cd5\u9006\u5e8f\u5199\u5165\u7684\u95ee\u9898\uff0c\u4f8b\u5982 STM32L4 \u7247\u5185 Flash\uff0c\u6240\u4ee5\u65e0\u6cd5\u4f7f\u7528 NG \u6a21\u5f0f\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5efa\u8bae\u4f7f\u7528 V3.0 \u7684 legacy \u6a21\u5f0f\n\n#### 1.1.2\u3001\u5ef6\u7eed V3.0 \u7684 legacy \u6a21\u5f0f\n\n> \u5bf9\u5e94\u6e90\u7801\u6587\u4ef6\u4e3a\uff1a `ef_env_legacy.c` \u53ca `ef_env_legacy_wl.c`\n\n**legacy** \u6a21\u5f0f\u4e5f\u5177\u6709\u78e8\u635f\u5e73\u8861\u53ca\u6389\u7535\u4fdd\u62a4\u529f\u80fd\uff0c\u76f8\u6bd4\u4e8e V 4.0 NG \u6a21\u5f0f\uff0c\u4f7f\u7528 legacy \u6a21\u5f0f\uff0c\u9700\u8981\u6709\u989d\u5916\u7684 RAM \u7a7a\u95f4\u6765\u4e34\u65f6\u7f13\u5b58\u6bcf\u4e2a ENV \uff0c\u6700\u7ec8\u8c03\u7528 save \u63a5\u53e3\uff0c\u7edf\u4e00\u64e6\u9664\u6247\u533a\u518d\u5b58\u50a8\u5230 Flash \u4e0a\u3002\n\n#### 1.1.3 ENV \u6a21\u5f0f\u5bf9\u6bd4\n\n|                      | V4.0 NG \u6a21\u5f0f                               | V3.0 legacy \u6a21\u5f0f         |\n| -------------------- | ------------------------------------------ | ------------------------ |\n| RAM \u8d44\u6e90\u5360\u7528         | \u4f4e                                         | \u9ad8                       |\n| \u652f\u6301 Flash \u5168\u9762\u6027    | \u4e2a\u522b Flash \u53d7\u9650\uff1a\u4f8b\u5982 STM32L4 \u7247\u5185         | \u6bd4\u8f83\u5168\u9762                 |\n| \u662f\u5426\u9700\u8981 GC \u5783\u573e\u56de\u6536 | \u9700\u8981 GC \uff0c\u8fd9\u4f1a\u5bfc\u81f4\u89e6\u53d1 GC \u65f6\uff0c\u5199\u5165\u901f\u5ea6\u53d8\u6162 | \u4e0d\u9700\u8981                   |\n| value \u7c7b\u578b\u9650\u5236       | \u65e0\u9650\u5236                                     | \u5bf9\u5b57\u7b26\u4e32\u7c7b\u578b\u652f\u6301\u7684\u6bd4\u8f83\u597d |\n| \u6389\u7535\u4fdd\u62a4             | \u652f\u6301                                       | \u652f\u6301                     |\n| \u78e8\u635f\u5e73\u8861             | \u652f\u6301                                       | \u652f\u6301                     |\n| \u589e\u91cf\u5347\u7ea7             | \u652f\u6301                                       | \u652f\u6301                     |\n\n### 1.2\u3001\u8d44\u6e90\u5360\u7528\n\n```\n\u6700\u4f4e\u8981\u6c42\uff1a ROM: 6K bytes     RAM: 0.1K bytes\n```\n\n### 1.3\u3001\u652f\u6301\u5e73\u53f0\n\n\u76ee\u524d\u5df2\u79fb\u690d\u786c\u4ef6\u5e73\u53f0\u6709 `stm32f10x`\u4e0e `stm32f4xx` \u7cfb\u5217\u7684\u7247\u5185Flash\uff0c\u7247\u5916\u7684 SPI Flash\uff08\u57fa\u4e8e [SFUD](https://github.com/armink/SFUD)\uff09\uff0c\u8fd9\u4e9b\u4e5f\u662f\u7b14\u8005\u4ea7\u54c1\u4f7f\u7528\u7684\u5e73\u53f0\u3002\u5176\u4f59\u5e73\u53f0\u7684\u79fb\u690d\u96be\u5ea6\u4e0d\u5927\uff0c\u5728\u9879\u76ee\u7684\u8bbe\u8ba1\u4e4b\u521d\u5c31\u6709\u8003\u8651\u9488\u5bf9\u6240\u6709\u5e73\u53f0\u7684\u9002\u914d\u6027\u95ee\u9898\uff0864\u4f4d\u9664\u5916\uff09\uff0c\u6240\u4ee5\u5bf9\u6240\u6709\u79fb\u690d\u63a5\u53e3\u90fd\u6709\u505a\u9884\u7559\u3002\u79fb\u690d\u53ea\u9700\u4fee\u6539 [`\\easyflash\\port\\ef_port.c`](https://github.com/armink/EasyFlash/blob/master/easyflash/port/ef_port.c) \u4e00\u4e2a\u6587\u4ef6\uff0c\u5b9e\u73b0\u91cc\u9762\u7684\u64e6\u3001\u5199\u3001\u8bfb\u53ca\u6253\u5370\u529f\u80fd\u5373\u53ef\u3002\n\n\u6b22\u8fce\u5927\u5bb6 **fork and pull request**([Github](https://github.com/armink/EasyFlash)|[OSChina](http://git.oschina.net/armink/EasyFlash)|[Coding](https://coding.net/u/armink/p/EasyFlash/git)) \u3002\u5982\u679c\u89c9\u5f97\u8fd9\u4e2a\u5f00\u6e90\u9879\u76ee\u5f88\u8d5e\uff0c\u53ef\u4ee5\u70b9\u51fb[\u9879\u76ee\u4e3b\u9875](https://github.com/armink/EasyFlash) \u53f3\u4e0a\u89d2\u7684 **Star**\uff0c\u540c\u65f6\u628a\u5b83\u63a8\u8350\u7ed9\u66f4\u591a\u6709\u9700\u8981\u7684\u670b\u53cb\u3002\n\n## 2\u3001\u6d41\u7a0b\n\n### 2.1\u3001ENV\uff1a\u73af\u5883\u53d8\u91cf\uff08KV\u6570\u636e\u5e93\uff09\n\n\u4e0b\u56fe\u4e3a\u901a\u8fc7\u63a7\u5236\u53f0\uff08\u7ec8\u7aef\uff09\u6765\u8c03\u7528\u73af\u5883\u53d8\u91cf\u7684\u5e38\u7528\u63a5\u53e3\uff0c\u6f14\u793a\u4e86\u4ee5\u4e0b\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u63a5\u53e3\u90fd\u652f\u6301\u88ab\u5e94\u7528\u5c42\u76f4\u63a5\u8c03\u7528\u3002\n\n- 1\u3001\u521b\u5efa\u201c\u6e29\u5ea6\u201d\u7684\u73af\u5883\u53d8\u91cf\uff0c\u540d\u4e3a `temp`\uff0c\u5e76\u4e14\u8d4b\u503c\u4e3a `123`\uff1b\n- 2\u3001\u4fdd\u5b58\u201c\u6e29\u5ea6\u201d\u5230Flash\u4e2d\u5e76\u91cd\u542f\uff08V4.0 \u7248\u672c\u7684\u6bcf\u4e2a\u64cd\u4f5c\u5b8c\u81ea\u52a8\u4fdd\u5b58\uff0c\u65e0\u9700\u989d\u5916\u4fdd\u5b58\uff09\uff1b\n- 3\u3001\u68c0\u67e5\u201c\u6e29\u5ea6\u201d\u662f\u5426\u88ab\u6210\u529f\u4fdd\u5b58\uff1b\n- 4\u3001\u4fee\u6539\u201c\u6e29\u5ea6\u201d\u503c\u4e3a `456` \u5e76\u4fdd\u5b58\u3001\u91cd\u542f\uff1b\n- 5\u3001\u68c0\u67e5\u201c\u6e29\u5ea6\u201d\u662f\u5426\u88ab\u6210\u529f\u4fee\u6539\uff1b\n- 6\u3001\u5220\u9664\u201c\u6e29\u5ea6\u201d\u7684\u73af\u5883\u53d8\u91cf\u3002\n\n![easy_flash_env](/docs/zh/images/EnvDemo.gif)\n\n### 2.2\u3001IAP\uff1a\u5728\u7ebf\u5347\u7ea7\n\n\u4e0b\u56fe\u6f14\u793a\u4e86\u901a\u8fc7\u63a7\u5236\u53f0\u6765\u8fdb\u884cIAP\u5347\u7ea7\u8f6f\u4ef6\u7684\u8fc7\u7a0b\uff0c\u4f7f\u7528\u7684\u662f\u5e93\u4e2d\u81ea\u5e26\u7684IAP\u529f\u80fd\u63a5\u53e3\uff0c\u6f14\u793a\u91c7\u7528\u7684\u662f\u4e32\u53e3+Ymodem\u534f\u8bae\u7684\u65b9\u5f0f\u3002\u4f60\u8fd8\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u901a\u8fc7CAN\u3001485\u3001\u4ee5\u592a\u7f51\u7b49\u603b\u7ebf\uff0c\u6765\u5b9e\u73b0\u8fdc\u7a0b\u7f51\u7edc\u66f4\u65b0\u3002\n\n![easy_flash_iap](/docs/zh/images/IapDemo.gif)\n\n### 2.3\u3001Log\uff1a\u65e5\u5fd7\u5b58\u50a8\n\n\u4e0b\u56fe\u8fc7\u7a0b\u4e3a\u901a\u8fc7\u63a7\u5236\u53f0\u8f93\u51fa\u65e5\u5fd7\uff0c\u5e76\u5c06\u8f93\u51fa\u7684\u65e5\u5fd7\u5b58\u50a8\u5230Flash\u4e2d\u3002\u91cd\u542f\u518d\u8bfb\u53d6\u4e0a\u6b21\u4fdd\u5b58\u7684\u65e5\u5fd7\uff0c\u6700\u540e\u6e05\u7a7aFlash\u65e5\u5fd7\u3002\n\n![easy_flash_log](/docs/zh/images/LogDemo.gif)\n\n## 3\u3001\u6587\u6863\n\n- API \u6587\u6863\uff1a[`\\docs\\zh\\api.md`](docs/zh/api.md)\n- \u79fb\u690d\u6587\u6863\uff1a[`\\docs\\zh\\port.md`](docs/zh/port.md)\n- V4.0 \u8fc1\u79fb\u6307\u5357\uff1a[`\\docs\\zh\\v4_migrate.md`](docs/zh/v4_migrate.md)\n- V4.0 ENV \u529f\u80fd\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\uff1a[`\\docs\\zh\\design.md`](docs/zh/design.md)\n\n\u52a1\u5fc5\u4fdd\u8bc1\u5728 **\u9605\u8bfb\u6587\u6863** \u540e\u518d\u79fb\u690d\u4f7f\u7528\u3002\n\n## 4\u3001\u652f\u6301\n\n ![support](/docs/zh/images/wechat_support.png)\n\n\u5982\u679c EasyFlash \u89e3\u51b3\u4e86\u4f60\u7684\u95ee\u9898\uff0c\u4e0d\u59a8\u626b\u63cf\u4e0a\u9762\u4e8c\u7ef4\u7801\u8bf7\u6211 **\u559d\u676f\u5496\u5561**~ \n\n## 5\u3001\u8bb8\u53ef\n\n\u91c7\u7528 MIT \u5f00\u6e90\u534f\u8bae\uff0c\u7ec6\u8282\u8bf7\u9605\u8bfb\u9879\u76ee\u4e2d\u7684 LICENSE \u6587\u4ef6\u5185\u5bb9\u3002\n\n---\n\n## 1 Introduction\n\n[EasyFlash](https://github.com/armink/EasyFlash) is an open source lightweight embedded flash memory library. It provides convenient application interface for MCU (Micro Control Unit). The developers can achieve more efficient and common application development based on Flash memory. The library currently provides **three useful features** \uff1a\n\n- **Env(environment variables)** Fast Saves product parameters. Support **write balance mode(wear leveling)** and **power fail safeguard**. \n\nEasyFlash can store **setting parameters** or **running logs** and other information which you want to keep after power down. It contains add, delete, modify and query methods. It helps developer to process the product parameters, and makes sure the product has better scalability after upgrade. Turns the Flash into a small NoSQL (non-relational databases) model and Key-Value stores database.\n\n- **IAP** : online upgrade is no longer a difficult thing.\n\nThe library encapsulates the IAP (In-Application Programming) feature common interface. Support CRC32 checksum. While supporting the bootloader and application upgrade.\n\n- **Log** : The logs can store to product's flash which has no file-system.\n\nIt's very suitable for small without a file system products. The developer can easy to locate and query problem when system crashes or freezes. You can use [EasyLogger](https://github.com/armink/EasyLogger)( A super-lightweight, high-performance C log library which open source by me. It provides a seamless interface with EasyFlash) at the same time. So, it's so easy to store the logs to flash.\n\n### 1.1 Resource consumption\n\n```\nMinimum : ROM: 6K bytes     RAM: 0.2K bytes\n```\n\n### 1.2 Supported platforms\n\nHardware platform has been ported SPI Flash, `stm32f10x` and `stm32f4xx` series of on-chip Flash. These are my product platforms. Remaining platform porting difficulty is little. To port it just modify [`\\easyflash\\port\\ef_port.c`](https://github.com/armink/EasyFlash/blob/master/easyflash/port/ef_port.c) file. Implement erase, write, read, print feature.\n\nWelcome everyone to **fork and pull request**([Github](https://github.com/armink/EasyFlash)|[OSChina](http://git.oschina.net/armink/EasyFlash)|[Coding](https://coding.net/u/armink/p/EasyFlash/git)). If you think this open source project is awesome. You can press the **Star** on the top right corner of [project home page](https://github.com/armink/EasyFlash), and recommend it to more friends.\n\n## 2 Flow\n\n### 2.1 Env(KV database)\n\nThe figure below shows an ENV's common interface be called by the console (terminal). These interfaces are supported by the application layer called.\n\n- 1.Create temperature environment variable. It's name is `temp` and value is `123`;\n- 2.Save temperature to flash and reboot;\n- 3.Check the temperature has saved successfully;\n- 4.Change the temperature value to `456` and save, reboot;\n- 5.Check the temperature has changed successfully;\n- 6.Delete temperature environment variable.\n\n![easy_flash_env](/docs/en/images/EnvDemo.gif)\n\n### 2.2 IAP\n\nThe figure below shows the process of upgrading software through the console by IAP. It use this library comes with IAP function interface. It uses a serial port + Ymodem protocol mode. You can also be achieved through CAN, 485, Ethernet bus to online upgrade.\n\n![easy_flash_iap](/docs/en/images/IapDemo.gif)\n\n### 2.3 Log\n\nThe following figure is the output of the log process through the console. The logs are saved to flash at real time. Then the board is rebooted and the logs back are read back from flash. At last logs will be erased.\n\n![easy_flash_log](/docs/en/images/LogDemo.gif)\n\n## 3 Documents\n\nAll documents are in the [`\\docs\\en\\`](https://github.com/armink/EasyFlash/tree/master/docs/en) folder. Please **read the documents** before porting it and using it.\n\n## 4 License\n\nUsing MIT open source license, please read the project LICENSE file."
 },
 {
  "repo": "cloudwu/pbc",
  "language": "C",
  "readme_contents": "## PBC\n\n[![travis-ci status](https://travis-ci.org/cloudwu/pbc.svg?branch=master)](https://travis-ci.org/cloudwu/pbc)\n\nPBC is a google protocol buffers library for C without code generation.\n\n## Quick Example\n\n    package tutorial;\n    \n    message Person {\n      required string name = 1;\n      required int32 id = 2;        // Unique ID number for this person.\n      optional string email = 3;\n    \n      enum PhoneType {\n        MOBILE = 0;\n        HOME = 1;\n        WORK = 2;\n      }\n    \n      message PhoneNumber {\n        required string number = 1;\n        optional PhoneType type = 2 [default = HOME];\n      }\n    \n      repeated PhoneNumber phone = 4;\n    }\n\n```C\nstruct pbc_rmessage * m = pbc_rmessage_new(env, \"tutorial.Person\", slice);\nprintf(\"name = %s\\n\", pbc_rmessage_string(m , \"name\" , 0 , NULL));\nprintf(\"id = %d\\n\", pbc_rmessage_integer(m , \"id\" , 0 , NULL));\nprintf(\"email = %s\\n\", pbc_rmessage_string(m , \"email\" , 0 , NULL));\n\nint phone_n = pbc_rmessage_size(m, \"phone\");\nint i;\n\nfor (i=0;i<phone_n;i++) {\n\tstruct pbc_rmessage * p = pbc_rmessage_message(m , \"phone\", i);\n\tprintf(\"\\tnumber[%d] = %s\\n\",i,pbc_rmessage_string(p , \"number\", i ,NULL));\n\tprintf(\"\\ttype[%d] = %s\\n\",i,pbc_rmessage_string(p, \"type\", i, NULL));\n}\n\npbc_rmessage_delete(m);\n```\n\n## Message API\n\nYou can use *wmessage* for encoding , and *rmessage* for decoding.\n\nSee test/addressbook.c for details.\n\n## Pattern API\n\nIf you need better performance , you can use pbc_pattern_xxx api .\n\nSee test/pattern.c for details.\n\nPattern api is faster and less memory used because it can access data in native C struct.\n\n## Extension\n\nPBC support extension in a very simple way . PBC add a specific prefix to every extension field name. \n\n## Service\n\nNot supported\n\n## Enum\n\nWith message API , you can use both string and integer as enum type . They must be integer in Pattern API. \n\n## Lua bindings\n\ncd binding/lua && make\nor\ncd binding/lua53 && make\n\nSee https://github.com/cloudwu/pbc/tree/master/binding/lua/README.md\n\n## Building pbc - Using vcpkg\n\nYou can download and install pbc using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:\n\n    git clone https://github.com/Microsoft/vcpkg.git\n    cd vcpkg\n    ./bootstrap-vcpkg.sh\n    ./vcpkg integrate install\n    ./vcpkg install pbc\n\nThe pbc port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.\n\n## Question ?\n\n* Send me email : http://www.codingnow.com/2000/gmail.gif\n* My Blog : http://blog.codingnow.com\n* Design : http://blog.codingnow.com/2011/12/protocol_buffers_for_c.html (in Chinese)\n* Build for Visual Studio 2012 : https://github.com/miaodadao/pbc\n\n\n"
 },
 {
  "repo": "hgpvision/darknet",
  "language": "C",
  "readme_contents": "# darknet\ndarknet\u662f\u4e00\u4e2a\u8f83\u4e3a\u8f7b\u578b\u7684\u5b8c\u5168\u57fa\u4e8eC\u4e0eCUDA\u7684\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u4e3b\u8981\u7279\u70b9\u5c31\u662f\u5bb9\u6613\u5b89\u88c5\uff0c\u6ca1\u6709\u4efb\u4f55\u4f9d\u8d56\u9879\uff08OpenCV\u90fd\u53ef\u4ee5\u4e0d\u7528\uff09\uff0c\u79fb\u690d\u6027\u975e\u5e38\u597d\uff0c\u652f\u6301CPU\u4e0eGPU\u4e24\u79cd\u8ba1\u7b97\u65b9\u5f0f\u3002\n\n\u66f4\u591a\u4fe1\u606f\uff08\u5305\u62ec\u5b89\u88c5\u3001\u4f7f\u7528\uff09\u53ef\u4ee5\u53c2\u8003\uff1a[Darknet: Open Source Neural Networks in C](https://pjreddie.com/darknet/)\n\n# \u4e3a\u4ec0\u4e48\u9009\u62e9darknet\uff1f\n\n\u76f8\u6bd4\u4e8eTensorFlow\u6765\u8bf4\uff0cdarknet\u5e76\u6ca1\u6709\u90a3\u4e48\u5f3a\u5927\uff0c\u4f46\u8fd9\u4e5f\u6210\u4e86darknet\u7684\u4f18\u52bf\uff1a\n\n1. darknet\u5b8c\u5168\u7531C\u8bed\u8a00\u5b9e\u73b0\uff0c\u6ca1\u6709\u4efb\u4f55\u4f9d\u8d56\u9879\uff0c\u5f53\u7136\u53ef\u4ee5\u4f7f\u7528OpenCV\uff0c\u4f46\u53ea\u662f\u7528\u5176\u6765\u663e\u793a\u56fe\u7247\u3001\u4e3a\u4e86\u66f4\u597d\u7684\u53ef\u89c6\u5316\uff1b\n\n2. darknet\u652f\u6301CPU\uff08\u6240\u4ee5\u6ca1\u6709GPU\u4e5f\u4e0d\u7528\u7d27\u7684\uff09\u4e0eGPU\uff08CUDA/cuDNN\uff0c\u4f7f\u7528GPU\u5f53\u7136\u66f4\u5757\u66f4\u597d\u4e86\uff09\uff1b\n\n3. \u6b63\u662f\u56e0\u4e3a\u5176\u8f83\u4e3a\u8f7b\u578b\uff0c\u6ca1\u6709\u50cfTensorFlow\u90a3\u822c\u5f3a\u5927\u7684API\uff0c\u6240\u4ee5\u7ed9\u6211\u7684\u611f\u89c9\u5c31\u662f\u6709\u53e6\u4e00\u79cd\u5473\u9053\u7684\u7075\u6d3b\u6027\uff0c\u9002\u5408\u7528\u6765\u7814\u7a76\u5e95\u5c42\uff0c\u53ef\u4ee5\u66f4\u4e3a\u65b9\u4fbf\u7684\u4ece\u5e95\u5c42\u5bf9\u5176\u8fdb\u884c\u6539\u8fdb\u4e0e\u6269\u5c55\uff1b\n\n4. darknet\u7684\u5b9e\u73b0\u4e0ecaffe\u7684\u5b9e\u73b0\u5b58\u5728\u76f8\u4f3c\u7684\u5730\u65b9\uff0c\u719f\u6089\u4e86darknet\uff0c\u76f8\u4fe1\u5bf9\u4e0a\u624bcaffe\u6709\u5e2e\u52a9\uff1b\n\n# \u672c\u9879\u76ee\u76ee\u7684\u4e0e\u72b6\u6001\n\n\u76ee\u7684\u5f88\u7b80\u5355\uff0c\u7814\u7a76darknet\u5e95\u5c42\uff0c\u7aa5\u63a2\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u539f\u7406\u4e0e\u5177\u4f53\u5b9e\u73b0\uff0c\u540c\u65f6\u5de9\u56faC\u8bed\u8a00\u7f16\u7a0b\uff08\u6240\u4ee5\u6ce8\u91ca\u4e2d\u4e0d\u5355\u6709\u5f88\u591a\u7684\u6846\u67b6\u539f\u7406/\u903b\u8f91\u5206\u6790\uff0c\u4e5f\u6709\u5f88\u591a\u8bed\u6cd5\u5206\u6790\uff09\u3002\u76ee\u524d\u53ea\u5b8c\u6210\u90e8\u5206\u4ee3\u7801\uff08\u4e3b\u8981\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u5206\u6790\uff0c\u5176\u6ce8\u91ca\u975e\u5e38\u8be6\u7ec6\uff08\u53ef\u80fd\u5f88\u591a\u4eba\u4f1a\u89c9\u5f97\u7f57\u55e6\u4e86\uff1a)\uff0c\u90a3\u5c31\u5f3a\u5fcd\u7740\u5427\uff5e\uff09\uff0c\u672a\u6765\u4f1a\u4e0d\u5b9a\u671f\u7684\u66f4\u65b0!\uff08\u5fd9ing\uff0c\u4ec0\u4e48\u65f6\u5019\u4f1a\u518d\u66f4\u5462\uff1f\uff09\n\n\u5f88\u5e0c\u671b\u6709\u76f8\u540c\u5174\u8da3\u7684\u4eba\u52a0\u5165\u6211\uff0c\u4e00\u8d77\u7814\u7a76\uff08\u82e5\u6709\u5174\u8da3\uff0c\u6b22\u8fce\u7ed9\u6211\u53d1\u90ae\u4ef6\uff5e\uff5e\uff09\uff01\n\n# \u5c0f\u5c0f\u58f0\u660e\n\n\u6ce8\u91ca\u4e2d\u6709\u4e9b\u5730\u65b9\u63d0\u53ca\u4e86\u53c2\u8003\u4ec0\u4e48\u4ec0\u4e48\u7684\uff0c\u8fd9\u4e9b\u591a\u534a\u662f\u6307\u6211\u6240\u4f5c\u7684\u56fe\u8868+\u6587\u5b57\u7528\u6765\u5e2e\u52a9\u7406\u89e3\u4ee3\u7801\u7684\u7b14\u8bb0\uff0c\u539f\u8c05\u6211\u8fd9\u4e9b\u7b14\u8bb0\u8fd8\u8eba\u5728\u6211\u7684\u7535\u8111\u91cc\uff0c\u5e76\u6ca1\u6709\u4e0a\u4f20\uff0c\u4f46\u4e0d\u7528\u7d27\uff0c\u56e0\u4e3a\u6ce8\u91ca\u771f\u7684\u771f\u7684\u5f88\u8be6\u7ec6\uff0c\u57fa\u672c\u4e0a\u4e0d\u7528\u56fe\u8868\u4e5f\u8bf4\u660e\u6e05\u695a\u4e86\uff5e\uff5e\n\n\u5927\u90e8\u5206\u7684\u4ee3\u7801\u90fd\u662f\u672c\u4eba\u4e2a\u4eba\u5b8c\u6210\u7684\uff08lonely...\uff09\uff0c\u6240\u4ee5\u96be\u514d\u4f1a\u6709\u7406\u89e3\u9519\u8bef\u7684\u5730\u65b9\uff08\u53ef\u80fd\u8fd8\u4e0d\u5c11\uff0c\u5bb3\u6015ing...\uff09\uff0c\u8fd8\u8bf7\u591a\u591a\u5305\u6db5\uff0c\u82e5\u53d1\u73b0\u4e0e\u60a8\u7406\u89e3\u76f8\u5de6\u7684\u5730\u65b9\uff0c\u6b22\u8fce\u53d1\u90ae\u4ef6\u7ed9\u6211\uff5e\uff5e\n\n# \u4e24\u4e09\u70b9\u5c0f\u8bf4\u660e\n\n1. src\u6587\u4ef6\u5939\u4e2d\u51e1\u662f.cu\u6587\u4ef6\uff0c\u90fd\u88ab\u6211\u6539\u4e3a.c\u7ed3\u5c3e\u4e86\uff08\u4e3a\u4e86\u4e00\u70b9\u70b9\u65b9\u4fbf\uff5e\uff09\uff0c\u66ff\u6362\u4e4b\u524d\u7684\u6587\u4ef6\u5168\u88ab\u6211\u653e\u5728\u4e86src/cu\u6587\u4ef6\u5939\u4e2d\uff08\u6ca1\u5565\u7528\uff0c\u53ef\u4ee5\u968f\u4fbf\u5220\u6389\uff5e\uff09\u3002\u5982\u679c\u4f60\u6ca1\u7528gpu\u7684\u8bdd\uff0c\u8fd9\u6ca1\u6709\u4efb\u4f55\u5f71\u54cd\uff0c\u56e0\u4e3a\u6ca1\u6709\u7528gpu\u5c31\u4e0d\u4f1a\u7528\u5230nvcc\u7f16\u8bd1\uff1b\u4f46\u662f\u5982\u679c\u4f60\u7528gpu\u7684\u8bdd\uff0c\u8fd8\u5f97\u9ebb\u70e6\u4f60\u5c06.c\u6539\u56de.cu\uff0c\u4e0d\u7136\u7f16\u8bd1\u4f1a\u51fa\u95ee\u9898\u7684\uff08\u4f60\u53ef\u4ee5\u770b\u4e00\u4e0bMakefile\u6587\u4ef6\uff0c.cu\u6587\u4ef6\u8981\u7528nvcc\u7f16\u8bd1\u7684\uff0c\u8981\u6539\u4e3a.c\u90a3\u5c31\u901a\u901a\u7528gcc\u7f16\u8bd1\u4e86\uff5e\uff5e\uff09\n\n2. \u5982\u679c\u4f60\u4e5f\u613f\u610f\u89e3\u6790\u4ee3\u7801\uff0c\u4e3a\u5176\u5199\u6ce8\u91ca\uff0c\u4e5f\u53ef\u4ee5pull requests\u7ed9\u6211\uff0c\u6211\u6765merge\uff08\u6ce8\u91ca\u98ce\u683c\u5982\u679c\u80fd\u591f\u4fdd\u6301\u4e00\u81f4\u5c31\u6700\u597d\u4e86\uff5e\uff5e\uff09\n\n3. Contributors:\n    * [Goffic](https://github.com/Goffic)\uff1a \u4e3arnn_layer.c\u6dfb\u52a0\u4e86\u6ce8\u91ca\n    * [LamHoCN](https://github.com/LamHoCN)\uff1a \u4fee\u6539\u4e86\u4e00\u4e9b\u6ce8\u91ca\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e9byolo\u7684\u5e94\u7528\u4ee3\u7801\uff08\u5df2push\u5230extension\u5206\u652f\u4e2d\uff09\n\n# \u7591\u60d1\u6c42\u89e3\n\n1. \u59cb\u7ec8\u4e0d\u660e\u767dsoftmax_layer\u5c42\u53cd\u5411\u4f20\u64ad\u51fd\u6570backward_softmax_layer()\u4e2d\u4e3a\u4ec0\u4e48\u4e0d\u7528\u5bf9softmax\u51fd\u6570\u6c42\u5bfc\uff1f\n\n2. region_layer\u5c42\u524d\u5411\u51fd\u6570forward_region_layer()\u5728\u6c42l.output\u8fc7\u7a0b\u4e2d\uff08\u4e5f\u5c31\u662f\u7b2c\u4e00\u6b21\u4f7f\u7528activate_array\u51fd\u6570\uff09\uff0c\u4e3a\u4ec0\u4e48\u53ea\u5bf9x,y\u8fdb\u884c\u4e86logistic\u6fc0\u6d3b\u51fd\u6570\u5904\u7406\uff0c\u800c\u6ca1\u6709\u5bf9w,h\u5904\u7406\uff08\u4e5f\u5c31\u662factivate_array()\u51fd\u6570\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u4e3a\u4ec0\u4e48\u662f2*l.w*l.h\uff0c\u800c\u4e0d\u662f4*l.w*l.h\uff09\uff1f\u8fd8\u6709\uff0cregion_layer\u8fd9\u4e00\u5c42\u6ca1\u6709\u8bad\u7ec3\u53c2\u6570\u5417\uff1f\n\n3. \u53e6\u5916\uff0c\u5c31\u662fregion_layer\u4e2dcost\u548cdelta\u7684\u8ba1\u7b97\u4e86\uff0c\u611f\u89c9\u5e76\u6ca1\u6709\u5f04\u61c2\uff1f\n"
 },
 {
  "repo": "bitcoin-core/secp256k1",
  "language": "C",
  "readme_contents": "libsecp256k1\n============\n\n[![Build Status](https://api.cirrus-ci.com/github/bitcoin-core/secp256k1.svg?branch=master)](https://cirrus-ci.com/github/bitcoin-core/secp256k1)\n![Dependencies: None](https://img.shields.io/badge/dependencies-none-success)\n[![irc.libera.chat #secp256k1](https://img.shields.io/badge/irc.libera.chat-%23secp256k1-success)](https://web.libera.chat/#secp256k1)\n\nOptimized C library for ECDSA signatures and secret/public key operations on curve secp256k1.\n\nThis library is intended to be the highest quality publicly available library for cryptography on the secp256k1 curve. However, the primary focus of its development has been for usage in the Bitcoin system and usage unlike Bitcoin's may be less well tested, verified, or suffer from a less well thought out interface. Correct usage requires some care and consideration that the library is fit for your application's purpose.\n\nFeatures:\n* secp256k1 ECDSA signing/verification and key generation.\n* Additive and multiplicative tweaking of secret/public keys.\n* Serialization/parsing of secret keys, public keys, signatures.\n* Constant time, constant memory access signing and public key generation.\n* Derandomized ECDSA (via RFC6979 or with a caller provided function.)\n* Very efficient implementation.\n* Suitable for embedded systems.\n* No runtime dependencies.\n* Optional module for public key recovery.\n* Optional module for ECDH key exchange.\n* Optional module for Schnorr signatures according to [BIP-340](https://github.com/bitcoin/bips/blob/master/bip-0340.mediawiki).\n\nImplementation details\n----------------------\n\n* General\n  * No runtime heap allocation.\n  * Extensive testing infrastructure.\n  * Structured to facilitate review and analysis.\n  * Intended to be portable to any system with a C89 compiler and uint64_t support.\n  * No use of floating types.\n  * Expose only higher level interfaces to minimize the API surface and improve application security. (\"Be difficult to use insecurely.\")\n* Field operations\n  * Optimized implementation of arithmetic modulo the curve's field size (2^256 - 0x1000003D1).\n    * Using 5 52-bit limbs (including hand-optimized assembly for x86_64, by Diederik Huys).\n    * Using 10 26-bit limbs (including hand-optimized assembly for 32-bit ARM, by Wladimir J. van der Laan).\n      * This is an experimental feature that has not received enough scrutiny to satisfy the standard of quality of this library but is made available for testing and review by the community.\n* Scalar operations\n  * Optimized implementation without data-dependent branches of arithmetic modulo the curve's order.\n    * Using 4 64-bit limbs (relying on __int128 support in the compiler).\n    * Using 8 32-bit limbs.\n* Modular inverses (both field elements and scalars) based on [safegcd](https://gcd.cr.yp.to/index.html) with some modifications, and a variable-time variant (by Peter Dettman).\n* Group operations\n  * Point addition formula specifically simplified for the curve equation (y^2 = x^3 + 7).\n  * Use addition between points in Jacobian and affine coordinates where possible.\n  * Use a unified addition/doubling formula where necessary to avoid data-dependent branches.\n  * Point/x comparison without a field inversion by comparison in the Jacobian coordinate space.\n* Point multiplication for verification (a*P + b*G).\n  * Use wNAF notation for point multiplicands.\n  * Use a much larger window for multiples of G, using precomputed multiples.\n  * Use Shamir's trick to do the multiplication with the public key and the generator simultaneously.\n  * Use secp256k1's efficiently-computable endomorphism to split the P multiplicand into 2 half-sized ones.\n* Point multiplication for signing\n  * Use a precomputed table of multiples of powers of 16 multiplied with the generator, so general multiplication becomes a series of additions.\n  * Intended to be completely free of timing sidechannels for secret-key operations (on reasonable hardware/toolchains)\n    * Access the table with branch-free conditional moves so memory access is uniform.\n    * No data-dependent branches\n  * Optional runtime blinding which attempts to frustrate differential power analysis.\n  * The precomputed tables add and eventually subtract points for which no known scalar (secret key) is known, preventing even an attacker with control over the secret key used to control the data internally.\n\nBuild steps\n-----------\n\nlibsecp256k1 is built using autotools:\n\n    $ ./autogen.sh\n    $ ./configure\n    $ make\n    $ make check  # run the test suite\n    $ sudo make install  # optional\n\nTo compile optional modules (such as Schnorr signatures), you need to run `./configure` with additional flags (such as `--enable-module-schnorrsig`). Run `./configure --help` to see the full list of available flags.\n\nUsage examples\n-----------\nUsage examples can be found in the [examples](examples) directory. To compile them you need to configure with `--enable-examples`.\n  * [ECDSA example](examples/ecdsa.c)\n  * [Schnorr signatures example](examples/schnorr.c)\n  * [Deriving a shared secret (ECDH) example](examples/ecdh.c)\n\nTo compile the Schnorr signature and ECDH examples, you also need to configure with `--enable-module-schnorrsig` and `--enable-module-ecdh`.\n\nTest coverage\n-----------\n\nThis library aims to have full coverage of the reachable lines and branches.\n\nTo create a test coverage report, configure with `--enable-coverage` (use of GCC is necessary):\n\n    $ ./configure --enable-coverage\n\nRun the tests:\n\n    $ make check\n\nTo create a report, `gcovr` is recommended, as it includes branch coverage reporting:\n\n    $ gcovr --exclude 'src/bench*' --print-summary\n\nTo create a HTML report with coloured and annotated source code:\n\n    $ mkdir -p coverage\n    $ gcovr --exclude 'src/bench*' --html --html-details -o coverage/coverage.html\n\nBenchmark\n------------\nIf configured with `--enable-benchmark` (which is the default), binaries for benchmarking the libsecp256k1 functions will be present in the root directory after the build.\n\nTo print the benchmark result to the command line:\n\n    $ ./bench_name\n\nTo create a CSV file for the benchmark result :\n\n    $ ./bench_name | sed '2d;s/ \\{1,\\}//g' > bench_name.csv\n\nReporting a vulnerability\n------------\n\nSee [SECURITY.md](SECURITY.md)\n"
 },
 {
  "repo": "mkirchner/linked-list-good-taste",
  "language": "C",
  "readme_contents": "# Linked lists, pointer tricks and good taste\n\n* [Introduction](#introduction)\n* [The code](#the-code)\n   * [The CS101 version](#the-cs101-version)\n   * [A more elegant solution](#a-more-elegant-solution)\n* [How does it work?](#how-does-it-work)\n   * [Integrating the head pointer](#integrating-the-head-pointer)\n   * [Maintaining a handle](#maintaining-a-handle)\n* [Going beyond](#going-beyond)\n   * [Inserting before existing items](#inserting-before-existing-items)\n   * [Quick refactor](#quick-refactor)\n   * [Implementing insert_before()](#implementing-insert_before)\n* [Conclusion](#conclusion)\n\n\n## Introduction\n\nIn a 2016 [TED interview][ted] (14:10) Linus Torvalds speaks about what he\nconsiders *good taste* in coding. As an example, he presents two\nimplementations of item removal in singly linked lists (reproduced below).  In\norder to remove the first item from a list, one of the implementations requires\na special case, the other one does not.  Linus, obviously, prefers the latter.\n\nHis comment is:\n\n> [...] I don't want you to understand why it doesn't have the if statement.\n> But I want you to understand that sometimes you can see a problem in a\n> different way and rewrite it so that a special case goes away and becomes the\n> normal case, and that's *good code*. [...] -- L. Torvalds\n\nThe code snippets he presents are C-style pseudocode and are simple enough to\nfollow. However, as Linus mentions in the comment, the snippets lack a\nconceptual explanation and it is not immediately evident how the more elegant\nsolution actually works.\n\nThe next two sections look at the technical approach in detail and demonstrate\nhow and why the indirect addressing approach is so neat. The last section\nextends the solution from item deletion to insertion.\n\n\n## The code\n\nThe basic data structure for a singly linked list of integers is shown in\nFigure 1.\n\n<p align=\"center\">\n<img alt=\"linked list\" src=\"img/linked-list.png\" width=\"600\">\n<br />\n<b>Figure 1</b>: Singly linked list of integers.\n</p>\n\nNumbers are arbitrarily chosen integer values and arrows indicate pointers.\n`head` is a pointer of type `list_item *` and each of the boxes\nis an instance of an `list_item` struct, each with a member variable (called\n`next` in the code) of type `list_item *` that points to the next item.\n\nThe C implementation of the data structure is:\n\n```c\nstruct list_item {\n        int value;\n        struct list_item *next;\n};\ntypedef struct list_item list_item;\n\nstruct list {\n        struct list_item *head;\n};\ntypedef struct list list;\n\n```\nWe also include a (minimal) API:\n\n```c\n/* The textbook version */\nvoid remove_cs101(list *l, list_item *target);\n/* A more elegant solution */\nvoid remove_elegant(list *l, list_item *target);\n```\n\nWith that in place, let's have a look at the implementations of\n`remove_cs101()` and `remove_elegant()`. The code of these examples is true\nto the pseudocode from Linus' example and also compiles and runs.\n\n### The CS101 version\n\n<p align=\"center\">\n<img alt=\"simple data model\" src=\"img/data-model-cs101.png\" width=\"600\">\n<br />\n<b>Figure 2</b>: The conceptual model for the list data structure in the CS101 algorithm.\n</p>\n\n```c\nvoid remove_cs101(list *l, list_item *target)\n{\n        list_item *cur = l->head, *prev = NULL;\n        while (cur != target) {\n                prev = cur;\n                cur = cur->next;\n        }\n        if (prev)\n                prev->next = cur->next;\n        else\n                l->head = cur->next;\n}\n```\n\nThe standard CS101 approach makes use of two traversal pointers `cur` and\n`prev`, marking the current and previous traversal position in the list,\nrespectively.  `cur` starts at the list head `head`, and advances until the\ntarget is found.  `prev` starts at `NULL` and is subsequently updated with the\nprevious value of `cur` every time `cur` advances. After the target is found,\nthe algorithm tests if `prev` is non-`NULL`. If yes, the item is not at the\nbeginning of the list and the removal consists of re-routing the linked list\naround `cur`. If `prev` is `NULL`, `cur` is pointing to the first element in\nthe list, in which case, removal means moving the list head forward.\n\n### A more elegant solution\n\nThe more elegant version has less code and does not require a separate branch\nto deal with deletion of the first element in a list.\n\n```c\nvoid remove_elegant(list *l, list_item *target)\n{\n        list_item **p = &l->head;\n        while (*p != target)\n                p = &(*p)->next;\n        *p = target->next;\n}\n```\n\nThe code uses an indirect pointer `p` that holds the address of a pointer to a\nlist item, starting with the address of `head`.  In every iteration, that\npointer is advanced to hold the address of the pointer to the next list item,\ni.e. the address of the `next` element in the current `list_item`.\nWhen the pointer to the list item `*p` equals `target`, we exit the search\nloop and remove the item from the list.\n\n\n## How does it work?\n\nThe key insight is that using an indirect pointer `p` has two conceptual\nbenefits:\n\n1. It allows us to interpret the linked list in a way that makes the `head`\n   pointer an integral part the data structure. This eliminates the need \n   for a special case to remove the first item.\n2. It also allows us to evaluate the condition of the `while` loop without\n   having to let go of the pointer that points to `target`. This allows us to\n   modify the pointer that points to `target` and to get away with a single\n   iterator as opposed to `prev` and `cur`.\n\nLet's look each of these points in turn.\n\n### Integrating the `head` pointer\n\nThe standard model interprets the linked list as a sequence of `list_item`\ninstances. The beginning of the sequence can be accessed through a `head`\npointer. This leads to the conceptual model illustrated in Figure 2 above. The `head` pointer is\nmerely considered as a handle to access the start of the list. `prev` and `cur`\nare pointers of type `list_item *` and always point to an item or `NULL`.\n\nThe elegant implementation uses indirect addressing scheme that yields a different\nview on the data structure:\n\n<p align=\"center\">\n<img alt=\"Data model for indirect addressing\" src=\"img/data-model-indirect.png\" width=\"600\">\n<br />\n<b>Figure 3</b>: The conceptual model for the list data structure in the more\nelegant approach.\n</p>\n\nHere, `p` is of type `list_item **` and holds the address of the pointer to\nthe current list item. When we advance the pointer, we forward to the address\nof the pointer to the next list item.\n\nIn code, this translates to `p = &(*p)->next`, meaning we\n\n1. `(*p)`: dereference the address to the pointer to the current list item\n2. `->next`: dereference that pointer again and select the field that holds\n   the address of the next list item\n3. `&`: take the address of that address field (i.e. get a pointer to it)\n\nThis corresponds to an interpretation of the data structure where the list is a\na sequence of pointers to `list_item`s (cf. Figure 3).\n\n### Maintaining a handle\n\nAn additional benefit of that particular interpretation is that it supports\nediting the `next` pointer of the predecessor of the current item throughout the\nentire traversal.\n\nWith `p` holding the address of a pointer to a list item, the comparison in the\nsearch loop becomes\n\n```c\nwhile (*p != target)\n```\n\nThe search loop will exit if `*p` equals `target`, and once it does, we are\nstill able to modify `*p` since we hold its address `p`. Thus, despite\niterating the loop until we hit `target`, we still maintain a handle (the\naddress of the `next` field or the `head` pointer) that can be used to directly\nmodify the pointer that points *to* the item.\n\nThis is the reason why we can modify the incoming pointer to an item to point\nto a different location using `*p = target->next` and why we do not need `prev`\nand `cur` pointers to traverse the list for item removal.\n\n## Going beyond\n\nIt turns out that the idea behind `remove_elegant()` can be applied to yield a\nparticularly concise implementation of another function in the list API:\n`insert_before()`, i.e. inserting a given item before another one.\n\n### Inserting before existing items\n\nFirst, let's add the following declaration to the list API in `list.h`:\n\n```c\nvoid insert_before(list *l, list_item *before, list_item *item);\n```\n\nThe function will take a pointer to a list `l`, a pointer `before` to an \nitem in that list and a pointer to a new list item `item` that the function\nwill insert before `before`.\n\n### Quick refactor\n\nBefore we move on, we refactor the search loop into a separate\nfunction\n\n```c\n\nstatic inline list_item **find_indirect(list *l, list_item *target)\n{\n        list_item **p = &l->head;\n        while (*p != target)\n                p = &(*p)->next;\n        return p;\n}\n\n```\n\nand use that function in `remove_elegant()` like so\n\n```c\nvoid remove_elegant(list *l, list_item *target)\n{\n        list_item **p = find_indirect(l, target);\n        *p = target->next;\n}\n```\n\n### Implementing `insert_before()`\n\nUsing `find_indirect()`, it is straightforward to implement `insert_before()`:\n\n```c\nvoid insert_before(list *l, list_item *before, list_item *item)\n{\n        list_item **p = find_indirect(l, before);\n        *p = item;\n        item->next = before;\n}\n```\n\nA particularly beautiful outcome is that the implementation has consistent\nsemantics for the edge cases: if `before` points to the list head, the new item\nwill be inserted at the beginning of the list, if `before` is `NULL` or invalid\n(i.e. the item does not exist in `l`), the new item will be appended at the\nend.\n\n\n## Conclusion\n\nThe premise of the more elegant solution for item deletion is a single, simple\nchange: using an indirect `list_item **` pointer to iterate over the pointers\nto the list items.  Everything else flows from there: there is no need for a\nspecial case or branching and a single iterator is sufficient to find and\nremove the target item.\nIt also turns out that the same approach provides an elegant solution for item\ninsertion in general and for insertion *before* an existing item in particular.\n\nSo, going back to Linus' initial comment: is it good taste? Hard to say, but\nit's certainly a different, creative and very elegant solution to a well-known\nCS task.\n\n[ted]: https://www.ted.com/talks/linus_torvalds_the_mind_behind_linux\n"
 },
 {
  "repo": "MatrixTM/MHDDoS",
  "language": "Python",
  "readme_contents": "<p align=\"center\"><img src=\"https://cdn.discordapp.com/attachments/938175699326484490/948263435412598864/unknown_2.png\" width=\"400px\" height=\"150px\" alt=\"ddos\"></p>\n\n<h1 align=\"center\">MHDDoS - DDoS Attack Script With 56 Methods</h1>\n<em><h5 align=\"center\">(Programming Language - Python 3)</h5></em>\n\n<p align=\"center\">\n<a href=\"#\"><img alt=\"MH-DDoS forks\" src=\"https://img.shields.io/github/forks/MatrixTM/MHDDoS?style=for-the-badge\"></a>\n<a href=\"#\"><img alt=\"MH-DDoS last commit (main)\" src=\"https://img.shields.io/github/last-commit/MatrixTM/MHDDoS/main?color=green&style=for-the-badge\"></a>\n<a href=\"#\"><img alt=\"MH-DDoS Repo stars\" src=\"https://img.shields.io/github/stars/MatrixTM/MHDDoS?style=for-the-badge&color=yellow\"></a>\n<a href=\"#\"><img alt=\"MH-DDoS License\" src=\"https://img.shields.io/github/license/MatrixTM/MHDDoS?color=orange&style=for-the-badge\"></a>\n<a href=\"https://github.com/MatrixTM/MHDDoS/issues\"><img alt=\"MatrixTM issues\" src=\"https://img.shields.io/github/issues/MatrixTM/MHDDoS?color=purple&style=for-the-badge\"></a>\n  \n<p align=\"center\">Please Don't Attack websites without the owners consent.</p>\n\n<p align=\"center\"><img src=\"https://i.imgur.com/aNrHJcA.png\" width=\"1078\" height=\"433\" alt=\"POWER\"></p>\n<p align=\"center\"><img src=\"https://i.imgur.com/4Q7v2wn.png\" width=\"1078\" height=\"296\" alt=\"SCRIPT\"></p>\n\n## Features And Methods\n\n * \ud83d\udca3 Layer7\n\n   * <img src=\"https://img.icons8.com/cotton/344/domain.png\" width=\"16\" height=\"16\" alt=\"get\"> GET | GET Flood\n   * <img src=\"https://cdn0.iconfinder.com/data/icons/database-storage-5/60/server__database__fire__burn__safety-512.png\" width=\"16\" height=\"16\" alt=\"post\"> POST | POST Flood\n   * <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/f/f9/OVH_Logo.svg/1200px-OVH_Logo.svg.png\" width=\"16\" height=\"16\" alt=\"ovh\"> OVH | Bypass OVH\n   * <img src=\"https://cdn-icons-png.flaticon.com/512/1691/1691948.png\" width=\"16\" height=\"16\" alt=\"ovh\"> RHEX | Random HEX\n   * <img src=\"https://cdn-icons-png.flaticon.com/512/4337/4337972.png\" width=\"16\" height=\"16\" alt=\"ovh\"> STOMP | Bypass chk_captcha\n   * <img src=\"https://cdn.iconscout.com/icon/premium/png-256-thumb/cyber-bullying-2557797-2152371.png\" width=\"16\" height=\"16\" alt=\"stress\"> STRESS | Send HTTP Packet With High Byte \n   * <img src=\"https://pbs.twimg.com/profile_images/1351562987224641544/IKb4q_yd_400x400.jpg\" width=\"16\" height=\"16\" alt=\"dyn\"> DYN | A New Method With Random SubDomain\n   * <img src=\"https://cdn-icons-png.flaticon.com/512/6991/6991643.png\" width=\"16\" height=\"16\" alt=\"downloader\"> DOWNLOADER | A New Method of Reading data slowly\n   * <img src=\"https://cdn2.iconfinder.com/data/icons/poison-and-venom-fill/160/loris2-512.png\" width=\"16\" height=\"16\" alt=\"slow\"> SLOW | Slowloris Old Method of DDoS\n   * <img src=\"https://lyrahosting.com/wp-content/uploads/2020/06/ddos-how-work-icon.png\" width=\"16\" height=\"16\" alt=\"head\"> HEAD | https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/HEAD\n   * <img src=\"https://img.icons8.com/plasticine/2x/null-symbol.png\" width=\"16\" height=\"16\" alt=\"null\"> NULL | Null UserAgent and ...\n   * <img src=\"https://i.pinimg.com/originals/03/2e/7d/032e7d0755cd511c753bcb6035d44f68.png\" width=\"16\" height=\"16\" alt=\"cookie\"> COOKIE | Random Cookie PHP 'if (isset($_COOKIE))'\n   * <img src=\"https://cdn0.iconfinder.com/data/icons/dicticons-files-folders/32/office_pps-512.png\" width=\"16\" height=\"16\" alt=\"pps\"> PPS |  Only 'GET / HTTP/1.1\\r\\n\\r\\n'\n   * <img src=\"https://cdn3.iconfinder.com/data/icons/internet-security-14/48/DDoS_website_webpage_bomb_virus_protection-512.png\" width=\"16\" height=\"16\" alt=\"even\"> EVEN | GET Method with more header\n   * <img src=\"https://projectshield.withgoogle.com/static/icons/favicon.ico\" width=\"16\" height=\"16\" alt=\"googleshield\"> GSB | Google Project Shield Bypass\n   * <img src=\"https://seeklogo.com/images/D/ddos-guard-logo-CFEFCA409C-seeklogo.com.png\" width=\"16\" height=\"16\" alt=\"DDoSGuard\"> DGB | DDoS Guard Bypass\n   * <img src=\"https://i.imgur.com/bGL8qfw.png\" width=\"16\" height=\"16\" alt=\"ArvanCloud\"> AVB | Arvan Cloud Bypass\n   * <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Google_%22G%22_Logo.svg/1024px-Google_%22G%22_Logo.svg.png\" width=\"16\" height=\"16\" alt=\"Google bot\"> BOT | Like Google bot\n   * <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Apache_HTTP_Server_Logo_%282016%29.svg/1000px-Apache_HTTP_Server_Logo_%282016%29.svg.png\" width=\"16\" height=\"16\" alt=\"Apache Webserver\"> APACHE | Apache Expliot\n   * <img src=\"https://icon-library.com/images/icon-for-wordpress/icon-for-wordpress-16.jpg\" width=\"16\" height=\"16\" alt=\"wordpress expliot\"> XMLRPC | WP XMLRPC expliot (add /xmlrpc.php)\n   * <img src=\"https://techcrunch.com/wp-content/uploads/2019/06/J2LlHqT3qJl0bG9Alpgc-1-730x438.png?w=730\" width=\"16\" height=\"16\" alt=\"CloudFlare\"> CFB | CloudFlare Bypass\n   * <img src=\"https://techcrunch.com/wp-content/uploads/2019/06/J2LlHqT3qJl0bG9Alpgc-1-730x438.png?w=730\" width=\"16\" height=\"16\" alt=\"CloudFlare UnderAttack Mode\"> CFBUAM | CloudFlare Under Attack Mode Bypass\n   * <img src=\"http://iclouddnsbypass.com/wp-content/uploads/2015/02/iCloudDNSBypassServer.ico\" width=\"16\" height=\"16\" alt=\"bypass\"> BYPASS |  Bypass Normal AntiDDoS\n   * <img src=\"https://cdn-icons-png.flaticon.com/512/905/905568.png\" width=\"16\" height=\"16\" alt=\"bypass\"> BOMB |  Bypass with codesenberg/bombardier\n   * \ud83d\udd2a KILLER | run many threads to kill a target\n   * \ud83e\uddc5 TOR | Bypass onion website\n\n\n* \ud83e\udde8 Layer4: \n  * <img src=\"https://raw.githubusercontent.com/kgretzky/pwndrop/master/media/pwndrop-logo-512.png\" width=\"16\" height=\"16\" alt=\"tcp\"> TCP | TCP Flood Bypass\n  * <img src=\"https://styles.redditmedia.com/t5_2rxmiq/styles/profileIcon_snoob94cdb09-c26c-4c24-bd0c-66238623cc22-headshot.png\" width=\"16\" height=\"16\" alt=\"udp\"> UDP | UDP Flood Bypass\n  * <img src=\"https://cdn-icons-png.flaticon.com/512/1918/1918576.png\" width=\"16\" height=\"16\" alt=\"syn\"> SYN | SYN Flood\n  * <img src=\"https://cdn-icons-png.flaticon.com/512/1017/1017466.png\" width=\"16\" height=\"16\" alt=\"cps\"> CPS | Open and close connections with proxy\n  * <img src=\"https://icon-library.com/images/icon-ping/icon-ping-28.jpg\" width=\"16\" height=\"16\" alt=\"icmp\"> ICMP | Icmp echo request flood (Layer3)\n  * <img src=\"https://s6.uupload.ir/files/1059643_g8hp.png\" width=\"16\" height=\"16\" alt=\"connection\"> CONNECTION | Open connection alive with proxy\n  * <img src=\"https://ia803109.us.archive.org/27/items/source-engine-video-projects/source-engine-video-projects_itemimage.png\" width=\"16\" height=\"16\" alt=\"vse\"> VSE | Send Valve Source Engine Protocol\n  * <img src=\"https://mycrackfree.com/wp-content/uploads/2018/08/TeamSpeak-Server-9.png\" width=\"16\" height=\"16\" alt=\"teamspeak 3\"> TS3 | Send Teamspeak 3 Status Ping Protocol\n  * <img src=\"https://cdn2.downdetector.com/static/uploads/logo/75ef9fcabc1abea8fce0ebd0236a4132710fcb2e.png\" width=\"16\" height=\"16\" alt=\"fivem\"> FIVEM | Send Fivem Status Ping Protocol\n  * <img src=\"https://cdn.iconscout.com/icon/free/png-512/redis-4-1175103.png\" width=\"16\" height=\"16\" alt=\"mem\"> MEM | Memcached Amplification\n  * <img src=\"https://lyrahosting.com/wp-content/uploads/2020/06/ddos-attack-icon.png\" width=\"16\" height=\"16\" alt=\"ntp\"> NTP | NTP Amplification\n  * <img src=\"https://cdn-icons-png.flaticon.com/512/4712/4712139.png\" width=\"16\" height=\"16\" alt=\"mcbot\"> MCBOT | Minecraft Bot Attack\n  * <img src=\"https://cdn.icon-icons.com/icons2/2699/PNG/512/minecraft_logo_icon_168974.png\" width=\"16\" height=\"16\" alt=\"minecraft\"> MINECRAFT | Minecraft Status Ping Protocol\n  * <img src=\"https://gam3r.ir/wp-content/uploads/2018/11/512dVKB22QL.png\" width=\"16\" height=\"16\" alt=\"minecraft pe\"> MCPE | Minecraft PE Status Ping Protocol\n  * <img src=\"https://cdn-icons-png.flaticon.com/512/2653/2653461.png\" width=\"16\" height=\"16\" alt=\"dns\"> DNS | DNS Amplification\n  * <img src=\"https://lyrahosting.com/wp-content/uploads/2020/06/ddos-attack-icon.png\" width=\"16\" height=\"16\" alt=\"chargen\"> CHAR | Chargen Amplification\n  * <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRct5OvjSCpUftyRMm3evgdPOa-f8LbwJFO-A&usqp=CAU\" width=\"16\" height=\"16\" alt=\"cldap\"> CLDAP | Cldap Amplification\n  * <img src=\"https://help.apple.com/assets/6171BD2C588E52621824409D/6171BD2D588E5262182440A4/en_US/8b631353e070420f47530bf95f1a7fae.png\" width=\"16\" height=\"16\" alt=\"ard\"> ARD | Apple Remote Desktop Amplification\n  * <img src=\"https://www.tenforums.com/geek/gars/images/2/types/thumb__emote__esktop__onnection.png\" width=\"16\" height=\"16\" alt=\"rdp\"> RDP |  Remote Desktop Protocol Amplification\n\n* \u2699\ufe0f Tools - Run With \n`\npython3 start.py tools\n`\n  * \ud83c\udf1f CFIP | Find Real IP Address Of Websites Powered By Cloudflare\n  * \ud83d\udd2a DNS | Show DNS Records Of Sites\n  * \ud83d\udccd  TSSRV | TeamSpeak SRV Resolver\n  * \u26a0  PING | PING Servers\n  * \ud83d\udccc CHECK | Check If Websites Status\n  * \ud83d\ude0e DSTAT | That Shows Bytes Received, bytes Sent and their amount\n\n* \ud83c\udfa9 Other\n  * \u274c STOP | STOP All Attacks\n  * \ud83c\udf20 TOOLS | Console Tools\n  * \ud83d\udc51 HELP | Show Usage Script\n\n\n<h1 align=\"center\">\nOur social's\ud83d\udcbb\n</h2> \n\n<div align=\"center\">\n   <img src=\"https://icon-library.com/images/github-icon-vector/github-icon-vector-27.jpg\" width=\"96\" height=\"96\" alt=\"github\" />\n   <img src=\"https://brandlogos.net/wp-content/uploads/2021/11/discord-logo.png\"  width=\"96\" height=\"96\" alt=\"discord\" />\n   <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Telegram_logo.svg/2048px-Telegram_logo.svg.png\" width=\"96\" height=\"96\" alt=\"telegram\" />\n</div>\n\n * [Matrix Team Telegram group](https://t.me/DD0SChat)\n * [Matrix community Telegram channel](https://t.me/MatrixORG)\n * [GitHub] : [github]\n### If u Like the project Leave a star on the repository!\n\n## Downloads\n\nYou can download it from [GitHub Releases](https://github.com/MatrixTM/MHDDoS/releases)\n\n### Getting Started\n\n**Requirements**\n\n* [dnspython](https://github.com/rthalley/dnspython)\n* [cfscrape](https://github.com/Anorov/cloudflare-scrape)\n* [impacket](https://github.com/SecureAuthCorp/impacket)\n* [requests](https://github.com/psf/requests)\n* [Python3][python3]\n* [PyRoxy](https://github.com/MatrixTM/PyRoxy)\n* [icmplib](https://github.com/ValentinBELYN/icmplib)\n* [certifi](https://github.com/certifi/python-certifi)\n* [psutil](https://github.com/giampaolo/psutil)\n* [yarl](https://github.com/aio-libs/yarl)\n---\n\n**Videos**\n\n* Aparat: https://www.aparat.com/v/bHcP9\n* YouTube : Coming soon...\n\n**Tutorial**\n\n* Aprat : https://aparat.com/v/XPn5Z\n* YouTube : Coming soon...\n---\n\n## Documentation\n\nYou can read it from [GitHub Wiki](https://github.com/MatrixTM/MHDDoS/wiki)\n\n**Clone and Install Script**\n\n```shell script\ngit clone https://github.com/MatrixTM/MHDDoS.git\ncd MHDDoS\npip install -r requirements.txt\n```\n\n**One-Line Installing on Fresh VPS**\n\n```shell script\napt -y update && apt -y install curl wget libcurl4 libssl-dev python3 python3-pip make cmake automake autoconf m4 build-essential ruby perl golang git && git clone https://github.com/MatrixTM/MHDDoS.git && cd MH* && pip3 install -r requirements.txt\n```\n\n[python3]: https://python.org 'Python3'\n[github]: https://github.com/MatrixTM/MHDDoS/issues 'GitHub'\n\n---\n\n**\ud83d\udcb0 Donation Links:**\n#### Donate Links\n\n<b>BTC</b>: <code>bc1q7dhut0fp3sqmz95kth0munte6exzlrne23jtjh</code></br>\n<b>ETH</b>: <code>0xff2fAF77705de1b842fCbA29c95E5C9e7dc266Dc</code></br>\n<b>USDT TRC20</b>: <code>TNeZtxhaYYseJoUS2LWmao6cDbYciprhKz</code></br></br>\n<b>Toman</b>: https://idpay.ir/mh-prodev</br>\n\n"
 },
 {
  "repo": "dylanaraps/pywal",
  "language": "Python",
  "readme_contents": "<h3 align=\"center\"><img src=\"https://i.imgur.com/5WgMACe.gif\" width=\"200px\"></h3>\n<p align=\"center\">Generate and change color-schemes on the fly.</p>\n\n<p align=\"center\">\n<a href=\"https://travis-ci.org/dylanaraps/pywal\"><img src=\"https://travis-ci.org/dylanaraps/pywal.svg?branch=master\"></a>\n<a href=\"./LICENSE.md\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\"></a>\n<a href=\"https://pypi.python.org/pypi/pywal/\"><img src=\"https://img.shields.io/pypi/v/pywal.svg\"></a>\n<a href=\"https://www.patreon.com/dyla\"><img src=\"https://img.shields.io/badge/donate-patreon-yellow.svg\"></a>\n<a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=V7QNJNKS3WYVS\"><img src=\"https://img.shields.io/badge/donate-paypal-green.svg\"></a>\n</p>\n\n<img src=\"https://i.imgur.com/HhK3LDv.jpg\" alt=\"img\" align=\"right\" width=\"400px\">\n\nPywal is a tool that generates a color palette from the dominant colors in an image. It then applies the colors system-wide and on-the-fly in all of your favourite programs.  \n\nThere are currently 5 supported color generation backends, each providing a different palette of colors from each image. You're bound to find an appealing color-scheme.\n\nPywal also supports predefined themes and has over 250 themes built-in. You can also create your own theme files to share with others.\n\nThe goal of Pywal was to be as out of the way as possible. It doesn't modify any of your existing configuration files. Instead it works around them and provides tools to integrate your system as you see fit.\n\nTerminal emulators and TTYs have their color-schemes updated in real-time with no delay. With minimal configuration this functionality can be extended to almost anything running on your system.\n\n### More: \\[[Installation](https://github.com/dylanaraps/pywal/wiki/Installation)] \\[[Getting Started](https://github.com/dylanaraps/pywal/wiki/Getting-Started)] \\[[Customization](https://github.com/dylanaraps/pywal/wiki/Customization)] \\[[Wiki](https://github.com/dylanaraps/pywal/wiki)] \\[[Screenshots](https://www.reddit.com/r/unixporn/search?q=wal&restrict_sr=on&sort=relevance&t=all)]\n"
 },
 {
  "repo": "EleutherAI/gpt-neo",
  "language": "Python",
  "readme_contents": "# GPT Neo\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5297715.svg)](https://doi.org/10.5281/zenodo.5297715) [![arXiv](https://img.shields.io/badge/arXiv-2101.00027-f9f107.svg)](https://arxiv.org/abs/2101.00027)\n\n**As of August, 2021 code is no longer maintained. It is preserved here in archival form for people who wish to continue to use it.*\n\n\ud83c\udf89 1T or bust my dudes \ud83c\udf89\n\nAn implementation of model & data parallel [GPT3](https://arxiv.org/abs/2005.14165)-like models using the [mesh-tensorflow](https://github.com/tensorflow/mesh) library.\n\n**If you're just here to play with our pre-trained models, we strongly recommend you try out the [HuggingFace Transformer integration](https://huggingface.co/EleutherAI).**\n\nTraining and inference is officially supported on TPU and should work on GPU as well. This repository will be (mostly) archived as we move focus to our GPU-specific repo, [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/).\n\nIn addition to the functionality offered by GPT-3, we also offer the following:\n* [Local attention](https://arxiv.org/abs/2004.05150)\n* [Linear attention](https://arxiv.org/abs/1812.01243)\n* [Mixture of Experts](https://arxiv.org/abs/1701.06538)\n* [Axial Positional embedding](https://arxiv.org/abs/1912.12180)\n\nNB, while neo can *technically* run a training step at 200B+ parameters, it is very inefficient at those scales. This, as well as the fact that many GPUs became available to us, among other things, prompted us to move development over to [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/).\n\n# Pretrained Models\n\n**Update 21/03/2021:**\n\nWe're proud to release two pretrained GPT-Neo models trained on The Pile, the weights and configs can be freely downloaded from [the-eye.eu](https://the-eye.eu/public/AI/gptneo-release/).\n\n1.3B: https://mystic.the-eye.eu/public/AI/gptneo-release/GPT3_XL/\n\n2.7B: https://mystic.the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/\n\nFor more information on how to get these set up, see the colab notebook, or read through the rest of the readme.\n\n## Model Evaluations\n\n#### Linguistic Reasoning\n\n| Model and Size   | Pile BPB   | Pile PPL  | Wikitext PPL | Lambada PPL | Lambada Acc | Winogrande | Hellaswag  |\n|------------------|------------|-----------|--------------|-------------|-------------|------------|------------|\n| **GPT-Neo 125M** | -----      | -----     | **32.285**   | **30.266**  | **37.36%**  | **50.43%** | **28.67%** |\n| GPT-3 125M       | -----      | -----     | -----        | 18.6        | 42.7%       | 52.0%      | 33.7%      |\n| **GPT-Neo 350M** | -----      | -----     | **22.5657**  | **13.876**  | **47.27%**  | **51.14%** | **32.16%** |\n| GPT-3 350M       | -----      | -----     | -----        | 9.09        | 54.3%       | 52.1%      | 43.6%      |\n| GPT-3 Ada        | 0.9631     | -----     | -----        | 9.954       | 51.60%      | 52.90%     | 35.93%     |\n| **GPT-Neo 1.3B** | **0.7527** | **6.159** | **13.10**    | **7.498**   | **57.23%**  | **55.01%** | **38.66%** |\n| GPT-3 1.3B       | -----      | -----     | -----        | 5.44        | 63.6%       | 58.7%      | 54.7%      |\n| GPT-2 1.5B       | 1.0468     | -----     | 17.48        | 10.634      | 51.21%      | 59.40%     | 40.03%     |\n| **GPT-Neo 2.7B** | **0.7165** | **5.646** | **11.39**    | **5.626**   | **62.22%**  | **56.50%** | **42.73%** |\n| GPT-3 2.7B       | -----      | -----     | -----        | 4.60        | 67.1%       | 62.3%      | 62.8%      |\n\n\n#### Physical and Scientific Reasoning\n\n| Model and Size   | MathQA     | PubMedQA   | Piqa       |\n|------------------|------------|------------|------------|\n| **GPT-Neo 125M** | **22.78%** | **55.10%** | **63.06%** |\n| GPT-3 125M       | -----      | -----      | 64.6%      |\n| **GPT-Neo 350M** | **23.45%** | **53.80%** | **65.07%** |\n| GPT-3 350M       | -----      | -----      | 70.2%      |\n| GPT-3 Ada        | 24.29%     | 52.80%     | 68.88%     |\n| **GPT-Neo 1.3B** | **24.05%** | **54.40%** | **71.11%** |\n| GPT-3 1.3B       | -----      | -----      | 75.1%      |\n| GPT-2 1.5B       | 23.64%     | 58.33%     | 70.78%     |\n| **GPT-Neo 2.7B** | **24.72%** | **57.54%** | **72.14%** |\n| GPT-3 2.7B       | -----      | -----      | 75.6%      |\n\n\n**Note:** All evaluations were done using our [evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness). Some results for GPT-2 and GPT-3 are inconsistent with the values reported in the respective papers. We are currently looking into why, and would greatly appreciate feedback and further testing of our eval harness.\n\n# Setup\n\n```bash\ngit clone https://github.com/EleutherAI/GPTNeo\ncd GPTNeo\npip3 install -r requirements.txt\n```\n# Training Setup\n\n## TPUs:\n\nSign up for [Google Cloud Platform](https://cloud.google.com/), and create a [storage bucket](https://cloud.google.com/storage). \n\nCreate your VM through a google shell (`https://ssh.cloud.google.com/`) with `ctpu up --vm-only` so that it can connect to your Google bucket and TPUs and install the requirements with pip (see above).\n\nGoogle colab provides tpu-v8s for free, which should be enough to finetune our models up to GPT3XL (1.5B parameter) sizes.\nClick [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb) to run through our example colab notebook.\n\nFor more detailed instructions, run through our [Training Guide](https://github.com/EleutherAI/GPTNeo#training-guide) below.\n\n## GPUs:\n\nYou can also choose to train GPTNeo locally on your GPUs. To do so, you can omit the Google cloud setup steps above, and git clone the repo locally. Run through the [Training Guide](https://github.com/EleutherAI/GPTNeo#training-guide) below, then when running main.py, you simply have to omit the `tpu` flag, and pass in GPU ids instead.\n\nNote: Some users have reported having difficulty getting MTF to recognize their GPUs. See [here](https://github.com/EleutherAI/gpt-neo/issues/150) for details and instructions on how to fix it.\n\n# Generating Text\n\nOnce you have a trained model, or you've downloaded one of our pre-trained models, generating text is as simple as running the main.py script with the `--predict` flag on. You can pass a path to your prompt txt file with the `--prompt` flag, like so:\n\n```bash\npython3 main.py --predict --prompt <example_prompt.txt> --tpu <tpu_name> --model <config_name>\n```\n\nor, if using GPUs:\n\n```bash\npython3 main.py --predict --prompt <example_prompt.txt> --gpu_ids <device:GPU:0 device:GPU:1> --model <config_name>\n```\n\n# Training Guide\n\n## 1. Create your Tokenizer (OPTIONAL)\n\nWe recommend you use [Huggingface's pretrained GPT2 tokenizer](https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2Tokenizer) with our repo (instructions provided below), but if you want to train a model with a different vocabulary size, we provide facilities to train your own tokenizer like so:\n\n```bash\npython data/train_tokenizer.py \\\n    --base_dir ./path/to/your/txt/files \\\n    --output_dir ./output/path \\\n    --file_type txt \\\n    --vocab_size 50257\n\n# if it succeeded, you should see the message\n# 'tokenizer saved at ./output/path/byte-level-bpe.tokenizer.json'\n```\n\n## 2. Tokenizing your Dataset\n\nIf you just want to test training, you can skip this step and download some dummy data like so:\n\n```\nwget https://storage.googleapis.com/connors-datasets/bundestag/bundestag_0.tfrecords\n```\n\nThen copy the data to your bucket, or if using GPUs, a local directory: \n\n```\ngsutil cp bundestag_0.tfrecords gs://<your bucket>/\n```\n\nIf using your own data to train, you can use the `data/create_tfrecords.py` script to encode your text data into tfrecords.\n\nYour data must either be in the form of lots of normal .txt files (one document per file), or in any format supported by [lm_dataformat](https://github.com/leogao2/lm_dataformat). \n\nYou can run the script without parameters to see help for all options.\n\nIn **document mode** Each example in the tfrecords is one (variably sized) document. This is to be used with the `documents_fixed` and `documents_random` sampling modes (For more details see the parameters reference section).\nDocument mode is the default mode.\n\nThe below command will tokenize all files in acceptable formats in *base_dir* using gpt2 tokenizer and save them to *output_dir*\n```\npython3 create_tfrecords.py --mode documents --input_dir <base> --name <name> --output_dir <output> --use_gpt2_tokenizer --minimum_size <min> \n```\n\n- `input_dir`: Defines the folder where your data is located. The script will encode all files present in this folder.\n- `name`: Name of output files will be `name_i.tfrecords` where i is the number of the file.\n- `output_dir`: Where to save the tfrecords to\n- `use_gpt2_tokenizer`: Whether to use the pretrained HuggingFace GPT2 tokenizer, in which case the separator will be set to [50256].\n- `encoder_path`: if not using the pretrained gpt2 tokenizer, use this flag to provide a path to your generated tokenizer json.\n- `separator`: Written in list format, the separator token(s) to insert between documents (e.g. \"[0]\"). Will depend on your encoder.\n- `minimum_size`: The minimum size (in tokens) a document must have, otherwise it is discarded. This is what will later determine your `stitch` parameter: `stitch * minimum_size` must always be greater or equal `n_ctx` (For more details see the parameters reference section).\n\n## 4. Using a Dataset in a Model\n\nTo use a dataset in a model, you must first register that dataset under `./configs/dataset_configs` folder. First choose a filename with a `.json` extension. That filename will serve as the dataset identification. The config should be filled out the following manner.\n\nIf you have a dataset encoded using the pretrained gpt2 tokenizer, you can specify that like so:\n\n```json\n{\n    \"n_vocab\": 50257,\n    \"path\": \"gs://neo-datasets/openwebtext-documents/openwebtext_*.tfrecords\",\n    \"eval_path\": \"gs://neo-datasets/openwebtext-documents/openwebtext_*.tfrecords\",\n    \"tokenizer_is_pretrained\": true,\n    \"tokenizer_path\": \"gpt2\"\n}\n```\n\nor if you've trained a custom tokenizer, like so:\n\n```json\n{\n    \"n_vocab\": 32768,\n    \"path\": \"./path/to/your/*.tfrecords\",\n    \"eval_path\": \"./path/to/your/eval/*.tfrecords\",\n    \"tokenizer_path\": \"./path/to/your/byte-level-bpe.tokenizer.json\"\n}\n```\n\nFinally, in your model config, add the filename that you created above to the `datasets` array.\n\nThe `<dataset id>` will be the filename, excluding the `.json`, that you created above\n\n```\n\"datasets\": [[<dataset id>, <stitch>, <datatype>, <weight>]] # datasets key defines at run time how each dataset is processed for training\n```\n\n## 5. Choose a model configuration\n\nOnce you have your datasets set up, find a suitable config in `/configs`.\n\nHere we use a GPT3-XL sized model as an example, but there are many more in `./configs`, all of which have short summaries in the Available Configs section.\n\nAll you need to do is edit the dataset id as described above, and edit `model_path` (where logs and checkpoints will be saved) to point to a cloud bucket you have write access to (or local path, if using GPUs).\n\n```json\n{\n    \"n_head\": 32,\n    \"n_vocab\": 50257,\n    \"embed_dropout\": 0.1,\n    \"lr\": 0.0002,\n    \"lr_decay\": \"cosine\",\n    \"warmup_steps\": 3000,\n    \"beta1\": 0.9,\n    \"beta2\": 0.95,\n    \"epsilon\": 1e-8,\n    \"opt_name\": \"adam\",\n    \"weight_decay\": 0.1,\n    \"train_batch_size\": 512,\n    \"attn_dropout\": 0.1,\n    \"train_steps\": 286150,\n    \"eval_steps\": 0,\n    \"predict_steps\": 1,\n    \"res_dropout\": 0.1,\n    \"eval_batch_size\": 128,\n    \"predict_batch_size\": 1,\n    \"iterations\": 2500,\n    \"n_embd\": 2048,\n    \"datasets\": [[\"your_dataset_name\", 25, \"documents_random\", 1.0]],\n    \"model_path\": \"gs://neo-models/GPT3_XL\",\n    \"n_ctx\": 2048,\n    \"n_layer\": 24,\n    \"scale_by_depth\": true,\n    \"scale_by_in\": false,\n    \"attention_types\" :  [[[\"global\"],24]],\n    \"mesh_shape\": \"x:128,y:2\",\n    \"layout\": \"batch:x,memory_length:y,embd:y\",\n    \"activation_function\": \"gelu\",\n    \"recompute_grad\": true,\n    \"gradient_clipping\": 1.0,\n    \"tokens_per_mb_per_replica\": 2048\n}\n```\n\n\n## 6. Run Training\n\n```\npython3 main.py --model <your_config_name> --steps_per_checkpoint <n> --tpu <tpu-name>\n```\n\n- `tpu`: Name of the TPU to use.\n- `steps_per_checkpoint`: The frequency in steps at which to save checkpoints.\n- `--auto_layout` and `--auto_layout_and_mesh_shape` (Optional): Disable training and instead auto generate a memory efficient `layout` (and `mesh_shape`)\n- `gpu_ids`: if training using GPUs, omit the `tpu` flag and pass in the ids of your gpus. In the example below, we train on 3 GPUs, specifying their device ids delimited by spaces:\n\n```\npython3 main.py --model <your_config_name> --steps_per_checkpoint <n> --gpu_ids <device:GPU:0 device:GPU:1>\n```\n\n# Available Configs\n\nWe have several model sizes available, but some of our configs require large TPUs and will need tweaking to run on smaller machines, or GPUs. Below is a short guide to each model in the configs directory:\n\nTODO\n\n# Extra Features: \n\n## Training (with Sacred)\n\n[Sacred](https://github.com/IDSIA/sacred) helps track experiments and is much nicer to work with than tensorboard.\n\nTo setup:\n\n1. Install Docker and Docker-compose\n\n2. Run `docker-compose up`\n\nTo use: \n\n1. Ensure model_dir doesn't have any metric logs in it (it trips up the metric stuff for tensorboard, which assumes that it's a continuation of the existing run). You can use `gsutil rm -r ...` to delete model dir\n\n2. Run `python3 run_experiment.py --tpu sometpuhere --model someconfig.json` Options are the same as `main.py`. \n\n3. You can go to http://server_ip_goes_here:8081/ to see the Omniboard overview. If you prefer to see a tensorboard, the script also spins one up and automatically assigns it a port. The script should print out the tensorboard port near the top of the log. \n\n## Peeking at a Dataset\n\nIf you are ever confused by the dataset of a particular config file, you can easily check the minimum and maximum token ids with a single command. This is useful for making sure that the vocabulary size of the model is at least as large as the maximum token id. Tensorflow will not error if you try to gather on a matrix with out of bounds indices, so you need to make sure your vocabulary size is sufficiently large.\n\n```bash\npython main --model {config_name} --check_dataset\n```\n\n## Masked Language Modeling\n\nIn addition to being able to train large GPT's, this repository also allows you to easily do masked language modeling (BERT, RoBERTa). In order to do so, you must follow two additional steps.\n\n1. When tokenizing your dataset, you must reserve a special id for the `[mask]` token.\n\n2. In the configs, you will have to define two additional fields\n\n```python\n\"mlm_training\": true,                           # must be set to true\n\"mlm_mask_id\": <mask id>                        # the mask id that you reserved from above\n```\n\nThat's all you need to train a model with the MLM objective, good for any type of data that you have encoded properly. If you would like to tweak the other related hyperparameters, please continue reading.\n\n```python\n\"mlm_cls_token_id\": <cls token id>,                # auto append specified CLS token id on the left\n\"mlm_mask_prob\": 0.15,                             # the probability of masking a token, defaults to 15%\n\"mlm_same_token_prob\": 0.10,                       # probability of keeping the token the same, defaults to 10%\n\"mlm_random_token_prob\": 0.10,                     # probability of tokens that are replaced with random tokens, 10% was recommended by the BERT paper\n\"mlm_mask_ignore_ids\": [<cls token>, <sep token>]  # ignore masking other special tokens, if any\n```\n\n## Parameter Reference\n\nPick a valid config from `/configs` and tweak the parameters as needed:\n\n- `n_heads`: The number of attention heads.\n- `n_embd`: Size of the hidden layers, must be divisible by `n_heads`.\n- `n_vocab`: Vocabulary size.\n- `embed_dropout`, `res_dropout`, `attn_dropout`: Dropout probability for word embedding/residuals/attention\n- `lr`: Learning rate\n- `warmup_steps`: Number of steps before full learning rate is reached (linear ramp from `0` to `lr`).\n- `lr_decay`: `cosine` or `linear`.\n- `opt_name`: `adam` or `adafactor`.\n- `beta1`, `beta2` and `epsilon`: `adam` optimizer params.\n- `beta1`, `ada_epsilon1` and `ada_epsilon2`: `adafactor` optimizer params.\n- `weight_decay`: Weight decay parameter, if not present no weight decay is used (the weight decay fix for Adam is used) (default: 0.01) (optional).\n- `train_batch_size`: Batch size during training.\n- `train_steps`: Number of training steps (batches), set to roughly ~1 epoch for now (total number of tokens in your dataset / number of tokens per batch (= `train_batch_size` / `n_ctx`)).\n- `eval_steps`: Number of steps to run for each evaluation. Set to `0` for no eval. i.e After every checkpoint, the model is tested for `eval_steps`\n- `iterations`: Number of steps queued to the TPU, must be smaller than `steps_per_checkpoint`. (default: 500)\n- `datasets`: List of tfrecords datasets to use. Each dataset is a list with the following parameters: `[train glob , eval glob, stitch, sampling_mode, weight]`. So for example for a single dataset (note the double list): `[[\"bundestag_*.tfrecords\", \"\", 10, \"random_sample\", 1.0]]`\n    + `dataset_id`: The name of a dataset configuration file in `./configs/dataset_configs`\n    + `stitch`: If `sampling_mode` `random_sample` is used, the input pipeline samples this amount of texts into one to sample from. You must select stitch so that `stitch * minimum_document_length >= n_ctx`\n    + `sampling_mode`: `chunks` (tfrecords are preprocessed into the correct length and are read sequentially) or `documents_random` (`stitch` amount of documents are concatenated and then a `n_ctx` chunk is randomly subsampled)\n    + `weights`: How much relative weight this dataset should have compared to others\n- `model`: Which model to train. Currently only `GPT` is supported, and it defaults to this if not present.\n- `model_path`: Google storage bucket location (or local path, if using GPUs) to save model checkpoints and logs.\n- `n_ctx`: Size of context window. Default is 2048\n- `n_layer`: Number of layers (blocks) in the model.\n- `scale_by_depth`: If true, the weight initialization of layers are scaled by their depth as in the GPT2 paper.\n- `scale_by_in`: If true, the weight initialization of layers are scaled by their number of inputs as in the GPT2 paper.\n- `mesh_shape`: A Mesh is an n-dimensional array of processors with named dimensions used for parallelism in the mesh-tensorflow library. Each Tensor is split evenly across mesh dimensions according to the layout (see below). The 'mesh_shape' is the shape of this array, and must be equal to the number of processors. e.g., for a v3-128 TPU \"mesh_shape\": \u201cx:16,y:8\u201d.\n- `layout`: A Tensor is laid out on its mesh with one slice on each processor. A Tensor \"layout\", is an injective partial map specifying which dimensions of the tensor are (evenly) split across which dimensions of the mesh. No dimension of a tensor may be split across two dimensions of its mesh and no two dimensions of a tensor may be split across the same dimension of its mesh. The user defines a global set of layout rules in the form of (tensor-dimension-name, mesh-dimension-name) pairs. A dimension of a tensor is split across a dimension of its mesh if there is a matching rule, e.g. (for the above example mesh_shape: \"layout\":\"batch:x,heads:y\"\n- `activation_function`: `selu` (self normalizing) or `gelu` (used by OA), activation function used in feed-forward passes. (default: gelu)\n- `attention_types`: the type of attention for each layer in a list of the following format [[[\"attention_type\"], n_layers]]. e.g. for a 12 layer net [[[\"global\"], 12]] or [[[\"local\"], 10], [[\"global\"], 2]].\n    + Choose from: `linear`, `global`, `local` or `none`. We have found a 50/50 mix of `global` and `linear` to work well. `none` allows you to create feed-forward only layers for more efficient [PAR Transformer](https://arxiv.org/abs/2009.04534) models.\n- `precision`: `float32` or `bfloat16`.\n- `tokens_per_mb_per_replica`: If not None, will split the batch up into smaller microbatches containing `tokens_per_mb_per_replica` tokens to avoid OOMs. Gradients are accumulated locally and reduced once. IMPORTANT: mb refers to *minibatch* not megabyte here. \n\n**Mixture of Experts**\n\n- `moe_layers`: A list of layer numbers to append a [mixture of experts](https://arxiv.org/abs/1701.06538) layer onto. E.G: `[2,4,6,8,10,12]`.\nWe have experimentally found a moe layer for every two self-attention layers to work well.\n-  `moe_params`: a dictionary of additional kwargs to pass in to the moe layer. E.G\n    `{\"moe_dropout_rate\": 0.0 }`\n    \n**Experimental features** \n\n- `axial_pos_emb_`: If true, uses [axial positional embedding](https://arxiv.org/abs/1912.12180. \n- `mlp_glu`: If true, uses a gated linear unit variant of feed forward layers.\n- `scalenorm`: If true, uses scalenorm instead of layernorm.\n- `rezero`: If true, uses [rezero](https://www.groundai.com/project/rezero-is-all-you-need-fast-convergence-at-large-depth/1) instead of layernorm.\n- `num_mem_kv`: adds memory / key values from the [all-attention paper](https://arxiv.org/pdf/1907.01470.pdf). Param is an int with the number of desired mem/key values.\n- `macaron`: if true - uses a [macaron transformer](https://arxiv.org/pdf/1906.02762.pdf) for each layer block.\n\n## TODO: \n\n- [x] finalize documentation\n- [ ] update configs\n\n## Citing GPT-Neo\n\nIf you have found GPT-Neo helpful in your work, you can cite this repository as\n\n```\n@software{gpt-neo,\n  author       = {Black, Sid and\n                  Gao, Leo and\n                  Wang, Phil and\n                  Leahy, Connor and\n                  Biderman, Stella},\n  title        = {{GPT-Neo: Large Scale Autoregressive Language \n                   Modeling with Mesh-Tensorflow}},\n  month        = mar,\n  year         = 2021,\n  note         = {{If you use this software, please cite it using \n                   these metadata.}},\n  publisher    = {Zenodo},\n  version      = {1.0},\n  doi          = {10.5281/zenodo.5297715},\n  url          = {https://doi.org/10.5281/zenodo.5297715}\n}\n\n```\nThe version number should be replaced with the version number you are using, and the year corresponds to the project's open-source release.\n\nIf you are specifically interested in citing the GPT-Neo models trained on [the Pile](https://arxiv.org/abs/2101.00027), we would appreciate also citing\n```\n@article{gao2020pile,\n  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},\n  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},\n  journal={arXiv preprint arXiv:2101.00027},\n  year={2020}\n}\n```\n"
 },
 {
  "repo": "facebookresearch/hydra",
  "language": "Python",
  "readme_contents": "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/facebookresearch/hydra/master/website/static/img/Hydra-Readme-logo2.svg\" alt=\"logo\" width=\"70%\" /></p>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/hydra-core/\">\n    <img src=\"https://img.shields.io/pypi/v/hydra-core\" alt=\"PyPI\" />\n  </a>\n  <a href=\"https://circleci.com/gh/facebookresearch/hydra\">\n    <img src=\"https://img.shields.io/circleci/build/github/facebookresearch/hydra?token=af199cd2deca9e70e53776f9ded96284b10687e9\" alt=\"CircleCI\" />\n  </a>\n  <a href=\"#\">\n    <img src=\"https://img.shields.io/pypi/l/hydra-core\" alt=\"PyPI - License\" />\n  </a>\n  <a href=\"#\">\n    <img src=\"https://img.shields.io/pypi/pyversions/hydra-core\" alt=\"PyPI - Python Version\" />\n  </a>\n  <a href=\"https://pepy.tech/project/hydra-core?versions=0.11.*&versions=1.0.*&versions=1.1.*\">\n    <img src=\"https://pepy.tech/badge/hydra-core/month\" alt=\"Downloads\" />\n  </a>\n  <a href=\"https://github.com/psf/black\">\n    <img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" alt=\"Code style: black\" />\n  </a>\n  <a href=\"https://lgtm.com/projects/g/facebookresearch/hydra/alerts/\">\n    <img src=\"https://img.shields.io/lgtm/alerts/g/facebookresearch/hydra.svg?logo=lgtm&logoWidth=18\" alt=\"Total alerts\" />\n  </a>\n  <a href=\"https://lgtm.com/projects/g/facebookresearch/hydra/context:python\">\n    <img src=\"https://img.shields.io/lgtm/grade/python/g/facebookresearch/hydra.svg?logo=lgtm&logoWidth=18\" alt=\"Language grade: Python\" />\n  </a>\n  <p align=\"center\">\n    <i>A framework for elegantly configuring complex applications.</i>\n  </p>\n  <p align=\"center\">\n    <i>Check the <a href=\"https://hydra.cc/\">website</a> for more information,<br>\n    or click the thumbnail below for a one-minute video introduction to Hydra.</i>\n  </p>\n  <p align=\"center\">\n   <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Slc3gRQpnBI\" target=\"_blank\">\n     <img src=\"http://img.youtube.com/vi/Slc3gRQpnBI/hqdefault.jpg\" alt=\"1 minute overview\" width=\"240\" height=\"180\" border=\"10\" />\n   </a>\n  </p>\n</p>\n\n----------------------\n\n\n### Releases\n\n#### Stable\n\n**Hydra 1.1** is the stable version of Hydra.\n- [Documentation](https://hydra.cc/docs/intro)\n- Installation : `pip install hydra-core --upgrade`\n\n\n### License\nHydra is licensed under [MIT License](LICENSE).\n\n## Community\n\nAsk questions in Github Discussions or StackOverflow (Use the tag #fb-hydra or #omegaconf):\n* [Github Discussions](https://github.com/facebookresearch/hydra/discussions)\n* [StackOverflow](https://stackexchange.com/filters/391828/hydra-questions)\n* [Twitter](https://twitter.com/Hydra_Framework)\n\n\n### Citing Hydra\nIf you use Hydra in your research please use the following BibTeX entry:\n```BibTeX\n@Misc{Yadan2019Hydra,\n  author =       {Omry Yadan},\n  title =        {Hydra - A framework for elegantly configuring complex applications},\n  howpublished = {Github},\n  year =         {2019},\n  url =          {https://github.com/facebookresearch/hydra}\n}\n```\n\n"
 },
 {
  "repo": "devicons/devicon",
  "language": "Python",
  "readme_contents": "<p align=\"center\">\n    <a href=\"https://github.com/devicons/devicon/releases\">\n        <img alt=\"GitHub release (latest by semver)\" src=\"https://img.shields.io/github/v/release/devicons/devicon?color=%2360be86&label=Latest%20release&style=for-the-badge&sort=semver\">\n    </a>\n    <a href=\"/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/devicons/devicon?color=%2360be86&style=for-the-badge\">\n    </a>\n    <a href=\"https://github.com/devicons/devicon/graphs/contributors\">\n        <img alt=\"GitHub contributors\" src=\"https://img.shields.io/github/contributors-anon/devicons/devicon?color=%2360be86&style=for-the-badge\">\n    </a>\n    <a href=\"https://github.com/devicons/devicon/actions\">\n        <img alt=\"GitHub branch checks state\" src=\"https://img.shields.io/github/checks-status/devicons/devicon/master?color=%2360be86&style=for-the-badge\">\n    </a>\n    <a href=\"https://github.com/devicons/devicon/issues?q=is%3Aopen+is%3Aissue+label%3Arequest%3Aicon\">\n        <img alt=\"GitHub issues by-label\" src=\"https://img.shields.io/github/issues/devicons/devicon/request:icon?color=%2360be86&label=icon%20requests&style=for-the-badge\">\n    </a>\n    <a href=\"https://github.com/devicons/devicon/stargazers\">\n        <img alt=\"GitHub Repo stars\" src=\"https://img.shields.io/github/stars/devicons/devicon?color=%2360be86&label=github%20stars&style=for-the-badge\">\n    </a>\n</p>\n<br />\n<div align=\"center\">\n    <a href=\"https://github.com/devicons/devicon\">\n        <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/devicon/devicon-original-wordmark.svg\" alt=\"Devicon Logo\" height=\"140\" />\n    </a>\n    <h5 align=\"center\">\n        devicon aims to gather all logos representing development languages and tools.\n    </h5>\n    <p align=\"center\">\n        <a target=\"_blank\" href=\"https://devicon.dev\">Demo</a>\n        &middot;\n        <a target=\"_blank\" href=\"https://github.com/devicons/devicon/issues/new?assignees=&labels=request%3Aicon&template=icon-request.md&title=Icon+request%3A+%5BNAME%5D\">Request Icon</a>\n        &middot;\n        <a href=\"#contribute\">Contribute</a>\n    </p>\n</div>\n\n<h2>TL;DR;</h2>\n\n```html\n<!-- in your header -->\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/gh/devicons/devicon@latest/devicon.min.css\">\n\n<!-- in your body -->\n<i class=\"devicon-devicon-plain\"></i>\n```\n\n<h2>Table of Contents</h2>\n<ol>\n    <li><a href=\"#about\">About the project</a></li>\n    <li><a href=\"#getting-started\">Getting started</a></li>\n    <li><a href=\"#request-icon\">Requesting icon</a></li>\n    <li><a href=\"#contribute\">Contributing</a></li>\n    <li><a href=\"#discord-server\">Discord server</a></li>\n    <li><a href=\"#develop-vs-master\"><code>develop</code> vs <code>master</code></a></li>\n    <li><a href=\"#stale-prs\">Stale pull requests</a></li>\n    <li><a href=\"#build-yourself\">Go build yourself</a></li>\n</ol>\n\n<h2 id=\"about\">About the project</h2>\n<p>\n    Devicon aims to gather all logos representing development languages and tools.\n    Each icon comes in several versions: font/SVG, original/plain/line, colored/not colored, wordmark/no wordmark.\n    Devicon has 150+ icons. And it's growing!<br />\n</p>\n<p>\n    See the <a href=\"/devicon.json\">devicon.json</a> or <a href=\"https://devicon.dev\">our website</a> for complete and up to date reference of \n    all available icons.\n</p>\n<sub>\n    All product names, logos, and brands are property of their respective owners. All company, product and service \n    names used in this website are for identification purposes only. Use of these names, logos, and brands does not \n    imply endorsement. Usage of these logos should be done according to the company/brand/service's brand policy.\n</sub>\n<p>\n    Thank you to our contributors and the <a href=\"https://icomoon.io/#home\">IcoMoon app</a>. Devicon would not be possible without you.\n</p>\n\n\n<h2 id=\"getting-started\">Getting started</h2>\n<p>\n    For a super fast setup go check <a href=\"https://devicon.dev\">devicon.dev</a>.<br />\n    You can either <a href=\"#getting-started-svg\">use the raw SVG</a> icons or our <a href=\"#getting-started-font\">devicon font</a> (which is \n    also available via CDN).\n</p>\n\n<h4 id=\"getting-started-font\">Use the <code>devicon</code> font (recommended)</h4>\n<p>\n    You can install devicon as a dependency to your project either with <code>npm</code> or <code>yarn</code>:\n</p>\n\n```bash\nnpm install --save devicon\nyarn add devicon\n```\n\n<p>\n    If you don't want to use a package manager you can also download\n    and include <a href=\"/devicon.min.css\">devicon.min.css</a> next to the <a href=\"/fonts\">font files</a> to your project.\n    See <a href=\"https://devicon.dev\">devicon.dev</a> for details about how to add devicon to your project via\n    a CDN.\n</p>\n<p>\n    After setting up you just have to include the stylesheet in your header\n    to get started:\n</p>\n\n```html\n<link rel=\"stylesheet\" href=\"devicon.min.css\">\n```\n\n<p>Start using icons with <code>&lt;i&gt;</code>-tag</p>\n\n```html\n<!--  for devicon plain version -->\n<i class=\"devicon-devicon-plain\"></i>\n\n<!--  for devicon plain version with wordmark -->\n<i class=\"devicon-devicon-plain-wordmark\"></i>\n\n<!--  for devicon plain version colored with devicon main color -->\n<i class=\"devicon-devicon-plain colored\"></i>\n\n<!--  for devicon plain version with wordmark colored with devicon main color -->\n<i class=\"devicon-devicon-plain-wordmark colored\"></i>\n```\n\n<p>\n    An alternate way to use <code>devicon</code> is by copy/paste the raw SVG code\n    to your project.\n</p>\n<h4 id=\"getting-started-svg\">Copy/paste SVG code (from the <a href=\"https://github.com/devicons/devicon/tree/master/icons\">svg folder</a> or the <a href=\"https://devicon.dev\">project page</a>)</h4>\n\n```html\n<!--  for devicon plain version -->\n<svg id=\"Devicon\" class='devicon-devicon-plain' xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 128 128\"><path id=\"plain\" fill=\"#60be86\" d=\"M64,7.83H4.77L14.95,95.13l49,25,.06,0,49.07-25L123.23,7.83Zm42.77,54.86c0,.88,0,1.67-.77,2L73.25,80.44l-2.42,1.13-.27-3.15V72.23l.24-1.57,1.09-.47L95.07,59.81l-21.54-9.6L64.35,68.34,58.9,78.87l-1.22,2.27-2.05-.9L22,64.71a2.42,2.42,0,0,1-1.45-2V56.91a2.39,2.39,0,0,1,1.42-2l34-15.73,3.21-1.44v9.66l.24,1.34-1.56.7L34.45,59.79,56.3,69.42l8.05-16,6.21-12.65,1.13-2.28,1.81.91L106,54.89c.73.35.76,1.14.76,2Z\"/></svg>\n```\n\nAdd css rules in your stylesheet\n```css\n.devicon-devicon-plain {\n  max-width: 2em;\n}\n\n/* if you want to change the original color */\n.devicon-devicon-plain path {\n  fill: #4691f6;\n}\n```\n\n<h4>You can also use the <code>img</code> tag and referencing an svg directly from the repo.</h4>\n\n```html\n<img src='https://cdn.jsdelivr.net/gh/devicons/devicon/icons/devicon/devicon-original.svg'>\n```\n\n<h2 id=\"request-icon\">Requesting icon</h2>\n<p>\n    When you want to request an icon please feel free to create an issue. See our <a href=\"https://github.com/devicons/devicon/wiki/Requesting-an-Icon\">Wiki</a> for more information.\n</p>\n\n<h2 id=\"contribute\">Contributing</h2>\n<p>\n    We are happy with every contribution, whether it's new icons, features, or maintainers. Please have a look at our <a href=\"https://github.com/devicons/devicon/wiki\">Wiki</a> to see how you can contribute to this project.\n</p>\n\n<h2 id=\"discord-server\">Discord server</h2>\n<p>\nWe are running a Discord server. You can go here to talk, discuss, and more with the maintainers and other people, too. Here's the invitation: https://discord.gg/hScy8KWACQ.\n<b>Note that the Discord server is unofficial, and Devicons is still being maintained via GitHub.</b>\n</p>\n\n<h2 id=\"develop-vs-master\"><code>develop</code> vs <code>master</code></h2>\n<p>\nAll official releases shall be in <code>master</code>. Any updates in between (icons, features, etc.) will be kept in <code>develop</code>. \n</p>\n<b><code>develop</code> contains:</b>\n<ul>\n    <li>\n        Latest SVGs (non-optimized).\n    </li>\n    <li>\n        No icons for the latest SVGs. These will be build at every release.\n    </li>\n    <li>\n        Experimental changes.\n    </li>\n</ul>\n<b><code>master</code> contains:</b>\n<ul>\n    <li>\n        Latest official release, which contains the SVGs and icons.\n    </li>\n    <li>\n        Official tested changes.\n    </li>\n</ul>\n\n<h2 id=\"stale-prs\">Stale pull requests</h2>\n<p>\nAfter a pull request has been open for over 30 days with no activity or response from the author, it'll be automatically marked as stale. We might fork your changes and merge the changes ourselves. Since GitHub tracks contributions by commits, you will be credited.\n</p>\n\n<h2 id=\"build-yourself\">Go build yourself</h2>\n<p>\n    Feel free to follow those steps when you want to build the font\n    by yourself.\n</p>\n<h5>Prerequisites</h5>\n<p>Install gulp (and gulp plugins)</p>\n\n```bash\nnpm install\n```\n\n<h5>Build the font and export stylesheet</h5>\nOpen <a href=\"https://icomoon.io/app/#/select\">icomoon.io</a> and import <a href=\"/icomoon.json\">icomoon.json</a>. Choose <i>yes</i> when being asked\nif you would like to restore the settings stored in the configuration file.\n\nThe next step is to click on <b>Generate font</b> and download the resulting archive. Extract the contents and you will find a <a href=\"/fonts\">fonts</a> directory next to a <code>style.css</code>. Replace the contents of the <code>fonts</code> folder, rename <code>style.css</code> as <a href=\"/devicon.css\">devicon.css</a> and follow the next step to build the final stylesheet.\n\n<h5>Build and minify stylesheet</h5>\n<p>\n    Run the following command to build the resulting file <code>devicon.min.css</code>\n</p>\n\n```bash\nnpm run build-css\n```\n\n<br />\n<div align=\"center\">\n    <img src=\"https://forthebadge.com/images/badges/built-with-love.svg\" />\n    <img src=\"https://forthebadge.com/images/badges/built-by-developers.svg\" />\n</div>\n"
 },
 {
  "repo": "sqlalchemy/sqlalchemy",
  "language": "Python",
  "readme_contents": "========================\nDeveloping new Dialects\n========================\n\n.. note::\n\n   When studying this file, it's probably a good idea to also\n   familiarize with the  README.unittests.rst file, which discusses\n   SQLAlchemy's usage and extension of the pytest test runner.\n\nWhile SQLAlchemy includes many dialects within the core distribution, the\ntrend for new dialects should be that they are published as external\nprojects.   SQLAlchemy has since version 0.5 featured a \"plugin\" system\nwhich allows external dialects to be integrated into SQLAlchemy using\nstandard setuptools entry points.  As of version 0.8, this system has\nbeen enhanced, so that a dialect can also be \"plugged in\" at runtime.\n\nOn the testing side, SQLAlchemy includes a \"dialect compliance\nsuite\" that is usable by third party libraries, in the source tree\nat ``lib/sqlalchemy/testing/suite``.   There's no need for a third party\ndialect to run through SQLAlchemy's full testing suite, as a large portion of\nthese tests do not have dialect-sensitive functionality.  The \"dialect\ncompliance suite\" should be viewed as the primary target for new dialects.\n\n\nDialect Layout\n===============\n\nThe file structure of a dialect is typically similar to the following::\n\n    sqlalchemy-<dialect>/\n                         setup.py\n                         setup.cfg\n                         sqlalchemy_<dialect>/\n                                              __init__.py\n                                              base.py\n                                              <dbapi>.py\n                                              requirements.py\n                         test/\n                                              __init__.py\n                                              conftest.py\n                                              test_suite.py\n                                              test_<dialect_specific_test>.py\n                                              ...\n\nAn example of this structure can be seen in the MS Access dialect at\nhttps://github.com/gordthompson/sqlalchemy-access .\n\nKey aspects of this file layout include:\n\n* setup.py - should specify setuptools entrypoints, allowing the\n  dialect to be usable from create_engine(), e.g.::\n\n        entry_points = {\n         'sqlalchemy.dialects': [\n              'access.pyodbc = sqlalchemy_access.pyodbc:AccessDialect_pyodbc',\n              ]\n        }\n\n  Above, the entrypoint ``access.pyodbc`` allow URLs to be used such as::\n\n    create_engine(\"access+pyodbc://user:pw@dsn\")\n\n* setup.cfg - this file contains the traditional contents such as\n  [tool:pytest] directives, but also contains new directives that are used\n  by SQLAlchemy's testing framework.  E.g. for Access::\n\n    [tool:pytest]\n    addopts= --tb native -v -r fxX --maxfail=25 -p no:warnings\n    python_files=test/*test_*.py\n\n    [sqla_testing]\n    requirement_cls=sqlalchemy_access.requirements:Requirements\n    profile_file=test/profiles.txt\n\n    [db]\n    default=access+pyodbc://admin@access_test\n    sqlite=sqlite:///:memory:\n\n  Above, the ``[sqla_testing]`` section contains configuration used by\n  SQLAlchemy's test plugin.  The ``[tool:pytest]`` section\n  include directives to help with these runners.  When using pytest\n  the test/conftest.py file will bootstrap SQLAlchemy's plugin.\n\n* test/conftest.py - This script bootstraps SQLAlchemy's pytest plugin\n  into the pytest runner.  This\n  script can also be used to install your third party dialect into\n  SQLAlchemy without using the setuptools entrypoint system; this allows\n  your dialect to be present without any explicit setup.py step needed.\n  The other portion invokes SQLAlchemy's pytest plugin::\n\n    from sqlalchemy.dialects import registry\n    import pytest\n\n    registry.register(\"access.pyodbc\", \"sqlalchemy_access.pyodbc\", \"AccessDialect_pyodbc\")\n\n    pytest.register_assert_rewrite(\"sqlalchemy.testing.assertions\")\n\n    from sqlalchemy.testing.plugin.pytestplugin import *\n\n  Where above, the ``registry`` module, introduced in SQLAlchemy 0.8, provides\n  an in-Python means of installing the dialect entrypoint(s) without the use\n  of setuptools, using the ``registry.register()`` function in a way that\n  is similar to the ``entry_points`` directive we placed in our ``setup.py``.\n  (The ``pytest.register_assert_rewrite`` is there just to suppress a spurious\n  warning from pytest.)\n\n* requirements.py - The ``requirements.py`` file is where directives\n  regarding database and dialect capabilities are set up.\n  SQLAlchemy's tests are often annotated with decorators   that mark\n  tests as \"skip\" or \"fail\" for particular backends.  Over time, this\n  system   has been refined such that specific database and DBAPI names\n  are mentioned   less and less, in favor of @requires directives which\n  state a particular capability.   The requirement directive is linked\n  to target dialects using a ``Requirements`` subclass.   The custom\n  ``Requirements`` subclass is specified in the ``requirements.py`` file\n  and   is made available to SQLAlchemy's test runner using the\n  ``requirement_cls`` directive   inside the ``[sqla_testing]`` section.\n\n  For a third-party dialect, the custom ``Requirements`` class can\n  usually specify a simple yes/no answer for a particular system. For\n  example, a requirements file that specifies a database that supports\n  the RETURNING construct but does not support nullable boolean\n  columns might look like this::\n\n      # sqlalchemy_access/requirements.py\n\n      from sqlalchemy.testing.requirements import SuiteRequirements\n\n      from sqlalchemy.testing import exclusions\n\n      class Requirements(SuiteRequirements):\n          @property\n          def nullable_booleans(self):\n              \"\"\"Target database allows boolean columns to store NULL.\"\"\"\n              # Access Yes/No doesn't allow null\n              return exclusions.closed()\n\n          @property\n          def returning(self):\n              return exclusions.open()\n\n  The ``SuiteRequirements`` class in\n  ``sqlalchemy.testing.requirements`` contains a large number of\n  requirements rules, which attempt to have reasonable defaults. The\n  tests will report on those requirements found as they are run.\n\n  The requirements system can also be used when running SQLAlchemy's\n  primary test suite against the external dialect.  In this use case,\n  a ``--dburi`` as well as a ``--requirements`` flag are passed to SQLAlchemy's\n  test runner so that exclusions specific to the dialect take place::\n\n    cd /path/to/sqlalchemy\n    pytest -v \\\n      --requirements sqlalchemy_access.requirements:Requirements \\\n      --dburi access+pyodbc://admin@access_test\n\n* test_suite.py - Finally, the ``test_suite.py`` module represents a\n  stub test suite, which pulls in the actual SQLAlchemy test suite.\n  To pull in the suite as a whole, it can   be imported in one step::\n\n      # test/test_suite.py\n\n      from sqlalchemy.testing.suite import *\n\n  That's all that's needed - the ``sqlalchemy.testing.suite`` package\n  contains an ever expanding series of tests, most of which should be\n  annotated with specific requirement decorators so that they can be\n  fully controlled.  In the case that the decorators are not covering\n  a particular test, a test can also be directly modified or bypassed.\n  In the example below, the Access dialect test suite overrides the\n  ``get_huge_int()`` test::\n\n      from sqlalchemy.testing.suite import *\n\n      from sqlalchemy.testing.suite import IntegerTest as _IntegerTest\n\n      class IntegerTest(_IntegerTest):\n\n          @testing.skip(\"access\")\n          def test_huge_int(self):\n              # bypass this test because Access ODBC fails with\n              # [ODBC Microsoft Access Driver] Optional feature not implemented.\n              return\n\nAsyncIO dialects\n----------------\n\nAs of version 1.4 SQLAlchemy supports also dialects that use\nasyncio drivers to interface with the database backend.\n\nSQLAlchemy's approach to asyncio drivers is that the connection and cursor\nobjects of the driver (if any) are adapted into a pep-249 compliant interface,\nusing the ``AdaptedConnection`` interface class. Refer to the internal asyncio\ndriver implementations such as that of ``asyncpg``, ``asyncmy`` and\n``aiosqlite`` for examples.\n\nGoing Forward\n==============\n\nThe third-party dialect can be distributed like any other Python\nmodule on PyPI. Links to prominent dialects can be featured within\nSQLAlchemy's own documentation; contact the developers (see AUTHORS)\nfor help with this.\n\nWhile SQLAlchemy includes many dialects built in, it remains to be\nseen if the project as a whole might move towards \"plugin\" model for\nall dialects, including all those currently built in.  Now that\nSQLAlchemy's dialect API is mature and the test suite is not far\nbehind, it may be that a better maintenance experience can be\ndelivered by having all dialects separately maintained and released.\n\nAs new versions of SQLAlchemy are released, the test suite and\nrequirements file will receive new tests and changes.  The dialect\nmaintainer would normally keep track of these changes and make\nadjustments as needed.\n\n"
 },
 {
  "repo": "worldveil/dejavu",
  "language": "Python",
  "readme_contents": "dejavu\n==========\n\nAudio fingerprinting and recognition algorithm implemented in Python, see the explanation here:  \n[How it works](http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/)\n\nDejavu can memorize audio by listening to it once and fingerprinting it. Then by playing a song and recording microphone input or reading from disk, Dejavu attempts to match the audio against the fingerprints held in the database, returning the song being played. \n\nNote: for voice recognition, *Dejavu is not the right tool!* Dejavu excels at recognition of exact signals with reasonable amounts of noise.\n\n## Quickstart with Docker\n\nFirst, install [Docker](https://docs.docker.com/get-docker/).\n\n```shell\n# build and then run our containers\n$ docker-compose build\n$ docker-compose up -d\n\n# get a shell inside the container\n$ docker-compose run python /bin/bash\nStarting dejavu_db_1 ... done\nroot@f9ea95ce5cea:/code# python example_docker_postgres.py \nFingerprinting channel 1/2 for test/woodward_43s.wav\nFingerprinting channel 1/2 for test/sean_secs.wav\n...\n\n# connect to the database and poke around\nroot@f9ea95ce5cea:/code# psql -h db -U postgres dejavu\nPassword for user postgres:  # type \"password\", as specified in the docker-compose.yml !\npsql (11.7 (Debian 11.7-0+deb10u1), server 10.7)\nType \"help\" for help.\n\ndejavu=# \\dt\n            List of relations\n Schema |     Name     | Type  |  Owner   \n--------+--------------+-------+----------\n public | fingerprints | table | postgres\n public | songs        | table | postgres\n(2 rows)\n\ndejavu=# select * from fingerprints limit 5;\n          hash          | song_id | offset |        date_created        |       date_modified        \n------------------------+---------+--------+----------------------------+----------------------------\n \\x71ffcb900d06fe642a18 |       1 |    137 | 2020-06-03 05:14:19.400153 | 2020-06-03 05:14:19.400153\n \\xf731d792977330e6cc9f |       1 |    148 | 2020-06-03 05:14:19.400153 | 2020-06-03 05:14:19.400153\n \\x71ff24aaeeb55d7b60c4 |       1 |    146 | 2020-06-03 05:14:19.400153 | 2020-06-03 05:14:19.400153\n \\x29349c79b317d45a45a8 |       1 |    101 | 2020-06-03 05:14:19.400153 | 2020-06-03 05:14:19.400153\n \\x5a052144e67d2248ccf4 |       1 |    123 | 2020-06-03 05:14:19.400153 | 2020-06-03 05:14:19.400153\n(10 rows)\n\n# then to shut it all down...\n$ docker-compose down\n```\n\nIf you want to be able to use the microphone with the Docker container, you'll need to do a [little extra work](https://stackoverflow.com/questions/43312975/record-sound-on-ubuntu-docker-image). I haven't had the time to write this up, but if anyone wants to make a PR, I'll happily merge.\n\n## Docker alternative on local machine\n\nFollow instructions in [INSTALLATION.md](INSTALLATION.md)\n\nNext, you'll need to create a MySQL database where Dejavu can store fingerprints. For example, on your local setup:\n\t\n\t$ mysql -u root -p\n\tEnter password: **********\n\tmysql> CREATE DATABASE IF NOT EXISTS dejavu;\n\nNow you're ready to start fingerprinting your audio collection! \n\nYou may also use Postgres, of course. The same method applies.\n\n## Fingerprinting\n\nLet's say we want to fingerprint all of July 2013's VA US Top 40 hits. \n\nStart by creating a Dejavu object with your configurations settings (Dejavu takes an ordinary Python dictionary for the settings).\n\n```python\n>>> from dejavu import Dejavu\n>>> config = {\n...     \"database\": {\n...         \"host\": \"127.0.0.1\",\n...         \"user\": \"root\",\n...         \"password\": <password above>, \n...         \"database\": <name of the database you created above>,\n...     }\n... }\n>>> djv = Dejavu(config)\n```\n\nNext, give the `fingerprint_directory` method three arguments:\n* input directory to look for audio files\n* audio extensions to look for in the input directory\n* number of processes (optional)\n\n```python\n>>> djv.fingerprint_directory(\"va_us_top_40/mp3\", [\".mp3\"], 3)\n```\n\nFor a large amount of files, this will take a while. However, Dejavu is robust enough you can kill and restart without affecting progress: Dejavu remembers which songs it fingerprinted and converted and which it didn't, and so won't repeat itself. \n\nYou'll have a lot of fingerprints once it completes a large folder of mp3s:\n```python\n>>> print djv.db.get_num_fingerprints()\n5442376\n```\n\nAlso, any subsequent calls to `fingerprint_file` or `fingerprint_directory` will fingerprint and add those songs to the database as well. It's meant to simulate a system where as new songs are released, they are fingerprinted and added to the database seemlessly without stopping the system. \n\n## Configuration options\n\nThe configuration object to the Dejavu constructor must be a dictionary. \n\nThe following keys are mandatory:\n\n* `database`, with a value as a dictionary with keys that the database you are using will accept. For example with MySQL, the keys must can be anything that the [`MySQLdb.connect()`](http://mysql-python.sourceforge.net/MySQLdb.html) function will accept. \n\nThe following keys are optional:\n\n* `fingerprint_limit`: allows you to control how many seconds of each audio file to fingerprint. Leaving out this key, or alternatively using `-1` and `None` will cause Dejavu to fingerprint the entire audio file. Default value is `None`.\n* `database_type`: `mysql` (the default value) and `postgres` are supported. If you'd like to add another subclass for `BaseDatabase` and implement a new type of database, please fork and send a pull request!\n\nAn example configuration is as follows:\n\n```python\n>>> from dejavu import Dejavu\n>>> config = {\n...     \"database\": {\n...         \"host\": \"127.0.0.1\",\n...         \"user\": \"root\",\n...         \"password\": \"Password123\", \n...         \"database\": \"dejavu_db\",\n...     },\n...     \"database_type\" : \"mysql\",\n...     \"fingerprint_limit\" : 10\n... }\n>>> djv = Dejavu(config)\n```\n\n## Tuning\n\nInside `config/settings.py`, you may want to adjust following parameters (some values are given below).\n\n    FINGERPRINT_REDUCTION = 30\n    PEAK_SORT = False\n    DEFAULT_OVERLAP_RATIO = 0.4\n    DEFAULT_FAN_VALUE = 5\n    DEFAULT_AMP_MIN = 10\n    PEAK_NEIGHBORHOOD_SIZE = 10\n    \nThese parameters are described within the file in detail. Read that in-order to understand the impact of changing these values.\n\n## Recognizing\n\nThere are two ways to recognize audio using Dejavu. You can recognize by reading and processing files on disk, or through your computer's microphone.\n\n### Recognizing: On Disk\n\nThrough the terminal:\n\n```bash\n$ python dejavu.py --recognize file sometrack.wav \n{'total_time': 2.863781690597534, 'fingerprint_time': 2.4306554794311523, 'query_time': 0.4067542552947998, 'align_time': 0.007731199264526367, 'results': [{'song_id': 1, 'song_name': 'Taylor Swift - Shake It Off', 'input_total_hashes': 76168, 'fingerprinted_hashes_in_db': 4919, 'hashes_matched_in_input': 794, 'input_confidence': 0.01, 'fingerprinted_confidence': 0.16, 'offset': -924, 'offset_seconds': -30.00018, 'file_sha1': b'3DC269DF7B8DB9B30D2604DA80783155912593E8'}, {...}, ...]}\n```\n\nor in scripting, assuming you've already instantiated a Dejavu object: \n\n```python\n>>> from dejavu.logic.recognizer.file_recognizer import FileRecognizer\n>>> song = djv.recognize(FileRecognizer, \"va_us_top_40/wav/Mirrors - Justin Timberlake.wav\")\n```\n\n### Recognizing: Through a Microphone\n\nWith scripting:\n\n```python\n>>> from dejavu.logic.recognizer.microphone_recognizer import MicrophoneRecognizer\n>>> song = djv.recognize(MicrophoneRecognizer, seconds=10) # Defaults to 10 seconds.\n```\n\nand with the command line script, you specify the number of seconds to listen:\n\n```bash\n$ python dejavu.py --recognize mic 10\n```\n\n## Testing\n\nTesting out different parameterizations of the fingerprinting algorithm is often useful as the corpus becomes larger and larger, and inevitable tradeoffs between speed and accuracy come into play. \n\n![Confidence](plots/confidence.png)\n\nTest your Dejavu settings on a corpus of audio files on a number of different metrics:\n\n* Confidence of match (number fingerprints aligned)\n* Offset matching accuracy\n* Song matching accuracy\n* Time to match\n\n![Accuracy](plots/matching_graph.png)\n\nAn example script is given in `test_dejavu.sh`, shown below:\n\n```bash\n#####################################\n### Dejavu example testing script ###\n#####################################\n\n###########\n# Clear out previous results\nrm -rf ./results ./temp_audio\n\n###########\n# Fingerprint files of extension mp3 in the ./mp3 folder\npython dejavu.py --fingerprint ./mp3/ mp3\n\n##########\n# Run a test suite on the ./mp3 folder by extracting 1, 2, 3, 4, and 5 \n# second clips sampled randomly from within each song 8 seconds \n# away from start or end, sampling offset with random seed = 42, and finally, \n# store results in ./results and log to ./results/dejavu-test.log\npython run_tests.py \\\n    --secs 5 \\\n    --temp ./temp_audio \\\n    --log-file ./results/dejavu-test.log \\\n    --padding 8 \\\n    --seed 42 \\\n    --results ./results \\\n    ./mp3\n```\n\nThe testing scripts are as of now are a bit rough, and could certainly use some love and attention if you're interested in submitting a PR! For example, underscores in audio filenames currently [breaks](https://github.com/worldveil/dejavu/issues/63) the test scripts. \n\n## How does it work?\n\nThe algorithm works off a fingerprint based system, much like:\n\n* [Shazam](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)\n* [MusicRetrieval](http://www.cs.cmu.edu/~yke/musicretrieval/)\n* [Chromaprint](https://oxygene.sk/2011/01/how-does-chromaprint-work/)\n\nThe \"fingerprints\" are locality sensitive hashes that are computed from the spectrogram of the audio. This is done by taking the FFT of the signal over overlapping windows of the song and identifying peaks. A very robust peak finding algorithm is needed, otherwise you'll have a terrible signal to noise ratio.\n\nHere I've taken the spectrogram over the first few seconds of \"Blurred Lines\". The spectrogram is a 2D plot and shows amplitude as a function of time (a particular window, actually) and frequency, binned logrithmically, just as the human ear percieves it. In the plot below you can see where local maxima occur in the amplitude space:\n\n![Spectrogram](plots/spectrogram_peaks.png)\n\nFinding these local maxima is a combination of a high pass filter (a threshold in amplitude space) and some image processing techniques to find maxima. A concept of a \"neighboorhood\" is needed - a local maxima with only its directly adjacent pixels is a poor peak - one that will not survive the noise of coming through speakers and through a microphone.\n\nIf we zoom in even closer, we can begin to imagine how to bin and discretize these peaks. Finding the peaks itself is the most computationally intensive part, but it's not the end. Peaks are combined using their discrete time and frequency bins to create a unique hash for that particular moment in the song - creating a fingerprint.\n\n![Spectgram zoomed](plots/spectrogram_zoomed.png)\n\nFor a more detailed look at the making of Dejavu, see my blog post [here](https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/).\n\n## How well it works\n\nTo truly get the benefit of an audio fingerprinting system, it can't take a long time to fingerprint. It's a bad user experience, and furthermore, a user may only decide to try to match the song with only a few precious seconds of audio left before the radio station goes to a commercial break.\n\nTo test Dejavu's speed and accuracy, I fingerprinted a list of 45 songs from the US VA Top 40 from July 2013 (I know, their counting is off somewhere). I tested in three ways:\n\n1. Reading from disk the raw mp3 -> wav data, and\n1. Playing the song over the speakers with Dejavu listening on the laptop microphone.\n1. Compressed streamed music played on my iPhone\n\nBelow are the results.\n\n### 1. Reading from Disk\n\nReading from disk was an overwhelming 100% recall - no mistakes were made over the 45 songs I fingerprinted. Since Dejavu gets all of the samples from the song (without noise), it would be nasty surprise if reading the same file from disk didn't work every time!\n\n### 2. Audio over laptop microphone\n\nHere I wrote a script to randomly chose `n` seconds of audio from the original mp3 file to play and have Dejavu listen over the microphone. To be fair I only allowed segments of audio that were more than 10 seconds from the starting/ending of the track to avoid listening to silence. \n\nAdditionally my friend was even talking and I was humming along a bit during the whole process, just to throw in some noise.\n\nHere are the results for different values of listening time (`n`):\n\n![Matching time](plots/accuracy.png)\n\nThis is pretty rad. For the percentages:\n\nNumber of Seconds | Number Correct | Percentage Accuracy\n----|----|----\n1 | 27 / 45 | 60.0%\n2 | 43 / 45 | 95.6%\n3 | 44 / 45 | 97.8%\n4 | 44 / 45 | 97.8%\n5 | 45 / 45 | 100.0%\n6 | 45 / 45 | 100.0%\n\nEven with only a single second, randomly chosen from anywhere in the song, Dejavu is getting 60%! One extra second to 2 seconds get us to around 96%, while getting perfect only took 5 seconds or more. Honestly when I was testing this myself, I found Dejavu beat me - listening to only 1-2 seconds of a song out of context to identify is pretty hard. I had even been listening to these same songs for two days straight while debugging...\n\nIn conclusion, Dejavu works amazingly well, even with next to nothing to work with. \n\n### 3. Compressed streamed music played on my iPhone\n\nJust to try it out, I tried playing music from my Spotify account (160 kbit/s compressed) through my iPhone's speakers with Dejavu again listening on my MacBook mic. I saw no degredation in performance; 1-2 seconds was enough to recognize any of the songs.\n\n## Performance\n\n### Speed\n\nOn my MacBook Pro, matching was done at 3x listening speed with a small constant overhead. To test, I tried different recording times and plotted the recording time plus the time to match. Since the speed is mostly invariant of the particular song and more dependent on the length of the spectrogram created, I tested on a single song, \"Get Lucky\" by Daft Punk:\n\n![Matching time](plots/matching_time.png)\n\nAs you can see, the relationship is quite linear. The line you see is a least-squares linear regression fit to the data, with the corresponding line equation:\n\n    1.364757 * record_time - 0.034373 = time_to_match\n    \nNotice of course since the matching itself is single threaded, the matching time includes the recording time. This makes sense with the 3x speed in purely matching, as:\n    \n    1 (recording) + 1/3 (matching) = 4/3 ~= 1.364757\n    \nif we disregard the miniscule constant term.\n\nThe overhead of peak finding is the bottleneck - I experimented with multithreading and realtime matching, and alas, it wasn't meant to be in Python. An equivalent Java or C/C++ implementation would most likely have little trouble keeping up, applying FFT and peakfinding in realtime.\n\nAn important caveat is of course, the round trip time (RTT) for making matches. Since my MySQL instance was local, I didn't have to deal with the latency penalty of transfering fingerprint matches over the air. This would add RTT to the constant term in the overall calculation, but would not effect the matching process. \n\n### Storage\n\nFor the 45 songs I fingerprinted, the database used 377 MB of space for 5.4 million fingerprints. In comparison, the disk usage is given below:\n\nAudio Information Type | Storage in MB \n----|----\nmp3 | 339\nwav | 1885\nfingerprints | 377\n\nThere's a pretty direct trade-off between the necessary record time and the amount of storage needed. Adjusting the amplitude threshold for peaks and the fan value for fingerprinting will add more fingerprints and bolster the accuracy at the expense of more space. "
 },
 {
  "repo": "aymericdamien/TopDeepLearning",
  "language": "Python",
  "readme_contents": "# Top Deep Learning Projects\nA list of popular github projects related to deep learning (ranked by stars).\n\nLast Update: 2020.07.09\n| Project Name | Stars | Description |\n| ------- | ------ | ------ |\n|[tensorflow](https://github.com/tensorflow/tensorflow)|146k|An Open Source Machine Learning Framework for Everyone|\n|[keras](https://github.com/keras-team/keras)|48.9k|Deep Learning for humans|\n|[opencv](https://github.com/opencv/opencv)|46.1k|Open Source Computer Vision Library|\n|[pytorch](https://github.com/pytorch/pytorch)|40k|Tensors and Dynamic neural networks in Python with strong GPU acceleration|\n|[TensorFlow-Examples](https://github.com/aymericdamien/TensorFlow-Examples)|38.1k|TensorFlow Tutorial and Examples for Beginners (support TF v1 & v2)|\n|[tesseract](https://github.com/tesseract-ocr/tesseract)|35.3k|Tesseract Open Source OCR Engine (main repository)|\n|[face_recognition](https://github.com/ageitgey/face_recognition)|35.2k|The world's simplest facial recognition api for Python and the command line|\n|[faceswap](https://github.com/deepfakes/faceswap)|31.4k|Deepfakes Software For All|\n|[transformers](https://github.com/huggingface/transformers)|30.4k|\ud83e\udd17Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.|\n|[100-Days-Of-ML-Code](https://github.com/Avik-Jain/100-Days-Of-ML-Code)|29.1k|100 Days of ML Coding|\n|[julia](https://github.com/JuliaLang/julia)|28.1k|The Julia Language: A fresh approach to technical computing.|\n|[gold-miner](https://github.com/xitu/gold-miner)|26.6k|\ud83e\udd47\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212\uff0c\u53ef\u80fd\u662f\u4e16\u754c\u6700\u5927\u6700\u597d\u7684\u82f1\u8bd1\u4e2d\u6280\u672f\u793e\u533a\uff0c\u6700\u61c2\u8bfb\u8005\u548c\u8bd1\u8005\u7684\u7ffb\u8bd1\u5e73\u53f0\uff1a|\n|[awesome-scalability](https://github.com/binhnguyennus/awesome-scalability)|26.6k|The Patterns of Scalable, Reliable, and Performant Large-Scale Systems|\n|[basics](https://github.com/madewithml/basics)|24.5k|\ud83d\udcda Learn ML with clean code, simplified math and illustrative visuals.|\n|[bert](https://github.com/google-research/bert)|23.9k|TensorFlow code and pre-trained models for BERT|\n|[funNLP](https://github.com/fighting41love/funNLP)|22.1k|(Machine Learning)NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\u3001nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0\u67e5\u3001XLM\uff1aFace\u2026|\n|[xgboost](https://github.com/dmlc/xgboost)|19.4k|Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, for Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink and DataFlow|\n|[Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)|18.4k|Clone a voice in 5 seconds to generate arbitrary speech in real-time|\n|[d2l-zh](https://github.com/d2l-ai/d2l-zh)|17.9k|\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\uff1a\u9762\u5411\u4e2d\u6587\u8bfb\u8005\u3001\u80fd\u8fd0\u884c\u3001\u53ef\u8ba8\u8bba\u3002\u82f1\u6587\u7248\u5373\u4f2f\u514b\u5229\u201c\u6df1\u5ea6\u5b66\u4e60\u5bfc\u8bba\u201d\u6559\u6750\u3002|\n|[openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)|17.8k|OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation|\n|[Coursera-ML-AndrewNg-Notes](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)|17.7k|\u5434\u6069\u8fbe\u8001\u5e08\u7684\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u4e2a\u4eba\u7b14\u8bb0|\n|[DeepFaceLab](https://github.com/iperov/DeepFaceLab)|17.3k|DeepFaceLab is the leading software for creating deepfakes.|\n|[pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)|17.3k|PyTorch Tutorial for Deep Learning Researchers|\n|[Mask_RCNN](https://github.com/matterport/Mask_RCNN)|17.2k|Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow|\n|[spaCy](https://github.com/explosion/spaCy)|16.8k|\ud83d\udcab Industrial-strength Natural Language Processing (NLP) with Python and Cython|\n|[NLP-progress](https://github.com/sebastianruder/NLP-progress)|16.2k|Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.|\n|[100-Days-Of-ML-Code](https://github.com/MLEveryday/100-Days-Of-ML-Code)|15.6k|100-Days-Of-ML-Code\u4e2d\u6587\u7248|\n|[cs-video-courses](https://github.com/Developer-Y/cs-video-courses)|14.9k|List of Computer Science courses with video lectures.|\n|[WaveFunctionCollapse](https://github.com/mxgmn/WaveFunctionCollapse)|14.7k|Bitmap & tilemap generation from a single example with the help of ideas from quantum mechanics.|\n|[lectures](https://github.com/oxford-cs-deepnlp-2017/lectures)|14.7k|Oxford Deep NLP 2017 course|\n|[reinforcement-learning](https://github.com/dennybritz/reinforcement-learning)|14.7k|Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accom\u2026|\n|[pwc](https://github.com/zziz/pwc)|14.7k|Papers with code. Sorted by stars. Updated weekly.|\n|[TensorFlow-Course](https://github.com/machinelearningmindset/TensorFlow-Course)|14.6k|Simple and ready-to-use tutorials for TensorFlow|\n|[DeepSpeech](https://github.com/mozilla/DeepSpeech)|14.4k|A TensorFlow implementation of Baidu's DeepSpeech architecture|\n|[pumpkin-book](https://github.com/datawhalechina/pumpkin-book)|14k|\u300a\u673a\u5668\u5b66\u4e60\u300b\uff08\u897f\u74dc\u4e66\uff09\u516c\u5f0f\u63a8\u5bfc\u89e3\u6790\uff0c\u5728\u7ebf\u9605\u8bfb\u5730\u5740\uff1ahttps://datawhalechina.github.io/pumpkin-book|\n|[tfjs](https://github.com/tensorflow/tfjs)|13.5k|A WebGL accelerated JavaScript library for training and deploying ML models.|\n|[examples](https://github.com/pytorch/examples)|13.5k|A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.|\n|[openface](https://github.com/cmusatyalab/openface)|13.5k|Face recognition with deep neural networks.|\n|[Qix](https://github.com/ty4z2008/Qix)|13.3k|Machine Learning\u3001Deep Learning\u3001PostgreSQL\u3001Distributed System\u3001Node.Js\u3001Golang|\n|[spleeter](https://github.com/deezer/spleeter)|12.7k|Deezer source separation library including pretrained models.|\n|[Virgilio](https://github.com/virgili0/Virgilio)|12.7k|Your new Mentor for Data Science E-Learning.|\n|[nndl.github.io](https://github.com/nndl/nndl.github.io)|12.7k|\u300a\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5b66\u4e60\u300b \u90b1\u9521\u9e4f\u8457 Neural Network and Deep Learning|\n|[Screenshot-to-code](https://github.com/emilwallner/Screenshot-to-code)|12.7k|A neural network that transforms a design mock-up into a static website.|\n|[pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)|12.4k|Image-to-Image Translation in PyTorch|\n|[pytorch-handbook](https://github.com/zergtant/pytorch-handbook)|11.9k|pytorch handbook\u662f\u4e00\u672c\u5f00\u6e90\u7684\u4e66\u7c4d\uff0c\u76ee\u6807\u662f\u5e2e\u52a9\u90a3\u4e9b\u5e0c\u671b\u548c\u4f7f\u7528PyTorch\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u5f00\u53d1\u548c\u7814\u7a76\u7684\u670b\u53cb\u5feb\u901f\u5165\u95e8\uff0c\u5176\u4e2d\u5305\u542b\u7684Pytorch\u6559\u7a0b\u5168\u90e8\u901a\u8fc7\u6d4b\u8bd5\u4fdd\u8bc1\u53ef\u4ee5\u6210\u529f\u8fd0\u884c|\n|[gun](https://github.com/amark/gun)|11.9k|An open source cybersecurity protocol for syncing decentralized graph data.|\n|[Paddle](https://github.com/PaddlePaddle/Paddle)|11.8k|PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice \uff08\u300e\u98de\u6868\u300f\u6838\u5fc3\u6846\u67b6\uff0c\u6df1\u5ea6\u5b66\u4e60&\u673a\u5668\u5b66\u4e60\u9ad8\u6027\u80fd\u5355\u673a\u3001\u5206\u5e03\u5f0f\u8bad\u2026|\n|[tensorflow-zh](https://github.com/jikexueyuanwiki/tensorflow-zh)|11.8k|\u8c37\u6b4c\u5168\u65b0\u5f00\u6e90\u4eba\u5de5\u667a\u80fd\u7cfb\u7edfTensorFlow\u5b98\u65b9\u6587\u6863\u4e2d\u6587\u7248|\n|[darknet](https://github.com/AlexeyAB/darknet)|11.4k|YOLOv4 - Neural Networks for Object Detection (Windows and Linux version of Darknet )|\n|[learnopencv](https://github.com/spmallick/learnopencv)|11.4k|Learn OpenCV : C++ and Python Examples|\n|[neural-networks-and-deep-learning](https://github.com/mnielsen/neural-networks-and-deep-learning)|11.3k|Code samples for my book \"Neural Networks and Deep Learning\"|\n|[google-research](https://github.com/google-research/google-research)|11.2k|Google Research|\n|[labelImg](https://github.com/tzutalin/labelImg)|11.2k|\ud83d\udd8d\ufe0f LabelImg is a graphical image annotation tool and label object bounding boxes in images|\n|[gensim](https://github.com/RaRe-Technologies/gensim)|11k|Topic Modelling for Humans|\n|[pix2code](https://github.com/tonybeltramelli/pix2code)|10.9k|pix2code: Generating Code from a Graphical User Interface Screenshot|\n|[facenet](https://github.com/davidsandberg/facenet)|10.8k|Face recognition using Tensorflow|\n|[DeOldify](https://github.com/jantic/DeOldify)|10.7k|A Deep Learning based project for colorizing and restoring old images (and video!)|\n|[python-machine-learning-book](https://github.com/rasbt/python-machine-learning-book)|10.7k|The \"Python Machine Learning (1st edition)\" book code repository and info resource|\n|[stanford-cs-229-machine-learning](https://github.com/afshinea/stanford-cs-229-machine-learning)|10.6k|VIP cheatsheets for Stanford's CS 229 Machine Learning|\n|[mmdetection](https://github.com/open-mmlab/mmdetection)|10.5k|OpenMMLab Detection Toolbox and Benchmark|\n|[face-api.js](https://github.com/justadudewhohacks/face-api.js)|10.4k|JavaScript API for face detection and face recognition in the browser and nodejs with tensorflow.js|\n|[Awesome-pytorch-list](https://github.com/bharathgs/Awesome-pytorch-list)|10.4k|A comprehensive list of pytorch related content on github,such as different models,implementations,helper libraries,t\u2026|\n|[nsfw_data_scraper](https://github.com/alex000kim/nsfw_data_scraper)|10.2k|Collection of scripts to aggregate image data for the purposes of training an NSFW Image Classifier|\n|[convnetjs](https://github.com/karpathy/convnetjs)|10k|Deep Learning in Javascript. Train Convolutional Neural Networks (or ordinary ones) in your browser.|\n|[CycleGAN](https://github.com/junyanz/CycleGAN)|9.8k|Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.|\n|[streamlit](https://github.com/streamlit/streamlit)|9.8k|Streamlit \u2014 The fastest way to build data apps in Python|\n|[DeepCreamPy](https://github.com/deeppomf/DeepCreamPy)|9.7k|Decensoring Hentai with Deep Neural Networks|\n|[stylegan](https://github.com/NVlabs/stylegan)|9.7k|StyleGAN - Official TensorFlow Implementation|\n|[Dive-into-DL-PyTorch](https://github.com/ShusenTang/Dive-into-DL-PyTorch)|9.6k|\u672c\u9879\u76ee\u5c06\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b(Dive into Deep Learning)\u539f\u4e66\u4e2d\u7684MXNet\u5b9e\u73b0\u6539\u4e3aPyTorch\u5b9e\u73b0\u3002|\n|[stanford-tensorflow-tutorials](https://github.com/chiphuyen/stanford-tensorflow-tutorials)|9.6k|This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research.|\n|[horovod](https://github.com/horovod/horovod)|9.6k|Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.|\n|[Deep-Learning-with-TensorFlow-book](https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book)|9.4k|\u6df1\u5ea6\u5b66\u4e60\u5165\u95e8\u5f00\u6e90\u4e66\uff0c\u57fa\u4e8eTensorFlow 2.0\u6848\u4f8b\u5b9e\u6218\u3002Open source Deep Learning book, based on TensorFlow 2.0 framework.|\n|[neural-doodle](https://github.com/alexjc/neural-doodle)|9.4k|Turn your two-bit doodles into fine artworks with deep neural networks, generate seamless textures from photos, transfer style from one image to another, perform example-based upscaling, but wait... there's more! (An implementation of Semantic Style Transfer.)|\n|[caire](https://github.com/esimov/caire)|9.3k|Content aware image resize library|\n|[fast-style-transfer](https://github.com/lengstrom/fast-style-transfer)|9.2k|TensorFlow CNN for fast style transfer \u26a1\ud83d\udda5\ud83c\udfa8\ud83d\uddbc|\n|[ncnn](https://github.com/Tencent/ncnn)|9.2k|ncnn is a high-performance neural network inference framework optimized for the mobile platform|\n|[kubeflow](https://github.com/kubeflow/kubeflow)|9.1k|Machine Learning Toolkit for Kubernetes|\n|[nltk](https://github.com/nltk/nltk)|9k|NLTK Source|\n|[flair](https://github.com/flairNLP/flair)|9k|A very simple framework for state-of-the-art Natural Language Processing (NLP)|\n|[ml-agents](https://github.com/Unity-Technologies/ml-agents)|9k|Unity Machine Learning Agents Toolkit|\n|[allennlp](https://github.com/allenai/allennlp)|8.8k|An open-source NLP research library, built on PyTorch.|\n|[botpress](https://github.com/botpress/botpress)|8.8k|\ud83e\udd16 The Conversational Platform with built-in language understanding (NLU), beautiful graphical interface and Dialog Manager (DM). Easily create chatbots and AI-based virtual assistants.|\n|[the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)|8.7k|A list of all named GANs!|\n|[EffectiveTensorflow](https://github.com/vahidk/EffectiveTensorflow)|8.6k|TensorFlow tutorials and best practices.|\n|[tfjs-core](https://github.com/tensorflow/tfjs-core)|8.5k|WebGL-accelerated ML // linear algebra // automatic differentiation for JavaScript.|\n|[fairseq](https://github.com/pytorch/fairseq)|8.4k|Facebook AI Research Sequence-to-Sequence Toolkit written in Python.|\n|[sonnet](https://github.com/deepmind/sonnet)|8.4k|TensorFlow-based neural network library|\n|[mit-deep-learning-book-pdf](https://github.com/janishar/mit-deep-learning-book-pdf)|8.3k|MIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville|\n|[TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials)|8.3k|TensorFlow Tutorials with YouTube Videos|\n|[pytorch_geometric](https://github.com/rusty1s/pytorch_geometric)|8.2k|Geometric Deep Learning Extension Library for PyTorch|\n|[tutorials](https://github.com/MorvanZhou/tutorials)|8.2k|\u673a\u5668\u5b66\u4e60\u76f8\u5173\u6559\u7a0b|\n|[fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)|8k|A MNIST-like fashion product database. Benchmark \ud83d\udc49|\n|[bert-as-service](https://github.com/hanxiao/bert-as-service)|7.9k|Mapping a variable-length sentence to a fixed-length vector using BERT model|\n|[pix2pix](https://github.com/phillipi/pix2pix)|7.8k|Image-to-image translation with conditional adversarial nets|\n|[mediapipe](https://github.com/google/mediapipe)|7.7k|MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, edge, cloud and the web.|\n|[recommenders](https://github.com/microsoft/recommenders)|7.7k|Best Practices on Recommendation Systems|\n|[mit-deep-learning](https://github.com/lexfridman/mit-deep-learning)|7.7k|Tutorials, assignments, and competitions for MIT Deep Learning related courses.|\n|[pytorch-book](https://github.com/chenyuntc/pytorch-book)|7.6k|PyTorch tutorials and fun projects including neural talk, neural style, poem writing, anime generation (\u300a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6PyTorch\uff1a\u5165\u95e8\u4e0e\u5b9e\u6218\u300b)|\n|[Winds](https://github.com/GetStream/Winds)|7.6k|A Beautiful Open Source RSS & Podcast App Powered by Getstream.io|\n|[vid2vid](https://github.com/NVIDIA/vid2vid)|7.4k|Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.|\n|[Learn_Machine_Learning_in_3_Months](https://github.com/llSourcell/Learn_Machine_Learning_in_3_Months)|7.3k|This is the code for \"Learn Machine Learning in 3 Months\" by Siraj Raval on Youtube|\n|[golearn](https://github.com/sjwhitworth/golearn)|7.3k|Machine Learning for Go|\n|[Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)|7.2k|Keras implementations of Generative Adversarial Networks.|\n|[mlcourse.ai](https://github.com/Yorko/mlcourse.ai)|7k|Open Machine Learning Course|\n|[faceai](https://github.com/vipstone/faceai)|7k|\u4e00\u6b3e\u5165\u95e8\u7ea7\u7684\u4eba\u8138\u3001\u89c6\u9891\u3001\u6587\u5b57\u68c0\u6d4b\u4ee5\u53ca\u8bc6\u522b\u7684\u9879\u76ee.|\n|[pysc2](https://github.com/deepmind/pysc2)|6.9k|StarCraft II Learning Environment|\n|[pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch)|6.9k|Pretrained ConvNets for pytorch: NASNet, ResNeXt, ResNet, InceptionV4, InceptionResnetV2, Xception, DPN, etc.|\n|[PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN)|6.7k|PyTorch implementations of Generative Adversarial Networks.|\n|[vision](https://github.com/pytorch/vision)|6.7k|Datasets, Transforms and Models specific to Computer Vision|\n|[nlp-tutorial](https://github.com/graykode/nlp-tutorial)|6.6k|Natural Language Processing Tutorial for Deep Learning Researchers|\n|[bullet3](https://github.com/bulletphysics/bullet3)|6.6k|Bullet Physics SDK: real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics,|\n|[DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)|6.6k|A tensorflow implementation of \"Deep Convolutional Generative Adversarial Networks\"|\n|[tfjs-models](https://github.com/tensorflow/tfjs-models)|6.5k|Pretrained models for TensorFlow.js|\n|[abu](https://github.com/bbfamily/abu)|6.5k|\u963f\u5e03\u91cf\u5316\u4ea4\u6613\u7cfb\u7edf(\u80a1\u7968\uff0c\u671f\u6743\uff0c\u671f\u8d27\uff0c\u6bd4\u7279\u5e01\uff0c\u673a\u5668\u5b66\u4e60) \u57fa\u4e8epython\u7684\u5f00\u6e90\u91cf\u5316\u4ea4\u6613\uff0c\u91cf\u5316\u6295\u8d44\u67b6\u6784|\n|[pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)|6.5k|The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate|\n|[tensorboardX](https://github.com/lanpa/tensorboardX)|6.4k|tensorboard for pytorch (and chainer, mxnet, numpy, ...)|\n|[machine-learning-course](https://github.com/machinelearningmindset/machine-learning-course)|6.4k|\ud83d\udcac Machine Learning Course with Python:|\n|[guess](https://github.com/guess-js/guess)|6.3k|\ud83d\udd2e Libraries & tools for enabling Machine Learning driven user-experiences on the web|\n|[pyro](https://github.com/pyro-ppl/pyro)|6.3k|Deep universal probabilistic programming with Python and PyTorch|\n|[lab](https://github.com/deepmind/lab)|6.2k|A customisable 3D platform for agent-based AI research|\n|[mml-book.github.io](https://github.com/mml-book/mml-book.github.io)|6.2k|Companion webpage to the book \"Mathematics For Machine Learning\"|\n|[Interview](https://github.com/apachecn/Interview)|6.2k|Interview = \u7b80\u5386\u6307\u5357 + LeetCode + Kaggle|\n|[tensorlayer](https://github.com/tensorlayer/tensorlayer)|6.2k|Deep Learning and Reinforcement Learning Library for Scientists and Engineers \ud83d\udd25|\n|[generative-models](https://github.com/wiseodd/generative-models)|6.1k|Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.|\n|[machine-learning-yearning-cn](https://github.com/deeplearning-ai/machine-learning-yearning-cn)|6.1k|Machine Learning Yearning \u4e2d\u6587\u7248 - \u300a\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u79d8\u7c4d\u300b - Andrew Ng \u8457|\n|[keras-yolo3](https://github.com/qqwweee/keras-yolo3)|6k|A Keras implementation of YOLOv3 (Tensorflow backend)|\n|[BossSensor](https://github.com/Hironsan/BossSensor)|5.9k|Hide screen when boss is approaching.|\n|[tensorflow2_tutorials_chinese](https://github.com/czy36mengfei/tensorflow2_tutorials_chinese)|5.9k|tensorflow2\u4e2d\u6587\u6559\u7a0b\uff0c\u6301\u7eed\u66f4\u65b0(\u5f53\u524d\u7248\u672c:tensorflow2.0)\uff0ctag: tensorflow 2.0 tutorials|\n|[TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)|5.9k|Simple tutorials using Google's TensorFlow Framework|\n|[argo](https://github.com/argoproj/argo)|5.9k|Argo Workflows: Get stuff done with Kubernetes.|\n|[python-machine-learning-book-2nd-edition](https://github.com/rasbt/python-machine-learning-book-2nd-edition)|5.8k|The \"Python Machine Learning (2nd edition)\" book code repository and info resource|\n|[dvc](https://github.com/iterative/dvc)|5.7k|\ud83e\udd89Data Version Control | Git for Data & Models|\n|[EasyPR](https://github.com/liuruoze/EasyPR)|5.7k|An easy, flexible, and accurate plate recognition project for Chinese licenses in unconstrained situations.|\n|[AdversarialNetsPapers](https://github.com/zhangqianhui/AdversarialNetsPapers)|5.6k|The classical paper list with code about generative adversarial nets|\n|[tensorpack](https://github.com/tensorpack/tensorpack)|5.6k|A Neural Net Training Interface on TensorFlow, with focus on speed + flexibility|\n|[photoprism](https://github.com/photoprism/photoprism)|5.6k|Personal Photo Management powered by Go and Google TensorFlow|\n|[tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook)|5.6k|Code for Tensorflow Machine Learning Cookbook|\n|[albumentations](https://github.com/albumentations-team/albumentations)|5.6k|fast image augmentation library and easy to use wrapper around other libraries|\n|[swift](https://github.com/tensorflow/swift)|5.6k|Swift for TensorFlow|\n|[darkflow](https://github.com/thtrieu/darkflow)|5.6k|Translate darknet to tensorflow. Load trained weights, retrain/fine-tune using tensorflow, export constant graph def to mobile devices|\n|[tensorflow_tutorials](https://github.com/pkmital/tensorflow_tutorials)|5.5k|From the basics to slightly more interesting applications of Tensorflow|\n|[deep-learning-coursera](https://github.com/Kulbear/deep-learning-coursera)|5.5k|Deep Learning Specialization by Andrew Ng on Coursera.|\n|[transferlearning](https://github.com/jindongwang/transferlearning)|5.5k|Everything about Transfer Learning and Domain Adaptation--\u8fc1\u79fb\u5b66\u4e60|\n|[ML-NLP](https://github.com/NLP-LOVE/ML-NLP)|5.5k|\u6b64\u9879\u76ee\u662f\u673a\u5668\u5b66\u4e60(Machine Learning)\u3001\u6df1\u5ea6\u5b66\u4e60(Deep Learning)\u3001NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\uff0c\u4e5f\u662f\u4f5c\u4e3a\u4e00\u4e2a\u7b97\u6cd5\u5de5\u7a0b\u5e08\u5fc5\u4f1a\u7684\u7406\u8bba\u57fa\u7840\u77e5\u8bc6\u3002|\n|[nmt](https://github.com/tensorflow/nmt)|5.5k|TensorFlow Neural Machine Translation Tutorial|\n|[faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch)|5.5k|A faster pytorch implementation of faster r-cnn|\n|[UGATIT](https://github.com/taki0112/UGATIT)|5.4k|Official Tensorflow implementation of U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Inst\u2026|\n|[pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)|5.4k|Create HTML profiling reports from pandas DataFrame objects|\n|[deep-residual-networks](https://github.com/KaimingHe/deep-residual-networks)|5.4k|Deep Residual Learning for Image Recognition|\n|[xlnet](https://github.com/zihangdai/xlnet)|5.3k|XLNet: Generalized Autoregressive Pretraining for Language Understanding|\n|[leeml-notes](https://github.com/datawhalechina/leeml-notes)|5.2k|\u674e\u5b8f\u6bc5\u300a\u673a\u5668\u5b66\u4e60\u300b\u7b14\u8bb0\uff0c\u5728\u7ebf\u9605\u8bfb\u5730\u5740\uff1ahttps://datawhalechina.github.io/leeml-notes|\n|[wav2letter](https://github.com/facebookresearch/wav2letter)|5.2k|Facebook AI Research's Automatic Speech Recognition Toolkit|\n|[neural-style](https://github.com/anishathalye/neural-style)|5.2k|Neural style in TensorFlow! \ud83c\udfa8|\n|[CVPR2020-Paper-Code-Interpretation](https://github.com/extreme-assistant/CVPR2020-Paper-Code-Interpretation)|5.2k|cvpr2020/cvpr2019\uff0fcvpr2018/cvpr2017 papers\uff0c\u6781\u5e02\u56e2\u961f\u6574\u7406|\n|[TensorFlow-2.x-Tutorials](https://github.com/dragen1860/TensorFlow-2.x-Tutorials)|5.2k|TensorFlow 2.x version's Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT exampl\u2026|\n|[yolov3](https://github.com/ultralytics/yolov3)|5.2k|YOLOv3 in PyTorch > ONNX > CoreML > iOS|\n|[cnn-text-classification-tf](https://github.com/dennybritz/cnn-text-classification-tf)|5.2k|Convolutional Neural Network for Text Classification in Tensorflow|\n|[seq2seq](https://github.com/google/seq2seq)|5.2k|A general-purpose encoder-decoder framework for Tensorflow|\n|[chineseocr_lite](https://github.com/ouyanghuiyu/chineseocr_lite)|5.1k|\u8d85\u8f7b\u91cf\u7ea7\u4e2d\u6587ocr\uff0c\u652f\u6301\u7ad6\u6392\u6587\u5b57\u8bc6\u522b, \u652f\u6301ncnn\u63a8\u7406 , dbnet(1.7M) + crnn(6.3M) + anglenet(1.5M) \u603b\u6a21\u578b\u4ec510M|\n|[featuretools](https://github.com/FeatureLabs/featuretools)|5k|An open source python library for automated feature engineering|\n|[labelme](https://github.com/wkentaro/labelme)|5k|Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).|\n|[ImageAI](https://github.com/OlafenwaMoses/ImageAI)|5k|A python library built to empower developers to build applications and systems with self-contained Computer Vision capabilities|\n|[nlp-recipes](https://github.com/microsoft/nlp-recipes)|5k|Natural Language Processing Best Practices & Examples|\n|[have-fun-with-machine-learning](https://github.com/humphd/have-fun-with-machine-learning)|4.9k|An absolute beginner's guide to Machine Learning and Image Classification with Neural Networks|\n|[eat_tensorflow2_in_30_days](https://github.com/lyhue1991/eat_tensorflow2_in_30_days)|4.9k|Tensorflow2.0 \ud83c\udf4e\ud83c\udf4a is delicious, just eat it! \ud83d\ude0b\ud83d\ude0b|\n|[tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet)|4.9k|A TensorFlow implementation of DeepMind's WaveNet paper|\n|[PyTorch-Tutorial](https://github.com/MorvanZhou/PyTorch-Tutorial)|4.9k|Build your neural network easy and fast|\n|[stylegan2](https://github.com/NVlabs/stylegan2)|4.9k|StyleGAN2 - Official TensorFlow Implementation|\n|[h2o-3](https://github.com/h2oai/h2o-3)|4.9k|Open Source Fast Scalable Machine Learning Platform For Smarter Applications: Deep Learning, Gradient Boosting & XGBo\u2026|\n|[awesome-machine-learning-on-source-code](https://github.com/src-d/awesome-machine-learning-on-source-code)|4.8k|Cool links & research papers related to Machine Learning applied to source code (MLonCode)|\n|[Learning-to-See-in-the-Dark](https://github.com/cchen156/Learning-to-See-in-the-Dark)|4.8k|Learning to See in the Dark. CVPR 2018|\n|[PyTorch-YOLOv3](https://github.com/eriklindernoren/PyTorch-YOLOv3)|4.8k|Minimal PyTorch implementation of YOLOv3|\n|[first-order-model](https://github.com/AliaksandrSiarohin/first-order-model)|4.8k|This repository contains the source code for the paper First Order Motion Model for Image Animation|\n|[models](https://github.com/PaddlePaddle/models)|4.8k|Pre-trained and Reproduced Deep Learning Models \uff08\u300e\u98de\u6868\u300f\u5b98\u65b9\u6a21\u578b\u5e93\uff0c\u5305\u542b\u591a\u79cd\u5b66\u672f\u524d\u6cbf\u548c\u5de5\u4e1a\u573a\u666f\u9a8c\u8bc1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09|\n|[smile](https://github.com/haifengl/smile)|4.8k|Statistical Machine Intelligence & Learning Engine|\n|[keras-js](https://github.com/transcranial/keras-js)|4.7k|Run Keras models in the browser, with GPU support using WebGL|\n|[carla](https://github.com/carla-simulator/carla)|4.7k|Open-source simulator for autonomous driving research.|\n|[keras-rl](https://github.com/keras-rl/keras-rl)|4.7k|Deep Reinforcement Learning for Keras.|\n|[useful-java-links](https://github.com/Vedenin/useful-java-links)|4.7k|A list of useful Java frameworks, libraries, software and hello worlds examples|\n|[Awesome-CoreML-Models](https://github.com/likedan/Awesome-CoreML-Models)|4.7k|Largest list of models for Core ML (for iOS 11+)|\n|[python-small-examples](https://github.com/jackzhenguo/python-small-examples)|4.7k|\u544a\u522b\u67af\u71e5\uff0c\u81f4\u529b\u4e8e\u6253\u9020 Python \u5bcc\u6709\u4f53\u7cfb\u4e14\u5b9e\u7528\u7684\u5c0f\u4f8b\u5b50\u3001\u5c0f\u6848\u4f8b\u3002|\n|[gcn](https://github.com/tkipf/gcn)|4.7k|Implementation of Graph Convolutional Networks in TensorFlow|\n|[introduction_to_ml_with_python](https://github.com/amueller/introduction_to_ml_with_python)|4.7k|Notebooks and code for the book \"Introduction to Machine Learning with Python\"|\n|[stargan](https://github.com/yunjey/stargan)|4.6k|StarGAN - Official PyTorch Implementation (CVPR 2018)|\n|[pix2pixHD](https://github.com/NVIDIA/pix2pixHD)|4.6k|Synthesizing and manipulating 2048x1024 images with conditional GANs|\n|[Data-Science-Wiki](https://github.com/Leo-G/Data-Science-Wiki)|4.6k|A wiki of DataScience, Statistics, Maths, R,Python, AI, Machine Learning, Automation, Devops Tools, Bash, Linux Tutor\u2026|\n|[MVision](https://github.com/Ewenwan/MVision)|4.6k|\u673a\u5668\u4eba\u89c6\u89c9 \u79fb\u52a8\u673a\u5668\u4eba VS-SLAM ORB-SLAM2 \u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b yolov3 \u884c\u4e3a\u68c0\u6d4b opencv PCL \u673a\u5668\u5b66\u4e60 \u65e0\u4eba\u9a7e\u9a76|\n|[cleverhans](https://github.com/tensorflow/cleverhans)|4.6k|An adversarial example library for constructing attacks, building defenses, and benchmarking both|\n|[vaex](https://github.com/vaexio/vaex)|4.6k|Out-of-Core DataFrames for Python, ML, visualize and explore big tabular data at a billion rows per second \ud83d\ude80|\n|[Grokking-Deep-Learning](https://github.com/iamtrask/Grokking-Deep-Learning)|4.6k|this repository accompanies the book \"Grokking Deep Learning\"|\n|[trax](https://github.com/google/trax)|4.5k|Trax \u2014 Deep Learning with Clear Code and Speed|\n|[graph_nets](https://github.com/deepmind/graph_nets)|4.5k|Build Graph Nets in Tensorflow|\n|[edward](https://github.com/blei-lab/edward)|4.5k|A probabilistic programming language in TensorFlow. Deep generative models, variational inference.|\n|[TensorFlow-World](https://github.com/astorfi/TensorFlow-World)|4.5k|\ud83c\udf0e Simple and ready-to-use tutorials for TensorFlow|\n|[imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)|4.5k|A Python Package to Tackle the Curse of Imbalanced Datasets in Machine Learning|\n|[machine-learning-mindmap](https://github.com/dformoso/machine-learning-mindmap)|4.5k|A mindmap summarising Machine Learning concepts, from Data Analysis to Deep Learning.|\n|[seq2seq-couplet](https://github.com/wb14123/seq2seq-couplet)|4.5k|Play couplet with seq2seq model. \u7528\u6df1\u5ea6\u5b66\u4e60\u5bf9\u5bf9\u8054\u3002|\n|[EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)|4.4k|A PyTorch implementation of EfficientNet|\n|[TensorFlow-Book](https://github.com/BinRoot/TensorFlow-Book)|4.4k|Accompanying source code for Machine Learning with TensorFlow. Refer to the book for step-by-step explanations.|\n|[stanza](https://github.com/stanfordnlp/stanza)|4.4k|Official Stanford NLP Python Library for Many Human Languages|\n|[amazon-dsstne](https://github.com/amzn/amazon-dsstne)|4.4k|Deep Scalable Sparse Tensor Network Engine (DSSTNE) is an Amazon developed library for building Deep Learning (DL) ma\u2026|\n|[cnn-explainer](https://github.com/poloclub/cnn-explainer)|4.4k|Learning Convolutional Neural Networks with Interactive Visualization.|\n|[Realtime_Multi-Person_Pose_Estimation](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation)|4.4k|Code repo for realtime multi-person pose estimation in CVPR'17 (Oral)|\n|[stanford-cs-230-deep-learning](https://github.com/afshinea/stanford-cs-230-deep-learning)|4.3k|VIP cheatsheets for Stanford's CS 230 Deep Learning|\n|[Real-Time-Person-Removal](https://github.com/jasonmayes/Real-Time-Person-Removal)|4.3k|Removing people from complex backgrounds in real time using TensorFlow.js in the web browser|\n|[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)|4.3k|Open Source Neural Machine Translation in PyTorch|\n|[tensorflow_practice](https://github.com/princewen/tensorflow_practice)|4.3k|tensorflow\u5b9e\u6218\u7ec3\u4e60\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u3001\u63a8\u8350\u7cfb\u7edf\u3001nlp\u7b49|\n|[pytorch-cnn-visualizations](https://github.com/utkuozbulak/pytorch-cnn-visualizations)|4.2k|Pytorch implementation of convolutional neural network visualization techniques|\n|[tensorspace](https://github.com/tensorspace-team/tensorspace)|4.2k|Neural network 3D visualization framework, build interactive and intuitive model in browsers, support pre-trained deep \u2026|\n|[DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples)|4.2k|Deep Learning Examples|\n|[sketch-code](https://github.com/ashnkumar/sketch-code)|4.2k|Keras model to generate HTML code from hand-drawn website mockups. Implements an image captioning architecture to dra\u2026|\n|[deeplearning-papernotes](https://github.com/dennybritz/deeplearning-papernotes)|4.2k|Summaries and notes on Deep Learning research papers|\n|[apex](https://github.com/NVIDIA/apex)|4.2k|A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch|\n|[AlphaPose](https://github.com/MVIG-SJTU/AlphaPose)|4.1k|Real-Time and Accurate Multi-Person Pose Estimation&Tracking System|\n|[attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)|4.1k|A PyTorch implementation of the Transformer model in \"Attention is All You Need\".|\n|[nmap](https://github.com/nmap/nmap)|4.1k|Nmap - the Network Mapper. Github mirror of official SVN repository.|\n|[Machine-learning-learning-notes](https://github.com/Vay-keen/Machine-learning-learning-notes)|4.1k|\u5468\u5fd7\u534e\u300a\u673a\u5668\u5b66\u4e60\u300b\u53c8\u79f0\u897f\u74dc\u4e66\u662f\u4e00\u672c\u8f83\u4e3a\u5168\u9762\u7684\u4e66\u7c4d\uff0c\u4e66\u4e2d\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e0d\u540c\u7c7b\u578b\u7684\u7b97\u6cd5(\u4f8b\u5982\uff1a\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u534a\u76d1\u7763\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u96c6\u6210\u964d\u7ef4\u3001\u7279\u5f81\u9009\u62e9\u7b49)\uff0c\u8bb0\u5f55\u4e86\u672c\u4eba\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u7406\u89e3\u601d\u8def\u4e0e\u6269\u5c55\u77e5\u8bc6\u70b9\uff0c\u5e0c\u671b\u5bf9\u65b0\u4eba\u9605\u8bfb\u897f\u74dc\u4e66\u6709\u2026|\n|[serenata-de-amor](https://github.com/okfn-brasil/serenata-de-amor)|4.1k|\ud83d\udd75 Artificial Intelligence for social control of public administration|\n|[practical-pytorch](https://github.com/spro/practical-pytorch)|4.1k|DEPRECATED and not maintained - see official repo at https://github.com/pytorch/tutorials|\n|[pytorch-image-models](https://github.com/rwightman/pytorch-image-models)|4.1k|PyTorch image models, scripts, pretrained weights -- (SE)ResNet/ResNeXT, DPN, EfficientNet, MixNet, MobileNet-V3/V2, MNASNet, Single-Path NAS, FBNet, and more|\n|[face-alignment](https://github.com/1adrianb/face-alignment)|4k|\ud83d\udd25 2D and 3D Face alignment library build using pytorch|\n|[learning-to-learn](https://github.com/deepmind/learning-to-learn)|4k|Learning to Learn in TensorFlow|\n|[machine-learning-notes](https://github.com/roboticcam/machine-learning-notes)|4k|My continuously updated Machine Learning, Probabilistic Models and Deep Learning notes and demos (1500+ slides) \u6211\u4e0d\u95f4\u65ad\u66f4\u2026|\n|[umap](https://github.com/lmcinnes/umap)|4k|Uniform Manifold Approximation and Projection|\n|[DeepLearningZeroToAll](https://github.com/hunkim/DeepLearningZeroToAll)|4k|TensorFlow Basic Tutorial Labs|\n|[gluon-cv](https://github.com/dmlc/gluon-cv)|4k|Gluon CV Toolkit|\n|[pipeline](https://github.com/PipelineAI/pipeline)|4k|PipelineAI Kubeflow Distribution|\n|[snorkel](https://github.com/snorkel-team/snorkel)|4k|A system for quickly generating training data with weak supervision|\n|[DIGITS](https://github.com/NVIDIA/DIGITS)|4k|Deep Learning GPU Training System|\n|[DenseNet](https://github.com/liuzhuang13/DenseNet)|4k|Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award).|\n|[awesome-project-ideas](https://github.com/NirantK/awesome-project-ideas)|4k|Curated list of Machine Learning, NLP, Vision, Recommender Systems Project Ideas|\n|[tutorials](https://github.com/pytorch/tutorials)|4k|PyTorch tutorials.|\n|[Deep-Learning-21-Examples](https://github.com/hzy46/Deep-Learning-21-Examples)|3.9k|\u300a21\u4e2a\u9879\u76ee\u73a9\u8f6c\u6df1\u5ea6\u5b66\u4e60\u2014\u2014\u2014\u57fa\u4e8eTensorFlow\u7684\u5b9e\u8df5\u8be6\u89e3\u300b\u914d\u5957\u4ee3\u7801|\n|[DeepLearningTutorials](https://github.com/lisa-lab/DeepLearningTutorials)|3.9k|Deep Learning Tutorial notes and code. See the wiki for more info.|\n|[textgenrnn](https://github.com/minimaxir/textgenrnn)|3.9k|Easily train your own text-generating neural network of any size and complexity on any text dataset with a few lines \u2026|\n|[lucid](https://github.com/tensorflow/lucid)|3.8k|A collection of infrastructure and tools for research in neural network interpretability.|\n|[nsfwjs](https://github.com/infinitered/nsfwjs)|3.8k|NSFW detection on the client-side via TensorFlow.js|\n|[ssd.pytorch](https://github.com/amdegroot/ssd.pytorch)|3.8k|A PyTorch Implementation of Single Shot MultiBox Detector|\n|[MachineLearning](https://github.com/wepe/MachineLearning)|3.8k|Basic Machine Learning and Deep Learning|\n|[Tensorflow-Tutorial](https://github.com/MorvanZhou/Tensorflow-Tutorial)|3.8k|Tensorflow tutorial from basic to hard|\n|[awesome-ml-for-cybersecurity](https://github.com/jivoi/awesome-ml-for-cybersecurity)|3.8k|Machine Learning for Cyber Security|\n|[daily-paper-computer-vision](https://github.com/amusi/daily-paper-computer-vision)|3.8k|\u8bb0\u5f55\u6bcf\u5929\u6574\u7406\u7684\u8ba1\u7b97\u673a\u89c6\u89c9/\u6df1\u5ea6\u5b66\u4e60/\u673a\u5668\u5b66\u4e60\u76f8\u5173\u65b9\u5411\u7684\u8bba\u6587|\n|[SSD-Tensorflow](https://github.com/balancap/SSD-Tensorflow)|3.8k|Single Shot MultiBox Detector in TensorFlow|\n|[cvat](https://github.com/opencv/cvat)|3.8k|Powerful and efficient Computer Vision Annotation Tool (CVAT)|\n|[deep-learning-roadmap](https://github.com/machinelearningmindset/deep-learning-roadmap)|3.8k|\ud83d\udce1 All You Need to Know About Deep Learning - A kick-starter|\n|[sqlflow](https://github.com/sql-machine-learning/sqlflow)|3.8k|Brings SQL and AI together.|\n|[mmf](https://github.com/facebookresearch/mmf)|3.7k|A modular framework for vision & language multimodal research from Facebook AI Research (FAIR)|\n|[tensorflow-docs](https://github.com/xitu/tensorflow-docs)|3.7k|TensorFlow \u6700\u65b0\u5b98\u65b9\u6587\u6863\u4e2d\u6587\u7248|\n|[iGAN](https://github.com/junyanz/iGAN)|3.7k|Interactive Image Generation via Generative Adversarial Networks|\n|[CapsNet-Tensorflow](https://github.com/naturomics/CapsNet-Tensorflow)|3.7k|A Tensorflow implementation of CapsNet(Capsules Net) in paper Dynamic Routing Between Capsules|\n|[Yet-Another-EfficientDet-Pytorch](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)|3.6k|The pytorch re-implement of the official efficientdet with SOTA performance in real time and pretrained weights.|\n|[pytorch-examples](https://github.com/jcjohnson/pytorch-examples)|3.6k|Simple examples to introduce PyTorch|\n|[ML_for_Hackers](https://github.com/johnmyleswhite/ML_for_Hackers)|3.6k|Code accompanying the book \"Machine Learning for Hackers\"|\n|[docs](https://github.com/tensorflow/docs)|3.6k|TensorFlow documentation|\n|[tensorflow-generative-model-collections](https://github.com/hwalsuklee/tensorflow-generative-model-collections)|3.6k|Collection of generative models in Tensorflow|\n|[DeepLearning.ai-Summary](https://github.com/mbadry1/DeepLearning.ai-Summary)|3.6k|This repository contains my personal notes and summaries on DeepLearning.ai specialization courses. I've enjoyed ever\u2026|\n|[BERT-pytorch](https://github.com/codertimo/BERT-pytorch)|3.6k|Google AI 2018 BERT pytorch implementation|\n|[pwnagotchi](https://github.com/evilsocket/pwnagotchi)|3.5k|(\u2310\u25a0_\u25a0) - Deep Reinforcement Learning instrumenting bettercap for WiFi pwning.|\n|[HyperLPR](https://github.com/zeusees/HyperLPR)|3.5k|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u9ad8\u6027\u80fd\u4e2d\u6587\u8f66\u724c\u8bc6\u522b High Performance Chinese License Plate Recognition Framework.|\n|[deep-learning](https://github.com/udacity/deep-learning)|3.5k|Repo for the Deep Learning Nanodegree Foundations program.|\n|[TensorFlowOnSpark](https://github.com/yahoo/TensorFlowOnSpark)|3.5k|TensorFlowOnSpark brings TensorFlow programs to Apache Spark clusters.|\n|[BigDL](https://github.com/intel-analytics/BigDL)|3.5k|BigDL: Distributed Deep Learning Framework for Apache Spark|\n|[AlgoWiki](https://github.com/vicky002/AlgoWiki)|3.5k|Repository which contains links and resources on different topics of Computer Science.|\n|[examples](https://github.com/tensorflow/examples)|3.5k|TensorFlow examples|\n|[tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn)|3.4k|Tensorflow Faster RCNN for Object Detection|\n|[tf-pose-estimation](https://github.com/ildoonet/tf-pose-estimation)|3.4k|Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference.|\n|[awesome-machine-learning-cn](https://github.com/jobbole/awesome-machine-learning-cn)|3.4k|\u673a\u5668\u5b66\u4e60\u8d44\u6e90\u5927\u5168\u4e2d\u6587\u7248\uff0c\u5305\u62ec\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u6846\u67b6\u3001\u5e93\u4ee5\u53ca\u8f6f\u4ef6|\n|[metaflow](https://github.com/Netflix/metaflow)|3.4k|Build and manage real-life data science projects with ease.|\n|[deep-reinforcement-learning](https://github.com/udacity/deep-reinforcement-learning)|3.3k|Repo for the Deep Reinforcement Learning Nanodegree program|\n|[semantic-segmentation-pytorch](https://github.com/CSAILVision/semantic-segmentation-pytorch)|3.3k|Pytorch implementation for Semantic Segmentation/Scene Parsing on MIT ADE20K dataset|\n|[gocv](https://github.com/hybridgroup/gocv)|3.3k|Go package for computer vision using OpenCV 4 and beyond.|\n|[d2l-pytorch](https://github.com/dsgiitr/d2l-pytorch)|3.3k|This project reproduces the book Dive Into Deep Learning (www.d2l.ai), adapting the code from MXNet into PyTorch.|\n|[Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)|3.3k|Pre-Training with Whole Word Masking for Chinese BERT\uff08\u4e2d\u6587BERT-wwm\u7cfb\u5217\u6a21\u578b\uff09|\n|[SmartCropper](https://github.com/pqpo/SmartCropper)|3.3k|\ud83d\udd25 A library for cropping image in a smart way that can identify the border and correct the cropped image. \u667a\u80fd\u56fe\u7247\u88c1\u526a\u6846\u67b6\u3002\u81ea\u52a8\u8bc6\u522b\u8fb9\u6846\uff0c\u624b\u52a8\u8c03\u8282\u9009\u533a\uff0c\u4f7f\u7528\u900f\u89c6\u53d8\u6362\u88c1\u526a\u5e76\u77eb\u6b63\u9009\u533a\uff1b\u9002\u7528\u4e8e\u8eab\u4efd\u8bc1\uff0c\u540d\u7247\uff0c\u6587\u6863\u7b49\u7167\u7247\u7684\u88c1\u526a\u3002|\n|[PyTorchZeroToAll](https://github.com/hunkim/PyTorchZeroToAll)|3.3k|Simple PyTorch Tutorials Zero to ALL!|\n|[pyAudioAnalysis](https://github.com/tyiannak/pyAudioAnalysis)|3.3k|Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications|\n|[InterpretableMLBook](https://github.com/MingchaoZhu/InterpretableMLBook)|3.3k|\u300a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60--\u9ed1\u76d2\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7406\u89e3\u6307\u5357\u300b\uff0c\u8be5\u4e66\u4e3a\u300aInterpretable Machine Learning\u300b\u4e2d\u6587\u7248|\n|[snips-nlu](https://github.com/snipsco/snips-nlu)|3.3k|Snips Python library to extract meaning from text|\n|[pyod](https://github.com/yzhao062/pyod)|3.3k|A Python Toolbox for Scalable Outlier Detection (Anomaly Detection)|\n|[DeepLearning](https://github.com/Mikoto10032/DeepLearning)|3.2k|\u6df1\u5ea6\u5b66\u4e60\u5165\u95e8\u6559\u7a0b, \u4f18\u79c0\u6587\u7ae0, Deep Learning Tutorial|\n|[vespa](https://github.com/vespa-engine/vespa)|3.2k|Vespa is an engine for low-latency computation over large data sets.|\n|[deep-voice-conversion](https://github.com/andabi/deep-voice-conversion)|3.2k|Deep neural networks for voice conversion (voice style transfer) in Tensorflow|\n|[lightfm](https://github.com/lyst/lightfm)|3.2k|A Python implementation of LightFM, a hybrid recommendation algorithm.|\n|[machine-learning](https://github.com/udacity/machine-learning)|3.2k|Content for Udacity's Machine Learning curriculum|\n|[skflow](https://github.com/tensorflow/skflow)|3.2k|Simplified interface for TensorFlow (mimicking Scikit Learn) for Deep Learning|\n|[Tensorflow-Project-Template](https://github.com/MrGemy95/Tensorflow-Project-Template)|3.2k|A best practice for tensorflow project template architecture.|\n|[EasyOCR](https://github.com/JaidedAI/EasyOCR)|3.2k|Ready-to-use OCR with 40+ languages supported including Chinese, Japanese, Korean and Thai|\n|[text-classification-cnn-rnn](https://github.com/gaussic/text-classification-cnn-rnn)|3.1k|CNN-RNN\u4e2d\u6587\u6587\u672c\u5206\u7c7b\uff0c\u57fa\u4e8eTensorFlow|\n|[MachineLearning_Python](https://github.com/lawlite19/MachineLearning_Python)|3.1k|\u673a\u5668\u5b66\u4e60\u7b97\u6cd5python\u5b9e\u73b0|\n|[imagededup](https://github.com/idealo/imagededup)|3.1k|\ud83d\ude0e Finding duplicate images made easy!|\n|[MatchZoo](https://github.com/NTMC-Community/MatchZoo)|3.1k|Facilitating the design, comparison and sharing of deep text matching models.|\n|[transformer](https://github.com/Kyubyong/transformer)|3.1k|A TensorFlow Implementation of the Transformer: Attention Is All You Need|\n|[tensorflow_poems](https://github.com/jinfagang/tensorflow_poems)|3.1k|\u4e2d\u6587\u53e4\u8bd7\u81ea\u52a8\u4f5c\u8bd7\u673a\u5668\u4eba\uff0c\u5c4c\u70b8\u5929\uff0c\u57fa\u4e8etensorflow1.10 api\uff0c\u6b63\u5728\u79ef\u6781\u7ef4\u62a4\u5347\u7ea7\u4e2d\uff0c\u5febstar\uff0c\u4fdd\u6301\u66f4\u65b0\uff01|\n|[Deep-Learning-Roadmap](https://github.com/astorfi/Deep-Learning-Roadmap)|3.1k|\ud83d\udce1 Organized Resources for Deep Learning Researchers and Developers|\n|[label-studio](https://github.com/heartexlabs/label-studio)|3.1k|Label Studio is a multi-type data labeling and annotation tool with standardized output format|\n|[ASRT_SpeechRecognition](https://github.com/nl8590687/ASRT_SpeechRecognition)|3.1k|A Deep-Learning-Based Chinese Speech Recognition System \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf|\n|[benchmark_results](https://github.com/foolwood/benchmark_results)|3.1k|Visual Tracking Paper List|\n|[Machine-Learning](https://github.com/Jack-Cherish/Machine-Learning)|3k|\u26a1\u673a\u5668\u5b66\u4e60\u5b9e\u6218\uff08Python3\uff09\uff1akNN\u3001\u51b3\u7b56\u6811\u3001\u8d1d\u53f6\u65af\u3001\u903b\u8f91\u56de\u5f52\u3001SVM\u3001\u7ebf\u6027\u56de\u5f52\u3001\u6811\u56de\u5f52|\n|[yolact](https://github.com/dbolya/yolact)|3k|A simple, fully convolutional model for real-time instance segmentation.|\n|[trfl](https://github.com/deepmind/trfl)|3k|TensorFlow Reinforcement Learning|\n|[pytorch-cifar](https://github.com/kuangliu/pytorch-cifar)|3k|95.16% on CIFAR10 with PyTorch|\n|[sacred](https://github.com/IDSIA/sacred)|3k|Sacred is a tool to help you configure, organize, log and reproduce experiments developed at IDSIA.|\n|[yolov5](https://github.com/ultralytics/yolov5)|3k|YOLOv5 in PyTorch > ONNX > CoreML > iOS|\n|[Reinforcement-Learning](https://github.com/andri27-ts/Reinforcement-Learning)|3k|Learn Deep Reinforcement Learning in 60 days! Lectures & Code in Python. Reinforcement Learning + Deep Learning|\n|[distiller](https://github.com/NervanaSystems/distiller)|3k|Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research. https://nervanasy\u2026|\n|[FastMaskRCNN](https://github.com/CharlesShang/FastMaskRCNN)|3k|Mask RCNN in TensorFlow|\n|[probability](https://github.com/tensorflow/probability)|3k|Probabilistic reasoning and statistical analysis in TensorFlow|\n|[DeepVideoAnalytics](https://github.com/AKSHAYUBHAT/DeepVideoAnalytics)|2.9k|A distributed visual search and visual data analytics platform.|\n|[tensorwatch](https://github.com/microsoft/tensorwatch)|2.9k|Debugging, monitoring and visualization for Python Machine Learning and Data Science|\n|[darts](https://github.com/quark0/darts)|2.9k|Differentiable architecture search for convolutional and recurrent networks|\n|[computervision-recipes](https://github.com/microsoft/computervision-recipes)|2.9k|Best Practices, code samples, and documentation for Computer Vision.|\n|[text-detection-ctpn](https://github.com/eragonruan/text-detection-ctpn)|2.9k|text detection mainly based on ctpn model in tensorflow, id card detect, connectionist text proposal network|\n|[tensorflow-windows-wheel](https://github.com/fo40225/tensorflow-windows-wheel)|2.9k|Tensorflow prebuilt binary for Windows|\n|[tensorflow-yolov3](https://github.com/YunYang1994/tensorflow-yolov3)|2.9k|\ud83d\udd25 Pure tensorflow Implement of YOLOv3 with support to train your own dataset|\n|[zhihu](https://github.com/NELSONZHAO/zhihu)|2.9k|This repo contains the source code in my personal column (https://zhuanlan.zhihu.com/zhaoyeyu), implemented using Python 3.6. Including Natural Language Processing and Computer Vision projects, such as text generation, machine translation, deep convolution GAN and other actual combat code.|\n|[BERT-BiLSTM-CRF-NER](https://github.com/macanv/BERT-BiLSTM-CRF-NER)|2.9k|Tensorflow solution of NER task Using BiLSTM-CRF model with Google BERT Fine-tuning And private Server services|\n|[TensorFlowSharp](https://github.com/migueldeicaza/TensorFlowSharp)|2.9k|TensorFlow API for .NET languages|\n|[ignite](https://github.com/pytorch/ignite)|2.9k|High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.|\n|[tensorflow-tutorial](https://github.com/caicloud/tensorflow-tutorial)|2.9k|Example TensorFlow codes and Caicloud TensorFlow as a Service dev environment.|\n|[100-Days-of-ML-Code-Chinese-Version](https://github.com/Avik-Jain/100-Days-of-ML-Code-Chinese-Version)|2.9k|Chinese Translation for Machine Learning Infographics|\n|[deep-learning-papers-translation](https://github.com/SnailTyan/deep-learning-papers-translation)|2.9k|\u6df1\u5ea6\u5b66\u4e60\u8bba\u6587\u7ffb\u8bd1\uff0c\u5305\u62ec\u5206\u7c7b\u8bba\u6587\uff0c\u68c0\u6d4b\u8bba\u6587\u7b49|\n|[DMTK](https://github.com/microsoft/DMTK)|2.8k|Microsoft Distributed Machine Learning Toolkit|\n|[caffe-tensorflow](https://github.com/ethereon/caffe-tensorflow)|2.8k|Caffe models in TensorFlow|\n|[libpostal](https://github.com/openvenues/libpostal)|2.8k|A C library for parsing/normalizing street addresses around the world. Powered by statistical NLP and open geo data.|\n|[pigo](https://github.com/esimov/pigo)|2.8k|Pure Go face detection, pupil/eyes localization and facial landmark points detection library|\n|[mindsdb](https://github.com/mindsdb/mindsdb)|2.8k|Machine Learning in one line of code|\n|[Tensorflow-Cookbook](https://github.com/taki0112/Tensorflow-Cookbook)|2.8k|Simple Tensorflow Cookbook for easy-to-use|\n|[easy-tensorflow](https://github.com/easy-tensorflow/easy-tensorflow)|2.8k|Simple and comprehensive tutorials in TensorFlow|\n|[DeepLearning](https://github.com/yusugomori/DeepLearning)|2.8k|Deep Learning (Python, C, C++, Java, Scala, Go)|\n|[pytorch-yolo-v3](https://github.com/ayooshkathuria/pytorch-yolo-v3)|2.8k|A PyTorch implementation of the YOLO v3 object detection algorithm|\n|[jukebox](https://github.com/openai/jukebox)|2.8k|Code for the paper \"Jukebox: A Generative Model for Music\"|\n|[makegirlsmoe_web](https://github.com/makegirlsmoe/makegirlsmoe_web)|2.8k|Create Anime Characters with MakeGirlsMoe|\n|[deep-learning-keras-tensorflow](https://github.com/leriomaggio/deep-learning-keras-tensorflow)|2.8k|Introduction to Deep Neural Networks with Keras and Tensorflow|\n|[SiamMask](https://github.com/foolwood/SiamMask)|2.7k|[CVPR2019] Fast Online Object Tracking and Segmentation: A Unifying Approach|\n|[tencent-ml-images](https://github.com/Tencent/tencent-ml-images)|2.7k|Largest multi-label image database; ResNet-101 model; 80.73% top-1 acc on ImageNet|\n|[DALI](https://github.com/NVIDIA/DALI)|2.7k|A library containing both highly optimized building blocks and an execution engine for data pre-processing in deep le\u2026|\n|[shogun](https://github.com/shogun-toolbox/shogun)|2.7k|Sh\u014dgun|\n|[optuna](https://github.com/optuna/optuna)|2.7k|A hyperparameter optimization framework|\n|[Automatic_Speech_Recognition](https://github.com/zzw922cn/Automatic_Speech_Recognition)|2.7k|End-to-end Automatic Speech Recognition for Madarian and English in Tensorflow|\n|[pytorch-semseg](https://github.com/meetshah1995/pytorch-semseg)|2.7k|Semantic Segmentation Architectures Implemented in PyTorch|\n|[pygcn](https://github.com/tkipf/pygcn)|2.7k|Graph Convolutional Networks in PyTorch|\n|[deep-learning-book](https://github.com/rasbt/deep-learning-book)|2.7k|Repository for \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in \u2026|\n|[pointnet](https://github.com/charlesq34/pointnet)|2.7k|PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation|\n|[CVPR2020-Code](https://github.com/amusi/CVPR2020-Code)|2.7k|CVPR 2020 \u8bba\u6587\u5f00\u6e90\u9879\u76ee\u5408\u96c6|\n|[Detectron.pytorch](https://github.com/roytseng-tw/Detectron.pytorch)|2.7k|A pytorch implementation of Detectron. Both training from scratch and inferring directly from pretrained Detectron we\u2026|\n|[LSTM-Human-Activity-Recognition](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition)|2.6k|Human Activity Recognition example using TensorFlow on smartphone sensors dataset and an LSTM RNN. Classifying the type of movement amongst six activity categories - Guillaume Chevalier|\n|[neural-style-tf](https://github.com/cysmith/neural-style-tf)|2.6k|TensorFlow (Python API) implementation of Neural Style|\n|[Pytorch-UNet](https://github.com/milesial/Pytorch-UNet)|2.6k|PyTorch implementation of the U-Net for image semantic segmentation with high quality images|\n|[kornia](https://github.com/kornia/kornia)|2.6k|Open Source Differentiable Computer Vision Library for PyTorch|\n|[Ad-papers](https://github.com/wzhe06/Ad-papers)|2.6k|Papers on Computational Advertising|\n|[deep-high-resolution-net.pytorch](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch)|2.6k|The project is an official implementation of our CVPR2019 paper \"Deep High-Resolution Representation Learning for Hum\u2026|\n|[VoTT](https://github.com/microsoft/VoTT)|2.6k|Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos.|\n|[espnet](https://github.com/espnet/espnet)|2.6k|End-to-End Speech Processing Toolkit|\n|[ltp](https://github.com/HIT-SCIR/ltp)|2.6k|Language Technology Platform|\n|[Learn_Deep_Learning_in_6_Weeks](https://github.com/llSourcell/Learn_Deep_Learning_in_6_Weeks)|2.6k|This is the Curriculum for \"Learn Deep Learning in 6 Weeks\" by Siraj Raval on Youtube|\n|[keras-vis](https://github.com/raghakot/keras-vis)|2.6k|Neural network visualization toolkit for keras|\n|[onnxruntime](https://github.com/microsoft/onnxruntime)|2.6k|ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator|\n|[3DDFA](https://github.com/cleardusk/3DDFA)|2.6k|The PyTorch improved version of TPAMI 2017 paper: Face Alignment in Full Pose Range: A 3D Total Solution.|\n|[olivia](https://github.com/olivia-ai/olivia)|2.6k|\ud83d\udc81\u200d\u2640\ufe0fYour new best friend powered by an artificial neural network|\n|[albert_zh](https://github.com/brightmart/albert_zh)|2.6k|A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS, \u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b|\n|[ai-deadlines](https://github.com/abhshkdz/ai-deadlines)|2.6k|\u23f0 AI conference deadline countdowns|\n|[opencvsharp](https://github.com/shimat/opencvsharp)|2.5k|.NET Framework wrapper for OpenCV|\n|[telegram-list](https://github.com/goq/telegram-list)|2.5k|List of telegram groups, channels & bots // \u0421\u043f\u0438\u0441\u043e\u043a \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0445 \u0433\u0440\u0443\u043f\u043f, \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0438 \u0431\u043e\u0442\u043e\u0432 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c\u0430 // \u0421\u043f\u0438\u0441\u043e\u043a \u0447\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0441\u0442\u043e\u0432|\n|[easy12306](https://github.com/zhaipro/easy12306)|2.5k|\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5b8c\u6210\u5bf912306\u9a8c\u8bc1\u7801\u7684\u81ea\u52a8\u8bc6\u522b|\n|[rust](https://github.com/tensorflow/rust)|2.5k|Rust language bindings for TensorFlow|\n|[miles-deep](https://github.com/ryanjay0/miles-deep)|2.5k|Deep Learning Porn Video Classifier/Editor with Caffe|\n|[VisualDL](https://github.com/PaddlePaddle/VisualDL)|2.5k|Deep Learning Visualization Toolkit\uff08\u300e\u98de\u6868\u300f\u6df1\u5ea6\u5b66\u4e60\u53ef\u89c6\u5316\u5de5\u5177 \uff09|\n|[SinGAN](https://github.com/tamarott/SinGAN)|2.5k|Official pytorch implementation of the paper: \"SinGAN: Learning a Generative Model from a Single Natural Image\"|\n|[t81_558_deep_learning](https://github.com/jeffheaton/t81_558_deep_learning)|2.5k|Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks|\n|[training](https://github.com/cloud-annotations/training)|2.5k|\ud83d\udc1d Custom Object Detection and Classification Training|\n|[PocketFlow](https://github.com/Tencent/PocketFlow)|2.5k|An Automatic Model Compression (AutoMC) framework for developing smaller and faster AI applications.|\n|[autogluon](https://github.com/awslabs/autogluon)|2.5k|AutoGluon: AutoML Toolkit for Deep Learning|\n|[simple-faster-rcnn-pytorch](https://github.com/chenyuntc/simple-faster-rcnn-pytorch)|2.5k|A simplified implemention of Faster R-CNN that replicate performance from origin paper|\n|[MITIE](https://github.com/mit-nlp/MITIE)|2.5k|MITIE: library and tools for information extraction|\n|[reinforcement-learning](https://github.com/rlcode/reinforcement-learning)|2.5k|Minimal and Clean Reinforcement Learning Examples|\n|[ISLR-python](https://github.com/JWarmenhoven/ISLR-python)|2.4k|An Introduction to Statistical Learning (James, Witten, Hastie, Tibshirani, 2013): Python code|\n|[EAST](https://github.com/argman/EAST)|2.4k|A tensorflow implementation of EAST text detector|\n|[DeepNLP-models-Pytorch](https://github.com/DSKSD/DeepNLP-models-Pytorch)|2.4k|Pytorch implementations of various Deep NLP models in cs-224n(Stanford Univ)|\n|[Machine-Learning-with-Python](https://github.com/susanli2016/Machine-Learning-with-Python)|2.4k|Python code for common Machine Learning Algorithms|\n|[stanford_dl_ex](https://github.com/amaas/stanford_dl_ex)|2.4k|Programming exercises for the Stanford Unsupervised Feature Learning and Deep Learning Tutorial|\n|[tensorflow-internals](https://github.com/horance-liu/tensorflow-internals)|2.4k|It is open source ebook about TensorFlow kernel and implementation mechanism.|\n|[DeepLearning](https://github.com/MingchaoZhu/DeepLearning)|2.4k|Python for\u300aDeep Learning\u300b\uff0c\u8be5\u4e66\u4e3a\u300a\u6df1\u5ea6\u5b66\u4e60\u300b(\u82b1\u4e66) \u6570\u5b66\u63a8\u5bfc\u3001\u539f\u7406\u5256\u6790\u4e0e\u6e90\u7801\u7ea7\u522b\u4ee3\u7801\u5b9e\u73b0|\n|[text](https://github.com/pytorch/text)|2.4k|Data loaders and abstractions for text and NLP|\n|[ALAE](https://github.com/podgorskiy/ALAE)|2.4k|[CVPR2020] Adversarial Latent Autoencoders|\n|[pytorch-summary](https://github.com/sksq96/pytorch-summary)|2.4k|Model summary in PyTorch similar to `model.summary()` in Keras|\n|[pytorch-doc-zh](https://github.com/apachecn/pytorch-doc-zh)|2.4k|Pytorch \u4e2d\u6587\u6587\u6863|\n|[Deep_reinforcement_learning_Course](https://github.com/simoninithomas/Deep_reinforcement_learning_Course)|2.4k|Implementations from the free course Deep Reinforcement Learning with Tensorflow|\n|[ML-Tutorial-Experiment](https://github.com/jiqizhixin/ML-Tutorial-Experiment)|2.4k|Coding the Machine Learning Tutorial for Learning to Learn|\n|[pytorch-Deep-Learning](https://github.com/Atcold/pytorch-Deep-Learning)|2.4k|Deep Learning (with PyTorch)|\n|[models](https://github.com/onnx/models)|2.4k|A collection of pre-trained, state-of-the-art models in the ONNX format|\n|[book](https://github.com/PaddlePaddle/book)|2.4k|Deep Learning 101 with PaddlePaddle \uff08\u300e\u98de\u6868\u300f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5165\u95e8\u6559\u7a0b\uff09|\n|[PyTorch_Tutorial](https://github.com/TingsongYu/PyTorch_Tutorial)|2.4k|\u300aPytorch\u6a21\u578b\u8bad\u7ec3\u5b9e\u7528\u6559\u7a0b\u300b\u4e2d\u914d\u5957\u4ee3\u7801|\n|[Dive-into-DL-TensorFlow2.0](https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0)|2.4k|\u672c\u9879\u76ee\u5c06\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b(Dive into Deep Learning)\u539f\u4e66\u4e2d\u7684MXNet\u5b9e\u73b0\u6539\u4e3aTensorFlow 2.0\u5b9e\u73b0\uff0c\u9879\u76ee\u5df2\u5f97\u5230\u674e\u6c90\u8001\u5e08\u7684\u540c\u610f|\n|[Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials](https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials)|2.4k|A comprehensive list of Deep Learning / Artificial Intelligence and Machine Learning tutorials - rapidly expanding in\u2026|\n|[segmentation_models](https://github.com/qubvel/segmentation_models)|2.4k|Segmentation models with pretrained backbones. Keras and TensorFlow Keras.|\n|[Awesome-PyTorch-Chinese](https://github.com/INTERMT/Awesome-PyTorch-Chinese)|2.3k|\u3010\u5e72\u8d27\u3011\u53f2\u4e0a\u6700\u5168\u7684PyTorch\u5b66\u4e60\u8d44\u6e90\u6c47\u603b|\n|[Flux.jl](https://github.com/FluxML/Flux.jl)|2.3k|Relax! Flux is the ML library that doesn't make you tensor|\n|[weld](https://github.com/weld-project/weld)|2.3k|High-performance runtime for data analytics applications|\n|[PyTorch-BigGraph](https://github.com/facebookresearch/PyTorch-BigGraph)|2.3k|Generate embeddings from large-scale graph-structured data.|\n|[byteps](https://github.com/bytedance/byteps)|2.3k|A high performance and generic framework for distributed DNN training|\n|[AI-Job-Notes](https://github.com/amusi/AI-Job-Notes)|2.3k|AI\u7b97\u6cd5\u5c97\u6c42\u804c\u653b\u7565\uff08\u6db5\u76d6\u51c6\u5907\u653b\u7565\u3001\u5237\u9898\u6307\u5357\u3001\u5185\u63a8\u548cAI\u516c\u53f8\u6e05\u5355\u7b49\u8d44\u6599\uff09|\n|[luminoth](https://github.com/tryolabs/luminoth)|2.3k|\u26a0\ufe0f UNMAINTAINED. Deep Learning toolkit for Computer Vision.|\n|[Alink](https://github.com/alibaba/Alink)|2.3k|Alink is the Machine Learning algorithm platform based on Flink, developed by the PAI team of Alibaba computing platf\u2026|\n|[introtodeeplearning](https://github.com/aamini/introtodeeplearning)|2.3k|Lab Materials for MIT 6.S191: Introduction to Deep Learning|\n|[TensorFlow-and-DeepLearning-Tutorial](https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial)|2.3k|A TensorFlow & Deep Learning online course I taught in 2016|\n|[srgan](https://github.com/tensorlayer/srgan)|2.3k|Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network|\n|[colorization](https://github.com/richzhang/colorization)|2.3k|Automatic colorization using deep neural networks. \"Colorful Image Colorization.\" In ECCV, 2016.|\n|[OpenNMT](https://github.com/OpenNMT/OpenNMT)|2.3k|Open Source Neural Machine Translation in Torch (deprecated)|\n|[Super-SloMo](https://github.com/avinashpaliwal/Super-SloMo)|2.3k|PyTorch implementation of Super SloMo by Jiang et al.|\n|[orange3](https://github.com/biolab/orange3)|2.3k|\ud83c\udf4a \ud83d\udcca \ud83d\udca1 Orange: Interactive data analysis https://orange.biolab.si|\n|[ENAS-pytorch](https://github.com/carpedm20/ENAS-pytorch)|2.3k|PyTorch implementation of \"Efficient Neural Architecture Search via Parameters Sharing\"|\n|[3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch)|2.3k|3D ResNets for Action Recognition (CVPR 2018)|\n|[pomegranate](https://github.com/jmschrei/pomegranate)|2.3k|Fast, flexible and easy to use probabilistic modelling in Python.|\n|[Faster-RCNN_TF](https://github.com/smallcorgi/Faster-RCNN_TF)|2.3k|Faster-RCNN in Tensorflow|\n|[datascience](https://github.com/r0f1/datascience)|2.3k|Curated list of Python resources for data science.|\n|[deep-learning-from-scratch](https://github.com/oreilly-japan/deep-learning-from-scratch)|2.3k|\u300e\u30bc\u30ed\u304b\u3089\u4f5c\u308b Deep Learning\u300f(O'Reilly Japan, 2016)|\n|[covid-chestxray-dataset](https://github.com/ieee8023/covid-chestxray-dataset)|2.2k|We are building an open database of COVID-19 cases with chest X-ray or CT images.|\n|[deepchem](https://github.com/deepchem/deepchem)|2.2k|Democratizing Deep-Learning for Drug Discovery, Quantum Chemistry, Materials Science and Biology|\n|[awesome-learning-resources](https://github.com/lauragift21/awesome-learning-resources)|2.2k|\ud83d\udd25 Awesome list of resources on Web Development.|\n|[omniscidb](https://github.com/omnisci/omniscidb)|2.2k|OmniSciDB (formerly MapD Core)|\n|[tablesaw](https://github.com/jtablesaw/tablesaw)|2.2k|Java dataframe and visualization library|\n|[Semantic-Segmentation-Suite](https://github.com/GeorgeSeif/Semantic-Segmentation-Suite)|2.2k|Semantic Segmentation Suite in TensorFlow. Implement, train, and test new Semantic Segmentation models easily!|\n|[FCOS](https://github.com/tianzhi0549/FCOS)|2.2k|FCOS: Fully Convolutional One-Stage Object Detection (ICCV'19)|\n|[spotlight](https://github.com/maciejkula/spotlight)|2.2k|Deep recommender models using PyTorch.|\n|[datasets](https://github.com/tensorflow/datasets)|2.2k|TFDS is a collection of datasets ready to use with TensorFlow, Jax, ...|\n|[Deep-Learning-for-Recommendation-Systems](https://github.com/robi56/Deep-Learning-for-Recommendation-Systems)|2.2k|This repository contains Deep Learning based articles , paper and repositories for Recommender Systems|\n|[gluon-nlp](https://github.com/dmlc/gluon-nlp)|2.1k|NLP made easy|\n|[dowhy](https://github.com/microsoft/dowhy)|2.1k|DoWhy is a Python library for causal inference that supports explicit modeling and testing of causal assumptions. DoWhy is based on a unified language for causal inference, combining causal graphical models and potential outcomes frameworks.|\n|[keras-attention-mechanism](https://github.com/philipperemy/keras-attention-mechanism)|2.1k|Attention mechanism Implementation for Keras.|\n|[Meta-Learning-Papers](https://github.com/floodsung/Meta-Learning-Papers)|2.1k|Meta Learning / Learning to Learn / One Shot Learning / Few Shot Learning|\n|[tacotron](https://github.com/keithito/tacotron)|2.1k|A TensorFlow implementation of Google's Tacotron speech synthesis with pre-trained model (unofficial)|\n|[machinelearninginaction](https://github.com/pbharrin/machinelearninginaction)|2.1k|Source Code for the book: Machine Learning in Action published by Manning|\n|[BuildingMachineLearningSystemsWithPython](https://github.com/luispedro/BuildingMachineLearningSystemsWithPython)|2.1k|Source Code for the book Building Machine Learning Systems with Python|\n|[CHINESE-OCR](https://github.com/xiaofengShi/CHINESE-OCR)|2.1k|[python3.6] \u8fd0\u7528tf\u5b9e\u73b0\u81ea\u7136\u573a\u666f\u6587\u5b57\u68c0\u6d4b,keras/pytorch\u5b9e\u73b0ctpn+crnn+ctc\u5b9e\u73b0\u4e0d\u5b9a\u957f\u573a\u666f\u6587\u5b57OCR\u8bc6\u522b|\n|[deepdetect](https://github.com/jolibrain/deepdetect)|2.1k|Deep Learning API and Server in C++11 support for Caffe, Caffe2, PyTorch,TensorRT, Dlib, NCNN, Tensorflow, XGBoost an\u2026|\n|[XLM](https://github.com/facebookresearch/XLM)|2.1k|PyTorch original implementation of Cross-lingual Language Model Pretraining.|\n|[tensorflow-on-raspberry-pi](https://github.com/samjabrahams/tensorflow-on-raspberry-pi)|2.1k|TensorFlow for Raspberry Pi|\n|[decaNLP](https://github.com/salesforce/decaNLP)|2.1k|The Natural Language Decathlon: A Multitask Challenge for NLP|\n|[AlphaZero_Gomoku](https://github.com/junxiaosong/AlphaZero_Gomoku)|2.1k|An implementation of the AlphaZero algorithm for Gomoku (also called Gobang or Five in a Row)|\n|[pytorch-beginner](https://github.com/L1aoXingyu/pytorch-beginner)|2.1k|pytorch tutorial for beginners|\n|[tangent](https://github.com/google/tangent)|2.1k|Source-to-Source Debuggable Derivatives in Pure Python|\n|[Person_reID_baseline_pytorch](https://github.com/layumi/Person_reID_baseline_pytorch)|2.1k|A tiny, friendly, strong pytorch implement of person re-identification baseline. Tutorial \ud83d\udc49https://github.com/layumi/\u2026|\n|[mmlspark](https://github.com/Azure/mmlspark)|2k|Microsoft Machine Learning for Apache Spark|\n|[FATE](https://github.com/FederatedAI/FATE)|2k|An Industrial Level Federated Learning Framework|\n|[text-to-image](https://github.com/paarthneekhara/text-to-image)|2k|Text to image synthesis using thought vectors|\n|[pytorch-sentiment-analysis](https://github.com/bentrevett/pytorch-sentiment-analysis)|2k|Tutorials on getting started with PyTorch and TorchText for sentiment analysis.|\n|[ELF](https://github.com/facebookresearch/ELF)|2k|An End-To-End, Lightweight and Flexible Platform for Game Research|\n|[catalyst](https://github.com/catalyst-team/catalyst)|2k|Accelerated DL R&D|\n|[neuralcoref](https://github.com/huggingface/neuralcoref)|2k|\u2728Fast Coreference Resolution in spaCy with Neural Networks|\n|[DeepRL-Agents](https://github.com/awjuliani/DeepRL-Agents)|2k|A set of Deep Reinforcement Learning Agents implemented in Tensorflow.|\n|[RL-Adventure](https://github.com/higgsfield/RL-Adventure)|2k|Pytorch Implementation of DQN / DDQN / Prioritized replay/ noisy networks/ distributional values/ Rainbow/ hierarchic\u2026|\n|[fe4ml-zh](https://github.com/apachecn/fe4ml-zh)|2k|\ud83d\udcd6 [\u8bd1] \u9762\u5411\u673a\u5668\u5b66\u4e60\u7684\u7279\u5f81\u5de5\u7a0b|\n|[lingvo](https://github.com/tensorflow/lingvo)|2k|Lingvo|\n|[pytorch-generative-model-collections](https://github.com/znxlwm/pytorch-generative-model-collections)|2k|Collection of generative models in Pytorch version.|\n|[ResNeSt](https://github.com/zhanghang1989/ResNeSt)|2k|ResNeSt: Split-Attention Networks|\n|[TensorFlow-Tutorials](https://github.com/golbin/TensorFlow-Tutorials)|2k|\ud150\uc11c\ud50c\ub85c\uc6b0\ub97c \uae30\ucd08\ubd80\ud130 \uc751\uc6a9\uae4c\uc9c0 \ub2e8\uacc4\ubcc4\ub85c \uc5f0\uc2b5\ud560 \uc218 \uc788\ub294 \uc18c\uc2a4 \ucf54\ub4dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4|\n|[MUNIT](https://github.com/NVlabs/MUNIT)|2k|Multimodal Unsupervised Image-to-Image Translation|\n|[oneDNN](https://github.com/oneapi-src/oneDNN)|2k|oneAPI Deep Neural Network Library (oneDNN)|\n|[deepvariant](https://github.com/google/deepvariant)|2k|DeepVariant is an analysis pipeline that uses a deep neural network to call genetic variants from next-generation DNA sequencing data.|\n|[texar](https://github.com/asyml/texar)|2k|Toolkit for Machine Learning, Natural Language Processing, and Text Generation, in TensorFlow|\n|[kaolin](https://github.com/NVIDIAGameWorks/kaolin)|2k|A PyTorch Library for Accelerating 3D Deep Learning Research|\n|[Clairvoyant](https://github.com/anfederico/Clairvoyant)|2k|Software designed to identify and monitor social/historical cues for short term stock movement|\n|[implicit](https://github.com/benfred/implicit)|2k|Fast Python Collaborative Filtering for Implicit Feedback Datasets|\n|[DeepRL](https://github.com/ShangtongZhang/DeepRL)|2k|Modularized Implementation of Deep RL Algorithms in PyTorch|\n|[lgo](https://github.com/yunabe/lgo)|2k|Interactive Go programming with Jupyter|\n|[kcws](https://github.com/koth/kcws)|2k|Deep Learning Chinese Word Segment|\n|[tensorflow-build-archived](https://github.com/lakshayg/tensorflow-build-archived)|2k|TensorFlow binaries supporting AVX, FMA, SSE|\n|[dm_control](https://github.com/deepmind/dm_control)|2k|DeepMind's software stack for physics-based simulation and Reinforcement Learning environments, using MuJoCo.|\n|[gpytorch](https://github.com/cornellius-gp/gpytorch)|2k|A highly efficient and modular implementation of Gaussian Processes in PyTorch|\n|[Neural-Photo-Editor](https://github.com/ajbrock/Neural-Photo-Editor)|1.9k|A simple interface for editing natural photos with generative neural networks.|\n|[alpha-zero-general](https://github.com/suragnair/alpha-zero-general)|1.9k|A clean implementation based on AlphaZero for any game in any framework + tutorial + Othello/Gobang/TicTacToe/Connect4|\n|[tacotron2](https://github.com/NVIDIA/tacotron2)|1.9k|Tacotron 2 - PyTorch implementation with faster-than-realtime inference|\n|[siamese-triplet](https://github.com/adambielski/siamese-triplet)|1.9k|Siamese and triplet networks with online pair/triplet mining in PyTorch|\n|[awesome-quant](https://github.com/thuquant/awesome-quant)|1.9k|\u4e2d\u56fd\u7684Quant\u76f8\u5173\u8d44\u6e90\u7d22\u5f15|\n|[image-super-resolution](https://github.com/idealo/image-super-resolution)|1.9k|\ud83d\udd0e Super-scale your images and run experiments with Residual Dense and Adversarial Networks.|\n|[generative_inpainting](https://github.com/JiahuiYu/generative_inpainting)|1.9k|DeepFill v1/v2 with Contextual Attention and Gated Convolution, CVPR 2018, and ICCV 2019 Oral|\n|[code-of-learn-deep-learning-with-pytorch](https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch)|1.9k|This is code of book \"Learn Deep Learning with PyTorch\"|\n|[gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)|1.9k|Python package to easily retrain OpenAI's GPT-2 text-generating model on new texts|\n|[DeepInterests](https://github.com/Honlan/DeepInterests)|1.9k|\u6df1\u5ea6\u6709\u8da3|\n|[segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch)|1.9k|Segmentation models with pretrained backbones. PyTorch.|\n|[human-pose-estimation.pytorch](https://github.com/microsoft/human-pose-estimation.pytorch)|1.9k|The project is an official implement of our ECCV2018 paper \"Simple Baselines for Human Pose Estimation and Tracking(h\u2026|\n|[BigGAN-PyTorch](https://github.com/ajbrock/BigGAN-PyTorch)|1.9k|The author's officially unofficial PyTorch BigGAN implementation.|\n|[pytorch-playground](https://github.com/aaron-xichen/pytorch-playground)|1.9k|Base pretrained models and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)|\n|[bertviz](https://github.com/jessevig/bertviz)|1.9k|Tool for visualizing attention in the Transformer model (BERT, GPT-2, Albert, XLNet, RoBERTa, CTRL, etc.)|\n|[face.evoLVe.PyTorch](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)|1.9k|\ud83d\udd25\ud83d\udd25High-Performance Face Recognition Library on PyTorch\ud83d\udd25\ud83d\udd25|\n|[Reco-papers](https://github.com/wzhe06/Reco-papers)|1.8k|Classic papers and resources on recommendation|\n|[coach](https://github.com/NervanaSystems/coach)|1.8k|Reinforcement Learning Coach by Intel AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms|\n|[sling](https://github.com/google/sling)|1.8k|SLING - A natural language frame semantics parser|\n|[pytorch-deeplab-xception](https://github.com/jfzhang95/pytorch-deeplab-xception)|1.8k|DeepLab v3+ model in PyTorch. Support different backbones.|\n|[mmskeleton](https://github.com/open-mmlab/mmskeleton)|1.8k|A OpenMMLAB toolbox for human pose estimation, skeleton-based action recognition, and action synthesis.|\n|[sru](https://github.com/asappresearch/sru)|1.8k|Training RNNs as Fast as CNNs (https://arxiv.org/abs/1709.02755)|\n|[pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq)|1.8k|Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText.|\n|[Deep-Learning-Interview-Book](https://github.com/amusi/Deep-Learning-Interview-Book)|1.8k|\u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\uff08\u542b\u6570\u5b66\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548cSLAM\u7b49\u65b9\u5411\uff09|\n|[pai](https://github.com/microsoft/pai)|1.8k|Resource scheduling and cluster management for AI|\n|[AI-Blocks](https://github.com/MrNothing/AI-Blocks)|1.8k|A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models!|\n|[scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)|1.8k|Sequential model-based optimization with a `scipy.optimize` interface|\n|[sequence_tagging](https://github.com/guillaumegenthial/sequence_tagging)|1.8k|Named Entity Recognition (LSTM + CRF) - Tensorflow|\n|[zh-NER-TF](https://github.com/Determined22/zh-NER-TF)|1.8k|A very simple BiLSTM-CRF model for Chinese Named Entity Recognition \u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (TensorFlow)|\n|[donkeycar](https://github.com/autorope/donkeycar)|1.8k|Open source hardware and software platform to build a small scale self driving car.|\n|[edge-connect](https://github.com/knazeri/edge-connect)|1.8k|EdgeConnect: Structure Guided Image Inpainting using Edge Prediction, ICCV 2019 https://arxiv.org/abs/1901.00212|\n|[awd-lstm-lm](https://github.com/salesforce/awd-lstm-lm)|1.7k|LSTM and QRNN Language Model Toolkit for PyTorch|\n|[pytorch-kaldi](https://github.com/mravanelli/pytorch-kaldi)|1.7k|pytorch-kaldi is a project for developing state-of-the-art DNN/RNN hybrid speech recognition systems. The DNN part is managed by pytorch, while feature extraction, label computation, and decoding are performed with the kaldi toolkit.|\n|[Bender](https://github.com/xmartlabs/Bender)|1.7k|Easily craft fast Neural Networks on iOS! Use TensorFlow models. Metal under the hood.|\n|[zi2zi](https://github.com/kaonashi-tyc/zi2zi)|1.7k|Learning Chinese Character style with conditional GAN|\n|[automl-gs](https://github.com/minimaxir/automl-gs)|1.7k|Provide an input CSV and a target field to predict, generate a model + code to run it.|\n|[stats](https://github.com/montanaflynn/stats)|1.7k|A well tested and comprehensive Golang statistics library package with no dependencies.|\n|[ranking](https://github.com/tensorflow/ranking)|1.7k|Learning to Rank in TensorFlow|\n|[mathAI](https://github.com/Roujack/mathAI)|1.7k|\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3002\u8f93\u5165\u4e00\u5f20\u5305\u542b\u6570\u5b66\u8ba1\u7b97\u9898\u7684\u56fe\u7247\uff0c\u8f93\u51fa\u8bc6\u522b\u51fa\u7684\u6570\u5b66\u8ba1\u7b97\u5f0f\u4ee5\u53ca\u8ba1\u7b97\u7ed3\u679c\u3002This is a mathematic expression recognition project.|\n|[spark-ml-source-analysis](https://github.com/endymecy/spark-ml-source-analysis)|1.7k|spark ml \u7b97\u6cd5\u539f\u7406\u5256\u6790\u4ee5\u53ca\u5177\u4f53\u7684\u6e90\u7801\u5b9e\u73b0\u5206\u6790|\n|[video-object-removal](https://github.com/zllrunning/video-object-removal)|1.7k|Just draw a bounding box and you can remove the object you want to remove.|\n|[datascience-pizza](https://github.com/PizzaDeDados/datascience-pizza)|1.7k|\ud83c\udf55 Reposit\u00f3rio para juntar informa\u00e7\u00f5es sobre materiais de estudo em an\u00e1lise de dados e \u00e1reas afins, empresas que trabalham com dados e dicion\u00e1rio de conceitos|\n|[data-science-interviews](https://github.com/alexeygrigorev/data-science-interviews)|1.7k|Data science interview questions and answers|\n|[yolov3-tf2](https://github.com/zzh8829/yolov3-tf2)|1.7k|YoloV3 Implemented in Tensorflow 2.0|\n|[ComputeLibrary](https://github.com/ARM-software/ComputeLibrary)|1.7k|The ARM Computer Vision and Machine Learning library is a set of functions optimised for both ARM CPUs and GPUs using SIMD technologies.|\n|[tacotron](https://github.com/Kyubyong/tacotron)|1.7k|A TensorFlow Implementation of Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model|\n|[DeepLearn](https://github.com/GauravBh1010tt/DeepLearn)|1.7k|Implementation of research papers on Deep Learning+ NLP+ CV in Python using Keras, Tensorflow and Scikit Learn.|\n|[analytics-zoo](https://github.com/intel-analytics/analytics-zoo)|1.7k|Distributed Tensorflow, Keras and PyTorch on Apache Spark/Flink & Ray|\n|[PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP)|1.7k|Basic Utilities for PyTorch Natural Language Processing (NLP)|\n|[captcha_break](https://github.com/ypwhs/captcha_break)|1.7k|\u9a8c\u8bc1\u7801\u8bc6\u522b|\n|[crnn](https://github.com/bgshih/crnn)|1.7k|Convolutional Recurrent Neural Network (CRNN) for image-based sequence recognition.|\n|[DeblurGAN](https://github.com/KupynOrest/DeblurGAN)|1.7k|Image Deblurring using Generative Adversarial Networks|\n|[robosat](https://github.com/mapbox/robosat)|1.6k|Semantic segmentation on aerial and satellite imagery. Extracts features such as: buildings, parking lots, roads, water, clouds|\n|[pointnet2](https://github.com/charlesq34/pointnet2)|1.6k|PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space|\n|[AutonomousDrivingCookbook](https://github.com/microsoft/AutonomousDrivingCookbook)|1.6k|Scenarios, tutorials and demos for Autonomous Driving|\n|[imgclsmob](https://github.com/osmr/imgclsmob)|1.6k|Sandbox for training convolutional networks for computer vision|\n|[tf_unet](https://github.com/jakeret/tf_unet)|1.6k|Generic U-Net Tensorflow implementation for image segmentation|\n|[torchsample](https://github.com/ncullen93/torchsample)|1.6k|High-Level Training, Data Augmentation, and Utilities for Pytorch|\n|[nlp](https://github.com/huggingface/nlp)|1.6k|\ud83e\udd17nlp \u2013 Datasets and evaluation metrics for Natural Language Processing in NumPy, Pandas, PyTorch and TensorFlow|\n|[hdbscan](https://github.com/scikit-learn-contrib/hdbscan)|1.6k|A high performance implementation of HDBSCAN clustering.|\n|[m2cgen](https://github.com/BayesWitnesses/m2cgen)|1.6k|Transform ML models into a native code (Java, C, Python, Go, JavaScript, Visual Basic, C#, R, PowerShell, PHP, Dart, Haskell, Ruby) with zero dependencies|\n|[fastNLP](https://github.com/fastnlp/fastNLP)|1.6k|fastNLP: A Modularized and Extensible NLP Framework. Currently still in incubation.|\n|[keras-yolo2](https://github.com/experiencor/keras-yolo2)|1.6k|Easy training on custom dataset. Various backends (MobileNet and SqueezeNet) supported. A YOLO demo to detect raccoon run entirely in brower is accessible at https://git.io/vF7vI (not on Windows).|\n|[Awesome-Chatbot](https://github.com/fendouai/Awesome-Chatbot)|1.6k|Awesome Chatbot Projects,Corpus,Papers,Tutorials.Chinese Chatbot =>:|\n|[knockknock](https://github.com/huggingface/knockknock)|1.6k|\ud83d\udeaa\u270aKnock Knock: Get notified when your training ends with only two additional lines of code|\n|[MTBook](https://github.com/NiuTrans/MTBook)|1.6k|\u300a\u673a\u5668\u7ffb\u8bd1\uff1a\u7edf\u8ba1\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u300b\u8096\u6850 \u6731\u9756\u6ce2 \u8457 - Machine Translation: Statistical Modeling and Deep Learning Methods|\n|[trains](https://github.com/allegroai/trains)|1.6k|TRAINS - Auto-Magical Experiment Manager & Version Control for AI - NOW WITH AUTO-MAGICAL DEVOPS!|\n|[self-driving-car](https://github.com/ndrplz/self-driving-car)|1.6k|Udacity Self-Driving Car Engineer Nanodegree projects.|\n|[cnn_captcha](https://github.com/nickliqian/cnn_captcha)|1.6k|use cnn recognize captcha by tensorflow. \u672c\u9879\u76ee\u9488\u5bf9\u5b57\u7b26\u578b\u56fe\u7247\u9a8c\u8bc1\u7801\uff0c\u4f7f\u7528tensorflow\u5b9e\u73b0\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u8fdb\u884c\u9a8c\u8bc1\u7801\u8bc6\u522b\u3002|\n|[XLearning](https://github.com/Qihoo360/XLearning)|1.6k|AI on Hadoop|\n|[Tacotron-2](https://github.com/Rayhane-mamah/Tacotron-2)|1.6k|DeepMind's Tacotron-2 Tensorflow implementation|\n|[fast-wavenet](https://github.com/tomlepaine/fast-wavenet)|1.6k|Speedy Wavenet generation using dynamic programming \u26a1|\n|[spacy-course](https://github.com/ines/spacy-course)|1.6k|\ud83d\udc69\u200d\ud83c\udfeb Advanced NLP with spaCy: A free online course|\n|[gandissect](https://github.com/CSAILVision/gandissect)|1.6k|Pytorch-based tools for visualizing and understanding the neurons of a GAN. https://gandissect.csail.mit.edu/|\n|[NCRFpp](https://github.com/jiesutd/NCRFpp)|1.6k|NCRF++, a Neural Sequence Labeling Toolkit. Easy use to any sequence labeling tasks (e.g. NER, POS, Segmentation). It includes character LSTM/CNN, word LSTM/CNN and softmax/CRF components.|\n|[stargan-v2](https://github.com/clovaai/stargan-v2)|1.6k|StarGAN v2 - Official PyTorch Implementation (CVPR 2020)|\n|[tinyflow](https://github.com/tqchen/tinyflow)|1.6k|Tutorial code on how to build your own Deep Learning System in 2k Lines|\n|[UNIT](https://github.com/mingyuliutw/UNIT)|1.6k|Unsupervised Image-to-Image Translation|\n|[ssd_keras](https://github.com/pierluigiferrari/ssd_keras)|1.6k|A Keras port of Single Shot MultiBox Detector|\n|[cosin](https://github.com/chatopera/cosin)|1.5k|\ud83c\udf32 \u6625\u677e\u5ba2\u670d\uff0c\u591a\u6e20\u9053\u667a\u80fd\u5ba2\u670d\u7cfb\u7edf\uff0c\u5f00\u6e90\u5ba2\u670d\u7cfb\u7edf|\n|[foolbox](https://github.com/bethgelab/foolbox)|1.5k|A Python toolbox to create adversarial examples that fool neural networks in PyTorch, TensorFlow, and JAX|\n|[capsule-networks](https://github.com/gram-ai/capsule-networks)|1.5k|A PyTorch implementation of the NIPS 2017 paper \"Dynamic Routing Between Capsules\".|\n|[flogo](https://github.com/TIBCOSoftware/flogo)|1.5k|Project Flogo is an open source ecosystem of opinionated event-driven capabilities to simplify building efficient & modern serverless functions, microservices & edge apps.|\n|[lip-reading-deeplearning](https://github.com/astorfi/lip-reading-deeplearning)|1.5k|\ud83d\udd13 Lip Reading - Cross Audio-Visual Recognition using 3D Architectures|\n|[hummingbird](https://github.com/microsoft/hummingbird)|1.5k|Hummingbird compiles trained ML models into tensor computation for faster inference.|\n|[deep-rl-tensorflow](https://github.com/carpedm20/deep-rl-tensorflow)|1.5k|TensorFlow implementation of Deep Reinforcement Learning papers|\n|[practical-machine-learning-with-python](https://github.com/dipanjanS/practical-machine-learning-with-python)|1.5k|Master the essential skills needed to recognize and solve complex real-world problems with Machine Learning and Deep Learning by leveraging the highly popular Python Machine Learning Eco-system.|\n|[NeuroNER](https://github.com/Franck-Dernoncourt/NeuroNER)|1.5k|Named-entity recognition using neural networks. Easy-to-use and state-of-the-art results.|\n|[wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)|1.5k|WaveNet vocoder|\n|[awesome-hand-pose-estimation](https://github.com/xinghaochen/awesome-hand-pose-estimation)|1.5k|Awesome work on hand pose estimation/tracking|\n|[mAP](https://github.com/Cartucho/mAP)|1.5k|mean Average Precision - This code evaluates the performance of your neural net for object recognition.|\n|[agents](https://github.com/tensorflow/agents)|1.5k|TF-Agents is a library for Reinforcement Learning in TensorFlow|\n|[CADL](https://github.com/pkmital/CADL)|1.5k|ARCHIVED: Contains historical course materials/Homework materials for the FREE MOOC course on \"Creative Applications of Deep Learning w/ Tensorflow\" #CADL|\n|[tensorflow-DeepFM](https://github.com/ChenglongChen/tensorflow-DeepFM)|1.5k|Tensorflow implementation of DeepFM for CTR prediction.|\n|[tensorflow-1.4-billion-password-analysis](https://github.com/philipperemy/tensorflow-1.4-billion-password-analysis)|1.5k|Deep Learning model to analyze a large corpus of clear text passwords.|\n|[DAT8](https://github.com/justmarkham/DAT8)|1.5k|General Assembly's 2015 Data Science course in Washington, DC|\n|[NeMo](https://github.com/NVIDIA/NeMo)|1.5k|NeMo: a toolkit for conversational AI|\n|[Machine-Learning-Flappy-Bird](https://github.com/ssusnic/Machine-Learning-Flappy-Bird)|1.5k|Machine Learning for Flappy Bird using Neural Network and Genetic Algorithm|\n|[ml-visuals](https://github.com/dair-ai/ml-visuals)|1.5k|Visuals contains figures and templates which you can reuse and customize to improve your scientific writing.|\n|[GANimation](https://github.com/albertpumarola/GANimation)|1.5k|GANimation: Anatomically-aware Facial Animation from a Single Image (ECCV'18 Oral) [PyTorch]|\n|[EagleEye](https://github.com/ThoughtfulDev/EagleEye)|1.5k|Stalk your Friends. Find their Instagram, FB and Twitter Profiles using Image Recognition and Reverse Image Search.|\n|[PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)|1.5k|A PyTorch CV Toolkit|\n|[spark](https://github.com/dotnet/spark)|1.5k|.NET for Apache\u00ae Spark\u2122 makes Apache Spark\u2122 easily accessible to .NET developers.|\n|[quiver](https://github.com/keplr-io/quiver)|1.5k|Interactive convnet features visualization for Keras|\n|[MachineLearning](https://github.com/jindongwang/MachineLearning)|1.5k|\u4e00\u4e9b\u5173\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5b66\u4e60\u8d44\u6599\u4e0e\u7814\u7a76\u4ecb\u7ecd|\n|[GAT](https://github.com/PetarV-/GAT)|1.5k|Graph Attention Networks (https://arxiv.org/abs/1710.10903)|\n|[mt-dnn](https://github.com/namisan/mt-dnn)|1.4k|Multi-Task Deep Neural Networks for Natural Language Understanding|\n|[deep-neuroevolution](https://github.com/uber-research/deep-neuroevolution)|1.4k|Deep Neuroevolution|\n|[a-PyTorch-Tutorial-to-Object-Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)|1.4k|SSD: Single Shot MultiBox Detector | a PyTorch Tutorial to Object Detection|\n|[labelbox](https://github.com/Labelbox/labelbox)|1.4k|Labelbox is the fastest way to annotate data to build and ship computer vision applications.|\n|[openvino](https://github.com/openvinotoolkit/openvino)|1.4k|OpenVINO\u2122 Toolkit repository|\n|[awesome-decision-tree-papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)|1.4k|A collection of research papers on decision, classification and regression trees with implementations.|\n|[project_alias](https://github.com/bjoernkarmann/project_alias)|1.4k|Alias is a teachable \u201cparasite\u201d that is designed to give users more control over their smart assistants, both when it comes to customisation and privacy. Through a simple app the user can train Alias to react on a custom wake-word/sound, and once trained, Alias can take control over your home assistant by activating it for you.|\n|[data-science-question-answer](https://github.com/ShuaiW/data-science-question-answer)|1.4k|A repo for data science related questions and answers|\n|[photo2cartoon](https://github.com/minivision-ai/photo2cartoon)|1.4k|\u4eba\u50cf\u5361\u901a\u5316\u63a2\u7d22\u9879\u76ee (photo-to-cartoon translation project)|\n|[video2x](https://github.com/k4yt3x/video2x)|1.4k|A lossless video/GIF/image upscaler achieved with waifu2x, Anime4K, SRMD and RealSR. Started in Hack the Valley 2, 2018.|\n|[Tengine](https://github.com/OAID/Tengine)|1.4k|Tengine is a lite, high performance, modular inference engine for embedded device|\n|[cuml](https://github.com/rapidsai/cuml)|1.4k|cuML - RAPIDS Machine Learning Library|\n|[BentoML](https://github.com/bentoml/BentoML)|1.4k|Model Serving Made Easy|\n|[GANotebooks](https://github.com/tjwei/GANotebooks)|1.4k|wgan, wgan2(improved, gp), infogan, and dcgan implementation in lasagne, keras, pytorch|\n|[MobileNet](https://github.com/Zehaos/MobileNet)|1.4k|MobileNet build with Tensorflow|\n|[CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch)|1.4k|Official implementation of Character Region Awareness for Text Detection (CRAFT)|\n|[mlr](https://github.com/mlr-org/mlr)|1.4k|Machine Learning in R|\n|[monodepth2](https://github.com/nianticlabs/monodepth2)|1.4k|Monocular depth estimation from a single image|\n|[TensorKart](https://github.com/kevinhughes27/TensorKart)|1.4k|self-driving MarioKart with TensorFlow|\n|[keras-contrib](https://github.com/keras-team/keras-contrib)|1.4k|Keras community contributions|\n|[stellargraph](https://github.com/stellargraph/stellargraph)|1.4k|StellarGraph - Machine Learning on Graphs|\n|[GDLnotes](https://github.com/ahangchen/GDLnotes)|1.4k|Google Deep Learning Notes\uff08TensorFlow\u6559\u7a0b\uff09|\n|[pydensecrf](https://github.com/lucasb-eyer/pydensecrf)|1.4k|Python wrapper to Philipp Kr\u00e4henb\u00fchl's dense (fully connected) CRFs with gaussian edge potentials.|\n|[seldon-server](https://github.com/SeldonIO/seldon-server)|1.4k|Machine Learning Platform and Recommendation Engine built on Kubernetes|\n|[chainercv](https://github.com/chainer/chainercv)|1.4k|ChainerCV: a Library for Deep Learning in Computer Vision|\n|[tensorflow-nlp](https://github.com/zhedongzheng/tensorflow-nlp)|1.4k|Building blocks for NLP and Text Generation in TensorFlow 2.x / 1.x|\n|[iOS_ML](https://github.com/alexsosn/iOS_ML)|1.4k|List of Machine Learning, AI, NLP solutions for iOS. The most recent version of this article can be found on my blog.|\n|[tfgo](https://github.com/galeone/tfgo)|1.4k|Tensorflow + Go, the gopher way|\n|[bi-att-flow](https://github.com/allenai/bi-att-flow)|1.4k|Bi-directional Attention Flow (BiDAF) network is a multi-stage hierarchical process that represents context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization.|\n|[CoreML-in-ARKit](https://github.com/hanleyweng/CoreML-in-ARKit)|1.4k|Simple project to detect objects and display 3D labels above them in AR. This serves as a basic Template for an ARKit project to use CoreML.|\n|[lightning](https://github.com/scikit-learn-contrib/lightning)|1.4k|Large-scale linear classification, regression and ranking in Python|\n|[DeepFace](https://github.com/RiweiChen/DeepFace)|1.4k|Face analysis mainly based on Caffe. At this time, face analysis tasks like detection, alignment and recognition have been done.|\n|[torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)|1.4k|An easy to use PyTorch to TensorRT converter|\n|[deepvoice3_pytorch](https://github.com/r9y9/deepvoice3_pytorch)|1.4k|PyTorch implementation of convolutional neural networks-based text-to-speech synthesis models|\n|[sphereface](https://github.com/wy1iu/sphereface)|1.4k|Implementation for <SphereFace: Deep Hypersphere Embedding for Face Recognition> in CVPR'17.|\n|[jeelizFaceFilter](https://github.com/jeeliz/jeelizFaceFilter)|1.4k|Javascript/WebGL lightweight face tracking library designed for augmented reality webcam filters. Features : multiple faces detection, rotation, mouth opening. Various integration examples are provided (Three.js, Babylon.js, FaceSwap, Canvas2D, CSS3D...).|\n|[kaggle-web-traffic](https://github.com/Arturus/kaggle-web-traffic)|1.4k|1st place solution|\n|[minimalRL](https://github.com/seungeunrho/minimalRL)|1.4k|Implementations of basic RL algorithms with minimal lines of codes! (pytorch based)|\n|[Machine-Learning-Notes](https://github.com/Sophia-11/Machine-Learning-Notes)|1.4k|\u5468\u5fd7\u534e\u300a\u673a\u5668\u5b66\u4e60\u300b\u624b\u63a8\u7b14\u8bb0|\n|[Gen.jl](https://github.com/probcomp/Gen.jl)|1.4k|A general-purpose probabilistic programming system with programmable inference|\n|[faster_rcnn_pytorch](https://github.com/longcw/faster_rcnn_pytorch)|1.4k|Faster RCNN with PyTorch|\n|[sod](https://github.com/symisc/sod)|1.4k|An Embedded Computer Vision & Machine Learning Library (CPU Optimized & IoT Capable)|\n|[keras_to_tensorflow](https://github.com/amir-abdi/keras_to_tensorflow)|1.4k|General code to convert a trained keras model into an inference tensorflow model|\n|[handtracking](https://github.com/victordibia/handtracking)|1.3k|Building a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow|\n|[word-rnn-tensorflow](https://github.com/hunkim/word-rnn-tensorflow)|1.3k|Multi-layer Recurrent Neural Networks (LSTM, RNN) for word-level language models in Python using TensorFlow.|\n|[TorchCraft](https://github.com/TorchCraft/TorchCraft)|1.3k|Connecting Torch to StarCraft|\n|[AndroidTensorFlowMachineLearningExample](https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample)|1.3k|Android TensorFlow MachineLearning Example (Building TensorFlow for Android)|\n|[magnitude](https://github.com/plasticityai/magnitude)|1.3k|A fast, efficient universal vector embedding utility package.|\n|[hackermath](https://github.com/amitkaps/hackermath)|1.3k|Introduction to Statistics and Basics of Mathematics for Data Science - The Hacker's Way|\n|[pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam)|1.3k|PyTorch implementation of Grad-CAM|\n|[grenade](https://github.com/HuwCampbell/grenade)|1.3k|Deep Learning in Haskell|\n|[captcha_trainer](https://github.com/kerlomz/captcha_trainer)|1.3k|[\u9a8c\u8bc1\u7801\u8bc6\u522b-\u8bad\u7ec3] This project is based on CNN/ResNet/DenseNet+GRU/LSTM+CTC/CrossEntropy to realize verification code identification. This project is only for training the model.|\n|[anago](https://github.com/Hironsan/anago)|1.3k|Bidirectional LSTM-CRF and ELMo for Named-Entity Recognition, Part-of-Speech Tagging and so on.|\n|[mne-python](https://github.com/mne-tools/mne-python)|1.3k|MNE : Magnetoencephalography (MEG) and Electroencephalography (EEG) in Python|\n|[eos](https://github.com/patrikhuber/eos)|1.3k|A lightweight 3D Morphable Face Model fitting library in modern C++14|\n|[ngraph](https://github.com/NervanaSystems/ngraph)|1.3k|nGraph - open source C++ library, compiler and runtime for Deep Learning|\n|[NSFWDetector](https://github.com/lovoo/NSFWDetector)|1.3k|A NSFW (aka porn) detector with CoreML|\n|[open_nsfw_android](https://github.com/devzwy/open_nsfw_android)|1.3k|\ud83d\udd25\ud83d\udd25\ud83d\udd25\u8272\u60c5\u56fe\u7247\u79bb\u7ebf\u8bc6\u522b\uff0c\u57fa\u4e8eTensorFlow\u5b9e\u73b0\u3002\u8bc6\u522b\u53ea\u970020ms,\u53ef\u65ad\u7f51\u6d4b\u8bd5\uff0c\u6210\u529f\u738799%\uff0c\u8c03\u7528\u53ea\u8981\u4e00\u884c\u4ee3\u7801\uff0c\u4ece\u96c5\u864e\u7684\u5f00\u6e90\u9879\u76eeopen_nsfw\u79fb\u690d\uff0c\u8be5\u6a21\u578b\u6587\u4ef6\u53ef\u7528\u4e8eiOS\u3001java\u3001C++\u7b49\u5e73\u53f0|\n|[cs230-code-examples](https://github.com/cs230-stanford/cs230-code-examples)|1.3k|Code examples in pyTorch and Tensorflow for CS230|\n|[pytorch-generative-adversarial-networks](https://github.com/devnag/pytorch-generative-adversarial-networks)|1.3k|A very simple generative adversarial network (GAN) in PyTorch|\n|[forecasting](https://github.com/microsoft/forecasting)|1.3k|Time Series Forecasting Best Practices & Examples|\n|[VIBE](https://github.com/mkocabas/VIBE)|1.3k|Official implementation of CVPR2020 paper \"VIBE: Video Inference for Human Body Pose and Shape Estimation\"|\n|[bulbea](https://github.com/achillesrasquinha/bulbea)|1.3k|\ud83d\udc17 \ud83d\udc3b Deep Learning based Python Library for Stock Market Prediction and Modelling|\n|[electra](https://github.com/google-research/electra)|1.3k|ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators|\n|[scattertext](https://github.com/JasonKessler/scattertext)|1.3k|Beautiful visualizations of how language differs among document types.|\n|[talos](https://github.com/autonomio/talos)|1.3k|Hyperparameter Optimization for TensorFlow, Keras and PyTorch|\n|[practicalAI-cn](https://github.com/MLEveryday/practicalAI-cn)|1.3k|AI\u5b9e\u6218-practicalAI \u4e2d\u6587\u7248|\n|[impersonator](https://github.com/svip-lab/impersonator)|1.3k|PyTorch implementation of our ICCV 2019 paper: Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis|\n|[gluon-ts](https://github.com/awslabs/gluon-ts)|1.3k|Probabilistic time series modeling in Python|\n|[dcgan-completion.tensorflow](https://github.com/bamos/dcgan-completion.tensorflow)|1.3k|Image Completion with Deep Learning in TensorFlow|\n|[EfficientDet.Pytorch](https://github.com/toandaominh1997/EfficientDet.Pytorch)|1.3k|Implementation EfficientDet: Scalable and Efficient Object Detection in PyTorch|\n|[NeuronBlocks](https://github.com/microsoft/NeuronBlocks)|1.3k|NLP DNN Toolkit - Building Your NLP DNN Models Like Playing Lego|\n|[yolo2-pytorch](https://github.com/longcw/yolo2-pytorch)|1.3k|YOLOv2 in PyTorch|\n|[EmojiIntelligence](https://github.com/BilalReffas/EmojiIntelligence)|1.3k|Neural Network built in Apple Playground using Swift|\n|[efficientnet](https://github.com/qubvel/efficientnet)|1.3k|Implementation of EfficientNet model. Keras and TensorFlow Keras.|\n|[YOLOv3_TensorFlow](https://github.com/wizyoung/YOLOv3_TensorFlow)|1.3k|Complete YOLO v3 TensorFlow implementation. Support training on your own dataset.|\n|[TensorFlow.NET](https://github.com/SciSharp/TensorFlow.NET)|1.3k|.NET Standard bindings for Google's TensorFlow for developing, training and deploying Machine Learning models in C#.|\n|[pytextrank](https://github.com/DerwenAI/pytextrank)|1.3k|Python implementation of TextRank for phrase extraction and summarization of text documents|\n|[infer](https://github.com/dotnet/infer)|1.3k|Infer.NET is a framework for running Bayesian inference in graphical models|\n|[uda](https://github.com/google-research/uda)|1.3k|Unsupervised Data Augmentation (UDA)|\n|[mmaction](https://github.com/open-mmlab/mmaction)|1.3k|An open-source toolbox for action understanding based on PyTorch|\n|[spark-nlp](https://github.com/JohnSnowLabs/spark-nlp)|1.3k|State of the Art Natural Language Processing|\n|[pytorch-semantic-segmentation](https://github.com/zijundeng/pytorch-semantic-segmentation)|1.3k|PyTorch for Semantic Segmentation|\n|[Deep-Learning-Boot-Camp](https://github.com/QuantScientist/Deep-Learning-Boot-Camp)|1.2k|A community run, 5-day PyTorch Deep Learning Bootcamp|\n|[pytorch-openai-transformer-lm](https://github.com/huggingface/pytorch-openai-transformer-lm)|1.2k|\ud83d\udc25A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI|\n|[hiddenlayer](https://github.com/waleedka/hiddenlayer)|1.2k|Neural network graphs and training metrics for PyTorch, Tensorflow, and Keras.|\n|[PaddleHub](https://github.com/PaddlePaddle/PaddleHub)|1.2k|Toolkit for Pre-trained Model Application of PaddlePaddle\uff08\u300e\u98de\u6868\u300f\u9884\u8bad\u7ec3\u6a21\u578b\u5e94\u7528\u5de5\u5177 \uff09|\n|[PhotographicImageSynthesis](https://github.com/CQFIO/PhotographicImageSynthesis)|1.2k|Photographic Image Synthesis with Cascaded Refinement Networks|\n|[alpr-unconstrained](https://github.com/sergiomsilva/alpr-unconstrained)|1.2k|License Plate Detection and Recognition in Unconstrained Scenarios|\n|[Deep-Image-Analogy](https://github.com/msracver/Deep-Image-Analogy)|1.2k|The source code of 'Visual Attribute Transfer through Deep Image Analogy'.|\n|[spektral](https://github.com/danielegrattarola/spektral)|1.2k|Graph Neural Networks with Keras and Tensorflow 2.|\n|[DeepAA](https://github.com/OsciiArt/DeepAA)|1.2k|make ASCII Art by Deep Learning|\n|[vvedenie-mashinnoe-obuchenie](https://github.com/demidovakatya/vvedenie-mashinnoe-obuchenie)|1.2k|\ud83d\udcdd \u041f\u043e\u0434\u0431\u043e\u0440\u043a\u0430 \u0440\u0435\u0441\u0443\u0440\u0441\u043e\u0432 \u043f\u043e \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u043c\u0443 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e|\n|[OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq)|1.2k|Toolkit for efficient experimentation with Speech Recognition, Text2Speech and NLP|\n|[Forge](https://github.com/hollance/Forge)|1.2k|A neural network toolkit for Metal|\n|[keras-yolo3](https://github.com/experiencor/keras-yolo3)|1.2k|Training and Detecting Objects with YOLO3|\n|[NiftyNet](https://github.com/NifTK/NiftyNet)|1.2k|[unmaintained] An open-source convolutional neural networks platform for research in medical image analysis and image-guided therapy|\n|[Awesome-TensorFlow-Chinese](https://github.com/fendouai/Awesome-TensorFlow-Chinese)|1.2k|Awesome-TensorFlow-Chinese\uff0cTensorFlow \u4e2d\u6587\u8d44\u6e90\u7cbe\u9009\uff0c\u5b98\u65b9\u7f51\u7ad9\uff0c\u5b89\u88c5\u6559\u7a0b\uff0c\u5165\u95e8\u6559\u7a0b\uff0c\u89c6\u9891\u6559\u7a0b\uff0c\u5b9e\u6218\u9879\u76ee\uff0c\u5b66\u4e60\u8def\u5f84\u3002QQ\u7fa4\uff1a167122861\uff0c\u516c\u4f17\u53f7\uff1a\u78d0\u521bAI\uff0c\u5fae\u4fe1\u7fa4\u4e8c\u7ef4\u7801\uff1ahttp://www.tensorflownews.com/|\n|[StudyBook](https://github.com/changwookjun/StudyBook)|1.2k|Study E-Book(ComputerVision DeepLearning MachineLearning Math NLP Python ReinforcementLearning)|\n|[awesome-semantic-segmentation-pytorch](https://github.com/Tramac/awesome-semantic-segmentation-pytorch)|1.2k|Semantic Segmentation on PyTorch (include FCN, PSPNet, Deeplabv3, Deeplabv3+, DANet, DenseASPP, BiSeNet, EncNet, DUNet, ICNet, ENet, OCNet, CCNet, PSANet, CGNet, ESPNet, LEDNet, DFANet)|\n|[RFBNet](https://github.com/ruinmessi/RFBNet)|1.2k|Receptive Field Block Net for Accurate and Fast Object Detection, ECCV 2018|\n|[GPflow](https://github.com/GPflow/GPflow)|1.2k|Gaussian processes in TensorFlow|\n|[dlwpt-code](https://github.com/deep-learning-with-pytorch/dlwpt-code)|1.2k|Code for the book Deep Learning with PyTorch by Eli Stevens, Luca Antiga, and Thomas Viehmann.|\n|[dlcv_for_beginners](https://github.com/frombeijingwithlove/dlcv_for_beginners)|1.2k|\u300a\u6df1\u5ea6\u5b66\u4e60\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u300b\u914d\u5957\u4ee3\u7801|\n|[Deep-Learning-with-PyTorch-Tutorials](https://github.com/dragen1860/Deep-Learning-with-PyTorch-Tutorials)|1.2k|\u6df1\u5ea6\u5b66\u4e60\u4e0ePyTorch\u5165\u95e8\u5b9e\u6218\u89c6\u9891\u6559\u7a0b \u914d\u5957\u6e90\u4ee3\u7801\u548cPPT|\n|[DLTK](https://github.com/DLTK/DLTK)|1.2k|Deep Learning Toolkit for Medical Image Analysis|\n|[FCN.tensorflow](https://github.com/shekkizh/FCN.tensorflow)|1.2k|Tensorflow implementation of Fully Convolutional Networks for Semantic Segmentation (http://fcn.berkeleyvision.org)|\n|[facenet-pytorch](https://github.com/timesler/facenet-pytorch)|1.2k|Pretrained Pytorch face detection (MTCNN) and recognition (InceptionResnet) models|\n|[h2o-tutorials](https://github.com/h2oai/h2o-tutorials)|1.2k|Tutorials and training material for the H2O Machine Learning Platform|\n|[nlp-journey](https://github.com/msgi/nlp-journey)|1.2k|Documents, papers and codes related to Natural Language Processing, including Topic Model, Word Embedding, Named Entity Recognition, Text Classificatin, Text Generation, Text Similarity, Machine Translation)\uff0cetc. All codes are implemented intensorflow 2.0.|\n|[object_detector_app](https://github.com/datitran/object_detector_app)|1.2k|Real-Time Object Recognition App with Tensorflow and OpenCV|\n|[tnt](https://github.com/pytorch/tnt)|1.2k|Simple tools for logging and visualizing, loading and training|\n|[tensorflow-deeplab-resnet](https://github.com/DrSleep/tensorflow-deeplab-resnet)|1.2k|DeepLab-ResNet rebuilt in TensorFlow|\n|[reproducible-image-denoising-state-of-the-art](https://github.com/wenbihan/reproducible-image-denoising-state-of-the-art)|1.2k|Collection of popular and reproducible image denoising works.|\n|[senet.pytorch](https://github.com/moskomule/senet.pytorch)|1.2k|PyTorch implementation of SENet|\n|[pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)|1.2k|An open source framework for seq2seq models in PyTorch.|\n|[efficient_densenet_pytorch](https://github.com/gpleiss/efficient_densenet_pytorch)|1.2k|A memory-efficient implementation of DenseNets|\n|[pytorch-retinanet](https://github.com/yhenon/pytorch-retinanet)|1.2k|Pytorch implementation of RetinaNet object detection.|\n|[cakechat](https://github.com/lukalabs/cakechat)|1.2k|CakeChat: Emotional Generative Dialog System|\n|[pytorch-fcn](https://github.com/wkentaro/pytorch-fcn)|1.2k|PyTorch Implementation of Fully Convolutional Networks. (Training code to reproduce the original result is available.)|\n|[python-machine-learning-book-3rd-edition](https://github.com/rasbt/python-machine-learning-book-3rd-edition)|1.2k|The \"Python Machine Learning (3rd edition)\" book code repository|\n|[tf-quant-finance](https://github.com/google/tf-quant-finance)|1.2k|High-performance TensorFlow library for quantitative finance.|\n|[gnes](https://github.com/gnes-ai/gnes)|1.1k|GNES is Generic Neural Elastic Search, a cloud-native semantic search system based on deep neural network.|\n|[Image-OutPainting](https://github.com/bendangnuksung/Image-OutPainting)|1.1k|\ud83c\udfd6 Keras Implementation of Painting outside the box|\n|[Fabrik](https://github.com/Cloud-CV/Fabrik)|1.1k|\ud83c\udfed Collaboratively build, visualize, and design neural nets in browser|\n|[attention-transfer](https://github.com/szagoruyko/attention-transfer)|1.1k|Improving Convolutional Networks via Attention Transfer (ICLR 2017)|\n|[pytorch-cifar100](https://github.com/weiaicunzai/pytorch-cifar100)|1.1k|Practice on cifar100(ResNet, DenseNet, VGG, GoogleNet, InceptionV3, InceptionV4, Inception-ResNetv2, Xception, Resnet In Resnet, ResNext,ShuffleNet, ShuffleNetv2, MobileNet, MobileNetv2, SqueezeNet, NasNet, Residual Attention Network, SENet)|\n|[MONAI](https://github.com/Project-MONAI/MONAI)|1.1k|AI Toolkit for Healthcare Imaging|\n|[tensorrec](https://github.com/jfkirk/tensorrec)|1.1k|A TensorFlow recommendation algorithm and framework in Python.|\n|[mleap](https://github.com/combust/mleap)|1.1k|MLeap: Deploy Spark Pipelines to Production|\n|[noreward-rl](https://github.com/pathak22/noreward-rl)|1.1k|[ICML 2017] TensorFlow code for Curiosity-driven Exploration for Deep Reinforcement Learning|\n|[PyTorchNLPBook](https://github.com/joosthub/PyTorchNLPBook)|1.1k|Code and data accompanying Natural Language Processing with PyTorch published by O'Reilly Media https://nlproc.info|\n|[mtcnn](https://github.com/ipazc/mtcnn)|1.1k|MTCNN face detection implementation for TensorFlow, as a PIP package.|\n|[WaveRNN](https://github.com/fatchord/WaveRNN)|1.1k|WaveRNN Vocoder + TTS|\n|[UNet-family](https://github.com/ShawnBIT/UNet-family)|1.1k|Paper and implementation of UNet-related model.|\n|[Awesome-pytorch-list-CNVersion](https://github.com/xavier-zy/Awesome-pytorch-list-CNVersion)|1.1k|Awesome-pytorch-list \u7ffb\u8bd1\u5de5\u4f5c\u8fdb\u884c\u4e2d......|\n|[tensorflow-fcn](https://github.com/MarvinTeichmann/tensorflow-fcn)|1.1k|An Implementation of Fully Convolutional Networks in Tensorflow.|\n|[BicycleGAN](https://github.com/junyanz/BicycleGAN)|1.1k|Toward Multimodal Image-to-Image Translation|\n|[TensorFlow2.0-Examples](https://github.com/YunYang1994/TensorFlow2.0-Examples)|1.1k|\ud83d\ude44 Difficult algorithm, simple code.|\n|[fast-autoaugment](https://github.com/kakaobrain/fast-autoaugment)|1.1k|Official Implementation of 'Fast AutoAugment' in PyTorch.|\n|[fastai_deeplearn_part1](https://github.com/reshamas/fastai_deeplearn_part1)|1.1k|Notes for Fastai Deep Learning Course|\n|[HyperGAN](https://github.com/HyperGAN/HyperGAN)|1.1k|Composable GAN framework with api and user interface|\n|[home](https://github.com/apachecn/home)|1.1k|ApacheCN \u5f00\u6e90\u7ec4\u7ec7\uff1a\u516c\u544a\u3001\u4ecb\u7ecd\u3001\u6210\u5458\u3001\u6d3b\u52a8\u3001\u4ea4\u6d41\u65b9\u5f0f|\n|[tfx](https://github.com/tensorflow/tfx)|1.1k|TFX is an end-to-end platform for deploying production ML pipelines|\n|[handwriting-synthesis](https://github.com/sjvasquez/handwriting-synthesis)|1.1k|Handwriting Synthesis with RNNs \u270f\ufe0f|\n|[image-quality-assessment](https://github.com/idealo/image-quality-assessment)|1.1k|Convolutional Neural Networks to predict the aesthetic and technical quality of images.|\n|[PerceptualSimilarity](https://github.com/richzhang/PerceptualSimilarity)|1.1k|Learned Perceptual Image Patch Similarity (LPIPS) metric. In CVPR, 2018.|\n|[lanenet-lane-detection](https://github.com/MaybeShewill-CV/lanenet-lane-detection)|1.1k|Unofficial implemention of lanenet model for real time lane detection using deep neural network model https://maybeshewill-cv.github.io/lanenet-lane-detection/|\n|[uTensor](https://github.com/uTensor/uTensor)|1.1k|TinyML AI inference library|\n|[torchgan](https://github.com/torchgan/torchgan)|1.1k|Research Framework for easy and efficient training of GANs based on Pytorch|\n|[merlin](https://github.com/CSTR-Edinburgh/merlin)|1.1k|This is now the official location of the Merlin project.|\n|[CLUE](https://github.com/CLUEbenchmark/CLUE)|1k|\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bc4 Chinese Language Understanding Evaluation Benchmark: datasets, baselines, pre-trained models, corpus and leaderboard|\n|[tfjs-node](https://github.com/tensorflow/tfjs-node)|1k|TensorFlow powered JavaScript library for training and deploying ML models on Node.js.|\n|[CycleGAN-TensorFlow](https://github.com/vanhuyz/CycleGAN-TensorFlow)|1k|An implementation of CycleGan using TensorFlow|\n|[EffectivePyTorch](https://github.com/vahidk/EffectivePyTorch)|1k|PyTorch tutorials and best practices.|\n|[hercules](https://github.com/src-d/hercules)|1k|Gaining advanced insights from Git repository history.|\n|[AdvancedEAST](https://github.com/huoyijie/AdvancedEAST)|1k|AdvancedEAST is an algorithm used for Scene image text detect, which is primarily based on EAST, and the significant improvement was also made, which make long text predictions more accurate.|\n|[one-pixel-attack-keras](https://github.com/Hyperparticle/one-pixel-attack-keras)|1k|Keras implementation of \"One pixel attack for fooling deep neural networks\" using differential evolution on Cifar10 and ImageNet|\n|[CRNN_Chinese_Characters_Rec](https://github.com/Sierkinhane/CRNN_Chinese_Characters_Rec)|1k|(CRNN) Chinese Characters Recognition.|\n|[hmtl](https://github.com/huggingface/hmtl)|1k|\ud83c\udf0aHMTL: Hierarchical Multi-Task Learning - A State-of-the-Art neural network model for several NLP tasks based on PyTorch and AllenNLP|\n|[rethinking-network-pruning](https://github.com/Eric-mingjie/rethinking-network-pruning)|1k|Rethinking the Value of Network Pruning (Pytorch) (ICLR 2019)|\n|[pytorch-classification](https://github.com/bearpaw/pytorch-classification)|1k|Classification with PyTorch.|\n|[a-PyTorch-Tutorial-to-Image-Captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)|1k|Show, Attend, and Tell | a PyTorch Tutorial to Image Captioning|\n|[reformer-pytorch](https://github.com/lucidrains/reformer-pytorch)|1k|Reformer, the efficient Transformer, in Pytorch|\n|[pytorch-YOLOv4](https://github.com/Tianxiaomo/pytorch-YOLOv4)|1k|PyTorch ,ONNX and TensorRT implementation of YOLOv4|\n|[FaceMaskDetection](https://github.com/AIZOOTech/FaceMaskDetection)|1k|\u5f00\u6e90\u4eba\u8138\u53e3\u7f69\u68c0\u6d4b\u6a21\u578b\u548c\u6570\u636e Detect faces and determine whether people are wearing mask.|\n|[gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch)|1k|Pretrained EfficientNet, EfficientNet-Lite, MixNet, MobileNetV3 / V2, MNASNet A1 and B1, FBNet, Single-Path NAS|\n|[open-reid](https://github.com/Cysu/open-reid)|1k|Open source person re-identification library in python|\n|[wgan-gp](https://github.com/caogang/wgan-gp)|1k|A pytorch implementation of Paper \"Improved Training of Wasserstein GANs\"|\n\n"
 },
 {
  "repo": "py-why/dowhy",
  "language": "Python",
  "readme_contents": "|BuildStatus|_ |PyPiVersion|_ |PythonSupport|_ |Downloads|_\n\n.. |PyPiVersion| image:: https://img.shields.io/pypi/v/dowhy.svg\n.. _PyPiVersion: https://pypi.org/project/dowhy/\n\n.. |PythonSupport| image:: https://img.shields.io/pypi/pyversions/dowhy.svg\n.. _PythonSupport: https://pypi.org/project/dowhy/\n\n.. |BuildStatus| image:: https://github.com/microsoft/dowhy/workflows/Python%20package/badge.svg\n.. _BuildStatus: https://github.com/microsoft/dowhy/actions\n\n.. |Downloads| image:: https://pepy.tech/badge/dowhy\n.. _Downloads: https://pepy.tech/project/dowhy\n\nDoWhy | An end-to-end library for causal inference\n===================================================\n\n  Introducing DoWhy and the 4 steps of causal inference | `Microsoft Research Blog <https://www.microsoft.com/en-us/research/blog/dowhy-a-library-for-causal-inference/>`_ | `Video Tutorial <https://note.microsoft.com/MSR-Webinar-DoWhy-Library-Registration-On-Demand.html>`_ | `Arxiv Paper <https://arxiv.org/abs/2011.04216>`_ | `Arxiv Paper (GCM-extension) <https://arxiv.org/abs/2206.06821>`_ | `Slides <https://www2.slideshare.net/AmitSharma315/dowhy-an-endtoend-library-for-causal-inference>`_\n\n  Read the `docs <https://py-why.github.io/dowhy/>`_ | Try it online! |Binder|_\n\n.. |Binder| image:: https://mybinder.org/badge_logo.svg\n.. _Binder: https://mybinder.org/v2/gh/microsoft/dowhy/main?filepath=docs%2Fsource%2F\n\n**Case Studies using DoWhy**: `Hotel booking cancellations <https://towardsdatascience.com/beyond-predictive-models-the-causal-story-behind-hotel-booking-cancellations-d29e8558cbaf>`_ | `Effect of customer loyalty programs <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_example_effect_of_memberrewards_program.ipynb>`_ | `Optimizing article headlines <https://medium.com/@akelleh/introducing-the-do-sampler-for-causal-inference-a3296ea9e78d>`_ | `Effect of home visits on infant health (IHDP) <https://towardsdatascience.com/implementing-causal-inference-a-key-step-towards-agi-de2cde8ea599>`_ | `Causes of customer churn/attrition <https://medium.com/geekculture/a-quickstart-for-causal-analysis-decision-making-with-dowhy-2ce2d4d1efa9>`_\n\n.. image:: https://raw.githubusercontent.com/microsoft/dowhy/main/docs/images/dowhy-schematic.png\n\nAs computing systems are more frequently and more actively intervening in societally critical domains such as healthcare, education, and governance, it is critical to correctly predict and understand the causal effects of these interventions. Without an A/B test, conventional machine learning methods, built on pattern recognition and correlational analyses, are insufficient for decision-making.\n\nMuch like machine learning libraries have done for prediction, **\"DoWhy\" is a Python library that aims to spark causal thinking and analysis**. DoWhy provides a principled four-step interface for causal inference that focuses on explicitly modeling causal assumptions and validating them as much as possible. The key feature of DoWhy is its state-of-the-art refutation API that can automatically test causal assumptions for any estimation method, thus making inference more robust and accessible to non-experts. DoWhy supports estimation of the average causal effect for backdoor, frontdoor, instrumental variable and other identification methods, and estimation of the conditional effect (CATE) through an integration with the EconML library.\n\nFor a quick introduction to causal inference, check out `amit-sharma/causal-inference-tutorial <https://github.com/amit-sharma/causal-inference-tutorial/>`_. We also gave a more comprehensive tutorial at the ACM Knowledge Discovery and Data Mining (`KDD 2018 <http://www.kdd.org/kdd2018/>`_) conference: `causalinference.gitlab.io/kdd-tutorial <http://causalinference.gitlab.io/kdd-tutorial/>`_. For an introduction to the four steps of causal inference and its implications for machine learning, you can access this video tutorial from Microsoft Research: `DoWhy Webinar <https://note.microsoft.com/MSR-Webinar-DoWhy-Library-Registration-On-Demand.html>`_.\n\nDocumentation for DoWhy is available at `py-why.github.io/dowhy <https://py-why.github.io/dowhy/>`_.\n\n.. i here comment toctree::\n.. i here comment   :maxdepth: 4\n.. i here comment   :caption: Contents:\n.. contents:: **Contents**\n\nNews\n-----\n**2022.05.27**:\n\n* **DoWhy now part of PyWhy**\n\n  We have moved DoWhy from microsoft/dowhy to py-why/dowhy. While GitHub will automatically\n  redirect your git command for cloning, pulling, etc., we recommend updating git remotes and bookmarks. Please note\n  that the **documentation** has now moved to https://py-why.github.io/dowhy with **no** redirect from the old URL.\n\n* **Experimental support for GCM-based inference**\n\n  We have started adding support for graphical causal model-based inference (or in short GCM-based). At the moment,\n  this includes support for interventions, counterfactuals, and attributing distribution changes. As part of this,\n  we also added features for Shapley value estimation and independence tests. We're still in the process of fleshing\n  everything out, including `documentation <https://py-why.github.io/dowhy/main/user_guide/gcm_based_inference/index.html>`_. Some of it is already on `main\n  <https://py-why.github.io/dowhy/main/user_guide/gcm_based_inference/index.html>`_, other parts are on feature branches (prefixed with ``gcm-``) with open\n  pull-requests, other parts will appear as new pull-requests in the next couple of weeks. Be sure to watch this space\n  here as we quickly expand functionality and documentation.\n\nThe need for causal inference\n----------------------------------\n\nPredictive models uncover patterns that connect the inputs and outcome in observed data. To intervene, however, we need to estimate the effect of changing an input from its current value, for which no data exists. Such questions, involving estimating a *counterfactual*, are common in decision-making scenarios.\n\n* Will it work?\n    * Does a proposed change to a system improve people's outcomes?\n* Why did it work?\n    * What led to a change in a system's outcome?\n* What should we do?\n    * What changes to a system are likely to improve outcomes for people?\n* What are the overall effects?\n    * How does the system interact with human behavior?\n    * What is the effect of a system's recommendations on people's activity?\n\nAnswering these questions requires causal reasoning. While many methods exist\nfor causal inference, it is hard to compare their assumptions and robustness of results. DoWhy makes three contributions,\n\n1. Provides a principled way of modeling a given problem as a causal graph so\n   that all assumptions are explicit.\n2. Provides a unified interface for many popular causal inference methods, combining the two major frameworks of graphical models and potential outcomes.\n3. Automatically tests for the validity of assumptions if possible and assesses\n   the robustness of the estimate to violations.\n\nTo see DoWhy in action, check out how it can be applied to estimate the effect\nof a subscription or rewards program for customers [`Rewards notebook\n<https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_example_effect_of_memberrewards_program.ipynb>`_] and for implementing and evaluating causal inference methods on benchmark datasets like the `Infant Health and Development Program (IHDP) <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_ihdp_data_example.ipynb>`_ dataset, `Infant Mortality (Twins) <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_twins_example.ipynb>`_ dataset, and the `Lalonde Jobs <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_lalonde_example.ipynb>`_ dataset.\n\n\nInstallation\n-------------\n\nDoWhy support Python 3.6+. To install, you can use pip or conda.\n\n**Latest Release**\n\nInstall the latest `release <https://pypi.org/project/dowhy/>`__ using pip.\n\n.. code:: shell\n\n   pip install dowhy\n\nInstall the latest `release <https://anaconda.org/conda-forge/dowhy>`__ using conda.\n\n.. code:: shell\n\n   conda install -c conda-forge dowhy\n\nIf you face \"Solving environment\" problems with conda, then try :code:`conda update --all` and then install dowhy. If that does not work, then use :code:`conda config --set channel_priority false` and try to install again. If the problem persists, please `add your issue here <https://github.com/microsoft/dowhy/issues/197>`_.\n\n**Development Version**\n\nIf you prefer the latest dev version, clone this repository and run the following command from the top-most folder of\nthe repository.\n\n.. code:: shell\n\n    pip install -e .\n\n**Requirements**\n\nDoWhy requires the following packages:\n\n* numpy\n* scipy\n* scikit-learn\n* pandas\n* networkx  (for analyzing causal graphs)\n* matplotlib (for general plotting)\n* sympy (for rendering symbolic expressions)\n\nIf you face any problems, try installing dependencies manually.\n\n.. code:: shell\n\n    pip install -r requirements.txt\n\nOptionally, if you wish to input graphs in the dot format, then install pydot (or pygraphviz).\n\n\nFor better-looking graphs, you can optionally install pygraphviz. To proceed,\nfirst install graphviz and then pygraphviz (on Ubuntu and Ubuntu WSL).\n\n.. code:: shell\n\n    sudo apt install graphviz libgraphviz-dev graphviz-dev pkg-config\n    ## from https://github.com/pygraphviz/pygraphviz/issues/71\n    pip install pygraphviz --install-option=\"--include-path=/usr/include/graphviz\" \\\n    --install-option=\"--library-path=/usr/lib/graphviz/\"\n\nSample causal inference analysis in DoWhy\n-------------------------------------------\nMost DoWhy\nanalyses for causal inference take 4 lines to write, assuming a\npandas dataframe df that contains the data:\n\n.. code:: python\n\n    from dowhy import CausalModel\n    import dowhy.datasets\n\n    # Load some sample data\n    data = dowhy.datasets.linear_dataset(\n        beta=10,\n        num_common_causes=5,\n        num_instruments=2,\n        num_samples=10000,\n        treatment_is_binary=True)\n\nDoWhy supports two formats for providing the causal graph: `gml <https://github.com/GunterMueller/UNI_PASSAU_FMI_Graph_Drawing>`_ (preferred) and `dot <http://www.graphviz.org/documentation/>`_. After loading in the data, we use the four main operations in DoWhy: *model*,\n*estimate*, *identify* and *refute*:\n\n.. code:: python\n\n    # I. Create a causal model from the data and given graph.\n    model = CausalModel(\n        data=data[\"df\"],\n        treatment=data[\"treatment_name\"],\n        outcome=data[\"outcome_name\"],\n        graph=data[\"gml_graph\"])\n\n    # II. Identify causal effect and return target estimands\n    identified_estimand = model.identify_effect()\n\n    # III. Estimate the target estimand using a statistical method.\n    estimate = model.estimate_effect(identified_estimand,\n                                     method_name=\"backdoor.propensity_score_matching\")\n\n    # IV. Refute the obtained estimate using multiple robustness checks.\n    refute_results = model.refute_estimate(identified_estimand, estimate,\n                                           method_name=\"random_common_cause\")\n\nDoWhy stresses on the interpretability of its output. At any point in the analysis,\nyou can inspect the untested assumptions, identified estimands (if any) and the\nestimate (if any). Here's a sample output of the linear regression estimator.\n\n.. image:: https://raw.githubusercontent.com/microsoft/dowhy/main/docs/images/regression_output.png\n\nFor a full code example, check out the `Getting Started with DoWhy <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_simple_example.ipynb>`_ notebook. You can also use Conditional Average Treatment Effect (CATE) estimation methods from other libraries such as EconML and CausalML, as shown in the `Conditional Treatment Effects <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy-conditional-treatment-effects.ipynb>`_ notebook. For more examples of using DoWhy, check out the Jupyter notebooks in `docs/source/example_notebooks <https://github.com/microsoft/dowhy/tree/main/docs/source/example_notebooks/>`_ or try them online at `Binder <https://mybinder.org/v2/gh/microsoft/dowhy/main?filepath=docs%2Fsource%2F>`_.\n\n\nGCM-based inference (experimental)\n----------------------------------\n\nGraphical causal model-based inference, or GCM-based inference for short, is an experimental addition to DoWhy. For\ndetails, check out the `documentation for the gcm sub-package <https://py-why.github.io/dowhy/main/user_guide/gcm_based_inference/index.html>`_. The basic\nrecipe for this API works as follows:\n\n.. code:: python\n\n    # 1. Modeling cause-effect relationships as a structural causal model\n    #    (causal graph + functional causal models):\n    scm = gcm.StructuralCausalModel(nx.DiGraph([('X', 'Y'), ('Y', 'Z')])) # X -> Y -> Z\n    scm.set_causal_mechanism('X', gcm.EmpiricalDistribution())\n    scm.set_causal_mechanism('Y', gcm.AdditiveNoiseModel(gcm.ml.create_linear_regressor()))\n    scm.set_causal_mechanism('Z', gcm.AdditiveNoiseModel(gcm.ml.create_linear_regressor()))\n\n    # 2. Fitting the SCM to the data:\n    gcm.fit(scm, data)\n\n    # 3. Answering a causal query based on the SCM:\n    results = gcm.<causal_query>(scm, ...)\n\nWhere <causal_query> can be one of multiple functions explained in `Answering Causal Questions <https://py-why.github.io/dowhy/main/user_guide/gcm_based_inference/answering_causal_questions/index.html>`_.\n\n\nA high-level Pandas API\n-----------------------\n\nWe've made an even simpler API for dowhy which is a light layer on top of the standard one. The goal is to make causal analysis much more like regular exploratory analysis. To use this API, simply\nimport :code:`dowhy.api`. This will magically add the :code:`causal` namespace to your\n:code:`pandas.DataFrame` s. Then,\nyou can use the namespace as follows.\n\n.. code:: python\n\n    import dowhy.api\n    import dowhy.datasets\n\n    data = dowhy.datasets.linear_dataset(beta=5,\n        num_common_causes=1,\n        num_instruments = 0,\n        num_samples=1000,\n        treatment_is_binary=True)\n\n    # data['df'] is just a regular pandas.DataFrame\n    data['df'].causal.do(x='v0', # name of treatment variable\n                         variable_types={'v0': 'b', 'y': 'c', 'W0': 'c'},\n                         outcome='y',\n                         common_causes=['W0']).groupby('v0').mean().plot(y='y', kind='bar')\n\n.. image:: https://raw.githubusercontent.com/microsoft/dowhy/main/docs/images/do_barplot.png\n\nFor some methods, the :code:`variable_types` field must be specified. It should be a :code:`dict`, where the keys are\nvariable names, and values are 'o' for ordered discrete, 'u' for un-ordered discrete, 'd' for discrete, or 'c'\nfor continuous.\n\n**Note:If the** :code:`variable_types` **is not specified we make use of the following implicit conversions:**\n::\n\n   int -> 'c'\n   float -> 'c'\n   binary -> 'b'\n   category -> 'd'\n\n**Currently we have not added support for timestamps.**\n\nThe :code:`do` method in the causal namespace generates a random sample from $P(outcome|do(X=x))$ of the\nsame length as your data set, and returns this outcome as a new :code:`DataFrame`. You can continue to perform\nthe usual :code:`DataFrame` operations with this sample, and so you can compute statistics and create plots\nfor causal outcomes!\n\nThe :code:`do` method is built on top of the lower-level :code:`dowhy` objects, so can still take a graph and perform\nidentification automatically when you provide a graph instead of :code:`common_causes`.\n\nFor more details, check out the `Pandas API\n<https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_causal_api.ipynb>`_ notebook or the `Do Sampler <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/do_sampler_demo.ipynb>`_\nnotebook.\n\nGraphical Models and Potential Outcomes: Best of both worlds\n============================================================\nDoWhy builds on two of the most powerful frameworks for causal inference:\ngraphical models and potential outcomes. It uses graph-based criteria and\ndo-calculus for modeling assumptions and identifying a non-parametric causal effect.\nFor estimation, it switches to methods based primarily on potential outcomes.\n\nA unifying language for causal inference\n----------------------------------------\n\nDoWhy is based on a simple unifying language for causal inference. Causal\ninference may seem tricky, but almost all methods follow four key steps:\n\n1. Model a causal inference problem using assumptions.\n2. Identify an expression for the causal effect under these assumptions (\"causal estimand\").\n3. Estimate the expression using statistical methods such as matching or instrumental variables.\n4. Finally, verify the validity of the estimate using a variety of robustness checks.\n\nThis workflow can be captured by four key verbs in DoWhy:\n\n- model\n- identify\n- estimate\n- refute\n\nUsing these verbs, DoWhy implements a causal inference engine that can support\na variety of methods. *model* encodes prior knowledge as a formal causal graph, *identify* uses\ngraph-based methods to identify the causal effect, *estimate* uses\nstatistical methods for estimating the identified estimand, and finally *refute*\ntries to refute the obtained estimate by testing robustness to assumptions.\n\nKey differences compared to available causal inference software\n----------------------------------------------------------------\nDoWhy brings three key differences compared to available software for causal inference:\n\n**Explicit identifying assumptions**\n    Assumptions are first-class citizens in DoWhy.\n\n    Each analysis starts with a\n    building a causal model. The assumptions can be viewed graphically or in terms\n    of conditional independence statements. Wherever possible, DoWhy can also\n    automatically test for stated assumptions using observed data.\n\n**Separation between identification and estimation**\n    Identification is the causal problem. Estimation is simply a statistical problem.\n\n    DoWhy\n    respects this boundary and treats them separately. This focuses the causal\n    inference effort on identification, and frees up estimation using any\n    available statistical estimator for a target estimand. In addition, multiple\n    estimation methods can be used for a single identified_estimand and\n    vice-versa.\n\n**Automated robustness checks**\n    What happens when key identifying assumptions may not be satisfied?\n\n    The most critical, and often skipped, part of causal analysis is checking the\n    robustness of an estimate to unverified assumptions. DoWhy makes it easy to\n    automatically run sensitivity and robustness checks on the obtained estimate.\n\nFinally, DoWhy is easily extensible, allowing other implementations of the\nfour verbs to co-exist (e.g., we support implementations of the *estimation* verb from\nEconML and CausalML libraries). The four verbs are mutually independent, so their\nimplementations can be combined in any way.\n\n\n\nBelow are more details about the current implementation of each of these verbs.\n\nFour steps of causal inference\n===============================\n\nI. Model a causal problem\n-----------------------------\n\nDoWhy creates an underlying causal graphical model for each problem. This\nserves to make each causal assumption explicit. This graph need not be\ncomplete---you can provide a partial graph, representing prior\nknowledge about some of the variables. DoWhy automatically considers the rest\nof the variables as potential confounders.\n\nCurrently, DoWhy supports two formats for graph input: `gml <https://github.com/GunterMueller/UNI_PASSAU_FMI_Graph_Drawing>`_ (preferred) and\n`dot <http://www.graphviz.org/documentation/>`_. We strongly suggest to use gml as the input format, as it works well with networkx. You can provide the graph either as a .gml file or as a string. If you prefer to use dot format, you will need to install additional packages (pydot or pygraphviz, see the installation section above). Both .dot files and string format are supported.\n\nWhile not recommended, you can also specify common causes and/or instruments directly\ninstead of providing a graph.\n\nSupported formats for specifying causal assumptions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n* **Graph**: Provide a causal graph in either gml or dot format. Can be a text file\n  or a string.\n* **Named variable sets**: Instead of the graph, provide variable names that\n  correspond to relevant categories, such as common causes, instrumental variables, effect\n  modifiers, frontdoor variables, etc.\n\nExamples of how to instantiate a causal model are in the `Getting Started\n<https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_simple_example.ipynb>`_\nnotebook.\n\n.. i comment image:: causal_model.png\n\nII. Identify a target estimand under the model\n----------------------------------------------\n\nBased on the causal graph, DoWhy finds all possible ways of identifying a desired causal effect based on\nthe graphical model. It uses graph-based criteria and do-calculus to find\npotential ways find expressions that can identify the causal effect.\n\nSupported identification criteria\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n* Back-door criterion\n* Front-door criterion\n* Instrumental Variables\n* Mediation (Direct and indirect effect identification)\n\nDifferent notebooks illustrate how to use these identification criteria. Check\nout the `Simple Backdoor <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_confounder_example.ipynb>`_ notebook for the back-door criterion, and the `Simple IV <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy-simple-iv-example.ipynb>`_ notebook for the instrumental variable criterion.\n\nIII. Estimate causal effect based on the identified estimand\n------------------------------------------------------------\n\nDoWhy supports methods based on both back-door criterion and instrumental\nvariables. It also provides a non-parametric confidence intervals and a permutation test for testing\nthe statistical significance of obtained estimate.\n\nSupported estimation methods\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n* Methods based on estimating the treatment assignment\n    * Propensity-based Stratification\n    * Propensity Score Matching\n    * Inverse Propensity Weighting\n\n* Methods based on estimating the outcome model\n    * Linear Regression\n    * Generalized Linear Models\n\n* Methods based on the instrumental variable equation\n    * Binary Instrument/Wald Estimator\n    * Two-stage least squares\n    * Regression discontinuity\n\n* Methods for front-door criterion and general mediation\n    * Two-stage linear regression\n\nExamples of using these methods are in the `Estimation methods\n<https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_estimation_methods.ipynb>`_\nnotebook.\n\nUsing EconML and CausalML estimation methods in DoWhy\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIt is easy to call external estimation methods using DoWhy. Currently we\nsupport integrations with the `EconML <https://github.com/microsoft/econml>`_ and `CausalML <https://github.com/uber/causalml>`_ packages. Here's an example\nof estimating conditional treatment effects using EconML's double machine\nlearning estimator.\n\n.. code:: python\n\n\tfrom sklearn.preprocessing import PolynomialFeatures\n\tfrom sklearn.linear_model import LassoCV\n\tfrom sklearn.ensemble import GradientBoostingRegressor\n\tdml_estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.econml.dml.DML\",\n                        control_value = 0,\n                        treatment_value = 1,\n                        target_units = lambda df: df[\"X0\"]>1,\n                        confidence_intervals=False,\n                        method_params={\n                            \"init_params\":{'model_y':GradientBoostingRegressor(),\n                                           'model_t': GradientBoostingRegressor(),\n                                           'model_final':LassoCV(),\n                                           'featurizer':PolynomialFeatures(degree=1, include_bias=True)},\n                            \"fit_params\":{}}\n\t\t\t\t\t\t)\n\n\nMore examples are in the `Conditional Treatment Effects with DoWhy\n<https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy-conditional-treatment-effects.ipynb>`_ notebook.\n\nIV. Refute the obtained estimate\n-------------------------------------\nHaving access to multiple refutation methods to validate an effect estimate from a\ncausal estimator is\na key benefit of using DoWhy.\n\nSupported refutation methods\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n* **Add Random Common Cause**: Does the estimation method change its estimate after\n  we add an independent random variable as a common cause to the dataset?\n  (*Hint: It should not*)\n* **Placebo Treatment**: What happens to the estimated causal effect when we\n  replace the true treatment variable with an independent random variable?\n  (*Hint: the effect should go to zero*)\n* **Dummy Outcome**: What happens to the estimated causal effect when we replace\n  the true outcome variable with an independent random variable? (*Hint: The\n  effect should go to zero*)\n* **Simulated Outcome**: What happens to the estimated causal effect when we\n  replace the dataset with a simulated dataset based on a known data-generating\n  process closest to the given dataset? (*Hint: It should match the effect parameter\n  from the data-generating process*)\n* **Add Unobserved Common Causes**: How sensitive is the effect estimate when we\n  add an additional common cause (confounder) to the dataset that is correlated\n  with the treatment and the outcome? (*Hint: It should not be too sensitive*)\n* **Data Subsets Validation**: Does the estimated effect change significantly when\n  we replace the given dataset with a randomly selected subset? (*Hint: It\n  should not*)\n* **Bootstrap Validation**: Does the estimated effect change significantly when we\n  replace the given dataset with bootstrapped samples from the same dataset? (*Hint: It should not*)\n\nExamples of using refutation methods are in the `Refutations <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_refuter_notebook.ipynb>`_ notebook. For an advanced refutation that uses a simulated dataset based on user-provided or learnt data-generating processes, check out the `Dummy Outcome Refuter <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_demo_dummy_outcome_refuter.ipynb>`_ notebook.\nAs a practical example, `this notebook <https://github.com/microsoft/dowhy/blob/main/docs/source/example_notebooks/dowhy_refutation_testing.ipynb>`_ shows an application of refutation methods on evaluating effect estimators for the Infant Health and Development Program (IHDP) and Lalonde datasets.\n\nCiting this package\n====================\nIf you find DoWhy useful for your research work, please cite us as follows:\n\nAmit Sharma, Emre Kiciman, et al. DoWhy: A Python package for causal inference. 2019. https://github.com/microsoft/dowhy\n\nBibtex::\n\n  @misc{dowhy,\n  author={Sharma, Amit and Kiciman, Emre and others},\n  title={Do{W}hy: {A Python package for causal inference}},\n  howpublished={https://github.com/microsoft/dowhy},\n  year={2019}\n  }\n\nAlternatively, you can cite our Arxiv paper on DoWhy.\n\nAmit Sharma, Emre Kiciman. DoWhy: An End-to-End Library for Causal Inference. 2020. https://arxiv.org/abs/2011.04216\n\nBibtex::\n\n  @article{dowhypaper,\n  title={DoWhy: An End-to-End Library for Causal Inference},\n  author={Sharma, Amit and Kiciman, Emre},\n  journal={arXiv preprint arXiv:2011.04216},\n  year={2020}\n  }\n\nAnd if you find the gcm package useful for your work, please also cite us as:\n\nPatrick Bl\u00f6baum, Peter G\u00f6tz, Kailash Budhathoki, Atalanti A. Mastakouri, Dominik Janzing. DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models. 2022. https://arxiv.org/abs/2206.06821\n\nBibtex::\n\n    @article{dowhy_gcm,\n      author = {Bl{\\\"o}baum, Patrick and G{\\\"o}tz, Peter and Budhathoki, Kailash and Mastakouri, Atalanti A. and Janzing, Dominik},\n      title = {DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models},\n      journal={arXiv preprint arXiv:2206.06821},\n      year={2022}\n    }\n\nRoadmap\n=======\nThe `projects <https://github.com/microsoft/dowhy/projects>`_ page lists the next steps for DoWhy. If you would like to contribute, have a look at the current projects. If you have a specific request for DoWhy, please `raise an issue <https://github.com/microsoft/dowhy/issues>`_.\n\nContributing\n============\n\nThis project welcomes contributions and suggestions. For a guide to contributing and a list of all contributors, check out `CONTRIBUTING.md <https://github.com/microsoft/dowhy/blob/main/CONTRIBUTING.md>`_ and our `docs for contributing code <https://github.com/py-why/dowhy/blob/main/docs/source/contributing/contributing-code.rst>`_. Our `contributor code of conduct is available here <https://github.com/py-why/governance/blob/main/CODE-OF-CONDUCT.md>`_. You can also join the DoWhy development channel on Discord: |discord|_\n\n.. |discord| image:: https://img.shields.io/discord/818456847551168542\n.. _discord: https://discord.gg/cSBGb3vsZb\n\n"
 },
 {
  "repo": "fluentpython/example-code",
  "language": "Python",
  "readme_contents": "Fluent Python, First Edition: example code\n==========================================\n\n**This repository is archived and will not be updated. Please visit https://github.com/fluentpython/example-code-2e**\n\nExample code for the book `Fluent Python, First Edition` by Luciano Ramalho (O'Reilly, 2015).\n\n* Code here may change and disappear without warning. \n\n* If a piece of code is not yet in the ebook, it's likely to be broken.\n\n* A major reorganization may happen when the last chapter is done. \n\n* No promises. No guarantees. Use at own risk.\n\n.. _Fluent Python: http://shop.oreilly.com/product/0636920032519.do \n"
 },
 {
  "repo": "kevinsawicki/http-request",
  "language": "Java",
  "readme_contents": "# Http Request [![Build Status](https://travis-ci.org/kevinsawicki/http-request.svg)](https://travis-ci.org/kevinsawicki/http-request)\n\nA simple convenience library for using a [HttpURLConnection](http://download.oracle.com/javase/6/docs/api/java/net/HttpURLConnection.html)\nto make requests and access the response.\n\nThis library is available under the [MIT License](http://www.opensource.org/licenses/mit-license.php).\n\n## Usage\n\nThe http-request library is available from [Maven Central](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22com.github.kevinsawicki%22%20AND%20a%3A%22http-request%22).\n\n```xml\n<dependency>\n  <groupId>com.github.kevinsawicki</groupId>\n  <artifactId>http-request</artifactId>\n  <version>6.0</version>\n</dependency>\n```\n\nNot using [Maven](http://maven.apache.org/)? Simply copy the [HttpRequest](https://raw.githubusercontent.com/kevinsawicki/http-request/master/lib/src/main/java/com/github/kevinsawicki/http/HttpRequest.java)\nclass into your project, update the package declaration, and you are good to go.\n\nJavadocs are available [here](http://kevinsawicki.github.com/http-request/apidocs/index.html).\n\n## FAQ\n\n### Who uses this?\n\nSee [here](https://github.com/kevinsawicki/http-request/wiki/Used-By) for a\nlist of known projects using this library.\n\n### Why was this written?\n\nThis library was written to make HTTP requests simple and easy when using a `HttpURLConnection`.\n\nLibraries like [Apache HttpComponents](http://hc.apache.org) are great but sometimes\nfor either simplicity, or perhaps for the environment you are deploying to (Android),\nyou just want to use a good old-fashioned `HttpURLConnection`.  This library seeks\nto add convenience and common patterns to the act of making HTTP requests such as\na fluid-interface for building requests and support for features such as multipart\nrequests.\n\n**Bottom line:** The single goal of this library is to improve the usability of the\n`HttpURLConnection` class.\n\n### What are the dependencies?\n\n**None**.  The goal of this library is to be a single class class with some inner static\nclasses.  The test project does require [Jetty](http://eclipse.org/jetty/) in order\nto test requests against an actual HTTP server implementation.\n\n### How are exceptions managed?\n\nThe `HttpRequest` class does not throw any checked exceptions, instead all low-level\nexceptions are wrapped up in a `HttpRequestException` which extends `RuntimeException`.\nYou can access the underlying exception by catching `HttpRequestException` and calling\n`getCause()` which will always return the original `IOException`.\n\n### Are requests asynchronous?\n\n**No**.  The underlying `HttpUrlConnection` object that each `HttpRequest`\nobject wraps has a synchronous API and therefore all methods on `HttpRequest`\nare also synchronous.\n\nTherefore it is important to not use an `HttpRequest` object on the main thread\nof your application.\n\nHere is a simple Android example of using it from an\n[AsyncTask](http://developer.android.com/reference/android/os/AsyncTask.html):\n\n```java\nprivate class DownloadTask extends AsyncTask<String, Long, File> {\n  protected File doInBackground(String... urls) {\n    try {\n      HttpRequest request =  HttpRequest.get(urls[0]);\n      File file = null;\n      if (request.ok()) {\n        file = File.createTempFile(\"download\", \".tmp\");\n        request.receive(file);\n        publishProgress(file.length());\n      }\n      return file;\n    } catch (HttpRequestException exception) {\n      return null;\n    }\n  }\n\n  protected void onProgressUpdate(Long... progress) {\n    Log.d(\"MyApp\", \"Downloaded bytes: \" + progress[0]);\n  }\n\n  protected void onPostExecute(File file) {\n    if (file != null)\n      Log.d(\"MyApp\", \"Downloaded file to: \" + file.getAbsolutePath());\n    else\n      Log.d(\"MyApp\", \"Download failed\");\n  }\n}\n\nnew DownloadTask().execute(\"http://google.com\");\n```\n\n## Examples\n\n### Perform a GET request and get the status of the response\n\n```java\nint response = HttpRequest.get(\"http://google.com\").code();\n```\n\n### Perform a GET request and get the body of the response\n\n```java\nString response = HttpRequest.get(\"http://google.com\").body();\nSystem.out.println(\"Response was: \" + response);\n```\n\n### Print the response of a GET request to standard out\n\n```java\nHttpRequest.get(\"http://google.com\").receive(System.out);\n```\n\n### Adding query parameters\n\n```java\nHttpRequest request = HttpRequest.get(\"http://google.com\", true, 'q', \"baseball gloves\", \"size\", 100);\nSystem.out.println(request.toString()); // GET http://google.com?q=baseball%20gloves&size=100\n```\n\n### Using arrays as query parameters\n\n```java\nint[] ids = new int[] { 22, 23 };\nHttpRequest request = HttpRequest.get(\"http://google.com\", true, \"id\", ids);\nSystem.out.println(request.toString()); // GET http://google.com?id[]=22&id[]=23\n```\n\n### Working with request/response headers\n\n```java\nString contentType = HttpRequest.get(\"http://google.com\")\n                                .accept(\"application/json\") //Sets request header\n                                .contentType(); //Gets response header\nSystem.out.println(\"Response content type was \" + contentType);\n```\n\n### Perform a POST request with some data and get the status of the response\n\n```java\nint response = HttpRequest.post(\"http://google.com\").send(\"name=kevin\").code();\n```\n\n### Authenticate using Basic authentication\n\n```java\nint response = HttpRequest.get(\"http://google.com\").basic(\"username\", \"p4ssw0rd\").code();\n```\n\n### Perform a multipart POST request\n\n```java\nHttpRequest request = HttpRequest.post(\"http://google.com\");\nrequest.part(\"status[body]\", \"Making a multipart request\");\nrequest.part(\"status[image]\", new File(\"/home/kevin/Pictures/ide.png\"));\nif (request.ok())\n  System.out.println(\"Status was updated\");\n```\n\n### Perform a POST request with form data\n\n```java\nMap<String, String> data = new HashMap<String, String>();\ndata.put(\"user\", \"A User\");\ndata.put(\"state\", \"CA\");\nif (HttpRequest.post(\"http://google.com\").form(data).created())\n  System.out.println(\"User was created\");\n```\n\n### Copy body of response to a file\n\n```java\nFile output = new File(\"/output/request.out\");\nHttpRequest.get(\"http://google.com\").receive(output);\n```\n### Post contents of a file\n\n```java\nFile input = new File(\"/input/data.txt\");\nint response = HttpRequest.post(\"http://google.com\").send(input).code();\n```\n\n### Using entity tags for caching\n\n```java\nFile latest = new File(\"/data/cache.json\");\nHttpRequest request = HttpRequest.get(\"http://google.com\");\n//Copy response to file\nrequest.receive(latest);\n//Store eTag of response\nString eTag = request.eTag();\n//Later on check if changes exist\nboolean unchanged = HttpRequest.get(\"http://google.com\")\n                               .ifNoneMatch(eTag)\n                               .notModified();\n```\n\n### Using gzip compression\n\n```java\nHttpRequest request = HttpRequest.get(\"http://google.com\");\n//Tell server to gzip response and automatically uncompress\nrequest.acceptGzipEncoding().uncompress(true);\nString uncompressed = request.body();\nSystem.out.println(\"Uncompressed response is: \" + uncompressed);\n```\n\n### Ignoring security when using HTTPS\n\n```java\nHttpRequest request = HttpRequest.get(\"https://google.com\");\n//Accept all certificates\nrequest.trustAllCerts();\n//Accept all hostnames\nrequest.trustAllHosts();\n```\n\n### Configuring an HTTP proxy\n\n```java\nHttpRequest request = HttpRequest.get(\"https://google.com\");\n//Configure proxy\nrequest.useProxy(\"localhost\", 8080);\n//Optional proxy basic authentication\nrequest.proxyBasic(\"username\", \"p4ssw0rd\");\n```\n\n### Following redirects\n\n```java\nint code = HttpRequest.get(\"http://google.com\").followRedirects(true).code();\n```\n\n### Custom connection factory\n\nLooking to use this library with [OkHttp](https://github.com/square/okhttp)?\nRead [here](https://gist.github.com/JakeWharton/5797571).\n\n```java\nHttpRequest.setConnectionFactory(new ConnectionFactory() {\n\n  public HttpURLConnection create(URL url) throws IOException {\n    if (!\"https\".equals(url.getProtocol()))\n      throw new IOException(\"Only secure requests are allowed\");\n    return (HttpURLConnection) url.openConnection();\n  }\n\n  public HttpURLConnection create(URL url, Proxy proxy) throws IOException {\n    if (!\"https\".equals(url.getProtocol()))\n      throw new IOException(\"Only secure requests are allowed\");\n    return (HttpURLConnection) url.openConnection(proxy);\n  }\n});\n```\n\n## Contributors\n\n* [Kevin Sawicki](https://github.com/kevinsawicki) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=kevinsawicki)\n* [Eddie Ringle](https://github.com/eddieringle) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=eddieringle)\n* [Sean Jensen-Grey](https://github.com/seanjensengrey) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=seanjensengrey)\n* [Levi Notik](https://github.com/levinotik) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=levinotik)\n* [Michael Wang](https://github.com/michael-wang) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=michael-wang)\n* [Julien HENRY](https://github.com/henryju) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=henryju)\n* [Benoit Lubek](https://github.com/BoD) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=BoD)\n* [Jake Wharton](https://github.com/JakeWharton) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=JakeWharton)\n* [Oskar Hagberg](https://github.com/oskarhagberg) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=oskarhagberg)\n* [David Pate](https://github.com/DavidTPate) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=DavidTPate)\n* [Anton Rieder](https://github.com/aried3r) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=aried3r)\n* [Jean-Baptiste Li\u00e8vremont](https://github.com/jblievremont) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=jblievremont)\n* [Roman Petrenko](https://github.com/romanzes) :: [contributions](https://github.com/kevinsawicki/http-request/commits?author=romanzes)\n"
 },
 {
  "repo": "flutter-webrtc/flutter-webrtc",
  "language": "Java",
  "readme_contents": "# Flutter-WebRTC\n\n[![Financial Contributors on Open Collective](https://opencollective.com/flutter-webrtc/all/badge.svg?label=financial+contributors)](https://opencollective.com/flutter-webrtc) [![pub package](https://img.shields.io/pub/v/flutter_webrtc.svg)](https://pub.dartlang.org/packages/flutter_webrtc) [![Gitter](https://badges.gitter.im/flutter-webrtc/Lobby.svg)](https://gitter.im/flutter-webrtc/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![slack](https://img.shields.io/badge/join-us%20on%20slack-gray.svg?longCache=true&logo=slack&colorB=brightgreen)](https://join.slack.com/t/flutterwebrtc/shared_invite/zt-q83o7y1s-FExGLWEvtkPKM8ku_F8cEQ)\n\nWebRTC plugin for Flutter Mobile/Desktop/Web\n\n</br>\n<p align=\"center\">\n<strong>Sponsored with \ud83d\udc96 &nbsp by</strong><br />\n<a href=\"https://getstream.io/chat/flutter/tutorial/?utm_source=https://github.com/flutter-webrtc/flutter-webrtc&utm_medium=github&utm_content=developer&utm_term=flutter\" target=\"_blank\">\n<img src=\"https://stream-blog-v2.imgix.net/blog/wp-content/uploads/f7401112f41742c4e173c30d4f318cb8/stream_logo_white.png?w=350\" alt=\"Stream Chat\" style=\"margin: 8px\" />\n</a>\n<br />\nEnterprise Grade APIs for Feeds & Chat. <a href=\"https://getstream.io/chat/flutter/tutorial/?utm_source=https://github.com/flutter-webrtc/flutter-webrtc&utm_medium=github&utm_content=developer&utm_term=flutter\" target=\"_blank\">Try the Flutter Chat tutorial</a> \ud83d\udcac\n</p>\n\n</br>\n<p align=\"center\">\n<a href=\"https://livekit.io/?utm_source=opencollective&utm_medium=github&utm_campaign=flutter-webrtc\" target=\"_blank\">\n<img src=\"https://avatars.githubusercontent.com/u/69438833?s=92&v=4\" alt=\"LiveKit\" style=\"margin: 8px\" />\n</a>\n<br />\n   <a href=\"https://livekit.io/?utm_source=opencollective&utm_medium=github&utm_campaign=flutter-webrtc\" target=\"_blank\">LiveKit</a> - Open source WebRTC infrastructure\n<p>\n\n## Functionality\n\n| Feature | Android | iOS | [Web](https://flutter.dev/web) | macOS | Windows | Linux | [Embedded](https://github.com/sony/flutter-elinux) | [Fuchsia](https://fuchsia.dev/) |\n| :-------------: | :-------------:| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| Audio/Video | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | [WIP] | [WIP] | |\n| Data Channel | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | [WIP] |  [WIP] ||\n| Screen Capture | :heavy_check_mark: | [:heavy_check_mark:(*)](https://github.com/flutter-webrtc/flutter-webrtc/wiki/iOS-Screen-Sharing) | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | | |\n| Unified-Plan | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | [WIP] | [WIP] | |\n| Simulcast | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | [WIP] | | | |\n| MediaRecorder | :warning: | :warning: | :heavy_check_mark: | | | | | |\n| Insertable Streams | | | | | | | | |\n## Usage\n\nAdd `flutter_webrtc` as a [dependency in your pubspec.yaml file](https://flutter.io/using-packages/).\n\n### iOS\n\nAdd the following entry to your _Info.plist_ file, located in `<project root>/ios/Runner/Info.plist`:\n\n```xml\n<key>NSCameraUsageDescription</key>\n<string>$(PRODUCT_NAME) Camera Usage!</string>\n<key>NSMicrophoneUsageDescription</key>\n<string>$(PRODUCT_NAME) Microphone Usage!</string>\n```\n\nThis entry allows your app to access camera and microphone.\n\n### Note for iOS.\nThe WebRTC.xframework compiled after the m104 release no longer supports iOS arm devices, so need to add the `config.build_settings['ONLY_ACTIVE_ARCH'] = 'YES'` to your ios/Podfile in your project\n\nios/Podfile\n\n```ruby\npost_install do |installer|\n  installer.pods_project.targets.each do |target|\n    flutter_additional_ios_build_settings(target)\n     target.build_configurations.each do |config|\n      # Workaround for https://github.com/flutter/flutter/issues/64502\n      config.build_settings['ONLY_ACTIVE_ARCH'] = 'YES' # <= this line\n     end\n  end\nend\n```\n\n### Android\n\nEnsure the following permission is present in your Android Manifest file, located in `<project root>/android/app/src/main/AndroidManifest.xml`:\n\n```xml\n<uses-feature android:name=\"android.hardware.camera\" />\n<uses-feature android:name=\"android.hardware.camera.autofocus\" />\n<uses-permission android:name=\"android.permission.CAMERA\" />\n<uses-permission android:name=\"android.permission.RECORD_AUDIO\" />\n<uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" />\n<uses-permission android:name=\"android.permission.CHANGE_NETWORK_STATE\" />\n<uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\" />\n```\n\nIf you need to use a Bluetooth device, please add:\n\n```xml\n<uses-permission android:name=\"android.permission.BLUETOOTH\" android:maxSdkVersion=\"30\" />\n<uses-permission android:name=\"android.permission.BLUETOOTH_ADMIN\" android:maxSdkVersion=\"30\" />\n<uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\" />\n```\n\nThe Flutter project template adds it, so it may already be there.\n\nAlso you will need to set your build settings to Java 8, because official WebRTC jar now uses static methods in `EglBase` interface. Just add this to your app level `build.gradle`:\n\n```groovy\nandroid {\n    //...\n    compileOptions {\n        sourceCompatibility JavaVersion.VERSION_1_8\n        targetCompatibility JavaVersion.VERSION_1_8\n    }\n}\n```\n\nIf necessary, in the same `build.gradle` you will need to increase `minSdkVersion` of `defaultConfig` up to `23` (currently default Flutter generator set it to `16`).\n\n### Important reminder\nWhen you compile the release apk, you need to add the following operations,\n[Setup Proguard Rules](https://github.com/flutter-webrtc/flutter-webrtc/commit/d32dab13b5a0bed80dd9d0f98990f107b9b514f4)\n\n## Contributing\n\nThe project is inseparable from the contributors of the community.\n\n- [CloudWebRTC](https://github.com/cloudwebrtc) - Original Author\n- [RainwayApp](https://github.com/rainwayapp) - Sponsor\n- [\u4ea2\u5c11\u519b](https://github.com/kangshaojun) - Sponsor\n- [ION](https://github.com/pion/ion) - Sponsor\n- [reSipWebRTC](https://github.com/reSipWebRTC) - Sponsor\n- [\u6c83\u5fb7\u7c73\u79d1\u6280](https://github.com/woodemi)-[36\u8bb0\u624b\u5199\u677f](https://www.36notes.com) - Sponsor\n- [\u963f\u65af\u7279\u7f51\u7edc\u79d1\u6280\u6709\u9650\u516c\u53f8](https://www.astgo.net/) - Sponsor\n\n### Example\n\nFor more examples, please refer to [flutter-webrtc-demo](https://github.com/cloudwebrtc/flutter-webrtc-demo/).\n\n## Contributors\n\n### Code Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](CONTRIBUTING.md)].\n<a href=\"https://github.com/cloudwebrtc/flutter-webrtc/graphs/contributors\"><img src=\"https://opencollective.com/flutter-webrtc/contributors.svg?width=890&button=false\" /></a>\n\n### Financial Contributors\n\nBecome a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/flutter-webrtc/contribute)]\n\n#### Individuals\n\n<a href=\"https://opencollective.com/flutter-webrtc\"><img src=\"https://opencollective.com/flutter-webrtc/individuals.svg?width=890\"></a>\n\n#### Organizations\n\nSupport this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/flutter-webrtc/contribute)]\n\n<a href=\"https://opencollective.com/flutter-webrtc/organization/0/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/0/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/1/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/1/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/2/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/2/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/3/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/3/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/4/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/4/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/5/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/5/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/6/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/6/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/7/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/7/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/8/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/8/avatar.svg\"></a>\n<a href=\"https://opencollective.com/flutter-webrtc/organization/9/website\"><img src=\"https://opencollective.com/flutter-webrtc/organization/9/avatar.svg\"></a>\n"
 },
 {
  "repo": "airbnb/native-navigation",
  "language": "Java",
  "readme_contents": "# Native Navigation\n\n[![Join the chat at https://gitter.im/airbnb/native-navigation](https://badges.gitter.im/airbnb/native-navigation.svg)](https://gitter.im/airbnb/native-navigation?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n[![npm Version](https://img.shields.io/npm/v/native-navigation.svg)](https://www.npmjs.com/package/native-navigation) \n[![License](https://img.shields.io/npm/l/native-navigation.svg)](https://www.npmjs.com/package/native-navigation) \n[![Build Status](https://travis-ci.org/airbnb/native-navigation.svg)](https://travis-ci.org/airbnb/native-navigation) \n\n## DISCLAIMER\n\n**This project is currently in beta**. \n\nMany of the core APIs are subject to change, and we do not consider this project \"Production Ready\" until it hits a 1.0 release. We encourage people to try this library out and provide us feedback as we get it to a stable state we are confident in, but not to rely on it for production use until then.\n\nRead more about our [Roadmap to 1.0](/docs/roadmap.md)\n\n## Contents\n\n- [Installation](#installation)\n- [Running the Example Project](#running-the-example-project)\n- [Guides](#guides)\n- [API Documentation](#api-documentation)\n- [Related Projects and Alternatives](#related-projects-and-alternatives)\n- [Contributing](#contributing)\n- [FAQ](#faq)\n- [License](#license)\n\n## Installation\n\nSee the [Installation Guide](/docs/installation.md)\n\n## Running the Example Project\n\nTo run the example project, first clone this repo:\n\n```bash\ngit clone https://github.com/airbnb/native-navigation.git\ncd native-navigation\n```\n\nBoth [`npm`](https://nodejs.org/) and the ruby [`bundler`](http://bundler.io/) gem are needed to run the project.\n\n```bash\nnpm install\n```\n\n```bash\nnpm start\n```\n\nThen, in another CLI window:\n\nTo run on iOS: `npm run run:ios`\n\nTo run on Android: `npm run run:android`\n\n\n## [Guides](/docs/guides/README.md)\n\n- [Basic Usage](/docs/guides/basic-usage.md)\n- [Integrating with existing apps](/docs/guides/integrating-with-existing-apps.md)\n- [Custom Navigation Implementations](/docs/guides/custom-navigation-implementations.md)\n- [Tabs](/docs/guides/tabs.md)\n- [Deep Linking](/docs/guides/deep-linking.md)\n- [Platform Differences](/docs/guides/platform-differences.md)\n- [Project Structure](/docs/guides/project-structure.md)\n- [Shared Element Transitions](/docs/guides/shared-element-transitions.md)\n\n## [API Documentation](/docs/api/README.md)\n\n- [`Navigator.registerScreen(...)`](/docs/api/navigator/registerScreen.md)\n- [`Navigator.push(...)`](/docs/api/navigator/push.md)\n- [`Navigator.present(...)`](/docs/api/navigator/present.md)\n- [`Navigator.pop(...)`](/docs/api/navigator/pop.md)\n- [`Navigator.dismiss(...)`](/docs/api/navigator/dismiss.md)\n- [`Config`](/docs/api/navigator-config.md)\n- [`Spacer`](/docs/api/navigator-spacer.md)\n- [`Tab`](/docs/api/navigator-tab.md)\n- [`TabBar`](/docs/api/navigator-tab-bar.md)\n- [`SharedElement`](/docs/api/navigator-shared-element.md)\n- [`SharedElementGroup`](/docs/api/navigator-shared-element-group.md)\n\n\n## Related Projects and Alternatives\n\nNative Navigation is a navigation library for the React Native platform. There are many navigation libraries in the React Native ecosystem. Native Navigation is unique in that it is built on top of the iOS and Android platform navigational components, and is thus more \"native\" than most other options which implement navigation from scratch in JavaScript on top of base React Native components like `View` and `Animated`.\n\n[React Native Navigation](https://github.com/wix/react-native-navigation) by Wix engineering is an alternative library that uses \"Native\" navigation components of each platform, and has been around longer than Native Navigation. If you need a stable / production-ready navigation library *today* that uses native platform based navigation components, we recommend you check this library out.\n\nIf you are investigating navigation solutions and you are okay with JavaScript-based solutions, we also encourage you to check out [React Navigation](https://reactnavigation.org/).\n\n## Contributing\n\nSee the [Contributors Guide](/CONTRIBUTING.md)\n\n## FAQ\n\nSee the [Frequently Asked Questions](/docs/FAQ.md) page\n\n## License\n\nThis project is licensed under the [MIT License](/LICENSE.md).\n"
 },
 {
  "repo": "zwwill/yanxuan-weex-demo",
  "language": "Java",
  "readme_contents": "English | [\u7b80\u4f53\u4e2d\u6587](README.zh-CN.md)\n\n# :art: High quality Weex Demo\n\n![](https://github.com/zwwill/yanxuan-weex-demo/raw/master/banner.png)\n\n> The following is a brief step to run the demo\n> further introduction\uff0cyou can read this [\u7f51\u6613\u4e25\u9009App\u611f\u53d7WEEX\u5f00\u53d1](https://github.com/zwwill/blog/issues/3)\n\n\n# Try\n\nopen [Weex Playground](http://weex.apache.org/cn/playground.html) , Scan the qrcode below\n\n![](https://github.com/zwwill/yanxuan-weex-demo/raw/master/erHome.png)\n\n\uff08no optimizing separately for android\uff09\n\n# Run\n\n## install\n\n```\n$ npm install\n```\n\n## run web\n\nbuilding web pro\n\n```\n$ npm run build \n```\n\nbuilding web pro & running service\n\n```\n$ npm run dev & npm run serve \n```\n\n## run ios\n\nios packaging requires developer accounts, f not, you can only install it on your own connected iphone through xcode debugging, or virtual machine\u3002\n\nThe following is the implementation of the non-developer account\n\ninstall ios platform\n\n``` \n$ weexpack platform add ios\n```\n\nbuild weex bundles\n\n```\n$ weex build ios\n```\n\nThis step is only for packaging, not fully executed, cancel before you enter the bundle id\u3002\n\nfurther info [https://segmentfault.com/a/1190000010984857](https://segmentfault.com/a/1190000010984857#articleHeader14)\n\nnext, use xcode to debug, refer to the native debug step.\n\n\uff08Using XCode to open file `platforms/ios/WeexDemo.xcworkspace`, simple configurate, then run or debug\uff09\n\n"
 },
 {
  "repo": "lovetuzitong/MultiImageSelector",
  "language": "Java",
  "readme_contents": "# MultiImageSelector\nImage selector for Android device. Support single choice and multi-choice.\n\n[![](https://jitpack.io/v/lovetuzitong/MultiImageSelector.svg)](https://jitpack.io/#lovetuzitong/MultiImageSelector)\n\n[\u4e2d\u6587\u6587\u6863](README_zh.md)\n\n###ART\n![Example1](art/example_1.png) ![Select1](art/select_1.png) ![Select2](art/select_2.png) ![Select3](art/select_3.png)\n\n-------------------\n\n###Run Demo\n\n>./gradlew installDebug\n\n###Quick Start\n* Step 0\nAdd module `multi-image-selector` as your dependence. in your `build.gradle` :\n```java\nrepositories {\n    maven { url \"https://jitpack.io\" }\n}\n\ndependencies {\n    compile 'com.github.lovetuzitong:MultiImageSelector:1.2'\n}\n```\n\n* Step 1 \nSet your `AndroidManifest.xml` as below:\n```xml\n<uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\" />\n<uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" />\n\n<application\n\n    ...\n\n    <!--Image Selector Entry-->\n    <activity\n        android:configChanges=\"orientation|screenSize\"\n        android:name=\"me.nereo.multi_image_selector.MultiImageSelectorActivity\" />\n</application>\n```\n\n* Step 2\nCall image selector simplest in your code, eg. ( From `version-1.1` )\n\n``` java\n// Multi image selector form an Activity\nMultiImageSelector.create(Context)\n        .start(Activity, REQUEST_IMAGE);\n```\n\nDetail Api.\n``` java\nMultiImageSelector.create(Context)\n        .showCamera(boolean) // show camera or not. true by default\n        .count(int) // max select image size, 9 by default. used width #.multi()\n        .single() // single mode\n        .multi() // multi mode, default mode;\n        .origin(ArrayList<String>) // original select data set, used width #.multi()\n        .start(Activity/Fragment, REQUEST_IMAGE);\n```\n\nAlso support traditional `Intent` :\n``` java\nIntent intent = new Intent(mContext, MultiImageSelectorActivity.class);\n// whether show camera\nintent.putExtra(MultiImageSelectorActivity.EXTRA_SHOW_CAMERA, true);\n// max select image amount\nintent.putExtra(MultiImageSelectorActivity.EXTRA_SELECT_COUNT, 9);\n// select mode (MultiImageSelectorActivity.MODE_SINGLE OR MultiImageSelectorActivity.MODE_MULTI)\nintent.putExtra(MultiImageSelectorActivity.EXTRA_SELECT_MODE, MultiImageSelectorActivity.MODE_MULTI);\n// default select images (support array list)\nintent.putStringArrayListExtra(MultiImageSelectorActivity.EXTRA_DEFAULT_SELECTED_LIST, defaultDataArray);\nstartActivityForResult(intent, REQUEST_IMAGE);\n```\n\n* Step 3\nReceive result in your `onActivityResult` Method. eg.\n```java\n@Override\nprotected void onActivityResult(int requestCode, int resultCode, Intent data) {\n    super.onActivityResult(requestCode, resultCode, data);\n    if(requestCode == REQUEST_IMAGE){\n        if(resultCode == RESULT_OK){\n\t        // Get the result list of select image paths\n            List<String> path = data.getStringArrayListExtra(MultiImageSelectorActivity.EXTRA_RESULT);\n            // do your logic ....\n        }\n    }\n}\n```\n\n* Step 4\nNo more steps, just enjoy. :)\n\n-------------------\n\n###Custom Activity Style\n* Custome your own Activity\n```java\nclass CustomerActivity extends Activity implements MultiImageSelectorFragment.Callback{\n\t@Override\n    protected void onCreate(Bundle savedInstanceState) {\n\t\t// customer logic here...\n\t\tBundle bundle = new Bundle();\n        bundle.putInt(MultiImageSelectorFragment.EXTRA_SELECT_COUNT, mDefaultCount);\n        bundle.putInt(MultiImageSelectorFragment.EXTRA_SELECT_MODE, mode);\n        bundle.putBoolean(MultiImageSelectorFragment.EXTRA_SHOW_CAMERA, isShow);\n        // Add fragment to your Activity\n        getSupportFragmentManager().beginTransaction()\n                .add(R.id.image_grid, Fragment.instantiate(this, MultiImageSelectorFragment.class.getName(), bundle))\n                .commit();\n\t}\n\t@Override\n    public void onSingleImageSelected(String path) {\n        // When select mode set to MODE_SINGLE, this method will received result from fragment\n    }\n\n    @Override\n    public void onImageSelected(String path) {\n        // You can specify your ActionBar behavior here \n    }\n\n    @Override\n    public void onImageUnselected(String path) {\n        // You can specify your ActionBar behavior here \n    }\n\n    @Override\n    public void onCameraShot(File imageFile) {\n        // When user take phone by camera, this method will be called.\n    }\n}\n```\n* Take a glance of `MultiImageSelectorActivity.java`\n\n-------------------\n\n###Change Log\n\n* 2016-5-18\n    1. Added. `JitPack` support\n    2. Added. Convenient way to call image selector. See `Step 2`\n    3. Fixed. Some NPE.\n\n* 2016-1-19\n    1. Fixed. cannot load some 0-size image\n    2. Added. When take a new photo, notify media scanner\n    3. Fixed. Can't take photo on RED-MI\n    4. Fixed. Performance when show Camera-Icon\n\n* 2015-5-5\n    1. Fixed. Can't display some images. (Issue by[sd6352051](https://github.com/sd6352051), [larry](https://github.com/18611480882))\n    2. Fixed. `ListPopupWindow` can not fill parent\n    3. Added. Add checked mask.\n\n* 2015-4-16\n    1. Fixed. Crack when rotate device. (Issue by [@Leminity](https://github.com/Leminity))\n    2. Fixed. PopupListView position error. (Issue by [@Slock](https://github.com/Slock))\n    3. Change. Demo application shortcut.\n    4. Change. Readme file.\n\n* 2015-4-9\n    1. Fixed. When set `EXTRA_SHOW_CAMERA` to `true`, the first grid item onclick event were messed.\n    2. Add. Support initial selected image list.\n\n-------------------\n\n###Thanks\n\n* [square-picasso](https://github.com/square/picasso) A powerful image downloading and caching library for Android \n\n-------------------\n\n###License\n>The MIT License (MIT)\n\n>Copyright (c) 2015 Nereo\n\n>Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n>The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software."
 },
 {
  "repo": "siyamed/android-shape-imageview",
  "language": "Java",
  "readme_contents": "# Shape Image View \n[![](https://travis-ci.org/siyamed/android-shape-imageview.svg?branch=master&style=flat)](https://travis-ci.org/siyamed/android-shape-imageview/) \n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.siyamed/android-shape-imageview/badge.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/com.github.siyamed/android-shape-imageview)\n\nProvides a set of custom shaped android imageview components, and a framework to define more shapes. Implements both **shader** and **bitmap mask** based image views. \n\n[Shader]: http://developer.android.com/reference/android/graphics/BitmapShader.html\n[Path.addPath]: http://developer.android.com/reference/android/graphics/Path.html#addPath(android.graphics.Path)\n[Path]: http://developer.android.com/reference/android/graphics/Path.html\n[xfermode]: http://developer.android.com/reference/android/graphics/Xfermode.html\n[svg_location]: library/src/main/res/raw\n[svg_rectangle]: http://www.w3schools.com/svg/svg_rect.asp \n[svg_circle]: http://www.w3schools.com/svg/svg_circle.asp \n[svg_ellipse]: http://www.w3schools.com/svg/svg_ellipse.asp\n[svg_polygon]: http://www.w3schools.com/svg/svg_polygon.asp\n[svg_path]: http://www.w3schools.com/svg/svg_path.asp\n[svg_group]: https://developer.mozilla.org/en-US/docs/Web/SVG/Element/g\n[svg_transformations]: https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/transform\n[sample_app_play_store]: https://play.google.com/store/apps/details?id=com.github.siyamed.shapeimageview.sample\n[youtube_video]: http://youtu.be/6fCkptmwxtQ\n\n* [Shader][Shader] based one uses *canvas draw methods* and *[Path][Path]* class, \n* Mask based one uses [xfermode][xfermode] to draw image on bitmaps defined by android shape XML's or resource bitmaps.\n\n<div>\n<a href=\"images/shader-buble.png\" style=\"float:left;\">\n<img src=\"images/shader-buble.png\" alt=\"Chat Bubble Image\" height=\"600px\"/>\n</a>\n<a href=\"images/all-samples.png\" >\n<img src=\"images/all-samples.png\" alt=\"Shape Image View\" height=\"600px\"/>\n</a>\n</div>\n\nThere are many projects online implementing such components, however one goal of this project is to provide a \nperformant/smooth scrolling **image view component framework** to define different shapes for imageviews. \n\n**For use with recycling view such as ListView or GridView please use shader based implementations.**\n\n[Sample app in play store][sample_app_play_store]\n\n[Youtube video][youtube_video]\n\n## How to use\n\nGradle dependency:\n```Groovy\ncompile 'com.github.siyamed:android-shape-imageview:0.9.+@aar'\n```\n\n###Shader Based ImageView's\n####BubbleImageView\n![Android Bubble ImageView](images/small-bubble.png)\n```XML\n<com.github.siyamed.shapeimageview.BubbleImageView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:src=\"@drawable/neo\"\n    app:siArrowPosition=\"right\"\n    app:siSquare=\"true\"/>\n```\n\nAttributes:\n* `siTriangleHeight` the height of the bubble pointer in dp\n* `siArrowPosition` where to point the arrow, currently `left|right`\n* `siSquare` set width and height to the minimum of the given values `true|false`\n\n####RoundedImageView\n![Android Rounded Rectangle ImageView](images/small-rounded.png)\n```XML\n<com.github.siyamed.shapeimageview.RoundedImageView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:src=\"@drawable/neo\"\n    app:siRadius=\"6dp\"\n    app:siBorderWidth=\"6dp\"\n    app:siBorderColor=\"@color/darkgray\"\n    app:siSquare=\"true\"/>\n```\n\nAttributes:\n* `siBorderColor` border color\n* `siBorderWidth` border width in dp\n* `siBorderAlpha` alpha value of the border between 0.0-1.0\n* `siRadius` corner radius in dp\n* `siSquare` set width and height to the minimum of the given values `true|false`\n\n####CircularImageView\n![Android Circular ImageView](images/small-circle.png)\n```XML\n<com.github.siyamed.shapeimageview.CircularImageView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:src=\"@drawable/neo\"\n    app:siBorderWidth=\"6dp\"\n    app:siBorderColor=\"@color/darkgray\"/>\n```\n\nAttributes:\n* `siBorderColor` border color\n* `siBorderWidth` border width in dp\n* `siBorderAlpha` alpha value of the border between 0.0-1.0\n\n####ShapeImageView\nThis view has the capability to process a provided SVG file (for a limited set of SVG elements), build a \n[Path][Path] object and draw it on the shader. The library includes SVG files defining a set of basic shapes and \nShapeImageView subclasses using those files. You can use whatever SVG you want to have a wonderful \nand creatively shaped images in your application. The included SVG files are under [library/src/main/res/raw][svg_location]\n\n\n| DiamondImageView                                       | PentagonImageView                                        | HexagonImageView                                         |\n| ------------------------------------------------------ | -------------------------------------------------------- | -------------------------------------------------------- |\n| ![Android Diamond ImageView](images/small-diamond.png) | ![Android Pentagon ImageView](images/small-pentagon.png) | ![Android Hexagon ImageView](images/small-hexagon.png)   |\n\n\n| OctogonImageView                                       | StarImageView                                            | HeartImageView                                       |\n| ------------------------------------------------------ | -------------------------------------------------------- | ---------------------------------------------------- |\n| ![Android Octogon ImageView](images/small-octogon.png) | ![Android Start ImageView](images/small-star.png)        | ![Android Heart ImageView](images/small-heart.png)   |\n\n\n```XML\n<com.github.siyamed.shapeimageview.{ClassName}\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:layout_margin=\"8dp\"\n    android:src=\"@drawable/neo\"\n    app:siBorderWidth=\"8dp\"\n    app:siBorderColor=\"@color/darkgray\"/>\n```\n\nAttributes:\n* `siBorderColor` border color\n* `siBorderWidth` border width in dp\n* `siBorderAlpha` alpha value of the border between 0.0-1.0\n* `siStrokeCap` border stroke cap type `butt|round|square`\n* `siStrokeJoin` border stroke join type `bevel|miter|round`\n* `siSquare` set width and height to the minimum of the given values `true|false`\n* `siShape` a reference to an SVG. This is used by ShapeImageView, not the subclasses of it.\n\n\nSVG elements that are supported are: [rectangle][svg_rectangle], [circle][svg_circle], \n[ellipse][svg_ellipse], [polygon][svg_polygon], [path][svg_path], [group][svg_group]. [Transformations][svg_transformations] on those elements are also supported. \n\nThe system converts an SVG file into a Path. For each element including the parent element `<svg>` a new Path is created, and all the children Path's are [added][Path.addPath] to their parent path. \n\n###Bitmap Mask Based ImageViews \n\nThis view uses extra bitmaps for bitmap masks. Therefore it would be good to use them for very custom shapes, \npossibly not in a recycling view. \n\n* With [mask bitmap](sample/src/main/res/drawable/star.png): \n\n![Android Star Shape ImageView ](images/small-mask-star.png)\n```XML\n<com.github.siyamed.shapeimageview.mask.PorterShapeImageView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    app:siShape=\"@drawable/star\"\n    android:src=\"@drawable/neo\"\n    app:siSquare=\"true\"/>\n```\n\n* With [shape XML](sample/src/main/res/drawable/shape_rounded_rectangle.xml):\n\n![Android Star Shape ImageView ](images/small-xmlshape-rounded.png)\n\n```XML\n<com.github.siyamed.shapeimageview.mask.PorterShapeImageView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    app:siShape=\"@drawable/shape_rounded_rectangle\"\n    android:src=\"@drawable/neo\"\n    app:siSquare=\"true\"/>\n```\n\nrounded rectangle shape definition in XML: \n\n```XML\n<shape android:shape=\"rectangle\" xmlns:android=\"http://schemas.android.com/apk/res/android\">\n    <corners\n        android:topLeftRadius=\"18dp\"\n        android:topRightRadius=\"18dp\"\n        android:bottomLeftRadius=\"18dp\"\n        android:bottomRightRadius=\"18dp\" />\n    <solid android:color=\"@color/black\" />\n</shape>\n```\n\nAttributes:\n* `siShape` the bitmap mask shape, either a shape drawable or a bitmap\n* `siSquare` set width and height to the minimum of the given values `true|false`\n\nThis method reads a shape file (either bitmap or an android shape xml), creates a bitmap object using this shape, and finally combines the bitmap of the real image to be shown and the mast bitmap using xfermode. \n\n## Sample\n\nSee/execute the [sample](sample) for a demonstration of the components.\n\nIf you are lazy check [this youtube video][youtube_video] demonstrating scrolling in the sample app\n\nYou can download the [sample app from play store][sample_app_play_store] \n\n## Proguard\n\n```\n-dontwarn android.support.v7.**\n-keep class android.support.v7.** { ; }\n-keep interface android.support.v7.* { ; }\n-keepattributes *Annotation,Signature\n-dontwarn com.github.siyamed.**\n-keep class com.github.siyamed.shapeimageview.**{ *; }\n```\n\n## References\n* [MostafaGazar/CustomShapeImageView](https://github.com/MostafaGazar/CustomShapeImageView): Used this project a basis for bitmap masks  \n* [geosolutions-it/mapsforge/svg-android](https://github.com/geosolutions-it/mapsforge/tree/master/svg-android): Used and modified to create a path from a svg file \n\n[![Android Shape Image View on Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-android--shape--imageview-brightgreen.svg?style=flat)](https://android-arsenal.com/details/1/932)\n"
 },
 {
  "repo": "yidongnan/grpc-spring-boot-starter",
  "language": "Java",
  "readme_contents": "# gRPC Spring Boot Starter\n\n[![Build master branch](https://github.com/yidongnan/grpc-spring-boot-starter/workflows/Build%20master%20branch/badge.svg)](https://github.com/yidongnan/grpc-spring-boot-starter/actions) [![Maven Central with version prefix filter](https://img.shields.io/maven-central/v/net.devh/grpc-spring-boot-starter.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22net.devh%22%20grpc) [![MIT License](https://img.shields.io/github/license/mashape/apistatus.svg)](LICENSE) [![Crowdin](https://badges.crowdin.net/grpc-spring-boot-starter/localized.svg)](https://crowdin.com/project/grpc-spring-boot-starter)\n\n[![Client-Javadoc](https://www.javadoc.io/badge/net.devh/grpc-client-spring-boot-autoconfigure.svg?label=Client-Javadoc)](https://www.javadoc.io/doc/net.devh/grpc-client-spring-boot-autoconfigure) [![Server-Javadoc](https://www.javadoc.io/badge/net.devh/grpc-server-spring-boot-autoconfigure.svg?label=Server-Javadoc)](https://www.javadoc.io/doc/net.devh/grpc-server-spring-boot-autoconfigure) [![Common-Javadoc](https://www.javadoc.io/badge/net.devh/grpc-common-spring-boot.svg?label=Common-Javadoc)](https://www.javadoc.io/doc/net.devh/grpc-common-spring-boot)\n\nREADME: [English](README.md) | [\u4e2d\u6587](README-zh-CN.md)\n\n**\u6587\u6863\uff1a** [English](https://yidongnan.github.io/grpc-spring-boot-starter/en/) | [\u4e2d\u6587](https://yidongnan.github.io/grpc-spring-boot-starter/zh-CN/)\n\n## \u7279\u6027\n\n* \u4f7f\u7528 `@GrpcService` \u6ce8\u89e3\u53ef\u4ee5\u5b9e\u73b0\u81ea\u52a8\u914d\u7f6e\u548c\u8fd0\u884c gRPC Server \u7aef\n\n* \u4f7f\u7528 `@GrpcClient` \u6ce8\u89e3\u53ef\u4ee5\u5b9e\u73b0\u81ea\u52a8\u521b\u5efa\u548c\u7ba1\u7406\u60a8\u7684 gRPC Channels \u548c stubs\n\n* \u652f\u6301\u5176\u4ed6 grpc-java \u7684\u53d8\u79cd (\u4f8b\u5982\uff1a [Reactive gRPC (RxJava)](https://github.com/salesforce/reactive-grpc/tree/master/rx-java), [grpc-kotlin](https://github.com/grpc/grpc-kotlin), ...)\n  * Server \u7aef\uff1a\u9002\u7528\u4e8e\u6240\u6709 grpc-java \u7684\u53d8\u79cd ( \u57fa\u4e8e `io.grpc.BindableService`)\n  * Client \u7aef\uff1a\u9700\u8981\u81ea\u5b9a\u4e49 `StubFactory` \u5f53\u524d\u5185\u7f6e\u652f\u6301\uff1a\n    * grpc-java\n    * (\u8bf7\u544a\u77e5\u6211\u4eec\u4e0d\u652f\u6301\u7684\u7ec4\u4ef6\uff0c\u6211\u4eec\u53ef\u4ee5\u6dfb\u52a0\u5bf9\u5b83\u4eec\u7684\u652f\u6301)\n\n* \u652f\u6301 [Spring-Security](https://github.com/spring-projects/spring-security)\n\n* \u652f\u6301 [Spring Cloud](https://spring.io/projects/spring-cloud)\n  * \u670d\u52a1\u7aef\uff1a\u5411\u670d\u52a1\u6ce8\u518c\u8be6\u60c5\u4e2d\u6dfb\u52a0 gRPC \u7aef\u53e3\u4fe1\u606f\u3002 \u76ee\u524d\u539f\u751f\u652f\u6301\uff1a\n    * [Consul](https://github.com/spring-cloud/spring-cloud-consul)\n    * [Eureka](https://github.com/spring-cloud/spring-cloud-netflix)\n    * [Nacos](https://github.com/spring-cloud-incubator/spring-cloud-alibaba)\n    * (\u8bf7\u544a\u8bc9\u6211\u4eec\u4e0d\u652f\u6301\u7684\u7ec4\u4ef6\uff0c\u6211\u4eec\u53ef\u4ee5\u6dfb\u52a0\u5bf9\u5b83\u4eec\u7684\u652f\u6301)\n  * \u5ba2\u6237\u7aef\uff1a\u4ece Spring \u7684 `DiscoveryClient` (\u6240\u6709\u53d8\u79cd) \u8bfb\u53d6\u670d\u52a1\u7684\u76ee\u6807\u5730\u5740\n\n* \u652f\u6301[Spring Sleuth](https://github.com/spring-cloud/spring-cloud-sleuth)\u4f5c\u4e3a\u5206\u5e03\u5f0f\u94fe\u8def\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848(\u5982\u679c[brave-instrument-grpc](https://mvnrepository.com/artifact/io.zipkin.brave/brave-instrumentation-grpc)\u5b58\u5728)\n\n* \u652f\u6301\u5168\u5c40\u548c\u81ea\u5b9a\u4e49\u7684 gRPC \u670d\u52a1\u7aef/\u5ba2\u6237\u7aef\u62e6\u622a\u5668\n\n* \u652f\u6301metric (\u57fa\u4e8e[micrometer](https://micrometer.io/)/[actuator](https://github.com/spring-projects/spring-boot/tree/master/spring-boot-project/spring-boot-actuator) )\n\n* \u4e5f\u9002\u7528\u4e8e (non-shaded) grpc-netty\n\n## \u7248\u672c\n\n\u6700\u65b0\u7248\u672c\u662f `2.13.1.RELEASE` \u5b83\u80fd\u8ddf Spring-Boot `2.5\u7684\u3002` \u548c Spring-Cloud `2020.0.5` \u642d\u914d\u4f7f\u7528\u3002 \u4f46\u5b83\u4e5f\u4e0e\u5404\u79cd\u5176\u4ed6\u7248\u672c\u517c\u5bb9\u3002 \u6211\u4eec\u7684 [\u6587\u6863](https://yidongnan.github.io/grpc-spring-boot-starter/en/versions.html) \u4e2d\u53ef\u4ee5\u627e\u5230\u6240\u6709\u7248\u672c\u53ca\u5176\u76f8\u5e94\u7684\u5e93\u7248\u672c\u7684\u6982\u89c8\u3002\n\n**\u6ce8\u610f:** \u8be5\u9879\u76ee\u4e5f\u53ef\u4ee5\u5728\u6ca1\u6709 Spring-Boot \u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\uff0c\u4f46\u662f\u60a8\u9700\u8981\u624b\u52a8\u914d\u7f6e\u4e00\u4e9b bean\u3002\n\n## \u7528\u6cd5\n\n### gRPC \u670d\u52a1\u7aef + \u5ba2\u6237\u7aef\n\n\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6dfb\u52a0 Maven \u4f9d\u8d56\u9879\uff1a\n\n````xml\n<dependency>\n  <groupId>net.devh</groupId>\n  <artifactId>grpc-spring-boot-starter</artifactId>\n  <version>2.13.1.RELEASE</version>\n</dependency>\n````\n\n\u4f7f\u7528 Gradle \u6dfb\u52a0\u4f9d\u8d56\uff1a\n\n````gradle\ndependencies {\n  implementation 'net.devh:grpc-spring-boot-starter:2.13.1.RELEASE'\n}\n````\n\n### gRPC \u670d\u52a1\u7aef\n\n\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6dfb\u52a0 Maven \u4f9d\u8d56\u9879\uff1a\n\n````xml\n<dependency>\n  <groupId>net.devh</groupId>\n  <artifactId>grpc-server-spring-boot-starter</artifactId>\n  <version>2.13.1.RELEASE</version>\n</dependency>\n````\n\n\u4f7f\u7528 Gradle \u6dfb\u52a0\u4f9d\u8d56\u9879\uff1a\n\n````gradle\ndependencies {\n  implementation 'net.devh:grpc-server-spring-boot-starter:2.13.1.RELEASE'\n}\n````\n\n\u5728\u670d\u52a1\u7aef\u63a5\u53e3\u5b9e\u73b0\u7c7b\u4e0a\u6dfb\u52a0 `@GrpcService` \u6ce8\u89e3\u3002\n\n````java\n@GrpcService\npublic class GrpcServerService extends GreeterGrpc.GreeterImplBase {\n\n    @Override\n    public void sayHello(HelloRequest req, StreamObserver<HelloReply> responseObserver) {\n        HelloReply reply = HelloReply.newBuilder().setMessage(\"Hello ==> \" + req.getName()).build();\n        responseObserver.onNext(reply);\n        responseObserver.onCompleted();\n    }\n\n}\n````\n\n\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cgRPC \u670d\u52a1\u5668\u5c06\u76d1\u542c\u7aef\u53e3 `9090`\u3002 \u7aef\u53e3\u7684\u914d\u7f6e\u548c\u5176\u4ed6\u7684 [\u8bbe\u7f6e](grpc-server-spring-boot-autoconfigure/src/main/java/net/devh/boot/grpc/server/config/GrpcServerProperties.java) \u53ef\u4ee5\u901a\u8fc7 Spring \u7684\u5c5e\u6027\u673a\u5236\u8fdb\u884c\u66f4\u6539\u3002 \u670d\u52a1\u7aef\u7684\u914d\u7f6e\u4f7f\u7528 `grpc.server.` \u524d\u7f00\u3002\n\n\u8be6\u60c5\u8bf7\u53c2\u9605\u6211\u4eec\u7684[\u6587\u6863](https://yidongnan.github.io/grpc-spring-boot-starter/)\u3002\n\n### gRPC \u5ba2\u6237\u7aef\n\n\u4f7f\u7528\u4e00\u4e0b\u547d\u4ee4\u6dfb\u52a0 Maven \u4f9d\u8d56\u9879\uff1a\n\n````xml\n<dependency>\n  <groupId>net.devh</groupId>\n  <artifactId>grpc-client-spring-boot-starter</artifactId>\n  <version>2.13.1.RELEASE</version>\n</dependency>\n````\n\n\u4f7f\u7528 Gradle \u6dfb\u52a0\u4f9d\u8d56\u9879\uff1a\n\n````gradle\ndependencies {\n  compile 'net.devh:grpc-client-spring-boot-starter:2.13.1.RELEASE'\n}\n````\n\n\u5728 grpc \u5ba2\u6237\u7aef\u7684\u7684 stub \u5b57\u6bb5\u4e0a\u6dfb\u52a0 `@GrpcClient(serverName)` \u6ce8\u89e3\u3002\n\n* \u8bf7\u4e0d\u8981\u5c06 @GrpcClient \u4e0e `@Autowireed` \u6216 `@Inject` \u4e00\u8d77\u4f7f\u7528\u3002\n\n  ````java\n  @GrpcClient(\"gRPC server name\")\n  private GreeterGrpc.GreeterBlockingStub greeterStub;\n  ````\n\n**\u6ce8\u610f:** \u4f60\u53ef\u4ee5\u5c06\u76f8\u540c\u7684 grpc \u670d\u52a1\u7aef\u540d\u79f0\u7528\u4e8e\u591a\u4e2a channel\uff0c \u4e5f\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684 stub \uff08\u751a\u81f3\u4f7f\u7528\u4e0d\u540c\u7684 stub \u62e6\u622a\u5668\uff09\n\n\u7136\u540e\u60a8\u53ef\u4ee5\u5411\u60a8\u7684\u670d\u52a1\u5668\u53d1\u9001\u67e5\u8be2\uff0c\u5c31\u50cf\u8fd9\u6837\uff1a\n\n````java\nHelloReply response = stub.sayHello(HelloRequest.newBuilder().setName(name).build());\n````\n\n\u53ef\u4ee5\u5355\u72ec\u914d\u7f6e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u76ee\u6807\u5730\u5740\u3002 \u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u60a8\u53ef\u4ee5\u4ec5\u4f9d\u9760\u9ed8\u8ba4\u914d\u7f6e\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7 `NameResolver.Factory` Bean \u7c7b\u81ea\u5b9a\u4e49\u9ed8\u8ba4\u7684 url \u6620\u5c04\u3002 \u5982\u679c\u60a8\u6ca1\u6709\u914d\u7f6e\u90a3\u4e2aBean\uff0c\u90a3\u4e48\u9ed8\u8ba4\u7684 uri \u5c06\u4f7f\u7528\u9ed8\u8ba4\u65b9\u6848\u548c\u540d\u79f0(\u5982\uff1a`dns:<name>`)\uff1a\n\n\u8fd9\u4e9b\u914d\u7f6e\u548c\u5176\u4ed6\u7684 [\u8bbe\u7f6e](grpc-client-spring-boot-autoconfigure/src/main/java/net/devh/boot/grpc/client/config/GrpcChannelProperties.java) \u53ef\u4ee5\u901a\u8fc7 Spring \u7684\u5c5e\u6027\u673a\u5236\u8fdb\u884c\u66f4\u6539\u3002 \u5ba2\u6237\u7aef\u4f7f\u7528`grpc.client.(serverName)\u3002` \u524d\u7f00\u3002\n\n\u8be6\u60c5\u8bf7\u53c2\u9605\u6211\u4eec\u7684[\u6587\u6863](https://yidongnan.github.io/grpc-spring-boot-starter/)\u3002\n\n## \u4f7f\u7528 (non-shaded) grpc-netty \u8fd0\u884c\n\n\u8fd9\u4e2a\u5e93\u652f\u6301`grpc-netty`\u548c`grpc-nety-shaded`\u3002 \u540e\u4e00\u79cd\u53ef\u80fd\u4f1a\u9632\u6b62\u4e0e\u4e0d\u517c\u5bb9\u7684 gRPC \u7248\u672c\u51b2\u7a81\u6216\u4e0d\u540c netty \u7248\u672c\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\n\n**\u6ce8\u610f:** \u5982\u679c\u5728classpath \u4e2d\u5b58\u5728 shaded netty\uff0c \u5219 shaded netty \u5c06\u4f7f\u7528\u6709\u7ebf\u4e0e non-shaded grpc-netty\u3002\n\n\u60a8\u53ef\u4ee5\u5728 Maven \u4e2d\u8fd9\u6837\u4f7f\u7528\u3002\n\n````xml\n<dependency>\n    <groupId>io.grpc</groupId>\n    <artifactId>grpc-netty</artifactId>\n    <version>${grpcVersion}</version>\n</dependency>\n\n<!-- For both -->\n<dependency>\n    <groupId>net.devh</groupId>\n    <artifactId>grpc-spring-boot-starter</artifactId>\n    <version>...</version>\n    <exclusions>\n        <exclusion>\n            <groupId>io.grpc</groupId>\n            <artifactId>grpc-netty-shaded</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<!-- For the server (only) -->\n<dependency>\n    <groupId>net.devh</groupId>\n    <artifactId>grpc-server-spring-boot-starter</artifactId>\n    <version>...</version>\n    <exclusions>\n        <exclusion>\n            <groupId>io.grpc</groupId>\n            <artifactId>grpc-netty-shaded</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<!-- For the client (only) -->\n<dependency>\n    <groupId>net.devh</groupId>\n    <artifactId>grpc-client-spring-boot-starter</artifactId>\n    <version>...</version>\n    <exclusions>\n        <exclusion>\n            <groupId>io.grpc</groupId>\n            <artifactId>grpc-netty-shaded</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n````\n\n\u7c7b\u4f3c\uff0c\u4f7f\u7528 Gradle \u7684\u5982\u4e0b\n\n````groovy\nimplementation \"io.grpc:grpc-netty:${grpcVersion}\"\n\nimplementation 'net.devh:grpc-spring-boot-starter:...' exclude group: 'io.grpc', module: 'grpc-netty-shaded' // For both\nimplementation 'net.devh:grpc-client-spring-boot-starter:...' exclude group: 'io.grpc', module: 'grpc-netty-shaded' // For the client (only)\nimplementation 'net.devh:grpc-server-spring-boot-starter:...' exclude group: 'io.grpc', module: 'grpc-netty-shaded' // For the server (only)\n````\n\n## \u793a\u4f8b\u9879\u76ee\n\n\u5728 [\u8fd9\u91cc](examples)\u53ef\u4ee5\u67e5\u770b\u66f4\u591a\u5173\u4e8e\u8be5\u9879\u76ee\u7684\u793a\u4f8b\u3002\n\n## \u6392\u9664\u6545\u969c\n\n\u8bf7\u53c2\u9605\u6211\u4eec\u7684[\u6587\u6863](https://yidongnan.github.io/grpc-spring-boot-starter/en/trouble-shooting)\u5bfb\u6c42\u5e2e\u52a9\u3002\n\n## \u53c2\u4e0e\u8d21\u732e\n\n\u6b22\u8fce\u60a8\u5bf9\u9879\u76ee\u4f5c\u51fa\u4efb\u4f55\u8d21\u732e\u3002 \u8be6\u89c1[CONTRIBUTING.md](CONTRIBUTING.md)\u3002\n"
 },
 {
  "repo": "liangfeidotme/MasteringAndroidDataBinding",
  "language": "Java",
  "readme_contents": "# \u7cbe\u901a Android Data Binding\n\n[![Build Status](https://travis-ci.org/LyndonChin/MasteringAndroidDataBinding.svg)](https://travis-ci.org/LyndonChin/MasteringAndroidDataBinding)\n\n* \u66f4\u591a\u5e72\u8d27\u53ef\u79fb\u6b65\u81f3[\u4e2a\u4eba\u4e3b\u9875](http://liangfei.me)\n* QQ \u4ea4\u6d41\u7fa4\uff1a**324112728** \uff0c\u6216\u8005[\u70b9\u51fb\u94fe\u63a5\u52a0\u5165QQ\u7fa4](http://jq.qq.com/?_wv=1027&k=2CokoRt)\n\n<img width=\"400px\" src=\"https://cdn.nlark.com/yuque/0/2019/png/124977/1559045910714-8948c8b2-2b86-44a3-a600-a4415db3c01f.png\"/>\n\n---\n\n\u5b98\u65b9\u867d\u7136\u5df2\u7ecf\u7ed9\u51fa\u4e86\u6559\u7a0b - [Data Binding Guide](https://developer.android.com/tools/data-binding/guide.html) [\uff08\u4e2d\u6587\u7248 - Data Binding\uff08\u6570\u636e\u7ed1\u5b9a\uff09\u7528\u6237\u6307\u5357\uff09](http://www.jianshu.com/p/b1df61a4df77) \uff0c\u4f46\u662f\u5b9e\u8df5\u4e4b\u540e\u53d1\u73b0\u69fd\u70b9\u5b9e\u5728\u592a\u591a\uff0c\u4e8e\u662f\u5c31\u6709\u4e86\u8fd9\u4e2a\u6559\u7a0b\uff0c\u9488\u5bf9\u6bcf\u4e2a\u77e5\u8bc6\u70b9\u7ed9\u51fa\u66f4\u8be6\u5b9e\u7684\u4f8b\u5b50\u540c\u65f6\u4e5f\u603b\u7ed3\u4e86\u9047\u5230\u7684\u4e00\u4e9b\u5751\uff0c\u5e0c\u671b\u5bf9\u4f60\u6709\u6240\u5e2e\u52a9\uff1a\uff09\n\n> \u6211\u73b0\u5728\u8f6c\u884c\u505a\u7eaf\u524d\u7aef\u5f00\u53d1\u4e86\uff0c\u5199\u4e86\u51e0\u4e2a\u6708 React/Vue \u4e4b\u540e\u53d1\u73b0\uff0cDataBinding \u771f\u662f\u4e00\u4e2a\u4f1f\u5927\u7684 MVVM \u6846\u67b6\uff0c\u5b83\u7f29\u5c0f\u4e86 Native \u5f00\u53d1\u548c\u524d\u7aef\u5f00\u53d1\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u6280\u672f\u4f1a\u8fc7\u65f6\uff0c\u7406\u5ff5\u6052\u4e45\u8fdc\u3002\n\n## \u51c6\u5907\n\n\u65b0\u5efa\u4e00\u4e2a Project\uff0c\u5efa\u8bae\u4f7f\u7528[\u65b0\u7248\u672c\u7684 Gradle \u63d2\u4ef6](build.gradle#L16)\uff08\u81f3\u5c11\u8981\u4fdd\u8bc1\u63d2\u4ef6\u7248\u672c\u4e0d\u4f4e\u4e8e **1.5.0**\uff09\uff1a\n\n```groovy\nclasspath 'com.android.tools.build:gradle:3.2.1'\n```\n\n\u7136\u540e\u4fee\u6539\u5bf9\u5e94\u6a21\u5757\uff08Module\uff09\u7684 [build.gradle](app/build.gradle#L6-L8)\uff1a\n\n```groovy\ndataBinding {\n    enabled true\n}\n```\n\n## \u57fa\u7840\n\n\u5de5\u7a0b\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u6700\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u8bf4\u660e Data Binding \u7684\u57fa\u672c\u7528\u6cd5\u3002\n\n### \u5e03\u5c40\u6587\u4ef6\n\n\u4f7f\u7528 Data Binding \u4e4b\u540e\uff0cxml \u7684\u5e03\u5c40\u6587\u4ef6\u5c31\u4e0d\u518d\u7528\u4e8e\u5355\u7eaf\u5730\u5c55\u793a UI \u5143\u7d20\uff0c\u8fd8\u9700\u8981\u5b9a\u4e49 UI \u5143\u7d20\u7528\u5230\u7684\u53d8\u91cf\u3002\u6240\u4ee5\uff0c\u5b83\u7684\u6839\u8282\u70b9\u4e0d\u518d\u662f\u4e00\u4e2a `ViewGroup`\uff0c\u800c\u662f\u53d8\u6210\u4e86 `layout`\uff0c\u5e76\u4e14\u65b0\u589e\u4e86\u4e00\u4e2a\u8282\u70b9 `data`\u3002\n\n```xml\n<layout xmlns:android=\"http://schemas.android.com/apk/res/android\">\n    <data>\n    </data>\n    <!--\u539f\u5148\u7684\u6839\u8282\u70b9\uff08Root Element\uff09-->\n    <LinearLayout>\n    ....\n    </LinearLayout>\n</layout>\n```\n\n\u8981\u5b9e\u73b0 MVVM \u7684 `ViewModel` \u5c31\u9700\u8981\u628a\u6570\u636e\uff08Model\uff09\u4e0e UI\uff08View\uff09 \u8fdb\u884c\u7ed1\u5b9a\uff0c`data` \u8282\u70b9\u7684\u4f5c\u7528\u5c31\u50cf\u4e00\u4e2a\u6865\u6881\uff0c\u642d\u5efa\u4e86 View \u548c Model \u4e4b\u95f4\u7684\u901a\u8def\u3002\n\n\u6211\u4eec\u5148\u5728 xml \u5e03\u5c40\u6587\u4ef6\u7684 `data` \u8282\u70b9\u4e2d\u58f0\u660e\u4e00\u4e2a `variable`\uff0c\u8fd9\u4e2a\u53d8\u91cf\u4f1a\u4e3a UI \u5143\u7d20\u63d0\u4f9b\u6570\u636e\uff08\u4f8b\u5982 `TextView` \u7684 `android:text`\uff09\uff0c\u7136\u540e\u5728 Java \u4ee3\u7801\u4e2d\u628a\u300e\u540e\u53f0\u300f\u6570\u636e\u4e0e\u8fd9\u4e2a `variable` \u8fdb\u884c\u7ed1\u5b9a\u3002\n\n\u4e0b\u9762\u6211\u4eec\u4f7f\u7528 Data Binding \u521b\u5efa\u4e00\u4e2a\u5c55\u793a\u7528\u6237\u4fe1\u606f\u7684\u8868\u683c\u3002\n\n### \u6570\u636e\u5bf9\u8c61\n\n\u6dfb\u52a0\u4e00\u4e2a POJO \u7c7b - [`User`](app/src/main/java/com/liangfeizc/databinding/model/User.java)\uff0c\u975e\u5e38\u7b80\u5355\uff0c\u4e24\u4e2a\u5c5e\u6027\u4ee5\u53ca\u4ed6\u4eec\u7684 getter \u548c setter\u3002\n\n```java\npublic class User {\n    private final String firstName;\n    private final String lastName;\n\n    public User(String firstName, String lastName) {\n        this.firstName = firstName;\n        this.lastName = lastName;\n    }\n\n    public String getFirstName() {\n        return firstName;\n    }\n\n    public String getLastName() {\n        return lastName;\n    }\n}\n```\n\n\u7a0d\u540e\uff0c\u6211\u4eec\u4f1a\u65b0\u5efa\u4e00\u4e2a `User` \u7c7b\u578b\u7684\u53d8\u91cf\uff0c\u7136\u540e\u628a\u5b83\u8ddf\u5e03\u5c40\u6587\u4ef6\u4e2d\u58f0\u660e\u7684\u53d8\u91cf\u8fdb\u884c\u7ed1\u5b9a\u3002\n\n### \u5b9a\u4e49 Variable\n\n\u56de\u5230\u5e03\u5c40\u6587\u4ef6\uff0c\u5728 `data` \u8282\u70b9\u4e2d\u58f0\u660e\u4e00\u4e2a `User` \u7c7b\u578b\u7684\u53d8\u91cf `user`\u3002\n\n```xml\n<data>\n\t<variable name=\"user\" type=\"com.liangfeizc.databindingsamples.basic.User\" />\n</data>\n```\n\n\u5176\u4e2d `type` \u5c5e\u6027\u5c31\u662f\u6211\u4eec\u5728 Java \u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684 `User` \u7c7b\u3002\n\n\u5f53\u7136\uff0c`data` \u8282\u70b9\u4e5f\u652f\u6301 `import`\uff0c\u6240\u4ee5\u4e0a\u9762\u7684\u4ee3\u7801\u53ef\u4ee5\u6362\u4e00\u79cd\u5f62\u5f0f\u6765\u5199\u3002\n\n```xml\n<data>\n    <import type=\"com.liangfeizc.databindingsamples.basic.User\" />\n    <variable name=\"user\" type=\"User\" />\n</data>\n```\n\n\u7136\u540e\u6211\u4eec\u521a\u624d\u5728 build.gradle \u4e2d\u6dfb\u52a0\u7684\u90a3\u4e2a\u63d2\u4ef6 - `com.android.databinding` \u4f1a\u6839\u636e xml \u6587\u4ef6\u7684\u540d\u79f0 **Generate** \u4e00\u4e2a\u7ee7\u627f\u81ea `ViewDataBinding` \u7684\u7c7b\u3002 \u5f53\u7136\uff0cIDE \u4e2d\u770b\u4e0d\u5230\u8fd9\u4e2a\u6587\u4ef6\uff0c\u9700\u8981\u624b\u52a8\u53bb build \u76ee\u5f55\u4e0b\u627e\u3002\n\n\u4f8b\u5982\uff0c\u8fd9\u91cc xml \u7684\u6587\u4ef6\u540d\u53eb `activity_basic.xml`\uff0c\u90a3\u4e48\u751f\u6210\u7684\u7c7b\u5c31\u662f `ActivityBasicBinding`\u3002\n\n**\u6ce8\u610f**\n\n`java.lang.*` \u5305\u4e2d\u7684\u7c7b\u4f1a\u88ab\u81ea\u52a8\u5bfc\u5165\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\uff0c\u4f8b\u5982\u8981\u5b9a\u4e49\u4e00\u4e2a `String` \u7c7b\u578b\u7684\u53d8\u91cf\uff1a\n\n```xml\n<variable name=\"firstName\" type=\"String\" />\n```\n\n### \u7ed1\u5b9a Variable\n\n\u4fee\u6539 [`BasicActivity`](app/src/main/java/com/liangfeizc/databinding/sample/basic/BasicActivity.java#L17-L20) \u7684 `onCreate` \u65b9\u6cd5\uff0c\u7528 `DatabindingUtil.setContentView()` \u6765\u66ff\u6362\u6389 `setContentView()`\uff0c\u7136\u540e\u521b\u5efa\u4e00\u4e2a `user` \u5bf9\u8c61\uff0c\u901a\u8fc7 `binding.setUser(user)` \u4e0e `variable` \u8fdb\u884c\u7ed1\u5b9a\u3002\n\n```java\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    ActivityBasicBinding binding = DataBindingUtil.setContentView(\n            this, R.layout.activity_basic);\n    User user = new User(\"fei\", \"Liang\");\n    binding.setUser(user);\n}\n```\n\n\u9664\u4e86\u4f7f\u7528\u6846\u67b6\u81ea\u52a8\u751f\u6210\u7684 `ActivityBasicBinding`\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u65b9\u5f0f\u81ea\u5b9a\u4e49\u7c7b\u540d\u3002\n\n```xml\n<data class=\"com.example.CustomBinding\">\n</data>\n```\n\n**\u6ce8\u610f**\n\n`ActivityBasicBinding` \u7c7b\u662f\u81ea\u52a8\u751f\u6210\u7684\uff0c\u6240\u6709\u7684 `set` \u65b9\u6cd5\u4e5f\u662f\u6839\u636e `variable` \u540d\u79f0\u751f\u6210\u7684\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e24\u4e2a\u53d8\u91cf\u3002\n\n```xml\n<data>\n    <variable name=\"firstName\" type=\"String\" />\n    <variable name=\"lastName\" type=\"String\" />\n</data>\n```\n\n\u90a3\u4e48\u5c31\u4f1a\u751f\u6210\u5bf9\u5e94\u7684\u4e24\u4e2a set \u65b9\u6cd5\u3002\n\n```java\nsetFirstName(String firstName);\nsetLastName(String lastName);\n```\n\n\n### \u4f7f\u7528 Variable\n\n\u6570\u636e\u4e0e Variable \u7ed1\u5b9a\u4e4b\u540e\uff0cxml \u7684 UI \u5143\u7d20\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4e86\u3002\n\n```xml\n<TextView\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:text=\"@{user.lastName}\" />\n```\n\n\u81f3\u6b64\uff0c\u4e00\u4e2a\u7b80\u5355\u7684\u6570\u636e\u7ed1\u5b9a\u5c31\u5b8c\u6210\u4e86\uff0c\u53ef\u53c2\u8003[\u5b8c\u6574\u4ee3\u7801](app/src/main/java/com/liangfeizc/databinding/sample/basic/)\n\n## \u9ad8\u7ea7\u7528\u6cd5\n\n### \u4f7f\u7528\u7c7b\u65b9\u6cd5\n\n\u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u9759\u6001\u65b9\u6cd5\n\n```java\npublic class MyStringUtils {\n    public static String capitalize(final String word) {\n        if (word.length() > 1) {\n            return String.valueOf(word.charAt(0)).toUpperCase() + word.substring(1);\n        }\n        return word;\n    }\n}\n```\n\n\u7136\u540e\u5728 xml \u7684 `data` \u8282\u70b9\u4e2d\u5bfc\u5165\uff1a\n\n```xml\n<import type=\"com.liangfeizc.databindingsamples.utils.MyStringUtils\" />\n```\n\n\u4f7f\u7528\u65b9\u6cd5\u4e0e Java \u8bed\u6cd5\u4e00\u6837\uff1a\n\n```java\n<TextView\n\tandroid:layout_width=\"wrap_content\"\n\tandroid:layout_height=\"wrap_content\"\n\tandroid:text=\"@{MyStringUtils.capitalize(user.firstName)}\" />\n```\n\n### \u7c7b\u578b\u522b\u540d\n\n\u5982\u679c\u6211\u4eec\u5728 `data` \u8282\u70b9\u4e86\u5bfc\u5165\u4e86\u4e24\u4e2a\u540c\u540d\u7684\u7c7b\u600e\u4e48\u529e\uff1f\n\n```xml\n<import type=\"com.example.home.data.User\" />\n<import type=\"com.examle.detail.data.User\" />\n<variable name=\"user\" type=\"User\" />\n```\n\n\u8fd9\u6837\u4e00\u6765\u51fa\u73b0\u4e86\u4e24\u4e2a `User` \u7c7b\uff0c\u90a3 `user` \u53d8\u91cf\u8981\u7528\u54ea\u4e00\u4e2a\u5462\uff1f\u4e0d\u7528\u62c5\u5fc3\uff0c`import` \u8fd8\u6709\u4e00\u4e2a `alias` \u5c5e\u6027\u3002\n\n```xml\n<import type=\"com.example.home.data.User\" />\n<import type=\"com.examle.detail.data.User\" alias=\"DetailUser\" />\n<variable name=\"user\" type=\"DetailUser\" />\n```\n\n### Null Coalescing \u8fd0\u7b97\u7b26\n\n```java\nandroid:text=\"@{user.displayName ?? user.lastName}\"\n```\n\n\u5c31\u7b49\u4ef7\u4e8e\n\n```java\nandroid:text=\"@{user.displayName != null ? user.displayName : user.lastName}\"\n```\n\n### \u5c5e\u6027\u503c\n\n\u901a\u8fc7 `@{}` \u53ef\u4ee5\u76f4\u63a5\u628a Java \u4e2d\u5b9a\u4e49\u7684\u5c5e\u6027\u503c\u8d4b\u503c\u7ed9 xml \u5c5e\u6027\u3002\n\n```xml\n<TextView\n   android:text=\"@{user.lastName}\"\n   android:layout_width=\"wrap_content\"\n   android:layout_height=\"wrap_content\"\n   android:visibility=\"@{user.isAdult ? View.VISIBLE : View.GONE}\"/>\n```\n\n### \u4f7f\u7528\u8d44\u6e90\u6570\u636e\n\n\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u5b98\u65b9\u6559\u7a0b\u6709\u9519\u8bef\uff0c\u53ef\u4ee5\u53c2\u8003[Android Data Binder \u7684\u4e00\u4e2abug](http://blog.csdn.net/feelang/article/details/46342699)\uff0c[\u5b8c\u6574\u4ee3\u7801\u5728\u6b64](app/src/main/res/layout/activity_resource.xml)\n\n```xml\n<TextView\n    android:padding=\"@{large? (int)@dimen/largePadding : (int)@dimen/smallPadding}\"\n    android:background=\"@android:color/black\"\n    android:textColor=\"@android:color/white\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:text=\"@string/hello_world\" />\n```\n\n## Observable Binding\n\n\u672c\u6765\u8fd9\u4e00\u8282\u7684\u6807\u9898\u5e94\u8be5\u53eb**\u53cc\u5411\u7ed1\u5b9a**\uff0c\u4f46\u662f\u5f88\u9057\u61be\uff0c\u73b0\u5728\u7684 **Data Binding** \u6682\u65f6\u652f\u6301\u5355\u5411\u7ed1\u5b9a\uff0c\u8fd8\u6ca1\u6709\u8fbe\u5230 **Angular.js** \u7684\u5a01\u529b\u3002\n\n\u8981\u5b9e\u73b0 Observable Binding\uff0c\u9996\u5148\u5f97\u6709\u4e00\u4e2a `implement` \u4e86\u63a5\u53e3 `android.databinding.Observable` \u7684\u7c7b\uff0c\u4e3a\u4e86\u65b9\u4fbf\uff0cAndroid \u539f\u751f\u63d0\u4f9b\u4e86\u5df2\u7ecf\u5c01\u88c5\u597d\u7684\u4e00\u4e2a\u7c7b - `BaseObservable`\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u76d1\u542c\u5668\u7684\u6ce8\u518c\u673a\u5236\u3002\n\n\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u7ee7\u627f `BaseObservable`\u3002\n\n```java\npublic class ObservableUser extends BaseObservable {\n    private String firstName;\n    private String lastName;\n\n    @Bindable\n    public String getFirstName() {\n        return firstName;\n    }\n\n    @Bindable\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setFirstName(String firstName) {\n        this.firstName = firstName;\n        notifyPropertyChanged(BR.firstName);\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n        notifyPropertyChanged(BR.lastName);\n    }\n}\n```\n\n`BR` \u662f\u7f16\u8bd1\u9636\u6bb5\u751f\u6210\u7684\u4e00\u4e2a\u7c7b\uff0c\u529f\u80fd\u4e0e `R.java` \u7c7b\u4f3c\uff0c\u7528 `@Bindable` \u6807\u8bb0\u8fc7 `getter` \u65b9\u6cd5\u4f1a\u5728 `BR` \u4e2d\u751f\u6210\u4e00\u4e2a *entry*\u3002\n\n\u901a\u8fc7\u4ee3\u7801\u53ef\u4ee5\u770b\u51fa\uff0c\u5f53\u6570\u636e\u53d1\u751f\u53d8\u5316\u65f6\u8fd8\u662f\u9700\u8981\u624b\u52a8\u53d1\u51fa\u901a\u77e5\u3002 \u901a\u8fc7\u8c03\u7528 `notifyPropertyChanged(BR.firstName)` \u53ef\u4ee5\u901a\u77e5\u7cfb\u7edf `BR.firstName` \u8fd9\u4e2a `entry` \u7684\u6570\u636e\u5df2\u7ecf\u53d1\u751f\u53d8\u5316\uff0c\u9700\u8981\u66f4\u65b0 UI\u3002\n\n\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u6709\u4e00\u79cd\u66f4\u7ec6\u7c92\u5ea6\u7684\u7ed1\u5b9a\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5177\u4f53\u5230\u6210\u5458\u53d8\u91cf\uff0c\u8fd9\u79cd\u65b9\u5f0f\u65e0\u9700\u7ee7\u627f `BaseObservable`\uff0c\u4e00\u4e2a\u7b80\u5355\u7684 **POJO** \u5c31\u53ef\u4ee5\u5b9e\u73b0\u3002\n\n```java\npublic class PlainUser {\n    public final ObservableField<String> firstName = new ObservableField<>();\n    public final ObservableField<String> lastName = new ObservableField<>();\n    public final ObservableInt age = new ObservableInt();\n}\n```\n\n\u7cfb\u7edf\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u6240\u6709\u7684 **primitive type** \u6240\u5bf9\u5e94\u7684 **Observable**\u7c7b\uff0c\u4f8b\u5982 `ObservableInt`\u3001`ObservableFloat`\u3001`ObservableBoolean` \u7b49\u7b49\uff0c\u8fd8\u6709\u4e00\u4e2a `ObservableField` \u5bf9\u5e94\u7740 **reference type**\u3002\n\n\u5269\u4e0b\u7684\u6570\u636e\u7ed1\u5b9a\u4e0e\u524d\u9762\u4ecb\u7ecd\u7684\u65b9\u5f0f\u4e00\u6837\uff0c\u5177\u4f53\u53ef\u53c2\u8003[ObservableActivity](app/src/main/java/com/liangfeizc/databinding/sample/observable/ObservableActivity.java)\u3002\n\n## \u5e26 ID \u7684 View\n\n**Data Binding** \u6709\u6548\u964d\u4f4e\u4e86\u4ee3\u7801\u7684\u5197\u4f59\u6027\uff0c\u751a\u81f3\u5b8c\u5168\u6ca1\u6709\u5fc5\u8981\u518d\u53bb\u83b7\u53d6\u4e00\u4e2a View \u5b9e\u4f8b\uff0c\u4f46\u662f\u60c5\u51b5\u4e0d\u662f\u7edd\u5bf9\u7684\uff0c\u4e07\u4e00\u6211\u4eec\u771f\u7684\u5c31\u9700\u8981\u4e86\u5462\uff1f\u4e0d\u7528\u62c5\u5fc3\uff0c\u53ea\u8981\u7ed9 View \u5b9a\u4e49\u4e00\u4e2a ID\uff0c**Data Binding** \u5c31\u4f1a\u4e3a\u6211\u4eec\u751f\u6210\u4e00\u4e2a\u5bf9\u5e94\u7684 `final` \u53d8\u91cf\u3002\n\n```xml\n<TextView\n    android:id=\"@+id/firstName\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\" />\n```\n\n\u4e0a\u9762\u4ee3\u7801\u4e2d\u5b9a\u4e49\u4e86\u4e00\u4e2a ID \u4e3a *firstName** \u7684 `TextView`\uff0c\u90a3\u4e48\u5b83\u5bf9\u5e94\u7684\u53d8\u91cf\u5c31\u662f\n\n```java\npublic final TextView firstName;\n```\n\n\u5177\u4f53\u4ee3\u7801\u53ef\u53c2\u8003 [ViewWithIDsActivity.java](app/src/main/java/com/liangfeizc/databinding/sample/viewid/ViewWithIDsActivity.java)\n\n## ViewStubs\n\nxml \u4e2d\u7684 `ViewStub` \u7ecf\u8fc7 binding \u4e4b\u540e\u4f1a\u8f6c\u6362\u6210 `ViewStubProxy`, \u5177\u4f53\u4ee3\u7801\u53ef\u53c2\u8003 [ViewStubActivity.java](app/src/main/java/com/liangfeizc/databinding/sample/viewstub/ViewStubActivity.java)\n\n\u7b80\u5355\u7528\u4ee3\u7801\u8bf4\u660e\u4e00\u4e0b\uff0cxml \u6587\u4ef6\u4e0e\u4e4b\u524d\u7684\u4ee3\u7801\u4e00\u6837\uff0c\u6839\u8282\u70b9\u6539\u4e3a `layout`\uff0c\u5728 `LinearLayout` \u4e2d\u6dfb\u52a0\u4e00\u4e2a `ViewStub`\uff0c\u6dfb\u52a0 **ID**\u3002\n\n```xml\n<layout xmlns:android=\"http://schemas.android.com/apk/res/android\">\n    <LinearLayout\n        ...>\n        <ViewStub\n            android:id=\"@+id/view_stub\"\n            android:layout=\"@layout/view_stub\"\n            ... />\n    </LinearLayout>\n</layout>\n```\n\n\u5728 Java \u4ee3\u7801\u4e2d\u83b7\u53d6 `binding` \u5b9e\u4f8b\uff0c\u4e3a `ViewStubProy` \u6ce8\u518c `ViewStub.OnInflateListener` \u4e8b\u4ef6\uff1a\n\n```java\nbinding = DataBindingUtil.setContentView(this, R.layout.activity_view_stub);\nbinding.viewStub.setOnInflateListener(new ViewStub.OnInflateListener() {\n\t@Override\n\tpublic void onInflate(ViewStub stub, View inflated) {\n\t\tViewStubBinding binding = DataBindingUtil.bind(inflated);\n\t\tUser user = new User(\"fee\", \"lang\");\n\t\tbinding.setUser(user);\n\t}\n});\n```\n\n## Dynamic Variables\n\n\u5b8c\u6574\u4ee3\u7801\u53ef\u4ee5\u53c2\u8003 [dynamic](app/src/main/java/com/liangfeizc/databinding/sample/dynamic)\n\n\u4ee5 `RecyclerView` \u4e3a\u4f8b\uff0c`Adapter` \u7684 **DataBinding** \u9700\u8981\u52a8\u6001\u751f\u6210\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5728 `onCreateViewHolder` \u7684\u65f6\u5019\u521b\u5efa\u8fd9\u4e2a **DataBinding**\uff0c\u7136\u540e\u5728 `onBindViewHolder` \u4e2d\u83b7\u53d6\u8fd9\u4e2a **DataBinding**\u3002\n\n```java\npublic static class BindingHolder extends RecyclerView.ViewHolder {\n    private ViewDataBinding binding;\n\n    public BindingHolder(View itemView) {\n        super(itemView);\n    }\n\n    public ViewDataBinding getBinding() {\n        return binding;\n    }\n\n    public void setBinding(ViewDataBinding binding) {\n        this.binding = binding;\n    }\n}\n\n@Override\npublic BindingHolder onCreateViewHolder(ViewGroup viewGroup, int i) {\n    ViewDataBinding binding = DataBindingUtil.inflate(\n            LayoutInflater.from(viewGroup.getContext()),\n            R.layout.list_item,\n            viewGroup,\n            false);\n    BindingHolder holder = new BindingHolder(binding.getRoot());\n    holder.setBinding(binding);\n    return holder;\n}\n\n@Override\npublic void onBindViewHolder(BindingHolder holder, int position) {\n    User user = users.get(position);\n    holder.getBinding().setVariable(BR.user, user);\n    holder.getBinding().executePendingBindings();\n}\n```\n\n\u6ce8\u610f\u6b64\u5904 `DataBindingUtil` \u7684\u7528\u6cd5\uff1a\n\n```java\nViewDataBinding binding = DataBindingUtil.inflate(\n\tLayoutInflater.from(viewGroup.getContext()),\n\tR.layout.list_item,\n\tviewGroup,\n\tfalse);\n```\n\n---\n\n\u8fd8\u6709\u53e6\u5916\u4e00\u79cd\u6bd4\u8f83\u7b80\u6d01\u7684\u65b9\u5f0f\uff0c\u76f4\u63a5\u5728\u6784\u9020 Holder \u65f6\u628a `View` \u4e0e\u81ea\u52a8\u751f\u6210\u7684 `XXXBinding` \u8fdb\u884c\u7ed1\u5b9a\u3002\n\n```java\npublic class UserAdapter extends RecyclerView.Adapter<UserAdapter.UserHolder> {\n    private static final int USER_COUNT = 10;\n\n    @NonNull\n    private List<User> mUsers;\n\n    public UserAdapter() {\n        mUsers = new ArrayList<>(10);\n        for (int i = 0; i < USER_COUNT; i ++) {\n            User user = new User(RandomNames.nextFirstName(), RandomNames.nextLastName());\n            mUsers.add(user);\n        }\n    }\n\n    public static class UserHolder extends RecyclerView.ViewHolder {\n        private UserItemBinding mBinding;\n\n        public UserHolder(View itemView) {\n            super(itemView);\n            mBinding = DataBindingUtil.bind(itemView);\n        }\n\n        public void bind(@NonNull User user) {\n            mBinding.setUser(user);\n        }\n    }\n\n    @Override\n    public UserHolder onCreateViewHolder(ViewGroup viewGroup, int i) {\n        View itemView = LayoutInflater.from(viewGroup.getContext())\n                .inflate(R.layout.user_item, viewGroup, false);\n        return new UserHolder(itemView);\n    }\n\n    @Override\n    public void onBindViewHolder(UserHolder holder, int position) {\n        holder.bind(mUsers.get(position));\n    }\n\n    @Override\n    public int getItemCount() {\n        return mUsers.size();\n    }\n}\n```\n\n## Attribute setters\n\n\u6709\u4e86 **Data Binding**\uff0c\u5373\u4f7f\u5c5e\u6027\u6ca1\u6709\u5728 `declare-styleable` \u4e2d\u5b9a\u4e49\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7 xml \u8fdb\u884c\u8d4b\u503c\u64cd\u4f5c\u3002\n\u4e3a\u4e86\u6f14\u793a\u8fd9\u4e2a\u529f\u80fd\uff0c\u6211\u81ea\u5b9a\u4e49\u4e86\u4e00\u4e2a View - [NameCard](app/src/main/java/com/liangfeizc/databinding/view/NameCard.java)\uff0c\u5c5e\u6027\u8d44\u6e90 [R.styleable.NameCard](app/src/main/res/values/styles.xml#L8-L10) \u4e2d\u53ea\u5b9a\u4e49\u4e86\u4e00\u4e2a `age` \u5c5e\u6027\uff0c\u5176\u4e2d `firstName` \u548c `lastName` \u53ea\u6709\u5bf9\u5e94\u7684\u4e24\u4e2a `setter` \u65b9\u6cd5\u3002\n\n\u53ea\u8981\u6709 `setter` \u65b9\u6cd5\u5c31\u53ef\u4ee5\u50cf\u4e0b\u9762\u4ee3\u7801\u4e00\u6837\u8d4b\u503c\uff1a\n\n```xml\n<com.liangfeizc.databindingsamples.attributesetters.UserView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"wrap_content\"\n    android:paddingLeft=\"@dimen/largePadding\"\n    app:onClickListener=\"@{activity.clickListener}\"\n    app:firstName=\"@{@string/firstName}\"\n    app:lastName=\"@{@string/lastName}\"\n    app:age=\"27\" />\n```\n\n`onClickListener` \u4e5f\u662f\u540c\u6837\u9053\u7406\uff0c\u53ea\u4e0d\u8fc7\u6211\u4eec\u662f\u5728 `Activity` \u4e2d\u5b9a\u4e49\u4e86\u4e00\u4e2a `Listener`\u3002\n\n## \u8f6c\u6362\u5668 (Converters)\n\n> **\u975e\u5e38\u91cd\u8981**\n\n> \u4f7f\u7528 **Converter** \u4e00\u5b9a\u8981\u4fdd\u8bc1\u5b83\u4e0d\u4f1a\u5f71\u54cd\u5230\u5176\u4ed6\u7684\u5c5e\u6027\uff0c\u4f8b\u5982\u8fd9\u4e2a `@BindingConversion`- [convertColorToString](app/src/main/java/com/liangfeizc/databinding/sample/converter/ConversionsActivity.java#L50-L63) \u5c31\u4f1a\u5f71\u54cd\u5230[android:visibility](app/src/main/res/layout/activity_basic.xml#L76), \u56e0\u4e3a\u4ed6\u4eec\u90fd\u662f\u90fd\u7b26\u5408\u4ece int \u5230 int \u7684\u8f6c\u6362\u3002\n\n\n\u5728 xml \u4e2d\u4e3a\u5c5e\u6027\u8d4b\u503c\u65f6\uff0c\u5982\u679c\u53d8\u91cf\u7684\u7c7b\u578b\u4e0e\u5c5e\u6027\u4e0d\u4e00\u81f4\uff0c\u901a\u8fc7 **DataBinding** \u53ef\u4ee5\u8fdb\u884c\u8f6c\u6362\u3002\n\n\u4f8b\u5982\uff0c\u4e0b\u9762\u4ee3\u7801\u4e2d\u5982\u679c\u8981\u4e3a\u5c5e\u6027 `android:background` \u8d4b\u503c\u4e00\u4e2a `int` \u578b\u7684 color \u53d8\u91cf\uff1a\n\n```xml\n<View\n    android:background=\"@{isError.get() ? @color/red : @color/white}\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"wrap_content\"\n    app:layout_height=\"@{height}\" />\n```\n\n\u53ea\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u6807\u8bb0\u4e86 `@BindingConversion` \u7684\u9759\u6001\u65b9\u6cd5\u5373\u53ef\uff08*\u65b9\u6cd5\u7684\u5b9a\u4e49\u4f4d\u7f6e\u53ef\u4ee5\u968f\u610f*\uff09\uff1a\n\n```java\n@BindingConversion\npublic static ColorDrawable convertColorToDrawable(int color) {\n    return new ColorDrawable(color);\n}\n```\n\n\u5177\u4f53\u4ee3\u7801\u53ef\u53c2\u8003 [ConversionsActivity.java](app/src/main/java/com/liangfeizc/databinding/sample/converter/ConversionsActivity.java)\u3002\n\n## include\n\n\u7528\u6cd5\u53ef\u4ee5\u53c2\u8003\u4ee3\u7801 [IncludeActivity.java](/app/src/main/java/com/liangfeizc/databinding/sample/include/IncludeActivity.java)\n\n\u5982\u679c\u5728\u975e\u6839\u8282\u70b9\u7684 ViewGroup \u4e2d\u4f7f\u7528 `include` \u4f1a\u5bfc\u81f4 crash\uff0c\u5df2\u7ecf\u5728 StackOverflow \u4e0a\u63d0\u4e86\u4e00\u4e2a\u95ee\u9898[Android Data Binding makes app crash when using include tag in a non-root ViewGroup](http://stackoverflow.com/questions/30887906/android-data-binding-makes-app-crash-when-using-include-tag-in-a-non-root-viewgr)\uff0c\u76f4\u3055\u308c\u305f\u305d\u3046\u3067\u3059\u3051\u3069\u3002\n"
 },
 {
  "repo": "react-native-google-signin/google-signin",
  "language": "Java",
  "readme_contents": "![React Native Google Sign In](img/header.png)\n\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/@react-native-google-signin/google-signin\"><img src=\"https://badge.fury.io/js/@react-native-google-signin%2Fgoogle-signin.svg\" alt=\"NPM Version\"></a>\n</p>\n\n### \ud83d\udea7\ud83d\udea7 Maintenance notice \ud83d\udea7\ud83d\udea7\n\nSee this [issue](https://github.com/react-native-google-signin/google-signin/issues/942)\n\n## Features\n\n- Support all 3 types of authentication methods (standard, with server-side validation or with offline access (aka server side access))\n- Promise-based API consistent between Android and iOS\n- Typings for TypeScript and Flow\n- Native sign in buttons\n\n## Requirements\n\n- RN >= 0.60\n\n## Project setup and initialization\n\n`yarn add @react-native-google-signin/google-signin`\n\nThen follow the [Android guide](docs/android-guide.md) and [iOS guide](docs/ios-guide.md)\n\n## Expo installation\n\n> This package cannot be used in the \"Expo Go\" app because [it requires custom native code](https://docs.expo.io/workflow/customizing/). _However, you can add custom native code to expo by following the guide below._\n\n- First install the package with yarn, npm, or [`expo install`](https://docs.expo.io/workflow/expo-cli/#expo-install).\n\n```sh\nexpo install @react-native-google-signin/google-signin\n```\n\nAfter installing this npm package, add the [config plugin](https://docs.expo.io/guides/config-plugins/) to the [`plugins`](https://docs.expo.io/versions/latest/config/app/#plugins) array of your `app.json` or `app.config.js`:\n\n```json\n{\n  \"expo\": {\n    \"android\": {\n      \"googleServicesFile\": \"./google-services.json\"\n    },\n    \"ios\": {\n      \"googleServicesFile\": \"./GoogleService-Info.plist\"\n    },\n    \"plugins\": [\"@react-native-google-signin/google-signin\"]\n  }\n}\n```\n\nNext, rebuild your app as described in the [\"Adding custom native code\"](https://docs.expo.io/workflow/customizing/) guide.\n\n## Public API\n\n### 1. GoogleSignin\n\n```js\nimport {\n  GoogleSignin,\n  GoogleSigninButton,\n  statusCodes,\n} from '@react-native-google-signin/google-signin';\n```\n\n#### `configure(options)`\n\nIt is mandatory to call this method before attempting to call `signIn()` and `signInSilently()`. This method is sync meaning you can call `signIn` / `signInSilently` right after it. In typical scenarios, `configure` needs to be called only once, after your app starts. In the native layer, this is a synchronous call. All parameters are optional.\n\nExample usage with default options: you get user email and basic profile info.\n\n```js\nimport { GoogleSignin } from '@react-native-google-signin/google-signin';\n\nGoogleSignin.configure();\n```\n\nAn example with all options enumerated:\n\n```js\nGoogleSignin.configure({\n  scopes: ['https://www.googleapis.com/auth/drive.readonly'], // what API you want to access on behalf of the user, default is email and profile\n  webClientId: '<FROM DEVELOPER CONSOLE>', // client ID of type WEB for your server (needed to verify user ID and offline access)\n  offlineAccess: true, // if you want to access Google API on behalf of the user FROM YOUR SERVER\n  hostedDomain: '', // specifies a hosted domain restriction\n  forceCodeForRefreshToken: true, // [Android] related to `serverAuthCode`, read the docs link below *.\n  accountName: '', // [Android] specifies an account name on the device that should be used\n  iosClientId: '<FROM DEVELOPER CONSOLE>', // [iOS] if you want to specify the client ID of type iOS (otherwise, it is taken from GoogleService-Info.plist)\n  googleServicePlistPath: '', // [iOS] if you renamed your GoogleService-Info file, new name here, e.g. GoogleService-Info-Staging\n  openIdRealm: '', // [iOS] The OpenID2 realm of the home web server. This allows Google to include the user's OpenID Identifier in the OpenID Connect ID token.\n  profileImageSize: 120, // [iOS] The desired height (and width) of the profile image. Defaults to 120px\n});\n```\n\n\\* [forceCodeForRefreshToken docs](https://developers.google.com/android/reference/com/google/android/gms/auth/api/signin/GoogleSignInOptions.Builder#public-googlesigninoptions.builder-requestserverauthcode-string-serverclientid,-boolean-forcecodeforrefreshtoken)\n\n#### `signIn(options: { loginHint?: string })`\n\nPrompts a modal to let the user sign in into your application. Resolved promise returns an [`userInfo` object](#3-userinfo). Rejects with error otherwise.\n\nOptions: an object which contains a single key:\n\n`loginHint`: [iOS-only] The user's ID, or email address, to be prefilled in the authentication UI if possible. [See docs here](<https://developers.google.com/identity/sign-in/ios/reference/Classes/GIDSignIn#/c:objc(cs)GIDSignIn(im)signInWithConfiguration:presentingViewController:hint:callback:>)\n\n```js\n// import statusCodes along with GoogleSignin\nimport { GoogleSignin, statusCodes } from '@react-native-google-signin/google-signin';\n\n// Somewhere in your code\nsignIn = async () => {\n  try {\n    await GoogleSignin.hasPlayServices();\n    const userInfo = await GoogleSignin.signIn();\n    this.setState({ userInfo });\n  } catch (error) {\n    if (error.code === statusCodes.SIGN_IN_CANCELLED) {\n      // user cancelled the login flow\n    } else if (error.code === statusCodes.IN_PROGRESS) {\n      // operation (e.g. sign in) is in progress already\n    } else if (error.code === statusCodes.PLAY_SERVICES_NOT_AVAILABLE) {\n      // play services not available or outdated\n    } else {\n      // some other error happened\n    }\n  }\n};\n```\n\n#### `addScopes(options: { scopes: Array<string> })`\n\nThis is an iOS-only method (calls `getCurrentUser()` on Android) that resolves with `null` or `userInfo` object.\n\nAs of version 8 of this package, you may not need this call: you can supply required scopes to the `configure` call.\n\nIf you want access to more scopes later, use this call.\n\nExample:\n\n```js\nconst user = await GoogleSignin.addScopes({\n  scopes: ['https://www.googleapis.com/auth/user.gender.read'],\n});\n```\n\n#### `signInSilently()`\n\nMay be called eg. in the `componentDidMount` of your main component. This method returns the [current user](#3-userinfo) and rejects with an error otherwise.\n\nTo see how to handle errors read [`signIn()` method](#signin)\n\n```js\ngetCurrentUserInfo = async () => {\n  try {\n    const userInfo = await GoogleSignin.signInSilently();\n    this.setState({ userInfo });\n  } catch (error) {\n    if (error.code === statusCodes.SIGN_IN_REQUIRED) {\n      // user has not signed in yet\n    } else {\n      // some other error\n    }\n  }\n};\n```\n\n#### `isSignedIn()`\n\nThis method may be used to find out whether some user is currently signed in. It returns a promise which resolves with a boolean value (it never rejects). In the native layer, this is a synchronous call. This means that it will resolve even when the device is offline. Note that it may happen that `isSignedIn()` resolves to true and calling `signInSilently()` rejects with an error (eg. due to a network issue).\n\n```js\nisSignedIn = async () => {\n  const isSignedIn = await GoogleSignin.isSignedIn();\n  this.setState({ isLoginScreenPresented: !isSignedIn });\n};\n```\n\n#### `getCurrentUser()`\n\nThis method resolves with `null` or `userInfo` object. The call never rejects and in the native layer, this is a synchronous call. Note that on Android, `accessToken` is always `null` in the response.\n\n```js\ngetCurrentUser = async () => {\n  const currentUser = await GoogleSignin.getCurrentUser();\n  this.setState({ currentUser });\n};\n```\n\n#### `clearCachedAccessToken(accessTokenString)`\n\nThis method only has an effect on Android. You may run into a 401 Unauthorized error when a token is invalid. Call this method to remove the token from local cache and then call `getTokens()` to get fresh tokens. Calling this method on iOS does nothing and always resolves. This is because on iOS, `getTokens()` always returns valid tokens, refreshing them first if they have expired or are about to expire (see [docs](https://developers.google.com/identity/sign-in/ios/reference/Classes/GIDAuthentication#-dowithfreshtokens:)).\n\n#### `getTokens()`\n\nResolves with an object containing `{ idToken: string, accessToken: string, }` or rejects with an error. Note that using `accessToken` for identity assertion on your backend server is [discouraged](https://developers.google.com/identity/sign-in/android/migration-guide).\n\n#### `signOut()`\n\nSigns out the current user.\n\n```js\nsignOut = async () => {\n  try {\n    await GoogleSignin.signOut();\n    this.setState({ user: null }); // Remember to remove the user from your app's state as well\n  } catch (error) {\n    console.error(error);\n  }\n};\n```\n\n#### `revokeAccess()`\n\nRemoves your application from the user authorized applications. Read more about it [here](https://developers.google.com/identity/sign-in/ios/disconnect#objective-c) and [here](<https://developers.google.com/android/reference/com/google/android/gms/auth/api/signin/GoogleSignInClient#revokeAccess()>).\n\n```js\nrevokeAccess = async () => {\n  try {\n    await GoogleSignin.revokeAccess();\n    // Google Account disconnected from your app.\n    // Perform clean-up actions, such as deleting data associated with the disconnected account.\n  } catch (error) {\n    console.error(error);\n  }\n};\n```\n\n#### `hasPlayServices(options)`\n\nChecks if device has Google Play Services installed. Always resolves to true on iOS.\n\nPresence of up-to-date Google Play Services is required to show the sign in modal, but it is _not_ required to perform calls to `configure` and `signInSilently`. Therefore, we recommend to call `hasPlayServices` directly before `signIn`.\n\n```js\ntry {\n  await GoogleSignin.hasPlayServices({ showPlayServicesUpdateDialog: true });\n  // google services are available\n} catch (err) {\n  console.error('play services are not available');\n}\n```\n\n`hasPlayServices` accepts one parameter, an object which contains a single key: `showPlayServicesUpdateDialog` (defaults to `true`). When `showPlayServicesUpdateDialog` is set to true the library will prompt the user to take action to solve the issue, as seen in the figure below.\n\nYou may also use this call at any time to find out if Google Play Services are available and react to the result as necessary.\n\n[![prompt install](img/prompt-install.png)](#prompt-install)\n\n#### `statusCodes`\n\nThese are useful when determining which kind of error has occured during sign in process. Import `statusCodes` along with `GoogleSignIn`. Under the hood these constants are derived from native GoogleSignIn error codes and are platform specific. Always prefer to compare `error.code` to `statusCodes.SIGN_IN_CANCELLED` or `statusCodes.IN_PROGRESS` and not relying on raw value of the `error.code`.\n\n| Name                          | Description                                                                                                                                                                                                                                                                                                                                                               |\n| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `SIGN_IN_CANCELLED`           | When user cancels the sign in flow                                                                                                                                                                                                                                                                                                                                        |\n| `IN_PROGRESS`                 | Trying to invoke another operation (eg. `signInSilently`) when previous one has not yet finished. If you call eg. `signInSilently` twice, 2 calls to `signInSilently` in the native module will be done. The promise from the first call to `signInSilently` will be rejected with this error, and the second will resolve / reject with the result of the native module. |\n| `SIGN_IN_REQUIRED`            | Useful for use with `signInSilently()` - no user has signed in yet                                                                                                                                                                                                                                                                                                        |\n| `PLAY_SERVICES_NOT_AVAILABLE` | Play services are not available or outdated, this can only happen on Android                                                                                                                                                                                                                                                                                              |\n\n[Example how to use `statusCodes`](#signin).\n\n### 2. GoogleSigninButton\n\n![signin button](img/signin-button.png)\n\n```js\nimport { GoogleSignin, GoogleSigninButton } from '@react-native-google-signin/google-signin';\n\n<GoogleSigninButton\n  style={{ width: 192, height: 48 }}\n  size={GoogleSigninButton.Size.Wide}\n  color={GoogleSigninButton.Color.Dark}\n  onPress={this._signIn}\n  disabled={this.state.isSigninInProgress}\n/>;\n```\n\n#### Props\n\n##### `size`\n\nPossible values:\n\n- Size.Icon: display only Google icon. Recommended size of 48 x 48.\n- Size.Standard: icon with 'Sign in'. Recommended size of 230 x 48.\n- Size.Wide: icon with 'Sign in with Google'. Recommended size of 312 x 48.\n\nDefault: `Size.Standard`. Given the `size` prop you pass, we'll automatically apply the recommended size, but you can override it by passing the style prop as in `style={{ width, height }}`.\n\n##### `color`\n\nPossible values:\n\n- Color.Dark: apply a blue background\n- Color.Light: apply a light gray background\n\n##### `disabled`\n\nBoolean. If true, all interactions for the button are disabled.\n\n##### `onPress`\n\nHandler to be called when the user taps the button\n\n##### [Inherited `View` props...](https://facebook.github.io/react-native/docs/view#props)\n\n### 3. `userInfo`\n\nExample `userInfo` which is returned after successful sign in.\n\n```\n{\n  idToken: string,\n  serverAuthCode: string,\n  scopes: Array<string>, // on iOS this is empty array if no additional scopes are defined\n  user: {\n    email: string,\n    id: string,\n    givenName: string,\n    familyName: string,\n    photo: string, // url\n    name: string // full name\n  }\n}\n```\n\n## Want to contribute?\n\nCheck out the [contributor guide](docs/CONTRIBUTING.md)!\n\n## Notes\n\nCalling the methods exposed by this package may involve remote network calls and you should thus take into account that such calls may take a long time to complete (eg. in case of poor network connection).\n\n**idToken Note**: idToken is not null only if you specify a valid `webClientId`. `webClientId` corresponds to your server clientID on the developers console. It **HAS TO BE** of type **WEB**\n\nRead [iOS documentation](https://developers.google.com/identity/sign-in/ios/backend-auth) and [Android documentation](https://developers.google.com/identity/sign-in/android/backend-auth) for more information\n\n**serverAuthCode Note**: serverAuthCode is not null only if you specify a valid `webClientId` and set `offlineAccess` to true. once you get the auth code, you can send it to your backend server and exchange the code for an access token. Only with this freshly acquired token can you access user data.\n\nRead [iOS documentation](https://developers.google.com/identity/sign-in/ios/offline-access) and [Android documentation](https://developers.google.com/identity/sign-in/android/offline-access) for more information.\n\n## Additional scopes\n\nThe default requested scopes are `email` and `profile`.\n\nIf you want to manage other data from your application (for example access user agenda or upload a file to drive) you need to request additional permissions. This can be accomplished by adding the necessary scopes when configuring the GoogleSignin instance.\n\nPlease visit https://developers.google.com/identity/protocols/googlescopes or https://developers.google.com/oauthplayground/ for a list of available scopes.\n\n## Troubleshooting\n\nPlease see the troubleshooting section in the [Android guide](docs/android-guide.md) and [iOS guide](docs/ios-guide.md).\n\n## Licence\n\n(MIT)\n"
 },
 {
  "repo": "lbryio/lbry-android",
  "language": "Java",
  "readme_contents": "# LBRY Android\n[![pipeline status](https://ci.lbry.tech/lbry/lbry-android/badges/master/pipeline.svg)](https://ci.lbry.tech/lbry/lbry-android/commits/master)\n[![GitHub license](https://img.shields.io/github/license/lbryio/lbry-android)](https://github.com/lbryio/lbry-android/blob/master/LICENSE)\n\nAn Android browser and wallet for the [LBRY](https://lbry.com) network.\n\n\n<img src=\"https://spee.ch/@lbry:3f/android-08-homepage.gif\" alt=\"LBRY Android GIF\" width=\"384px\" />\n\n\n## Installation\nThe minimum supported Android version is 5.0 Lollipop. There are two ways to install:\n\n1. Via the Google Play Store. Anyone can join the [open beta](https://play.google.com/apps/testing/io.lbry.browser) in order to install the app from the Play Store.\n1. Direct APK install available at [http://build.lbry.io/android/latest.apk](http://build.lbry.io/android/latest.apk). You will need to enable installation from third-party sources on your device in order to install from this source.\n\n## Usage\nThe app can be launched by opening **LBRY** from the device's app drawer or via the shortcut on the home screen if that was created upon installation.\n\n## Running from Source\nClone the repository and open the project in Android Studio. Android Studio will automatically run the initial build process.\n\nCreate file 'twitter.properties' in app/ folder with the following content:\n\n```\ntwitterConsumerKey=XXXXXX\n\ntwitterConsumerSecret=XXXXXX\n```\n\nClick the Sync button and when process finishes, the Run button to launch the app on your simulator or connected debugging device after the build process is complete.\n\n## Contributing\nContributions to this project are welcome, encouraged, and compensated. For more details, see https://lbry.io/faq/contributing\n\n## License\nThis project is MIT licensed. For the full license, see [LICENSE](LICENSE).\n\n## Security\nWe take security seriously. Please contact security@lbry.com regarding any security issues. Our PGP key is [here](https://lbry.com/faq/pgp-key) if you need it.\n\n## Contact\nThe primary contact for this project is [@akinwale](https://github.com/akinwale) (akinwale@lbry.com)\n"
 },
 {
  "repo": "iyegoroff/react-native-image-filter-kit",
  "language": "Java",
  "readme_contents": "# react-native-image-filter-kit\n\n[![npm version](https://badge.fury.io/js/react-native-image-filter-kit.svg)](https://badge.fury.io/js/react-native-image-filter-kit)\n[![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg)](https://github.com/standard/standard)\n[![Dependency Status](https://david-dm.org/iyegoroff/react-native-image-filter-kit.svg)](https://david-dm.org/iyegoroff/react-native-image-filter-kit)\n[![devDependencies Status](https://david-dm.org/iyegoroff/react-native-image-filter-kit/dev-status.svg)](https://david-dm.org/iyegoroff/react-native-image-filter-kit?type=dev)\n[![typings included](https://img.shields.io/badge/typings-included-brightgreen.svg?t=1495378566925)](src/typings/index.d.ts)\n[![npm](https://img.shields.io/npm/l/express.svg)](https://www.npmjs.com/package/react-native-image-filter-kit)\n\n<!-- [![CircleCI](https://circleci.com/gh/iyegoroff/react-native-image-filter-kit.svg?style=svg)](https://circleci.com/gh/iyegoroff/react-native-image-filter-kit) -->\n\nVarious image filters for iOS & Android.\n\n## Status\n\n- iOS & Android:\n  - filter components work as combinable wrappers for standard `Image` and `ImageBackground` components\n  - resulting images are being cached in memory and can be\n    [extracted into temporary files](docs/image_extraction.md) of original resolution\n  - [additional filters](https://github.com/iyegoroff/react-native-image-filter-kit/tree/master/examples) can be developed as separate modules\n- react-native:\n\n  - supported versions:\n\n    | react-native     | min Android SDK | min iOS version |\n    | ---------------- | --------------- | --------------- |\n    | >=0.64.0         | 21              | 9.0             |\n    | >=0.57.1 <0.64.0 | 17              | 9.0             |\n\n## Installation\n\n<table>\n<td>\n<details style=\"border: 1px solid; border-radius: 5px; padding: 5px\">\n  <summary>with react-native \"<strong>&gt;=0.64.0</strong>\"</summary>\n\n### 1. Install latest version from npm\n\n`$ npm i react-native-image-filter-kit -S`\n\n### 2. Install pods\n\n`$ cd ios && pod install && cd ..`\n  \n### 3. Add ProGuard rules\n  \n  - Add ProGuard rules to `android/app/proguard-rules.pro`:\n\n    ```\n    # react-native-image-filter-kit\n    -keep class com.facebook.react.views.image.** { *; }\n    -keep class com.facebook.drawee.** { *; }\n    ```\n    \n    Thanks @NikitaDudin for [pointing this out](/../../issues/89)!\n  \n\n</details>\n</td>\n</table>\n<table>\n<td>\n<details style=\"border: 1px solid; border-radius: 5px; padding: 5px\">\n  <summary>with react-native \"<strong>&gt;=0.61.0 &lt;0.64.0</strong>\"</summary>\n\n### 1. Install v0.7.3 from npm\n\n`$ npm i react-native-image-filter-kit@0.7.3 -S`\n\n### 2. Install pods\n\n`$ cd ios && pod install && cd ..`\n\n### 3. Enable renderscript\n\n- Modify `android/build.gradle`:\n\n  ```diff\n  buildscript {\n    ext {\n  -   buildToolsVersion = \"28.0.3\"\n  -   minSdkVersion = 16\n  -   compileSdkVersion = 28\n  -   targetSdkVersion = 28\n  +   buildToolsVersion = \"29.0.3\"\n  +   minSdkVersion = 17\n  +   compileSdkVersion = 29\n  +   targetSdkVersion = 29\n  +   renderscriptVersion = 21\n  ...\n\n    dependencies {\n  -   classpath(\"com.android.tools.build:gradle:3.4.2\")\n  +   classpath(\"com.android.tools.build:gradle:3.6.0\")\n  ```\n\n- Modify `android/app/build.gradle`:\n\n  ```diff\n  android {\n    compileSdkVersion rootProject.ext.compileSdkVersion\n  + buildToolsVersion rootProject.ext.buildToolsVersion\n\n    ...\n\n    defaultConfig {\n      ...\n  +   renderscriptTargetApi rootProject.ext.renderscriptVersion\n  +   renderscriptSupportModeEnabled true\n    }\n  ```\n\n- Modify `android/gradle/wrapper/gradle-wrapper.properties`:\n\n  ```diff\n  - distributionUrl=https\\://services.gradle.org/distributions/gradle-5.5-all.zip\n  + distributionUrl=https\\://services.gradle.org/distributions/gradle-6.2-all.zip\n  ```\n\n</details>\n</td>\n</table>\n<table>\n<td>\n<details style=\"border: 1px solid; border-radius: 5px; padding: 5px\">\n  <summary>with react-native \"<strong>&gt;=0.60.0 &lt;0.61.0</strong>\"</summary>\n\n### 1. Install v0.5.18 from npm\n\n`$ npm i react-native-image-filter-kit@0.5.18 -S`\n\n### 2. Install pods\n\n`$ cd ios && pod install && cd ..`\n\n### 3. Enable renderscript\n\n- Modify `android/build.gradle`:\n\n  ```diff\n  buildscript {\n    ext {\n  -   buildToolsVersion = \"28.0.3\"\n  -   minSdkVersion = 16\n  -   compileSdkVersion = 28\n  -   targetSdkVersion = 28\n  +   buildToolsVersion = \"29.0.3\"\n  +   minSdkVersion = 17\n  +   compileSdkVersion = 29\n  +   targetSdkVersion = 29\n  ...\n\n    dependencies {\n  -   classpath(\"com.android.tools.build:gradle:3.4.2\")\n  +   classpath(\"com.android.tools.build:gradle:3.6.0\")\n  ...\n\n  allprojects {\n    repositories {\n      ...\n  +   maven { url 'https://jitpack.io' }\n    }\n  }\n  ```\n\n- Modify `android/app/build.gradle`:\n\n  ```diff\n  android {\n    compileSdkVersion rootProject.ext.compileSdkVersion\n  + buildToolsVersion rootProject.ext.buildToolsVersion\n\n    ...\n\n    defaultConfig {\n      ...\n  +   renderscriptTargetApi 21\n  +   renderscriptSupportModeEnabled true\n    }\n  ```\n\n- Modify `android/gradle/wrapper/gradle-wrapper.properties`:\n\n  ```diff\n  - distributionUrl=https\\://services.gradle.org/distributions/gradle-5.5-all.zip\n  + distributionUrl=https\\://services.gradle.org/distributions/gradle-6.2-all.zip\n  ```\n\n</details>\n</td>\n</table>\n<table>\n<td>\n<details style=\"border: 1px solid; border-radius: 5px; padding: 5px\">\n  <summary>with react-native \"<strong>&gt;=0.58.0 &lt;0.60.0</strong>\"</summary>\n\n### 1. Install v0.4.14 from npm\n\n`$ npm i react-native-image-filter-kit@0.4.14 -S`\n\n### 2-a. Link native modules\n\n`$ react-native link react-native-image-filter-kit`\n\n### 2-b. Install pods\n\nIf you use Cocoapods add the following line to your Podfile:\n\n```sh\npod 'React', :path => '../node_modules/react-native'\npod 'RNImageFilterKit', :path => '../node_modules/react-native-image-filter-kit'\n```\n\n`$ cd ios && pod install && cd ..`\n\n### 2-c. Manual installation\n\nInstall manually if `react-native link` failed - [link](docs/manual_installation.md)\n\n### 3. Enable renderscript\n\n- Modify `android/build.gradle`:\n  ```diff\n  buildscript {\n    ext {\n      ...\n  -   minSdkVersion = 16\n  +   minSdkVersion = 17\n  ```\n- Modify `android/app/build.gradle`:\n\n  ```diff\n  defaultConfig {\n    ...\n  + renderscriptTargetApi 21\n  + renderscriptSupportModeEnabled true\n  }\n  ```\n\n</details>\n</td>\n</table>\n<table>\n<td>\n<details style=\"border: 1px solid; border-radius: 5px; padding: 5px\">\n  <summary>with react-native \"<strong>&gt;=0.57.1 &lt;0.58.0</strong>\"</summary>\n\n### 1. Install v0.3.9 from npm\n\n`$ npm i react-native-image-filter-kit@0.3.9 -S`\n\n### 2-a. Link native modules\n\n`$ react-native link react-native-image-filter-kit`\n\n### 2-b. Install pods\n\nIf you use Cocoapods add the following line to your Podfile:\n\n```sh\npod 'React', :path => '../node_modules/react-native'\npod 'RNImageFilterKit', :path => '../node_modules/react-native-image-filter-kit'\n```\n\n`$ cd ios && pod install && cd ..`\n\n### 2-c. Manual installation\n\nInstall manually if `react-native link` failed - [link](docs/manual_installation.md)\n\n### 3. Final step\n\nOpen `android/build.gradle` and change `minSdkVersion` to 17.\n\n</details>\n</td>\n</table>\n\n## Scope\n\nThe purpose of this module is to support most of the native image filters on each platform and to provide a common interface for these filters. If the filter exists only on one platform, then its counterpart will be implemented using `renderscript` on Android and `cikernel` on iOS. If you need only [color matrix](docs/color_matrix_filters.md) filters - better use a [lightweight predecessor](https://github.com/iyegoroff/react-native-color-matrix-image-filters) of this module.\n\n## Example\n\n```javascript\nimport { Image } from 'react-native'\nimport {\n  SoftLightBlend,\n  Emboss,\n  Earlybird,\n  Invert,\n  RadialGradient\n} from 'react-native-image-filter-kit'\n\nconst result = (\n  <Earlybird\n    image={\n      <SoftLightBlend\n        resizeCanvasTo={'dstImage'}\n        dstTransform={{\n          scale: 'CONTAIN'\n        }}\n        dstImage={\n          <Emboss\n            image={\n              <Image\n                style={{ width: 320, height: 320 }}\n                source={require('./parrot.png')}\n                resizeMode={'contain'}\n              />\n            }\n          />\n        }\n        srcTransform={{\n          anchor: { x: 0.5, y: 1 },\n          translate: { x: 0.5, y: 1 }\n        }}\n        srcImage={\n          <Invert\n            image={\n              <RadialGradient\n                colors={['rgba(0, 0, 255, 1)', '#00ff00', 'red']}\n                stops={[0.25, 0.75, 1]}\n                center={{ x: '50w', y: '100h' }}\n              />\n            }\n          />\n        }\n      />\n    }\n  />\n)\n```\n\n<table>\n  <tr>\n    <th>original image</th>\n    <th>result</th>\n  </tr>\n  <tr>\n    <th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/parrot.png\" align=\"left\" width=\"300\"></th>\n    <th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/earlybird.png\" align=\"left\" width=\"300\"></th>\n  </tr>\n</table>\n&nbsp;\n<details>\n  <summary>filter steps</summary>\n  <table>\n  <tr>\n    <th>\n      <table>\n        <tr><th>original image</th></tr>\n        <tr><th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/parrot.png\" align=\"left\" width=\"170\"></th></tr>\n      </table>\n    </th>\n    <th>\n      <table>\n        <tr><th>Emboss</th></tr>\n        <tr><th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/emboss.png\" align=\"left\" width=\"170\"></th></tr>\n      </table>\n    </th>\n    <th rowspan=\"2\">\n      <table>\n        <tr><th>SoftLightBlend</th></tr>\n        <tr><th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/soft_light_blend.png\" align=\"left\" width=\"170\"></th></tr>\n      </table>\n    </th>\n    <th rowspan=\"2\">\n      <table>\n        <tr><th>Earlybird</th></tr>\n        <tr><th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/earlybird.png\" align=\"left\" width=\"170\"></th></tr>\n      </table>\n    </th>\n  </tr>\n  <tr>\n    <td>\n      <table>\n        <tr><th>RadialGradient</th></tr>\n        <tr><th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/radial_gradient.png\" align=\"left\" width=\"170\"></th></tr>\n      </table>\n    </td>\n    <td>\n      <table>\n        <tr><th>Invert</th></tr>\n        <tr><th><img src=\"https://raw.githubusercontent.com/iyegoroff/react-native-image-filter-kit/master/img/invert.png\" align=\"left\" width=\"170\"></th></tr>\n      </table>\n    </td>\n  </tr>\n</table>\n\n</details>\n\n## Reference\n\n- [Types](docs/types.md)\n- [Functions](docs/functions.md)\n- [Image extraction](docs/image_extraction.md)\n- [Color matrix filters](docs/color_matrix_filters.md)\n- [Blur filters](docs/blur_filters.md)\n- [Convolve matrix filters](docs/convolve_matrix_filters.md)\n- [Generators](docs/generators.md)\n- [Composition filters](docs/composition_filters.md)\n- [Blend filters](docs/blend_filters.md)\n- [CSSGram filters](docs/cssgram_filters.md)\n- [Android-only filters](src/native-platform-filters/shapes.android.ts)\n- [iOS-only filters](src/native-platform-filters/shapes.ios.ts)\n\n## Caveats\n\n- `blurRadius` Image prop will not work in conjunction with this library, instead of it just use [BoxBlur](docs/blur_filters.md#BoxBlur) filter\n- When running on pre-Lollipop (SDK < 21) Android devices you may experience [TooManyBitmapsException](https://frescolib.org/javadoc/reference/com/facebook/imagepipeline/common/TooManyBitmapsException.html), which results in image is not being rendered (this can be logged with [onFilteringError](docs/types.md#ImageFilter) prop). It looks like this is a relatively rare case which arises on low-end devices when filtering wallpaper-sized images (like 1920 \u00d7 1080 pixels). The common workarounds are:\n\n  - using smaller images\n  - using [ColorMatrix](docs/color_matrix_filters.md#ColorMatrix) filter with [concatColorMatrices](docs/functions.md#concatColorMatrices) instead of wrapping the image with multiple color matrix based filters\n  - adding `android:largeHeap=\"true\"` to `android/app/src/main/AndroidManifest.xml`\n  - replacing standard `MainReactPackage` with [alternative](android/src/main/java/iyegoroff/imagefilterkit/MainReactPackageWithFrescoCache.java) one provided by this module:\n\n    ```diff\n    ...\n    + import iyegoroff.imagefilterkit.MainReactPackageWithFrescoCache;\n\n      public class MainApplication extends Application implements ReactApplication {\n\n      ...\n          List<ReactPackage> packages = new PackageList(this).getPackages();\n          // Packages that cannot be autolinked yet can be added manually here, for example:\n          // packages.add(new MyReactNativePackage());\n    -     return packages;\n    +     return MainReactPackageWithFrescoCache.inject(packages);\n        }\n    ...\n    ```\n\n    After this change `ImageFilter` will not throw `TooManyBitmapsException` immediately and will clear Fresco image caches, trim bitmap pool memory and try to      filter the image again several times until succeed or reach the limit of retries, specified by [clearCachesMaxRetries](docs/types.md#ImageFilter) prop.\n\n\n- If you are using `react-native-asset` with \"<strong>&lt;=0.4.14</strong>\" version of this library - switch to `iyegoroff/react-native-asset#with-key`. In order to prevent unlinking of `.cikernel` files provided by `react-native-image-filter-kit` use `react-native-asset` the following way: `npx iyegoroff/react-native-asset#with-key -a YOUR-OWN-ASSETS -k YOUR-APP-ID`\n\n## Credits\n\n- CSSGram filters are taken from [cssgram](https://github.com/una/cssgram) project by @una\n- `EdgeDetection`, `Emboss` and `FuzzyGlass` filters are taken from [android-graphics-demo](https://github.com/chiuki/android-graphics-demo) project by @chiuki\n- Parrot [image](https://commons.wikimedia.org/wiki/File:Ara_macao_-flying_away-8a.jpg) by\n  [Robert01](https://de.wikipedia.org/wiki/Benutzer:Robert01)\n- Blend filters are based on `skia` [sources](https://github.com/google/skia/blob/master/src/gpu/glsl/GrGLSLBlend.cpp)\n- File save functionality is based on [react-native-view-shot](https://github.com/gre/react-native-view-shot) project by @gre\n"
 },
 {
  "repo": "ThomasLengeling/KinectPV2",
  "language": "Java",
  "readme_contents": "KinectPV2\n==========\n\n### Kinect v2 library for Processing\n\n---\nVersion 0.7.8\n\nLibrary is curretly on develop with the Windows SDK Version 1409 (9/16/2014)\n\nLibrary for Mac check out  the [OpenKinect-for-Processing library](https://github.com/shiffman/OpenKinect-for-Processing).\n\n---\n\n####Requirements\n\n- A Kinect for Windows v2 Device (K4W2).\n- [Kinect SDK v2](http://www.microsoft.com/en-us/kinectforwindows/default.aspx)\n- 64bit computer with a dedicated USB 3.0.\n- Windows 10, 8, 8.1\n- [Processing 3.0](http://processing.org/)\n- Update your latest video card driver.\n- Install DirectX 11.\n\n#### Reference and tutorial\n\n[Webpage](http://codigogenerativo.com/code/kinectpv2-k4w2-processing-library/) tutorial with a couple of useful examples. \n\n\n### Install for Processing 3.0\n\n- Install the [Kinect for Windows SDK v2](http://www.microsoft.com/en-us/kinectforwindows/default.aspx)\n    - Install the library using Processing Contributed Library Manager\n    - Manual install, download the latest KinectPV2 version from the releases tab, and copy the KinectPV2 folder into your processing libraries sketch folder. \n- Enjoy\n\nFor Processing 2.2.1 please use the [KinectPV2 0.7.2 version](https://github.com/ThomasLengeling/KinectPV2/releases/tag/0.7.2).\n\n#### Examples\n\n- TestImages, Test all Frames/Images for the Kinect.\n- SkeletonMaskDepth, Skeleton positions are mapped to match the depth and body index frames.\n- SkeletonColor, Skeleton is mapped to match the color frame.\n- Skeleton3d, 3d Skeleton example needs love.\n- SimpleFaceTracking, simple face tracking with mode detection.\n- PointCloudOGL, Point cloud depth render using openGL and shaders.\n- PointCloudDepth, point cloud in a single 2d Image and threshold example.\n- PointCloudColor, Point cloud in color, using openGL and shaders.\n- MaskTest, Body Index test, and body index with depth.\n- Mask_findUsers, find number of users base on body index information.\n- MapDepthToColor, depth to color mapping, depth frame is aligned with color frame.\n- HDFaceVertex, Face vertices are match with the color frame.\n- HDColor, 1920 x 1080 RGB frame.\n- DepthTest, Depth test with raw depth data.\n- CoordinateMapperRGBDepth, example broken, check 0.7.2 version.\n- RecordPointCloud, simple OBJ recording of the point cloud positions.\n- [OpenCV](https://github.com/atduskgreg/opencv-processing) examples:\n  - Live Capture App\n  - Find Contours with depth or bodyIndex\n\n#### Build\n\nTo build the KinectPV2 library from source code, look at the Build_libs folder\n- KinectPV2_vc2012, builds the .dll library with JNI code.\n- KinectPV2_Eclipse, builds the .jar library for processing.\n\n\n#### Know issues\n\n- Missing \"msvcp110.dll\", or  \"Kinect20.Face.dll: Can't find dependent libraries on thread\"\n    - Install [Visual 2012 C++ Redistributable Packages](https://www.microsoft.com/en-us/download/details.aspx?id=30679).\n    - Check if the Kinect v2 SDK 2.0 is installed correctly.\n- Problems with the video, update your video card driver and install DirectX 11.\n\n\n---\n\n#### Todo\n\n- Heart rate detection\n- Multiple Devices\n- [Kinect Fusion](http://msdn.microsoft.com/en-us/library/dn188670.aspx)\n\n---\n\n### Simple Device\n\nTo include the library into a processing sketch you just need to import it and initialize it.\n\n```java\nimport KinectPV2.*;\nKinectPV2 kinect;\n\nvoid setup() {\n  kinect = new KinectPV2(this);\n  //Start up methods go here\n  kinect.init();\n}\n```\n\n#### Images\n\nTo obtain the color Image, depth Image, infrared Image, bodyIndex Image and long Exposure Image as a PImage you need to active with the following method\n\n```java\n void enableColorImg(boolean toggle);\n void enableDepthImg(boolean toggle);\n void enableInfraredImg(boolean toggle);\n void enableBodyTrackImg(boolean toggle);\n void enableInfraredLongExposureImg(boolean toggle);\n \n PImage getColorImage();\n PImage getDepthImage();\n PImage getInfraredImage();\n PImage getBodyTrackImage();\n PImage getInfraredLongExposureImage();\n```\njust initialize in the setup()\n\n```java\n  kinect = new KinectPV2(this);\n  kinect.enableColorImg(true);\n  kinect.init();\n```\n\nget the PImage in the draw()\n\n```java\nPImage imgC = kinect.getColorImage();\nimage(imgC, 0, 0);\n```\n\nRaw Data is only available for the color, depth ad bodytrack frames/images. Once the frames are activated just call them.\n\n```java\n  //raw Data int valeus from [0 - 4500]\n  int [] rawData = kinect.getRawDepthData();\n  \n  //values for [0 - 256] strip\n  int [] rawData256 = kinect.getRawDepth256Data();\n  \n  //raw body data 0-6 users 255 nothing\n  int [] rawData = kinect.getRawBodyTrack();\n  \n  //unpacket color values\n  int [] colorRaw = kinect.getRawColor();\n  \n```\n\n\n#### License\n\nMIT License http://en.wikipedia.org/wiki/MIT_License\n\n\n"
 },
 {
  "repo": "MoshDev/BackgroundViewPager",
  "language": "Java",
  "readme_contents": "BackgroundViewPager\n===================\n\nIts similar to the Launcher Background effect, when user switch between fragments, the background will scroll within specific ratio to cover all fragments.\n\ncheckout the code, it includes a sample, just replace the background to any image you have.\n\nUsage\n==================\nSimply add your \"BackgroundViewPager\" into your layout xml, and assign a drawable to it,\n\n    <com.infusionapps.widget.BackgroundViewPager\n        android:id=\"@+id/flowViewPager1\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:layout_centerInParent=\"true\"\n        flow:image=\"@drawable/bg5\" />\n        \n        \nContact Me\n=================\nTwitter : https://twitter.com/MoshErsan\nGmail: mr.ersan85@gmail.com\n        \n        \nLicense\n=================\n\n    The MIT License (MIT)\n        \n    Copyright (c) 2014 Mohammad Ersan\n    \n    Permission is hereby granted, free of charge, to any person obtaining a copy of \n    this software and associated documentation files (the \"Software\"), to deal in\n    the Software without restriction, including without limitation the rights to\n    use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n    the Software, and to permit persons to whom the Software is furnished to do so,\n    subject to the following conditions:\n    \n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n    \n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n    FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n    COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n    IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n    CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
 },
 {
  "repo": "vincenzopalazzo/material-ui-swing",
  "language": "Java",
  "readme_contents": "# Material-UI-Swing\n[![Maven Central](https://img.shields.io/maven-central/v/io.github.vincenzopalazzo/material-ui-swing?color=%237cc4f4&style=for-the-badge)](https://search.maven.org/search?q=g:%22io.github.vincenzopalazzo%22%20AND%20a:%22material-ui-swing%22)\n![GitHub Workflow Status](https://img.shields.io/github/workflow/status/vincenzopalazzo/material-ui-swing/build?style=for-the-badge)\n![GitHub last commit](https://img.shields.io/github/last-commit/vincenzopalazzo/material-ui-swing?color=%237cc4f4&style=for-the-badge)\n![GitHub All Releases](https://img.shields.io/github/downloads/vincenzopalazzo/material-ui-swing/total?color=%234caf50&style=for-the-badge)\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/material-ui-swing/material-ui-swing-icon/main/svg/java-red-icon.svg\" />\n</div>\n\n## Description\n![GitHub issues](https://img.shields.io/github/issues/vincenzopalazzo/material-ui-swing.svg?style=for-the-badge)\n![GitHub pull requests](https://img.shields.io/github/issues-pr/vincenzopalazzo/material-ui-swing.svg?style=for-the-badge)\n[![Donation](https://img.shields.io/website/http/material-ui-swing.github.io/material-ui-swing-donations.svg?style=for-the-badge&up_color=yellow&up_message=Donation)](https://material-ui-swing.github.io/material-ui-swing-donations)\n[![Gitter chat](https://img.shields.io/gitter/room/vincenzopalazzo/material-ui-swing.svg?style=for-the-badge)](https://gitter.im/material-ui-swing/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\nA modern, Material Design UI for Java Swing\n\n# Overview\n\n- [Introduction](https://github.com/vincenzopalazzo/material-ui-swing/wiki/Introduction)\n- [Component Supported](https://github.com/vincenzopalazzo/material-ui-swing/wiki/Component-Supported)\n- [Material Theming](https://vincenzopalazzo.medium.com/swing-theming-basic-introduction-to-materialtheme-system-150395392a7)\n- [Develop a new Theme](https://github.com/vincenzopalazzo/material-ui-swing/wiki)\n- [Screenshots](https://github.com/vincenzopalazzo/material-ui-swing/wiki/Screenshots)\n- [How Contribute](/docs/dev/MAINTAINERS.md)\n- [Built with](https://github.com/vincenzopalazzo/material-ui-swing/wiki/Built-with)\n- [Community](https://gitter.im/material-ui-swing/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link)\n- [License](https://github.com/vincenzopalazzo/material-ui-swing/tree/development#license)\n\n ## Repository\n\n _Maven_\n ```xml\n<dependency>\n  <groupId>io.github.vincenzopalazzo</groupId>\n  <artifactId>material-ui-swing</artifactId>\n  <version>1.1.2</version>\n</dependency>\n ```\n\n _Gradle (Groovy)_\n```groovy\nimplementation 'io.github.vincenzopalazzo:material-ui-swing:1.1.2'\n```\n\n _Gradle (Kotlin DSL)_\n```kotlin\nimplementation(\"io.github.vincenzopalazzo:material-ui-swing:1.1.2\")\n```\n\nOthers version [here](https://search.maven.org/artifact/io.github.vincenzopalazzo/material-ui-swing)\n\n### Snapshot version\n\nEach master version has a SNAPSHOT version that is the official version `x.x.x + 1`, so for example for the version `v1.1.2-rc1`\nthe version on if exist a new version of the master branch is `v1.1.2-rc2-SNAPSHOT`\n\nAn example of gradle configuration is reported below\n\n_Gradle (Kotlin DSL)_\n```kotlin\nconfigurations.all {\n    resolutionStrategy.cacheDynamicVersionsFor(0, \"seconds\")\n    resolutionStrategy.cacheChangingModulesFor(0, \"seconds\")\n}\n\nrepositories {\n    ... other suff\n    maven(\"https://oss.sonatype.org/content/repositories/snapshots\")\n}\n\ndependencies {\n    ... other stuff\n    implementation(\"io.github.vincenzopalazzo:material-ui-swing:1.1.3-rc1-SNAPSHOT\")\n}\n\n```\n\n## Code Style\n> We live in a world where robots can drive a car, so we shouldn't just write code, we should write elegant code.\n\nThis repository use [google-java-format](https://github.com/sherter/google-java-format-gradle-plugin) to maintains the code of the repository elegant, so\nbefore submit the code check the Java format with the following command on the root of the directory\n\n```bash\n./gradlew verifyGoogleJavaFormat\n```\n\nIf any error are reported please run the following command to fix them\n\n```bash\n./gradlew googleJavaFormat\n```\n\nP.S.: The gradle plugin works with all the JDK versions >= 9 (or better with java byte code version compatible with the version  55.0)\n\nFor more details about the JDK support see [this issue](https://github.com/sherter/google-java-format-gradle-plugin/issues/58) \nand to know more about the Google Java code Style see [this reference](https://google.github.io/styleguide/javaguide.html)\n\n## Build with Material-UI-Swing\n_**List of projects with Material-UI-Swing theme**_\n- [Krayon for SBGN](https://github.com/wiese42/krayon4sbgn)\n- [JMars 5](https://JMars.mars.asu.edu): Used by NASA and United Arab Emirates (UAE) for the last Mars missions.\n\nContact us if you use this look and feel, so we can add your project to the list \ud83d\ude42\n\n### _Donors_\n- [lilili87222](https://github.com/lilili87222)\n- Arizona State University\n- [zanderson9](https://github.com/zanderson9)\n\nYou can support the project on the [Material-UI-Swing donation site](https://material-ui-swing.github.io/material-ui-swing-donations/)\n\n## Supporters\n\n<div align=\"center\">\n  <img src=\"docs/jetbrains-logos/jetbrains-variant-4.png\" width=\"325\" height=\"180\"/>\n</div>\n\n[JetBrains' support](https://www.jetbrains.com/?from=material-ui-swing)\n\n<div align=\"center\">\n  <img src=\"https://www.yourkit.com/images/yklogo.png\"/>\n</div>\nThe YourKit is used also by Google, Microsoft, PayPal, ecc.\n\n- **YourKit**: it supports open source projects with innovative and intelligent tools\nfor monitoring and profiling Java and .NET applications.\nYourKit is the creator of <a href=\"https://www.yourkit.com/java/profiler/\">YourKit Java Profiler</a>,\n<a href=\"https://www.yourkit.com/.net/profiler/\">YourKit .NET Profiler</a>,\nand <a href=\"https://www.yourkit.com/youmonitor/\">YourKit YouMonitor</a>.\n\n## License\n\n<div align=\"center\">\n  <img src=\"https://opensource.org/files/osi_keyhole_300X300_90ppi_0.png\" width=\"150\" height=\"150\"/>\n</div>\n\nCopyright (c) 2018 atharva washimkar.\n\nCopyright (c) 2019-2020 atharva washimkar, Vincenzo Palazzo vincenzopalazzodev@gmail.com\n\nCopyright (c) 2021 Vincenzo Palazzo vincenzopalazzodev@gmail.com\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), \nto deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, \nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, \nDAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE \nOR OTHER DEALINGS IN THE SOFTWARE.\n\n"
 },
 {
  "repo": "RurioLuca/QrCardParsing",
  "language": "Java",
  "readme_contents": "# QrCardParsing\n\n[![Platform (Android)](https://img.shields.io/badge/platform-Android-blue.svg?style=flat-square)](http://www.android.com)\n[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-QrCardParsing-green.svg?style=true)](https://android-arsenal.com/details/1/3978)\n[![GitHub license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/RurioLuca/QrCardParsing/blob/master/License)\n<!-- [![Codewake](https://www.codewake.com/badges/ask_question_flat_square.svg)](https://www.codewake.com/p/qrcardparsing) -->\n[ ![Download](https://api.bintray.com/packages/auron/maven/mecard-parser/images/download.svg) ](https://bintray.com/auron/maven/mecard-parser/_latestVersion) \n[![API](https://img.shields.io/badge/API-14%2B-blue.svg?style=flat)](https://android-arsenal.com/api?level=14)\n [![GitHub stars](https://img.shields.io/github/stars/RurioLuca/QrCardParsing.svg)](https://github.com/RurioLuca/QrCardParsing/stargazers)\n \nAndroid Libraries to parsing and generate content of:\n\n- MeCard\n- VCard\n- VEvent\n- WifiCard \n- GeoCard\n\n![Screen](https://raw.githubusercontent.com/RurioLuca/MeCardParsing/master/img/screen.png)\n\n\n### Requirements\n\nThe library requires Android **API Level 14+**.\n\nif you need a higher backward compatibility, use version 1.1.3 or lower (api level 9+)\n### Import\n\nin build.gradle\n\n```Gradle\nallprojects {\n    repositories {\n        jcenter()\n    }\n}\n\n```\n```Gradle\ndependencies {\n// ... other dependencies here\nimplementation 'it.auron:mecard-parser:1.1.5'\n}\n```\n\n### How to use\n\n#### Generate MeCard content\n```java\n        MeCard meCard =new MeCard();\n        meCard.setName(\"Luca\");\n        meCard.setSurname(\"Rurio\");\n        meCard.setDate(\"1989-07-19\");\n        meCard.setEmail(\"rurio.luca@gmail.com\");\n        meCard.setNote(\"generate MeCard string content!\");\n        meCard.addTelephone(\"+39 3486454313\");\n        meCard.addTelephone(\"+39 3476512321\");\n        meCard.setUrl(\"https://github.com/RurioLuca\");\n        meCard.setAddress(\"via del corso , Rome , Italy\");\n        meCard.setOrg(\"your company\");\n        String meCardContent=meCard.buildString();\n        \n        //sample using QrGen to generate QrCode bitmap\n        imageView.setImageBitmap(QRCode.from(meCardcontent).bitmap());\n```\n#### Parsing MeCard content\n\n```java\n\n   String meCardString = \"MECARD:N:Rurio,Luca;TEL:+39 3486454313;EMAIL:rurio.luca@gmail.com;URL:https://github.com/RurioLuca;NOTE:generate MeCard!;BDAY:1989-07-19;ADR:via del corso , Rome , Italy;ORG:your company;\";\n       \n   MeCard meCard = MeCardParser.parse(meCardString);\n\n   String name = meCard.getName(); \n   //output :Luca\n    String name = meCard.getEmail(); \n    //output :rurio.luca@gmail.com\n    \n  meCard.setDate(\"1989-07-19\");\n  String meCardContent=meCard.buildString();\n  \n   //sample using QrGen to generate QrCode bitmap\n  imageView.setImageBitmap(QRCode.from(meCardcontent).bitmap());\n  \n```\n\n\n#### Generate VCard content\n\n```java\n\n        VCard vCard=new VCard();\n        vCard.setName(\"Luca\");\n        vCard.setAddress(\"via del corso\");\n        vCard.setCompany(\"freelancer\");\n        vCard.addEmail(\"rurio.luca@gmail.com\");\n        vCard.addTelephone(\"+39 3486454314\");\n        vCard.setFormattedName(\"Rurio Luca\");\n        vCard.setTitle(\"Developer\");\n        vCard.setUrl(\"https://github.com/RurioLuca/MeCardParsing/\");\n        imageView.setImageBitmap(QRCode.from(vCard.buildString()).bitmap());\n        \n```\n#### Parsing VCard content\n\n```java\n\n String vCardString = \"BEGIN:VCARD\\n\" +\n                \"N:Luca\\n\" +\n                \"FN:Rurio Luca\\n\" +\n                \"ORG:freelancer\\n\" +\n                \"TITLE:Developer\\n\" +\n                \"EMAIL:rurio.luca@gmail.com\\n\" +\n                \"URL:https://yoursite.com\\n\" +\n                \"END:VCARD\";\n\n        VCard vCard = VCardParser.parse(vCardString);\n\n        vCard.setNote(\"vCard generate and modified!\");\n        vCard.addTelephone(\"+39 3486454314\");\n        String vCardcontent = vCard.buildString();\n        //sample generate bitmap using QrGen\n        imageView.setImageBitmap(QRCode.from(vCardcontent).bitmap());\n\n\n```\n\n#### Generate Wifi content\n\n```java\n\n        WifiCard wifiCard = new WifiCard();\n        wifiCard.setSid(\"Vodafone Wifi32341\");\n        wifiCard.setPassword(\"administrator\");\n        wifiCard.setType(\"WPA\");\n        \n         //sample generate Qr code using Qrgen\n        imageView.setImageBitmap(QRCode.from(wifiCard.buildString()).bitmap());\n        \n```\n\n#### Parsing Wifi content\n\n```java\n\n        String wifiString = \"WIFI:S:Vodafone Wifi32341;T:WPA;P:administrator;;\";\n        WifiCard wifiCard = WifiCardParser.parse(wifiString);\n\n        wifiCard.setPassword(\"administrator2016\");\n\n\n        String wifiCardcontent = wifiCard.buildString();\n\n        //sample generate Qr code using Qrgen\n        imageView.setImageBitmap(QRCode.from(wifiCardcontent).bitmap());\n\n\n```\n\n#### Generate VEvent content\n\n```java\n\n        VEvent vEvent = new VEvent();\n        vEvent.Summary(\"Google IO\");\n        vEvent.Location(\"Shoreline Amphitheatre Mountain View, California\");\n        vEvent.Url(\"www.sample.com\");\n        vEvent.setDtStart(\"20170611T130000Z\");\n        vEvent.setDtEnd(\"20170611T153400Z\");\n        \n         //sample generate Qr code using Qrgen\n        imageView.setImageBitmap(QRCode.from(vEvent.buildString()).bitmap());\n        \n```\n\n#### Parsing VEvent content\n\n```java\n\n        String vEventString = \"BEGIN:VEVENT\\n\" +\n                \"SUMMARY:Google IO\\n\" +\n                \"LOCATION:Shoreline Amphitheatre Mountain View, California\\n\" +\n                \"DTSTART:20170611T130000Z\\n\" +\n                \"DTEND:20170611T153400Z\\n\" +\n                \"END:VEVENT\";\n                \n       VEvent vEvent = VEventParser.parse(vEventString);\n       vEvent.setSummary(\"Google I/O\");\n      \n      SimpleDateFormat simpleDateFormat = new SimpleDateFormat(VEventCostant.DATE_FORMAT);\n        try {\n            Calendar calendar = Calendar.getInstance();\n            calendar.setTime(simpleDateFormat.parse(vEvent.getDtEnd()));\n            calendar.set(Calendar.DAY_OF_MONTH, 12);\n            calendar.set(Calendar.HOUR_OF_DAY, 14);\n            calendar.set(Calendar.MINUTE, 00);\n            vEvent.setDtEnd(simpleDateFormat.format(calendar.getTime()));\n        } catch (ParseException e) {\n            e.printStackTrace();\n        }\n        \n\n\n        String vEventcontent = vEvent.buildString();\n\n        //sample generate Qr code using Qrgen\n        imageView.setImageBitmap(QRCode.from(vEventcontent).bitmap());\n\n\n```\n\n#### Generate Geolocation content\n\n```java\n\n        GeoCard geoCard = new GeoCard();\n\n        geoCard.setLat(41.8919300);\n        geoCard.setLon(12.5113300);\n        \n         //sample generate Qr code using Qrgen\n        imageView.setImageBitmap(QRCode.from(geoCard.buildString()).bitmap());\n        \n```\n\n#### Parsing Geolocation content\n\n```java\n\n      String geoString = \"geo:20.33470,20.39448\";\n      GeoCard geoCard = GeoCardParser.parse(geoString);\n       \n       // set Rome location\n       geoCard.setLat(41.8919300);\n       geoCard.setLon(12.5113300);\n\n\n        String geoCardcontent = geoCard.buildString();\n\n        //sample generate Qr code using Qrgen\n        imageView.setImageBitmap(QRCode.from(geoCardcontent).bitmap());\n\n\n```\n\n### Developed By\nRurio Luca- [rurio.luca@gmail.com](mailto:rurio.luca@gmail.com)\n\n[![Linkedin](https://raw.githubusercontent.com/RurioLuca/MeCardParsing/master/img/social/linkedin-icon.png) ](https://it.linkedin.com/in/luca-rurio-5a4462107)\n\n### App using QrCardParsing\n\n\n  * [Material Qr](https://play.google.com/store/apps/details?id=qrreader.com.studios.it.qrreader)\n  * [Mercatini Usato](https://play.google.com/store/apps/details?id=it.auron.mercatino&hl=it)\n\nsend me your apps!\nrurio.luca@gmail.com\n\n# License\n\nThe MIT License (MIT)\n\nCopyright (c) 2016 Rurio Luca\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
 },
 {
  "repo": "7heaven/UILibrary",
  "language": "Java",
  "readme_contents": "[![License](http://img.shields.io/:license-mit-blue.svg)](LICENSE)\n# UILibrary\n\n---\n\n\u628a\u5e73\u65f6\u5199\u7684\u7528\u5230\u9879\u76ee\u91cc\u9762\u7684\u4e00\u4e9bUI\u76f8\u5173\u7684View\u3001Drawable\u7b49\u6574\u7406\u4e86\u4e00\u4e0b\uff0c\u6301\u7eed\u66f4\u65b0\n\n\u4e3b\u8981\u6709\u4ee5\u4e0b\u4e00\u4e9b\u6548\u679c:\n\n**\u8ddf\u7740Path\u8fd0\u52a8\u7684ProgressBar\u6548\u679c**\n\n![PathProgressProvider+ProgressiveDrawable](./arts/rendering_1.gif)\n\n**QQ\u7fa4\u5934\u50cf\u6548\u679c\u7684\u63a7\u4ef6**\n\n![GroupedAvatarDrawable](./arts/rendering_2.gif)\n\n**\u82f9\u679capp\u4e0b\u8f7d\u7684\u8fdb\u5ea6\u6548\u679c**\n\n![AppStoreStyleProgressProvider+ProgressiveDrawable](./arts/rendering_3.gif)\n\n**\u4efb\u610f\u5f62\u72b6\u7684\u906e\u7f69**\n\n![MaskDrawable+PolygonShape](./arts/rendering_4.gif)\n\n**\u5e26\u5706\u89d2\u7684\u4efb\u610f\u6b63\u591a\u8fb9\u5f62\u548c\u661f\u578b**\n\n![PolygonShape](./arts/rendering_5.gif)\n\n**\u5e26\u52a8\u753b\u6548\u679c\u5207\u6362ScaleType\u7684ImageView**\n\n![AnimatedImageView](./arts/rendering_6.gif)\n\n"
 },
 {
  "repo": "cucumber-attic/cuke4duke",
  "language": "Java",
  "readme_contents": "h1. WARNING\n\nThis project is defunkt, and has been replaced by \"Cucumber-JVM\":https://github.com/cucumber/cucumber-jvm. For more info see \"this thread\":http://groups.google.com/group/cukes/browse_thread/thread/299d94d38500e8c3.\n\nh1. Cuke4Duke\n\nCuke4Duke is an addon to Cucumber, making it possible to write step definitions in several different JVM languages.\n\nh2. Building Cuke4Duke\n\nFirst of all, you need \"Maven\":http://maven.apache.org/ installed.\nThen you'll need \"git\":http://git-scm.com/\n\nYou'll also need \"JRuby\":http://jruby.org installed to build Cuke4Duke. If you're on OS X or Linux it's recommended you install JRuby\nwith \"RVM\":http://rvm.beginrescueend.com\n\nWith JRuby installed - bootstrap your environment by installing some gems:\n\nUsing RVM:\n<pre>\nmkdir -p ~/.m2/repository/.jruby\nGEM_HOME=~/.m2/repository/.jruby GEM_PATH=~/.m2/repository/.jruby gem install bundler\nGEM_HOME=~/.m2/repository/.jruby GEM_PATH=~/.m2/repository/.jruby ~/.m2/repository/.jruby/bin/bundle install\nGEM_HOME=~/.m2/repository/.jruby GEM_PATH=~/.m2/repository/.jruby ~/.m2/repository/.jruby/bin/rake install\n</pre>\n\nNot using RVM:\n<pre>\nmkdir -p ~/.m2/repository/.jruby\nGEM_HOME=~/.m2/repository/.jruby GEM_PATH=~/.m2/repository/.jruby jruby -S gem install bundler\nGEM_HOME=~/.m2/repository/.jruby GEM_PATH=~/.m2/repository/.jruby jruby -S bundle install\nGEM_HOME=~/.m2/repository/.jruby GEM_PATH=~/.m2/repository/.jruby jruby -S rake install\n</pre>\n\nWith the gems installed, build the whole shebang (including the examples):\n\nUsing RVM:\n<pre>GEM_HOME=~/.m2/repository/.jruby GEM_PATH=~/.m2/repository/.jruby ~/.m2/repository/.jruby/bin/rake build_all</pre>\n\nNot using RVM:\n<pre>jruby -S rake build_all</pre>\n\nh2. Release process\n\nFirst, bump the release number:\n\n<pre>rake remove_snapshots</pre>\n\nBuild again:\n\n<pre>rake build_all</pre>\n\nIf all is OK, commit:\n\n<pre>git commit -m \"Release\"</pre>\n\nAnd release:\n\n<pre>rake release</pre>\n\nFinally, bump version:\n\n<pre>rake add_snapshots</pre>\n\nAnd commit again:\n\n<pre>git commit -m \"Starting new development cycle\"</pre>\n"
 },
 {
  "repo": "HouariZegai/Calculator",
  "language": "Java",
  "readme_contents": "# Calculator App\nA very basic calculator application created with Java **Swing**. \n\n[![License MIT](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n## Thank You!\nPlease \u2b50\ufe0f this repo and share it with others\n\n### Screenshots\nScientific / Dark |  Standard / Colored\n:------------------:|:-------------------\n![Calculator - screenshot](screenshots/dark.PNG) | ![Calculator - screenshot](screenshots/colored.PNG)\n\n### Requirements \ud83d\udd27\n* Java version 8 or higher.\n\n### Installation \ud83d\udd0c\n1. Press the **Fork** button (top right the page) to save copy of this project on your account.\n\n2. Download the repository files (project) from the download section or clone this project by typing in the bash the following command:\n\n       git clone https://github.com/HouariZegai/Calculator.git\n3. Imported it in Intellij IDEA or any other Java IDE.\n4. Run the application :D\n\n### Contributing \ud83d\udca1\nIf you want to contribute to this project and make it better with new ideas, your pull request is very welcomed.\nIf you find any issue just put it in the repository issue section, thank you.\n"
 },
 {
  "repo": "tencentyun/wafer-java-server-sdk",
  "language": "Java",
  "readme_contents": "Wafer \u670d\u52a1\u7aef SDK - Java\n=================================\n\n[![license](https://img.shields.io/github/license/tencentyun/weapp-java-server-sdk.svg?style=flat-square)](LICENSE)\n\n\u672c\u9879\u76ee\u662f [Wafer](https://github.com/tencentyun/wafer) \u7ec4\u6210\u90e8\u5206\uff0c\u4ee5 SDK \u7684\u5f62\u5f0f\u4e3a\u4e1a\u52a1\u670d\u52a1\u5668\u63d0\u4f9b\u4ee5\u4e0b\u670d\u52a1\uff1a\n\n+ [\u4f1a\u8bdd\u670d\u52a1](https://github.com/tencentyun/wafer/wiki/\u4f1a\u8bdd\u670d\u52a1)\n+ [\u4fe1\u9053\u670d\u52a1](https://github.com/tencentyun/wafer/wiki/\u4fe1\u9053\u670d\u52a1)\n\n## SDK \u83b7\u53d6\n\n\u672c\u9879\u76ee\u9075\u5b88 [MIT](LICENSE) \u534f\u8bae\uff0c\u53ef\u4ee5\u76f4\u63a5[\u4e0b\u8f7d SDK \u6e90\u7801][sdk-download]\u8fdb\u884c\u4fee\u6539\u3001\u7f16\u8bd1\u548c\u53d1\u5e03\u3002\n\n> \u5982\u679c\u4f7f\u7528[\u81ea\u52a8\u90e8\u7f72](https://github.com/tencentyun/wafer/wiki/%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2)\u5e76\u9009\u62e9 Java \u8bed\u8a00\uff0c\u5219\u5206\u914d\u7684\u4e1a\u52a1\u670d\u52a1\u5668\u91cc\u5df2\u7ecf\u90e8\u7f72\u4e86\u672c SDK \u548c Demo \u7684\u53d1\u884c\u7248\u672c\u3002\n\n## API\n\n\u8bf7\u53c2\u8003[\u7ebf\u4e0a API \u6587\u6863][api-url]\u3002\n\n## \u4f7f\u7528\u793a\u4f8b\uff08Servlet\uff09\n\n### \u914d\u7f6e SDK\n\nSDK \u5fc5\u987b\u7ecf\u8fc7\u521d\u59cb\u5316\u914d\u7f6e\u4e4b\u540e\u624d\u80fd\u4f7f\u7528\u3002\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u4ee3\u7801\u521d\u59cb\u5316\u6216\u8005\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316\u3002\u521d\u59cb\u5316\u914d\u7f6e\u5efa\u8bae\u5728 `Servlet::init()` \u91cc\u8fdb\u884c\u3002\n\n\u4f7f\u7528\u4ee3\u7801\u521d\u59cb\u5316\uff1a\n\n```java\nimport com.qcloud.weapp.*;\n\nConfiguration configuration = new Configuration();\n\n// \u4e1a\u52a1\u670d\u52a1\u5668\u8bbf\u95ee\u57df\u540d\nconfiguration.setServerHost(\"199447.qcloud.la\");\n// \u9274\u6743\u670d\u52a1\u5730\u5740\nconfiguration.setAuthServerUrl(\"http://10.0.12.135/mina_auth/\");\n// \u4fe1\u9053\u670d\u52a1\u5730\u5740\nconfiguration.setTunnelServerUrl(\"https://ws.qcloud.com/\");\n// \u4fe1\u9053\u670d\u52a1\u7b7e\u540d key\nconfiguration.setTunnelSignatureKey(\"my$ecretkey\");\n// \u7f51\u7edc\u8bf7\u6c42\u8d85\u65f6\u8bbe\u7f6e\uff0c\u5355\u4f4d\u4e3a\u79d2\nconfiguration.setNetworkTimeout(30);\n\nConfigurationManager.setup(configuration);\n```\n\n\u4f7f\u7528\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316\uff1a\n\n```java\nimport com.qcloud.weapp.*;\n\nvar configFilePath = \"/etc/qcloud/sdk.config\";\nConfigurationManager.setupFromFile(configFilePath);\n```\n\n\u5173\u4e8e SDK \u914d\u7f6e\u5b57\u6bb5\u7684\u542b\u4e49\u4ee5\u53ca\u914d\u7f6e\u6587\u4ef6\u683c\u5f0f\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003[\u670d\u52a1\u7aef SDK \u914d\u7f6e][sdk-config-wiki]\u3002\n\n### \u4f7f\u7528 SDK \u63d0\u4f9b\u767b\u5f55\u670d\u52a1\n\n#### \u767b\u5f55\n\n\u4e1a\u52a1\u670d\u52a1\u5668\u63d0\u4f9b\u4e00\u4e2a\u8def\u7531\u5904\u7406\u5ba2\u6237\u7aef\u7684\u767b\u5f55\u8bf7\u6c42\uff0c\u76f4\u63a5\u628a\u8be5\u8bf7\u6c42\u4ea4\u7ed9 SDK \u6765\u5904\u7406\u5373\u53ef\u5b8c\u6210\u767b\u5f55\u3002\u767b\u5f55\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u83b7\u53d6\u7528\u6237\u4fe1\u606f\u3002\n\n```java\nimport com.qcloud.weapp.*;\nimport com.qcloud.weapp.authorization.*;\n\n@WebServlet(\"/login\")\npublic class LoginServlet extends HttpServlet {\n    /**\n     * \u5904\u7406\u767b\u5f55\u8bf7\u6c42\n     * */\n    protected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        // \u901a\u8fc7 ServletRequest \u548c ServletResponse \u521d\u59cb\u5316\u767b\u5f55\u670d\u52a1\n        LoginService service = new LoginService(request, response);\n        try {\n            // \u8c03\u7528\u767b\u5f55\u63a5\u53e3\uff0c\u5982\u679c\u767b\u5f55\u6210\u529f\u53ef\u4ee5\u83b7\u5f97\u767b\u5f55\u4fe1\u606f\n            UserInfo userInfo = service.login();\n            System.out.println(\"========= LoginSuccess, UserInfo: ==========\");\n            System.out.println(userInfo.toString());\n        } catch (LoginServiceException e) {\n            // \u767b\u5f55\u5931\u8d25\u4f1a\u629b\u51fa\u767b\u5f55\u5931\u8d25\u5f02\u5e38\n            e.printStackTrace();\n        } catch (ConfigurationException e) {\n            // SDK \u5982\u679c\u8fd8\u6ca1\u6709\u914d\u7f6e\u4f1a\u629b\u51fa\u914d\u7f6e\u5f02\u5e38\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n> \u5982\u679c\u767b\u5f55\u5931\u8d25\uff0c[login()][login-api] \u65b9\u6cd5\u4f1a\u629b\u51fa\u5f02\u5e38\uff0c\u9700\u8981\u4f7f\u7528 try-catch \u6765\u6355\u83b7\u5f02\u5e38\u3002\u8be5\u5f02\u5e38\u53ef\u4ee5\u4e0d\u7528\u5904\u7406\uff0c\u629b\u51fa\u6765\u662f\u4e3a\u4e86\u65b9\u4fbf\u4e1a\u52a1\u670d\u52a1\u5668\u53ef\u4ee5\u8fdb\u884c\u8bb0\u5f55\u548c\u76d1\u63a7\u3002\n\n#### \u83b7\u53d6\u4f1a\u8bdd\u72b6\u6001\n\n\u5ba2\u6237\u7aef\u4ea4\u7ed9\u4e1a\u52a1\u670d\u52a1\u5668\u7684\u8bf7\u6c42\uff0c\u4e1a\u52a1\u670d\u52a1\u5668\u53ef\u4ee5\u901a\u8fc7 SDK \u6765\u68c0\u67e5\u8be5\u8bf7\u6c42\u662f\u5426\u5305\u542b\u5408\u6cd5\u7684\u5fae\u4fe1\u5c0f\u7a0b\u5e8f\u4f1a\u8bdd\u3002\u5982\u679c\u5305\u542b\uff0c\u5219\u4f1a\u8fd4\u56de\u4f1a\u8bdd\u5bf9\u5e94\u7684\u7528\u6237\u4fe1\u606f\u3002\n\n```java\nimport com.qcloud.weapp.*;\nimport com.qcloud.weapp.authorization.*;\n\n@WebServlet(\"/user\")\npublic class UserServlet extends HttpServlet {\n    /**\n     * \u4ece\u8bf7\u6c42\u4e2d\u83b7\u53d6\u4f1a\u8bdd\u4e2d\u7684\u7528\u6237\u4fe1\u606f\n     */\n    protected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        LoginService service = new LoginService(request, response);        \n        try {\n            // \u8c03\u7528\u68c0\u67e5\u767b\u5f55\u63a5\u53e3\uff0c\u6210\u529f\u540e\u53ef\u4ee5\u83b7\u5f97\u7528\u6237\u4fe1\u606f\uff0c\u8fdb\u884c\u6b63\u5e38\u7684\u4e1a\u52a1\u8bf7\u6c42\n            UserInfo userInfo = service.check();\n            \n            // \u83b7\u53d6\u4f1a\u8bdd\u6210\u529f\uff0c\u8f93\u51fa\u83b7\u5f97\u7684\u7528\u6237\u4fe1\u606f            \n            JSONObject result = new JSONObject();\n            JSONObject data = new JSONObject();\n            data.put(\"userInfo\", new JSONObject(userInfo));\n            result.put(\"code\", 0);\n            result.put(\"message\", \"OK\");\n            result.put(\"data\", data);            \n            response.setContentType(\"application/json\");\n            response.setCharacterEncoding(\"utf-8\");\n            response.getWriter().write(result.toString());\n            \n        } catch (LoginServiceException e) {\n            e.printStackTrace();\n        } catch (JSONException e) {\n            e.printStackTrace();\n        } catch (ConfigurationException e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n```\n\n> \u5982\u679c\u68c0\u67e5\u4f1a\u8bdd\u5931\u8d25\uff0c\u6216\u8005\u4f1a\u8bdd\u65e0\u6548\uff0c[check()][check-api] \u65b9\u6cd5\u4f1a\u629b\u51fa\u5f02\u5e38\uff0c\u9700\u8981\u4f7f\u7528 try-catch \u6765\u6355\u83b7\u5f02\u5e38\u3002\u8be5\u5f02\u5e38\u53ef\u4ee5\u4e0d\u7528\u5904\u7406\uff0c\u629b\u51fa\u6765\u662f\u4e3a\u4e86\u65b9\u4fbf\u4e1a\u52a1\u670d\u52a1\u5668\u53ef\u4ee5\u8fdb\u884c\u8bb0\u5f55\u548c\u76d1\u63a7\u3002\n\n\u9605\u8bfb\u89e3\u51b3\u65b9\u6848\u6587\u6863\u4e2d\u7684[\u4f1a\u8bdd\u670d\u52a1][session-service-wiki]\u4e86\u89e3\u66f4\u591a\u89e3\u51b3\u65b9\u6848\u4e2d\u5173\u4e8e\u9274\u6743\u670d\u52a1\u7684\u6280\u672f\u8d44\u6599\u3002\n\n### \u4f7f\u7528 SDK \u63d0\u4f9b\u4fe1\u9053\u670d\u52a1\n\n\u4e1a\u52a1\u5728\u4e00\u4e2a\u8def\u7531\u4e0a\u63d0\u4f9b\u4fe1\u9053\u670d\u52a1\uff0c\u53ea\u9700\u628a\u8be5\u8def\u7531\u4e0a\u7684\u8bf7\u6c42\u90fd\u4ea4\u7ed9 SDK \u7684\u4fe1\u9053\u670d\u52a1\u5904\u7406\u5373\u53ef\u3002\n\n```java\nimport com.qcloud.weapp.*;\nimport com.qcloud.weapp.tunnel.*;\nimport com.qcloud.weapp.demo.ChatTunnelHandler;\n\n@WebServlet(\"/tunnel\")\npublic class TunnelServlet extends HttpServlet {\n    /**\n     * \u628a\u6240\u6709\u7684\u8bf7\u6c42\u4ea4\u7ed9 SDK \u5904\u7406\uff0c\u63d0\u4f9b TunnelHandler \u5904\u7406\u4fe1\u9053\u4e8b\u4ef6\n     */\n    protected void service(HttpServletRequest request, HttpServletResponse response)\n            throws ServletException, IOException {\n        \n        // \u521b\u5efa\u4fe1\u9053\u670d\u52a1\u5904\u7406\u4fe1\u9053\u76f8\u5173\u8bf7\u6c42\n        TunnelService tunnelService = new TunnelService(request, response);\n        \n        try {\n            // \u914d\u7f6e\u662f\u53ef\u9009\u7684\uff0c\u914d\u7f6e CheckLogin \u4e3a true \u7684\u8bdd\uff0c\u4f1a\u5728\u96a7\u9053\u5efa\u7acb\u4e4b\u524d\u83b7\u53d6\u7528\u6237\u4fe1\u606f\uff0c\u4ee5\u4fbf\u4e1a\u52a1\u5c06\u96a7\u9053\u548c\u7528\u6237\u5173\u8054\u8d77\u6765\n            TunnelHandleOptions options = new TunnelHandleOptions();\n            options.setCheckLogin(true);\n            \n            // \u9700\u8981\u5b9e\u73b0\u4fe1\u9053\u5904\u7406\u5668\uff0cChatTunnelHandler \u662f\u4e00\u4e2a\u5b9e\u73b0\u7684\u8303\u4f8b\n            tunnelService.handle(new ChatTunnelHandler(), options);\n        } catch (ConfigurationException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n\u4f7f\u7528\u4fe1\u9053\u670d\u52a1\u9700\u8981\u5b9e\u73b0\u5904\u7406\u5668\uff0c\u6765\u83b7\u53d6\u5904\u7406\u4fe1\u9053\u7684\u5404\u79cd\u4e8b\u4ef6\uff0c\u5177\u4f53\u53ef\u53c2\u8003\u63a5\u53e3 [TunnelHandler][tunnel-handler-api] \u7684 API \u6587\u6863\u4ee5\u53ca\u914d\u5957 Demo \u4e2d\u7684 [ChatTunnelHandler][chat-handler-source] \u7684\u5b9e\u73b0\u3002\n\n\u9605\u8bfb\u89e3\u51b3\u65b9\u6848\u6587\u6863\u4e2d\u7684[\u4fe1\u9053\u670d\u52a1][tunnel-service-wiki]\u4e86\u89e3\u66f4\u591a\u89e3\u51b3\u65b9\u6848\u4e2d\u5173\u4e8e\u9274\u6743\u670d\u52a1\u7684\u6280\u672f\u8d44\u6599\u3002\n\n\n## \u5728DEMO\u57fa\u7840\u4e0a\u8fdb\u884c\u5f00\u53d1\n\u5c06\u7f16\u8bd1\u597d\u7684\u6587\u4ef6\u653e\u5230  /var/lib/tomcat/webapps/ \u76ee\u5f55\u4e0b\n\ntomcat\u4ee3\u7801\u914d\u7f6e    /etc/tomcat\n\n\u91cd\u542ftomcat\u670d\u52a1    service tomcat restart\n\n\n\n\n## \u53cd\u9988\u548c\u8d21\u732e\n\n\u5982\u6709\u95ee\u9898\uff0c\u6b22\u8fce\u4f7f\u7528 [Issues][new-issue] \u63d0\u51fa\uff0c\u4e5f\u6b22\u8fce\u5e7f\u5927\u5f00\u53d1\u8005\u7ed9\u6211\u4eec\u63d0 [Pull Request][pr]\u3002\n\n[wafer]: https://github.com/tencentyun/wafer \"\u67e5\u770b Wafer \u6839\u9879\u76ee\"\n[sdk-download]: https://github.com/tencentyun/wafer-java-server-sdk/archive/master.zip \"\u4e0b\u8f7d Java SDK \u6e90\u7801\"\n[api-url]: https://tencentyun.github.io/wafer-java-server-sdk/api/ \"\u67e5\u770b Java SDK API \u6587\u6863\"\n[sdk-config-wiki]: https://github.com/tencentyun/wafer/wiki/%E6%9C%8D%E5%8A%A1%E7%AB%AF-SDK-%E9%85%8D%E7%BD%AE \"\u67e5\u770b\u670d\u52a1\u7aef SDK \u914d\u7f6e\"\n[session-service-wiki]: https://github.com/tencentyun/wafer/wiki/\u4f1a\u8bdd\u670d\u52a1 \"\u67e5\u770b\u5173\u4e8e\u4f1a\u8bdd\u670d\u52a1\u7684\u66f4\u591a\u8d44\u6599\"\n[tunnel-service-wiki]: https://github.com/tencentyun/wafer/wiki/\u4fe1\u9053\u670d\u52a1 \"\u67e5\u770b\u5173\u4e8e\u4fe1\u9053\u670d\u52a1\u7684\u66f4\u591a\u8d44\u6599\"\n[login-api]: https://tencentyun.github.io/wafer-java-server-sdk/api/com/qcloud/weapp/authorization/LoginService.html#login-- \"\u67e5\u770b LoginService::login() \u65b9\u6cd5 API\"\n[check-api]: https://tencentyun.github.io/wafer-java-server-sdk/api/com/qcloud/weapp/authorization/LoginService.html#check-- \"\u67e5\u770b LoginService::ckeck() \u65b9\u6cd5 API\"\n[tunnel-handler-api]: https://tencentyun.github.io/wafer-java-server-sdk/api/com/qcloud/weapp/tunnel/TunnelHandler.html \"\u67e5\u770b TunnelHandler \u63a5\u53e3 API \u6587\u6863\"\n[chat-handler-source]: https://github.com/tencentyun/wafer-java-server-sdk/blob/master/com.qcloud.weapp.demo/src/com/qcloud/weapp/demo/ChatTunnelHandler.java \"\u67e5\u770b ChatTunnelHandler \u793a\u4f8b\u4ee3\u7801\"\n\n[new-issue]: https://github.com/CFETeam/wafer-server-sdk-csharp/issues/new \"\u53cd\u9988\u5efa\u8bae\u548c\u95ee\u9898\"\n[pr]: https://github.com/CFETeam/wafer-server-sdk-csharp/pulls \"\u521b\u5efa Pull Request\"\n\n## LICENSE\n\n[MIT](LICENSE)\n"
 },
 {
  "repo": "szhnet/kcp-netty",
  "language": "Java",
  "readme_contents": "# kcp-netty\n\n![Build Status][1] [![Powered][3]][4] [![Powered][5]][6] [![MIT licensed][7]][8]\n\nJava implementation of KCP based on Netty\n\nTo add a dependency using Maven:\n```xml\n<dependency>\n    <groupId>io.jpower.kcp</groupId>\n    <artifactId>kcp-netty</artifactId>\n    <version>1.5.0</version>\n</dependency>\n```\n\n## How to use\nYou can find the examples in the [examples][20] directory.\n\n[1]: https://github.com/szhnet/kcp-netty/actions/workflows/maven.yml/badge.svg\n[3]: https://img.shields.io/badge/KCP-Powered-blue.svg\n[4]: https://github.com/skywind3000/kcp\n[5]: https://img.shields.io/badge/Netty-Powered-blue.svg\n[6]: https://netty.io\n[7]: https://img.shields.io/badge/license-MIT-yellow.svg\n[8]: https://github.com/szhnet/kcp-netty/blob/master/LICENSE\n[20]: https://github.com/szhnet/kcp-netty/tree/master/kcp-example/src/main/java/io/jpower/kcp/example\n"
 },
 {
  "repo": "electrojustin/triad-decompiler",
  "language": "C",
  "readme_contents": "Triad decompiler version 0.4 Alpha Test.\n\nNot intended to be used for copyright infringement or other illegal activities.\n\nWhat is triad:\nTRiad Is A Decompiler\nTriad is a tiny, free and open source, Capstone based x86 decompiler that will\ntake in ELF files as input and spit out pseudo-C.\n\nInstallation:\nTriad requires Capstone to be installed first.\nhttp://www.capstone-engine.org/\nFor 32 bit tests, gcc-multilib is also required.\n\nFirst, it will be necessary to build triad. \"make triad\" should suffice.\nAfter its components are built, the triad binary will be placed in the build \ndirectory. To copy the binary into /usr/bin, simply use \"sudo make install.\"\n\nUsage: triad <flags> <file name> <(optional)start address> \n<(optional) cutoff address>\n\nSimply run the triad binary from the command line and specify an ELF to \ndecompile as a parameter. By default, triad will try to find the main function \nof the given file and start decompiling from there.\n\nSometimes ELFs have all symbols stripped, so triad will be unable to find main.\nIn such a scenario, the user may simply specify a starting address as the second\ncommand line parameter. But, an incorrect starting address will likely result in\nincorrect decompilation or no decompilation.\n\nOccasionally it is ambiguous as to where a function actually ends. If a user\nthinks he/she knows better where a particular function ends than triad and has\nspecified a start address, he/she can specify a cutoff address. The default\ncutoff address is the end of the segment containing the entry point.\n\nTriad has the ability to follow function calls and automatically decompile\ncallees. This is especially helpful when dealing with stripped binaries or other\nbinaries in which relevant code isn't clearly distinguishable from data.\n\nFlags:\n\n\t-f: Full decompilation. This is the default.\n\n\t-p: Partial decompilation. Recovered control flow is always going to be\n\tbad, so Triad has an option to only partially decompile code. This\n\tmeans Triad will identify variables and parameters, try to recover\n\tcalling convention, and translate most instructions back into their C\n\toperator equivalents, but Triad will leave jumps and comparisons as is\n\twith the philosophy that the user knows best how to follow them.\n\n\t-d: Disassemble. Make no attempt to decompile code, simply print out a\n\tdisassembly in AT&T syntax.\n\n\t-s: Disable call following, just decompile main/whatever code was at the\n\tspecified address.\n\n\t-h: Print all constants in hexadecimal format.\n\nLimitations PLEASE READ BEFORE SUBMITTING A BUG REPORT:\nTriad really only works on x86 and x86_64 ELF executables. Other architectures\nmay be possible in the future, but there are currently no plans to add them.\n\nThe triad decompiler is still very much an alpha. The project is nowhere near\ncompletion and as such is missing some critical features, contains numerous\nbugs, has several odd quirks, and has a propensity for segfaulting.\n\nMissing features include support for switch decompilation and full support for\nstrings and statically allocated arrays (dynamically allocated arrays will\nactually probably work to one degree or another, but the syntax will be\nmost unusual e.g. *(char*)(eax + (12)) = 96 instead of array[12] = 'a').\nStruct analysis will be a long ways a way as well, and unions may never work\nproperly.\n\nThe only supported binary format currently supported is the Executable and\nLinkable Format (ELF), commonly used on UNIX like systems, such as LINUX.\n\nControl flow decompilation should be mostly correct, but it may look funky.\nContinues, and forward gotos inside of conditional statements might wind up as\nif-else statements. This is actually semantically equivalent, just different\nfrom original source.\n\nOptimization and computed jumps will probably cause a program to be decompiled\ncompletely incorrectly.\n\nTriad was designed and tested for programs compiled using gcc.\n\nIt is important to understand that the generated source code will NEVER be\nexactly the original source (unless the program was compiled with debug \nsymbols, of course).\n\nIf triad segfaults on you, feel free to tell me. Include a stack trace and a\ndescription of the conditions that triggered the crash if at all possible.\nFor obvious reasons, it is quite important that triad crash as little as\npossible.\n\n\"Hacking\"/Modding notes:\nI will be honest, the code is a bit of a mess. It is a short mess,\nprobably less than 2 KLOC, but the amount of pointer arithmetic and number of\nglobals used is not for the faint of heart.\n\nThat said, feel free to \"hack\" in features! The license is just MIT, so do \nwhatever. Feel free to contact me if you have any questions about how the code\nworks or think you have a cool feature that should be merged into the codebase.\nI tried to document the source, but I'm sure certain lines will leave many \nprogrammers confused and/or horrified.\n\nMy email is just electrojustin@gmail.com\n"
 },
 {
  "repo": "sonots/cumo",
  "language": "C",
  "readme_contents": "# Cumo\n\nCumo (pronounced \"koomo\") is a CUDA-aware, GPU-optimized numerical library that offers a significant performance boost over [Ruby Numo](https://github.com/ruby-numo), while (mostly) maintaining drop-in compatibility.\n\n<img src=\"https://raw.githubusercontent.com/sonots/cumo-logo/master/logo_transparent.png\" alt=\"cumo logo\" title=\"cumo logo\" width=\"50%\">\n\n## Requirements\n\n* Ruby 2.5 or later\n* NVIDIA GPU Compute Capability 3.5 (Kepler) or later\n* CUDA 9.0 or later\n\n## Preparation\n\nInstall CUDA and set your environment variables as follows:\n\n```bash\nexport CUDA_PATH=\"/usr/local/cuda\"\nexport CPATH=\"$CUDA_PATH/include:$CPATH\"\nexport LD_LIBRARY_PATH=\"$CUDA_PATH/lib64:$CUDA_PATH/lib:$LD_LIBRARY_PATH\"\nexport PATH=\"$CUDA_PATH/bin:$PATH\"\nexport LIBRARY_PATH=\"$CUDA_PATH/lib64:$CUDA_PATH/lib:$LIBRARY_PATH\"\n```\n\nTo use cuDNN features, install cuDNN and set your environment variables as follows:\n\n```\nexport CUDNN_ROOT_DIR=/path/to/cudnn\nexport CPATH=$CUDNN_ROOT_DIR/include:$CPATH\nexport LD_LIBRARY_PATH=$CUDNN_ROOT_DIR/lib64:$LD_LIBRARY_PATH\nexport LIBRARY_PATH=$CUDNN_ROOT_DIR/lib64:$LIBRARY_PATH\n```\n\nFYI: I use [cudnnenv](https://github.com/unnonouno/cudnnenv) to install cudnn under my home directory like `export CUDNN_ROOT_DIR=/home/sonots/.cudnn/active/cuda`.\n\n## Installation\n\nAdd the following line to your Gemfile:\n\n```ruby\ngem 'cumo'\n```\n\nAnd then execute:\n\n    $ bundle\n\nOr install it yourself as:\n\n    $ gem install cumo\n\n## How To Use\n\n### Quick start\n\nAn example:\n\n```ruby\n[1] pry(main)> require \"cumo/narray\"\n=> true\n[2] pry(main)> a = Cumo::DFloat.new(3,5).seq\n=> Cumo::DFloat#shape=[3,5]\n[[0, 1, 2, 3, 4],\n [5, 6, 7, 8, 9],\n [10, 11, 12, 13, 14]]\n[3] pry(main)> a.shape\n=> [3, 5]\n[4] pry(main)> a.ndim\n=> 2\n[5] pry(main)> a.class\n=> Cumo::DFloat\n[6] pry(main)> a.size\n=> 15\n```\n\n### Switching from Numo to Cumo\n\nThe following find-and-replace should just work:\n\n```\nfind . -type f | xargs sed -i -e 's/Numo/Cumo/g' -e 's/numo/cumo/g'\n```\n\nIf you want to dynamically switch between Numo and Cumo, something like the following will work:\n\n```ruby\nif gpu\n  require 'cumo/narray'\n  xm = Cumo\nelse\n  require 'numo/narray'\n  xm = Numo\nend\n\na = xm::DFloat.new(3,5).seq\n```\n\n### Incompatibility With Numo\n\nThe following methods behave incompatibly with Numo by default for performance reasons:\n\n* `extract`\n* `[]`\n* `count_true`\n* `count_false`\n\nNumo returns a Ruby numeric object for 0-dimensional NArray, while Cumo returns the 0-dimensional NArray instead of a Ruby numeric object.\nCumo differs in this way to avoid synchronization and minimize CPU \u21c4 GPU data transfer.\n\nSet the `CUMO_COMPATIBLE_MODE` environment variable to `ON` to force Numo NArray compatibility (for worse performance).\n\nYou may enable or disable `compatible_mode` as:\n\n```\nrequire 'cumo'\nCumo.enable_compatible_mode # enable\nCumo.compatible_mode_enabled? #=> true\nCumo.disable_compatible_mode # disable\nCumo.compatible_mode_enabled? #=> false\n```\n\nYou can also use the following methods which behave like Numo's NArray methods. The behavior of these methods does not depend on `compatible_mode`.\n\n* `extract_cpu`\n* `aref_cpu(*idx)`\n* `count_true_cpu`\n* `count_false_cpu`\n\n### Select a GPU device ID\n\nSet the `CUDA_VISIBLE_DEVICES=id` environment variable, or\n\n```\nrequire 'cumo'\nCumo::CUDA::Runtime.cudaSetDevice(id)\n```\n\nwhere `id` is an integer.\n\n### Disable GPU Memory Pool\n\nGPU memory pool is enabled by default. To disable it, set `CUMO_MEMORY_POOL=OFF`, or:\n\n```\nrequire 'cumo'\nCumo::CUDA::MemoryPool.disable\n```\n\n## Documentation\n\nSee https://github.com/ruby-numo/numo-narray#documentation, replacing Numo with Cumo.\n\n## Contributions\n\nThis project is under active development. See [issues](https://github.com/sonots/cumo/issues) for future works.\n\n## Development\n\nInstall ruby dependencies:\n\n```\nbundle install --path vendor/bundle\n```\n\nCompile:\n\n```\nbundle exec rake compile\n```\n\nRun tests:\n\n```\nbundle exec rake test\n```\n\nGenerate docs:\n\n```\nbundle exec rake docs\n```\n\n## Advanced Development Tips\n\n### ccache\n\n[ccache](https://ccache.samba.org/) would be useful to speedup compilation time.\nInstall ccache and configure with:\n\n\n```bash\nexport PATH=\"$HOME/opt/ccache/bin:$PATH\"\nln -sf \"$HOME/opt/ccache/bin/ccache\" \"$HOME/opt/ccache/bin/gcc\"\nln -sf \"$HOME/opt/ccache/bin/ccache\" \"$HOME/opt/ccache/bin/g++\"\nln -sf \"$HOME/opt/ccache/bin/ccache\" \"$HOME/opt/ccache/bin/nvcc\"\n```\n\n### Build in parallel\n\nSet `MAKEFLAGS` to specify `make` command options. You can build in parallel as:\n\n```\nbundle exec env MAKEFLAG=-j8 rake compile\n```\n\n### Specify nvcc --generate-code options\n\n```\nbundle exec env CUMO_NVCC_GENERATE_CODE=arch=compute_60,code=sm_60 rake compile\n```\n\nThis is useful even on development because it makes it possible to skip JIT compilation of PTX to cubin during runtime.\n\n### Run tests with gdb\n\nCompile with debugging enabled:\n\n```\nbundle exec DEBUG=1 rake compile\n```\n\nRun tests with gdb:\n\n```\nbundle exec gdb -x run.gdb --args ruby test/narray_test.rb\n```\n\nYou may put a breakpoint by calling `cumo_debug_breakpoint()` at C source codes.\n\n### Run tests only a specific line\n`--location` option is available as:\n\n```\nbundle exec ruby test/narray_test.rb --location 121\n```\n\n### Compile and run tests only a specific type\n\n`DTYPE` environment variable is available as:\n\n```\nbundle exec DTYPE=dfloat rake compile\n```\n\n```\nbundle exec DTYPE=dfloat ruby test/narray_test.rb\n```\n\n### Run program always synchronizing CPU and GPU\n\n```\nbundle exec CUDA_LAUNCH_BLOCKING=1\n```\n\n### Show GPU synchronization warnings\n\nCumo shows warnings if CPU and GPU synchronization occurs if:\n\n```\nexport CUMO_SHOW_WARNING=ON\n```\n\nBy default, Cumo shows warnings that occurred at the same place only once.\nTo show all, multiple warnings, set:\n\n```\nexport CUMO_SHOW_WARNING=ON\nexport CUMO_SHOW_WARNING_ONCE=OFF\n```\n\n## Contributing\n\nBug reports and pull requests are welcome on GitHub at https://github.com/sonots/cumo.\n\n## License\n\n* [LICENSE.txt](./LICENSE.txt)\n* [3rd_party/LICENSE.txt](./3rd_party/LICENSE.txt)\n\n## Related Materials\n\n* [Fast Numerical Computing and Deep Learning in Ruby with Cumo](https://speakerdeck.com/sonots/fast-numerical-computing-and-deep-learning-in-ruby-with-cumo) - Presentation Slide at [RubyKaigi 2018](https://rubykaigi.org/2018/presentations/sonots.html#may31)\n"
 },
 {
  "repo": "mcauser/BLACK_F407VE",
  "language": "C",
  "readme_contents": "# MCUDev Black STM32F407VET6\n\nMicroPython board definition files for the MCUDev black STM32F407VET6 dev board.\n\n**Brand:** MCUDev\n\n**Markings:** STM32F4XX STM32_F4VE V2.0 1509\n\n![board](docs/STM32F407VET6.jpg)\n\nYou can buy one for around $16 AUD (Oct 2019) on [AliExpress].\n\n### Build the firmware\n\nClone the board definitions to your [MicroPython](https://github.com/micropython/micropython) `ports/stm32/boards` folder.\n\n```bash\ncd micropython/ports/stm32/boards\ngit clone https://github.com/mcauser/BLACK_F407VE.git\n\ncd ..\nmake BOARD=BLACK_F407VE\n```\n\n### Flashing via DFU\n\nThis board can be flashed using DFU. To put the board in DFU mode, disconnect\nUSB, set BOOT0 to ON by connecting pin BT0 to 3V3 and reconnect USB.\n\nNow you can flash the board using USB with the command:\n\n```bash\nmake BOARD=BLACK_F407VE deploy\n```\n\nOnce the upload is complete, disconnect USB, set BOOT0 to OFF by connecting\npin BT0 to GND and reconnect USB.\n\nAlternatively, you can use the MicroPython command `pyb.bootloader()`\nto get into DFU mode without needing to use the switch.\n\nCurrently, you need to unplug and replug the board in order to switch from DFU\nmode back to regular mode.\n\n### Accessing the board\n\nOnce built and deployed, you can access the MicroPython REPL (the Python prompt) via USB serial.\n\n```bash\nscreen /dev/tty.usbmodem1422 115200\n# or\nscreen /dev/ttyACM0 115200\n```\n\n### Specifications\n\n* STM32F407VET6 ARM Cortex M4\n* 168MHz, 210 DMIPS / 1.25 DMIPS / MHz\n* 1.8V - 3.6V operating voltage\n* 8MHz system crystal\n* 32.768KHz RTC crystal\n* 2.54mm pitch pins\n* JTAG/SWD header\n* 512 KByte Flash, 192 + 4 KByte SRAM\n* 3x SPI, 3x USART, 2x UART, 2x I2S, 3x I2C\n* 1x FSMC, 1x SDIO, 2x CAN\n* 1x USB 2.0 FS / HS controller (with dedicated DMA)\n* 1x USB HS ULPI (for external USB HS PHY)\n* Micro SD\n* Winbond W25Q16 16Mbit SPI Flash\n* RTC battery CR1220\n* 1x 10/100 Ethernet MAC\n* 1x 8 to 12-bit Parallel Camera interface\n* 3x ADC (12-bit / 16-channel)\n* 2x DAC (12-bit)\n* 12x general timers, 2x advanced timers\n* AMS1117-3.3V: 3.3V LDO voltage regulator, max current 800mA\n* Micro USB for power and comms\n* Red power LED D1\n* Red user LED D2 (PA6) active low\n* Red user LED D3 (PA7) active low\n* 2x jumpers for bootloader selection\n* Reset button, Wakeup button, 2x user buttons K0 (PE4) and K1 (PE3)\n* 2x24 side pins + 2x16 bottom pins + 1x4 ISP pins\n* 2x16 FMSC LCD Interface\n* NRF24L01 socket\n* M3 mounting holes\n* Dimensions: 85.1mm x 72.45mm\n\n### Exposed Port Pins\n\n* PA0-PA15\n* PB0-PB15\n* PC0-PC13 (PC14 OSC32_IN and PC15 OSC32_OUT not broken out)\n* PD0-PD15\n* PE0-PE15\n\n### Peripherals\n\n#### TFT (J1)\n\n* 1 GND\n* 2 RST\n* 3 PD10 FSMC_D15\n* 4 PD9 FSMC_D14\n* 5 PD8 FSMC_D13\n* 6 PE15 FSMC_D12\n* 7 PE14 FSMC_D11\n* 8 PE13 FSMC_D10\n* 9 PE12 FSMC_D9\n* 10 PE11 FSMC_D8\n* 11 PE10 FSMC_D7\n* 12 PE9 FSMC_D6\n* 13 PE8 FSMC_D5\n* 14 PE7 FSMC_D4\n* 15 PD1 FSMC_D3\n* 16 PD0 FSMC_D2\n* 17 PD15 FSMC_D1\n* 18 PD14 FSMC_D0\n* 19 PD4 FSMC_NOE\n* 20 PD5 FSMC_NWE\n* 21 PD13 FSMC_A18\n* 22 PD7 FSMC_NE1\n* 23 PB13 T_SCK\n* 24 PB12 T_CS\n* 25 PB15 T_MOSI\n* 26 PB14 T_MISO\n* 27 PC5 T_PEN\n* 28 PB1 LCD_BL\n* 29 NC\n* 30 GND\n* 31 3V3\n* 32 GND\n\n#### SPI Flash W25Q16 (U3)\n\n* 1 PB0 F_CS\n* 2 PB4 SPI1_MISO\n* 3 WP 3V3\n* 4 GND\n* 5 PB5 SPI1_MOSI\n* 6 PB3 SPI1_SCK\n* 7 HOLD 3V3\n* 8 VCC 3V3\n\n#### JTAG/SWD debug (P1)\n\n* 1 3V3 Vref\n* 2 3V3 Vsupply\n* 3 PB4 TRST\n* 4 GND\n* 5 PA15 TDI\n* 6 GND\n* 7 PA13 TMS\n* 8 GND\n* 9 PA14 TCK\n* 10 GND\n* 11 NC RTCK\n* 12 GND\n* 13 PB3 TDO\n* 14 GND\n* 15 RST\n* 16 GND\n* 17 NC\n* 18 GND\n* 19 NC\n* 20 GND\n\n#### ISP (J6)\n\n* 1 5V\n* 2 GND\n* 3 PA10 RXD1\n* 4 PA9 TXD1\n\n#### USB (J4)\n\n* 1 VCC 5V\n* 2 PA11 USB_DM\n* 3 PA12 USB_DP\n* 4 NCC\n* 5 GND\n\n#### Micro SD (U5)\n\n* 1 PC10 SDIO_D2\n* 2 PC11 SDIO_D3\n* 3 PD2 SDIO_CMD\n* 4 3V3\n* 5 PC12 SDIO_SCK\n* 6 GND\n* 7 PC8 SDIO_D0\n* 8 PC9 SDIO_D1\n* 9 GND\n\n#### NRF24L01 (JP2)\n\n* 1 GND\n* 2 3V3\n* 3 PB6 NRF_CE\n* 4 PB7 NRF_CS\n* 5 PB3 SPI1_SCK\n* 6 PB5 SPI1_MOSI\n* 7 PB4 SPI1_MISO\n* 8 PB8 NRF_IRQ\n\n#### User Button (K0)\n\n* PE4 KEY0, active low\n\n#### User Button (K1)\n\n* PE3 KEY1, active low\n\n#### User Button (WK_UP)\n\n* PA0 WK_UP, active high\n\n#### User LED (D2)\n\n* PA6 LED0\n\n#### User LED (D3)\n\n* PA7 LED1\n\n#### Battery (Q1)\n\n* 1 BAT54C\n* 2 3V3\n* 3 GND VBAT\n\n### Links\n\n* [STM32F407VE on st.com](http://www.st.com/content/st_com/en/products/microcontrollers/stm32-32-bit-arm-cortex-mcus/stm32-high-performance-mcus/stm32f4-series/stm32f407-417/stm32f407ve.html)\n* Buy on [AliExpress] or search for \"STM32F407VET6\"\n* Buy on [Taobao](https://item.taobao.com/item.htm?id=523083108350)\n* [STM32F407VET6 datasheet](https://github.com/mcauser/BLACK_F407VE/blob/master/docs/STM32F407VET6_datasheet.pdf)\n* [STM32F407VET6 schematics](https://github.com/mcauser/BLACK_F407VE/blob/master/docs/STM32F407VET6_schematics.pdf)\n\n### Related boards\n\n* [MCUDev Black STM32F407VET6](https://github.com/mcauser/BLACK_F407VE) - this board\n* [MCUDev Black STM32F407ZET6](https://github.com/mcauser/BLACK_F407ZE)\n* [MCUDev Black STM32F407ZGT6](https://github.com/mcauser/BLACK_F407ZG)\n* [MCUDev DevEBox STM32F407VET6](https://github.com/mcauser/MCUDEV_DEVEBOX_F407VET6)\n* [MCUDev DevEBox STM32F407VGT6](https://github.com/mcauser/MCUDEV_DEVEBOX_F407VGT6)\n* [VCC GND STM32F407VET6 Mini](https://github.com/mcauser/VCC_GND_F407VE)\n* [VCC GND STM32F407ZGT6 Mini](https://github.com/mcauser/VCC_GND_F407ZG)\n\n[AliExpress]: https://www.aliexpress.com/item/32618222721.html\n\n## License\n\nLicensed under the [MIT License](http://opensource.org/licenses/MIT).\n"
 },
 {
  "repo": "ShyykoSerhiy/gfm-plugin",
  "language": "C",
  "readme_contents": "# Deprecated\nYou should check out https://github.com/Hinaser/gfm-advanced instead.\n\n# gfm-plugin\nGitHub markdown plugin for intellij platform.\n\n### Features\n\n*   GitHub flavored preview.\n*   Full offline support.\n*   Plays well with other markdown plugins.\n\n### License\n\n[MIT License](http://opensource.org/licenses/mit-license.php).\n\nMany thanks to the [TeamDev](http://www.teamdev.com) for providing a free license for [JxBrowser](http://www.teamdev.com/jxbrowser), an amazing lightweight Chromium-based Swing/JavaFX component.\n"
 },
 {
  "repo": "HKUST-KnowComp/JWE",
  "language": "C",
  "readme_contents": "# JWE\nSource codes of our EMNLP2017 paper [Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components](http://www.cse.ust.hk/~yqsong/papers/2017-EMNLP-ChineseEmbedding.pdf)\n\n## Preparation\nYou need to prepare a training corpus and the Chinese subcharacter radicals or components. \n* Training corpus. Download [Chinese Wikipedia Dump](http://download.wikipedia.com/zhwiki).\nPreprocess the corpus. First, use wiki extractor or python gensim.corpora to extract plain text from the xml file; Second, remove non Chinese characters by using regular expression; Third, use opencc to convert traditional Chinese to simplified Chinese; Finally, perform Chinese word segmentation by using THULAC package. We provide the corpus after preprocessing at the onlibe baidu [box](https://pan.baidu.com/s/1jINyG6q).\n\n* Subcharacter radicals and components.  Deploy the scrapy codes in `JWE/ChineseCharCrawler` on [Scrapy Cloud](https://scrapinghub.com), you can crawl the resource from [HTTPCN](http://tool.httpcn.com/zi/). We provide a copy of the data in `./subcharacters` for reserach convenience. The copyright and all rights therein of the subcharacter data are reserved by the website [HTTPCN](http://tool.httpcn.com/zi/). \n\n## Model Training\n- `cd JWE/src`, compile the code by `make all`. \n- run `./jwe` for parameters details.\n- run `./run.sh` to start the model training, you may modify the parameters in file `run.sh`.\n- Input files format:\nCorpus `wiki.txt` contains segmented Chinese words with UTF-8  encoding;\nSubcharacters `comp.txt` contains a list of components which are seperated by blank spaces; `char2comp.txt`, each line consists of a Chinese character and its components in the following format:\n\n```\n\u4fa9 \u4ebb \u4eba \u4e91\n\u4fa8 \u4ebb \u4e54\n\u4fa7 \u4ebb \u8d1d \u5202\n\u4fa6 \u4ebb \u535c \u8d1d\n```\n\n## Model Evaluation\n\nTwo Chinese word similarity datasets `240.txt` and `297.txt` and one Chinese analogy dataset `analogy.txt` in `JWE/evaluation` folder are provided by [(Chen et al., IJCAI, 2015)](https://github.com/Leonard-Xu/CWE/tree/master/data).\n\ncd `JWE/src`, then \n- run `python word_sim.py -s <similarity_file> -e <embed_file>` for word similarity evaluation, where `similarity_file` is the word similarity file, e.g., `240.txt` or `297.txt`, `embed_file` is the trained word embedding file.\n- run `python word_analogy.py -a <analogy_file> -e <embed_file>` or `./word_analogy <embed_file> <analogy_file>` for word analogy evaluation.\n"
 },
 {
  "repo": "zlargon/lssdp",
  "language": "C",
  "readme_contents": "# lssdp\nlight weight SSDP library\n\n### What is SSDP\n\nThe Simple Service Discovery Protocol (SSDP) is a network protocol based on the Internet Protocol Suite for advertisement and discovery of network services and presence information.\n\n====\n\n### Support Platform\n\n* Linux Ubuntu\n* MAC OSX\n* Android\n* iOS\n\n====\n\n### How To Build and Test\n\n```\nmake clean\nmake\n\ncd test\n./daemon.exe\n```\n\n====\n\n#### lssdp_ctx:\n\nlssdp context\n\n**port** - SSDP UDP port, 1900 port is general.\n\n**sock** - SSDP socket, created by `lssdp_socket_create`, and close by `lssdp_socket_close`\n\n**neighbor_list** - neighbor list, when received *NOTIFY* or *RESPONSE* packet, neighbor list will be updated.\n\n**neighbor_timeout** - this value will be used by `lssdp_neighbor_check_timeout`. If neighbor is timeout, then remove from neighbor list.\n\n**debug** - SSDP debug mode, show debug message.\n\n**interface** - Network Interface list. Call `lssdp_network_interface_update` to update the list.\n\n**interface_num** - the number of Network Interface list.\n\n**header.search_target** - SSDP Search Target (ST). A potential search target.\n\n**header.unique_service_name** - SSDP Unique Service Name (USN). A composite identifier for the advertisement.\n\n**header.location = prefix + domain + suffix** - [http://] + IP + [:PORT/URI]\n\n**header.sm_id** - Optional field.\n\n**header.device_type** - Optional field.\n\n**network_interface_changed_callback** - when interface is changed, this callback would be invoked.\n\n**neighbor_list_changed_callback** - when neighbor list is changed, this callback would be invoked.\n\n**packet_received_callback** - when received any SSDP packet, this callback would be invoked. It callback is usally used for debugging.\n\n====\n\n#### Function API (8)\n\n##### 01. lssdp_network_interface_update\n\nupdate network interface.\n\n```\n- lssdp.interface, lssdp.interface_num will be updated.\n```\n\n\n##### 02. lssdp_socket_create\n\ncreate SSDP socket.\n\n```\n- SSDP port must be setup ready before call this function. (lssdp.port > 0)\n\n- if SSDP socket is already exist (lssdp.sock > 0), the socket will be closed, and create a new one.\n\n- SSDP neighbor list will be force clean up.\n```\n\n##### 03. lssdp_socket_close\n\nclose SSDP socket.\n\n```\n- if SSDP socket <= 0, will be ignore, and lssdp.sock will be set -1.\n- SSDP neighbor list will be force clean up.\n```\n\n##### 04. lssdp_socket_read\n\nread SSDP socket.\n\n```\n1. if read success, packet_received_callback will be invoked.\n\n2. if received SSDP packet is match to Search Target (lssdp.header.search_target),\n   - M-SEARCH: send RESPONSE back\n   - NOTIFY/RESPONSE: add/update to SSDP neighbor list\n```\n\n```\n- SSDP socket and port must be setup ready before call this function. (sock, port > 0)\n- if SSDP neighbor list has been changed, neighbor_list_changed_callback will be invoked.\n```\n\n##### 05. lssdp_send_msearch\n\nsend SSDP M-SEARCH packet to multicast address (239.255.255.250)\n\n```\n- SSDP port must be setup ready before call this function. (lssdp.port > 0)\n```\n\n##### 06. lssdp_send_notify\n\nsend SSDP NOTIFY packet to multicast address (239.255.255.250)\n\n```\n- SSDP port must be setup ready before call this function. (lssdp.port > 0)\n```\n\n##### 07. lssdp_neighbor_check_timeout\n\ncheck neighbor in list is timeout or not. (lssdp.neighbor_timeout)\n\nthe timeout neighbor will be remove from the list.\n\n```\n- if neighbor be removed, neighbor_list_changed_callback will be invoked.\n```\n\n##### 08. lssdp_set_log_callback\n\nsetup SSDP log callback. All SSDP library log will be forward to here.\n"
 },
 {
  "repo": "NSProgrammer/ZipUtilities",
  "language": "C",
  "readme_contents": "# ZipUtilities\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/NSProgrammer/ZipUtilities/master/LICENSE.md) [![CocoaPods compatible](https://img.shields.io/cocoapods/v/ZipUtilities.svg?style=flat)](https://CocoaPods.org) [![Carthage compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat)](https://github.com/Carthage/Carthage)\n\n![Targets](https://img.shields.io/badge/Targets-OSX.Framework_iOS.Framework_iOS.lib-lightgrey.svg)\n\n[![Build Status](https://travis-ci.org/NSProgrammer/ZipUtilities.svg?branch=master)](https://travis-ci.org/NSProgrammer/ZipUtilities)\n\n## Introduction\n\n*ZipUtilities*, prefixed with `NOZ` for _Nolan O'Brien ZipUtilities_, is a library of zipping and unzipping utilities for iOS and Mac OS X.\n\n## Background\n\nThe need can occasionally arise where easy compressing and decompressing of data is desired from a simple API.  There are many zipping/unzipping utilities out there but all of them can be found wanting in some way or another.\n\n- too low level\n- too complex\n- poor code quality\n- old coding practices/style (basically really old)\n- lack a service oriented approach (request, operation and response pattern)\n\nThe goal is to provide an easy to use modern interface for archiving and unarchiving zip files. As a particular focus, providing a service oriented approach can provide powerful support for NSOperation composition.\n\n## Install\n\nThe _ZipUtilities_ Xcode project has targets to build iOS and OSX dynamic frameworks. You can build these and add them to your project manually, or add a subproject in Xcode.\n\nAlternatively you may use one of the following dependency managers:\n\n#### CocoaPods\n\nAdd _ZipUtilities_ to your `Podfile`\n\n```ruby\npod 'ZipUtilities', '~> 1.13.0'\n```\n\n#### Carthage\n\nAdd _ZipUtilities_ to your `Cartfile`\n\n```ruby\ngithub \"NSProgrammer/ZipUtilities\"\n```\n\n## Documentation\n\nYou can either build the documentation with appledoc locally or you can visit the documentation online at http://cocoadocs.org/docsets/ZipUtilities\n\n## Overview\n\n### Service Oriented Interfaces (NSOperations)\n\nThe primary value of _ZipUtilities_ is that it provides an easy to use interface for archiving data or files into a single zip archive and unarchiving a zip archive to the contained files.  The primary approach for _ZipUtilities_ is to provide a service oriented pattern for compressing and decompressing.\n\n**`NOZCompress.h`**\n\n`NOZCompress.h` contains the service oriented interfaces related to compressing into a zip archive.\n\n- `NOZCompressRequest` is the object that encapsulates the _what_ and _how_ for the compression operation to act upon\n- `NOZCompressOperation` is the `NSOperation` subclass object that performs the compression. By being an `NSOperation`, consumers can take advantage of cancelling, prioritization and dependencies.  Progress is also provided with the operation and can be observed via _KVO_ on the `progress` property or via the delegate callback.\n- `NOZCompressDelegate` is the delegate for the `NOZCompressOperation`. It provides callbacks for progress and completion.\n- `NOZCompressResult` is the object that encapsulates the result of a compress operation. It holds whether or not the operation succeed, the error if it didn't succeed, the path to the created zip archive and other informative metrics like duration and compression ratio.\n\n*Example:*\n\n```obj-c\n- (NSOperation *)startCompression\n{\n\tNOZCompressRequest *request = [[NOZCompressRequest alloc] initWithDestinationPath:self.zipFilePath];\n    [request addEntriesInDirectory:self.sourceDirectoryPath \n                       filterBlock:^BOOL(NSString *filePath) {\n        return [filePath.lastPathComponent hasPrefix:@\".\"];\n    }\n        compressionSelectionBlock:NULL];\n    [request addDataEntry:self.data name:@\"Aesop.txt\"];\n    for (id<NOZZippableEntry> entry in self.additionalEntries) {\n        [request addEntry:entry];\n    }\n\n    NOZCompressionOperation *op = [[NOZCompressOperation alloc] initWithRequest:request delegate:self];\n    [self.operationQueue addOperation:op];\n\n    // return operation so that a handle can be maintained and cancelled if necessary\n    return op;\n}\n\n- (void)compressOperation:(NOZCompressOperation *)op didCompleteWithResult:(NOZCompressResult *)result\n{\n    dispatch_async(dispatch_get_main_queue(), ^{\n\t    self.completionBlock(result.didSuccess, result.operationError);\n    });\n}\n\n- (void)compressOperation:(NOZCompressOperation *)op didUpdateProgress:(float)progress\n{\n\tdispatch_async(dispatch_get_main_queue(), ^{\n\t    self.progressBlock(progress);\n\t});\n}\n```\n\n```swift\nfunc startCompression() -> NSOperation\n{\n    let request = NOZCompressRequest.init(destinationPath: self.zipFilePath)\n    request.addEntriesInDirectory(self.sourceDirectoryPath, filterBlock: { (filePath: String) -> Bool in\n        return ((filePath as NSString).lastPathComponent as NSString).hasPrefix(\".\")\n    }, compressionSelectionBlock: nil)\n    request.addDataEntry(self.data name:\"Aesop.txt\")\n    for entry in self.additionalEntries {\n        request.addEntry(entry)\n    }\n\n    let operation = NOZCompressOperation.init(request: request, delegate: self)\n    zipQueue?.addOperation(operation)\n    \n    // return operation so that a handle can be maintained and cancelled if necessary\n    return operation\n}\n\nfunc compressOperation(op: NOZCompressOperation, didCompleteWithResult result: NOZCompressResult)\n{\n    dispatch_async(dispatch_get_main_queue(), {\n        self.completionBlock(result.didSuccess, result.operationError);\n    })\n}\n\nfunc compressOperation(op: NOZCompressOperation, didUpdateProgress progress: Float)\n{\n    dispatch_async(dispatch_get_main_queue(), {\n        self.progressBlock(progress);\n    })\n}\n```\n\n**`NOZDecompress.h`**\n\n`NOZDecompress.h` contains the service oriented interfaces related to decompressing from a zip archive.\n\n- `NOZDecompressRequest` is the object that encapsulates the _what_ and _how_ for the decompression operation to act upon\n- `NOZDecompressOperation` is the `NSOperation` subclass object that performs the compression. By being an `NSOperation`, consumers can take advantage of cancelling, prioritization and dependencies.  Progress is also provided with the operation and can be observed via _KVO_ on the `progress` property or via the delegate callback.\n- `NOZDecompressDelegate` is the delegate for the `NOZDecompressOperation`. It provides callbacks for progress, overwriting output files and completion.\n- `NOZDecompressResult` is the object that encapsulates the result of a compress operation. It holds whether or not the operation succeed, the error if it didn't succeed, the paths to the output unarchived files and other informative metrics like duration and compression ratio.\n\n*Example:*\n\n```obj-c\n- (NSOperation *)startDecompression\n{\n    NOZDecompressRequest *request = [[NOZDecompressRequest alloc] initWithSourceFilePath:self.zipFilePath];\n\n    NOZDecompressOperation *op = [[NOZDecompressOperation alloc] initWithRequest:request delegate:self];\n    [self.operationQueue addOperation:op];\n\n    // return operation so that a handle can be maintained and cancelled if necessary\n    return op;\n}\n\n- (void)decompressOperation:(NOZDecompressOperation *)op didCompleteWithResult:(NOZDecompressResult *)result\n{\n    dispatch_async(dispatch_get_main_queue(), ^{\n\t    self.completionBlock(result.didSuccess, result.destinationFiles, result.operationError);\n    });\n}\n\n- (void)decompressOperation:(NOZDecompressOperation *)op didUpdateProgress:(float)progress\n{\n\tdispatch_async(dispatch_get_main_queue(), ^{\n\t    self.progressBlock(progress);\n\t});\n}\n```\n\n```swift\nfunc startDecompression() -> NSOperation\n{\n    let request = NOZDecompressRequest.init(sourceFilePath: self.zipFilePath)\n    let operation = NOZDecompressOperation.init(request: request, delegate: self)\n    zipQueue?.addOperation(operation)\n    return operation\n}\n\nfunc decompressOperation(op: NOZDecompressOperation, didCompleteWithResult result: NOZDecompressResult)\n{\n    dispatch_async(dispatch_get_main_queue(), {\n        self.completionBlock(result.didSuccess, result.destinationFiles, result.operationError);\n    })\n}\n\nfunc decompressOperation(op: NOZDecompressOperation, didUpdateProgress progress: Float)\n{\n    dispatch_async(dispatch_get_main_queue(), {\n        self.progressBlock(progress);\n    })\n}\n```\n\n### Manual Zipping and Unzipping\n\nAdditional, the underlying objects for zipping and unzipping are exposed for direct use if NSOperation support is not needed.\n\n**`NOZZipper.h`**\n\n`NOZZipper` is an object that encapsulates the work for zipping sources (NSData, streams and/or\nfiles) into an on disk zip archive file.\n\n*Example:*\n\n```obj-c\n- (BOOL)zipThingsUpAndReturnError:(out NSError **)error\n{\n    NOZZipper *zipper = [[NOZZipper alloc] initWithZipFile:pathToCreateZipFile];\n    if (![zipper openWithMode:NOZZipperModeCreate error:error]) {\n        return NO;\n    }\n\n    __block int64_t totalBytesCompressed = 0;\n\n    NOZFileZipEntry *textFileZipEntry = [[NOZFileZipEntry alloc] initWithFilePath:textFilePath];\n    textFileZipEntry.comment = @\"This is a heavily compressed text file.\";\n    textFileZipEntry.compressionLevel = NOZCompressionLevelMax;\n\n    NSData *jpegData = UIImageJPEGRepresentation(someImage, 0.8f);\n    NOZDataZipEntry *jpegEntry = [[NOZDataZipEntry alloc] initWithData:jpegData name:@\"image.jpg\"];\n    jpegEntry.comment = @\"This is a JPEG so it doesn't need more compression.\";\n    jpegEntry.compressionMode = NOZCompressionModeNone;\n\n    if (![zipper addEntry:textFileZipEntry\n            progressBlock:^(int64_t totalBytes, int64_t bytesComplete, int64_t bytesCompletedThisPass, BOOL *abort) {\n            totalBytesCompressed = bytesCompletedThisPass;\n          }\n                    error:error]) {\n        return NO;\n    }\n\n    if (![zipper addEntry:jpegEntry\n            progressBlock:^(int64_t totalBytes, int64_t bytesComplete, int64_t bytesCompletedThisPass, BOOL *abort) {\n            totalBytesCompressed = bytesCompletedThisPass;\n         }\n                    error:error]) {\n        return NO;\n    }\n\n    zipper.globalComment = @\"This is a global comment for the entire archive.\";\n    if (![zipper closeAndReturnError:error]) {\n        return NO;\n    }\n\n    int64_t archiveSize = (int64_t)[[[NSFileManager defaultFileManager] attributesOfItemAtPath:zipper.zipFilePath] fileSize];\n    NSLog(@\"Compressed to %@ with compression ratio of %.4f:1\", zipper.zipFilePath, (double)totalBytesCompressed / (double)archiveSize);\n    return YES;\n}\n\n```\n\n**`NOZUnzipper.h`**\n\n`NOZUnzipper` is an object that encapsulates the work for unzipping from a zip archive file on disk\ninto destinations (NSData, streams and/or files).\n\n*Example:*\n\n```obj-c\n- (BOOL)unzipThingsAndReturnError:(out NSError **)error\n{\n    NSAssert(![NSThread isMainThread]); // do this work on a background thread\n\n    NOZUnzipper *unzipper = [[NOZUnzipper alloc] initWithZipFile:zipFilePath];\n    if (![unzipper openAndReturnError:error]) {\n        return NO;\n    }\n\n    if (nil == [unzipper readCentralDirectoryAndReturnError:error]) {\n        return NO;\n    }\n\n    __block NSError *enumError = nil;\n    [unzipper enumerateManifestEntriesUsingBlock:^(NOZCentralDirectoryRecord * record, NSUInteger index, BOOL * stop) {\n        NSString *extension = record.name.pathExtension;\n        if ([extension isEqualToString:@\"jpg\"]) {\n            *stop = ![self readImageFromUnzipper:unzipper withRecord:record error:&enumError];\n        } else if ([extension isEqualToString:@\"json\"]) {\n            *stop = ![self readJSONFromUnzipper:unzipper withRecord:record error:&enumError];\n        } else {\n            *stop = ![self extractFileFromUnzipper:unzipper withRecord:record error:&enumError];\n        }\n    }];\n\n    if (enumError) {\n        *error = enumError;\n        return NO;\n    }\n\n    if (![unzipper closeAndReturnError:error]) {\n        return NO;\n    }\n\n    return YES;\n}\n\n- (BOOL)readImageFromUnzipper:(NOZUnzipper *)unzipper withRecord:(NOZCentralDirectoryRecord *)record error:(out NSError **)error\n{\n    CGImageSourceRef imageSource = CGImageSourceCreateIncremental(NULL);\n    custom_defer(^{ // This is Obj-C equivalent to 'defer' in Swift.  See http://www.openradar.me/21684961 for more info.\n        if (imageSource) {\n            CFRelease(imageSource);\n        }\n    });\n\n    NSMutableData *imageData = [NSMutableData dataWithCapacity:record.uncompressedSize];\n    if (![unzipper enumerateByteRangesOfRecord:record\n                                 progressBlock:NULL\n                                    usingBlock:^(const void * bytes, NSRange byteRange, BOOL * stop) {\n                                        [imageData appendBytes:bytes length:byteRange.length];\n                                        CGImageSourceUpdate(imageSource, imageData, NO);\n                                    }\n                                         error:error]) {\n        return NO;\n    }\n\n    CGImageSourceUpdate(imageSource, (__bridge CFDataRef)imageData, YES);\n    CGImageRef imageRef = CGImageSourceCreateImageAtIndex(imageSource, 0, NULL);\n    if (!imageRef) {\n        *error = ... some error ...;\n        return NO;\n    }\n\n    custom_defer(^{\n        CFRelease(imageRef);\n    });\n\n    UIImage *image = [UIImage imageWithCGImage:imageRef];\n    if (!image) {\n        *error = ... some error ...;\n        return NO;\n    }\n\n    self.image = image;\n    return YES;\n}\n\n- (BOOL)readJSONFromUnzipper:(NOZUnzipper *)unzipper withRecord:(NOZCentralDirectoryRecord *)record error:(out NSError **)error\n{\n    NSData *jsonData = [unzipper readDataFromRecord:record\n                                      progressBlock:NULL\n                                              error:error];\n    if (!jsonData) {\n        return NO;\n    }\n\n    id jsonObject = [NSJSONSerialization JSONObjectWithData:jsonData\n                                                    options:0\n                                                      error:error];\n    if (!jsonObject) {\n        return NO;\n    }\n\n    self.json = jsonObject;\n    return YES;\n}\n\n- (BOOL)extractFileFromUnzipper:(NOZUnzipper *)unzipper withRecord:(NOZCentralDirectoryRecord *)record error:(out NSError **)error\n{\n    if (record.isZeroLength || record.isMacOSXAttribute || record.isMacOSXDSStore) {\n        return YES;\n    }\n\n    return [self saveRecord:record toDirectory:someDestinationRootDirectory options:NOZUnzipperSaveRecordOptionsNone progressBlock:NULL error:error];\n}\n```\n\n### Extensibility - Modular Compression Encoders/Decoders\n\n**`NOZEncoder` and `NOZDecoder`**\n\n_ZipUtilities_ provides a modular approach to compressing and decompressing individual entries of a zip archive.  The _Zip_ file format specifies what compression method is used for any given entry in an archive.  The two most common algorithms for zip archivers and unarchivers are *Deflate* and *Raw*.  Given those are the two most common, _ZipUtilities_ comes with those algorithms built in with *Deflate* being provided from the _zlib_ library present on iOS and OS X and *Raw* simply being unmodified bytes (no compression).  With the combination of `NOZCompressionLevel` and `NOZCompressionMethod` you can optimize the way you compress multiple entries in a file.  For example: you might have a text file, an image and a binary to archive.  You could add the text file with `NOZCompressionLevelDefault` and `NOZCompressionMethodDeflate`, the image with `NOZCompressionMethodNone` and the binary with `NOZCompressionLevelVeryLow` and `NOZCompressionMethodDeflate` (aka Fast).\n\nSince _ZipUtilities_ takes a modular approach for compression methods, adding support for additional compression encoders and decoders is very straightforward.  You simply implement the `NOZEncoder` and `NOZDecoder` protocols and register them for the related `NOZCompressionMethod` with a `NOZCompressionLibrary` (including the `sharedInstance`).  For instance, you might want to add _BZIP2_ support: just implement `MyBZIP2Encoder<NOZEncoder>` and `MyBZIP2Decoder<NOZDecoder>` and update the known encoders and decoders for `NOZCompressionMethodBZip2` in _ZipUtilities_ before you start zipping or unzipping with the `NOZCompressionLibrary`.\n\n*Example:*\n\n```objc\n[[NOZCompressionLibrary sharedInstance] setEncoder:[[MyBZIP2Encoder alloc] init] forMethod:NOZCompressionMethodBZip2];\n[[NOZCompressionLibrary sharedInstance] setDecoder:[[MyBZIP2Decoder alloc] init] forMethod:NOZCompressionMethodBZip2];\n```\n\n*Apple compression library as an extra*\n\n`NOZXAppleCompressionCoder` has been written as an example of how to construct your own coders.  Supports all algorithms provided by libcompression, including LZMA which is specified as a known compression method in the ZIP archive format.\n\n*Example of registering the Apple compression library coders:*\n\n```objc\n- (BOOL)updateRegisteredCodersWithAppleCompressionCoders\n{\n    if (![NOZXAppleCompressionCoder isSupported]) {\n        // Apple's Compression Lib is only supported on iOS 9+ and Mac OS X 10.11+\n        return NO;\n    }\n\n    NOZCompressionLibrary *library = [NOZCompressionLibrary sharedInstance];\n\n    // DEFLATE\n    // Replace existing default DEFLATE coders with Apple Compression variant\n\n    [library setEncoder:[NOZXAppleCompressionCoder encoderWithAlgorithm:COMPRESSION_ZLIB]\n              forMethod:NOZCompressionMethodDeflate];\n    [library setDecoder:[NOZXAppleCompressionCoder decoderWithAlgorithm:COMPRESSION_ZLIB]\n              forMethod:NOZCompressionMethodDeflate];\n    \n    // LZMA\n\n    [library setEncoder:[NOZXAppleCompressionCoder encoderWithAlgorithm:COMPRESSION_LZMA]\n              forMethod:NOZCompressionMethodLZMA];\n    [library setDecoder:[NOZXAppleCompressionCoder decoderWithAlgorithm:COMPRESSION_LZMA]\n              forMethod:NOZCompressionMethodLZMA];\n\n    // The following coders are not defined as known ZIP compression methods, \n    // however that doesn't mean we can't extend the enumeration of ZIP methods\n    // to have custom compression methods.\n    //\n    // Since compression_algorithm enum values are all beyond the defined ZIP methods values\n    // and are all within 16 bits, we can just use the values directly.\n    // Puts the burden on the decoder to know that these non-ZIP compression methods\n    // are for their respective algorithm.\n\n    // LZ4\n\n    [library setEncoder:[NOZXAppleCompressionCoder encoderWithAlgorithm:COMPRESSION_LZ4]\n              forMethod:(NOZCompressionMethod)COMPRESSION_LZ4];\n    [library setDecoder:[NOZXAppleCompressionCoder decoderWithAlgorithm:COMPRESSION_LZ4]\n              forMethod:(NOZCompressionMethod)COMPRESSION_LZ4];\n\n    // Apple LZFSE - the new hotness for compression from Apple\n\n    [library setEncoder:[NOZXAppleCompressionCoder encoderWithAlgorithm:COMPRESSION_LZFSE]\n              forMethod:(NOZCompressionMethod)COMPRESSION_LZFSE];\n    [library setDecoder:[NOZXAppleCompressionCoder decoderWithAlgorithm:COMPRESSION_LZFSE]\n              forMethod:(NOZCompressionMethod)COMPRESSION_LZFSE];\n\n    return YES;\n}\n```\n\n## ZipUtilities CLI (aka _noz_)\n\nZipUtilities includes a command-line interface for convenient tooling integration and scriptability.  It can be built via the Xcode project directly or installed via [Homebrew](https://brew.sh).\n\n### Installing via Homebrew\n\nTapping the NSProgrammer formulae:\n```\nbrew tap NSProgrammer/macos\n```\n\nInstalling _noz_:\n```\nbrew install noz\n```\n\nUpgrading _noz_ (when already installed):\n```\nbrew upgrade noz\n```\n\n### Using _noz_\n\n**List _noz_ compression methods**\n```\nnoz -A\n```\n\n**Dump info of zip archive**\n```\nnoz -D [dump_options] -i zip_file\n\ndump_options:\n-------------------------------\n\t-L                list all entries\n\t-v                verbose info\n\t-s                silence the archive info (the default info that is output)\n```\n\n**Compress file**\n```\nnoz -c -m METHOD [-l LEVEL] -i in_file -o out_file\n\nExample: noz -c -m Brotli -l 12 -i payload.json -o payload.json.br\n(Use 'noz -A' for available compression methods and levels)\n```\n\n**Decompress file**\n```\nnoz -d -m METHOD -i in_file -o out_file\n\nExample: noz -d -m Brotli -i payload.json.br -o payload.json\n(Use 'noz -A' for available compression methods)\n```\n\n**Zip up an archive**\n```\nnoz -z [zip_options] -o output_file -i input_file1 [file_options] [... [-i intput_fileN [file_options]]]\n\nzip_options:\n-------------------------------\n\t-c COMMENT        provide an archive comment\n\t-M METHOD NUMBER  map a METHOD to a different archive number... this impacts unzipping!\n\nzip file_options:\n-------------------------------\n\t-c COMMENT        provide an archive entry comment\n\t-n NAME           override the name\n\t-m METHOD         specify a compression method, default is \"deflate\" (see METHODS below)\n\t-l LEVEL          specify a compression level, levels are defined per METHOD each with their own default\n\t-h                permit hidden file(s)\n\t-f                don't recurse into the director if provided path was a directory (default is to recurse)\n```\n\n**Unzip an archive**\n```\nnoz -u [unzip_options] -i zip_file [-e [entry_options] entry1 [... [-e [entry_options] entryN]]]\n\nunzip_options:\n-------------------------------\n\t-f                forcibly guess METHOD when unzipping an unknown METHOD\n\t-F                forcibly fail when unzipping an unknown METHOD\n\t-M METHOD NUMBER  map a METHOD to a different archive number... this impacts unzipping!\n\t-b BASE_PATH      provide the base path output to.  Default is directory named after archive, './zip_file/'\n\nunzip entry_options:\n-------------------------------\n\t-o OUTPUT_FILE    provide the specific output file\n\t-m METHOD         override the method to unzip the entry with\n```\n\n## TODO\n\n### Eventually\n\n- 64-bit file support (big file archiving)\n- add password support\n  - This is low priority as encrypting zip file content is not the appropriate way to secure data\n- add support for per entry \"extra info\" in an archive\n- expand on progress info\n  - state transitions\n  - what files are being zipped/unzipped\n  - per file progress\n\n## Dependencies\n\n### ZStandard\nZipUtilities includes Facebook's ZStandard (zstd) compression library.\n[www.zstd.net](http://www.zstd.net)\n\n### Brotli\nZipUtilities includes Google's Brotli (br) compression library.\n[Brotli Github](https://github.com/google/brotli)\n\n### Test files for zipping/unzipping\nAs a part of unit testing Aesop's Fables, the Star Wars Episode VII trailer and Maniac Mansion are used for unit testing.  Aesop's Fables and Maniac Mansion no longer hold a copyright anymore and can be freely be distributed including the unorthodox use as test files for unit testing zip archiving and unarchiving.  The Star Wars Episode VII Trailer is free for distribution and also provides a useful file for testing by being a large file.\n\n![MANIAC.EXE](https://media.giphy.com/media/DPQ4G030oJdcc/giphy.gif)\n"
 },
 {
  "repo": "jamesmunns/nrf52dk-sys",
  "language": "C",
  "readme_contents": "# nRF52dk-sys\n\nThis is a (work in progress) rust crate to support the [Nordic nRF52 Development Kit](https://www.nordicsemi.com/eng/Products/Bluetooth-low-energy/nRF52-DK) using Rust.\n\nThis crate uses the Nordic SoftDevice S132 (a binary Bluetooth stack), as well as the Nordic SDK (an open source hardware abstraction layer), which are written in C, and provides Rust bindings for those items.\n\nThis project aims to be a reference on how to combine C and Rust components, in order to create a Bluetooth peripheral which uses Rust for the main application software.\n\n## Software Prerequisites\n\nThis project requires the following tools before building:\n\nTool                | Recommended Version   | Link/Install\n:---                | :------------------   | :---\nClang               | 3.9                   | [debian/ubuntu](http://apt.llvm.org/) or [source](http://releases.llvm.org/download.html)\narm-none-eabi-gcc   | 6.1                   | [Current Version](https://developer.arm.com/open-source/gnu-toolchain/gnu-rm/downloads)\nRust (nightly)      | nightly-2017-11-15    | [rustup.rs](https://www.rustup.rs/)\nRust source         | nightly-2017-11-15    | `rustup component add rust-src`\nXargo               | 0.3.8                 | `cargo install xargo --vers 0.3.8`\nBindgen             | 0.31.3                | `cargo install bindgen --vers 0.31.3`\n\nIf you would like more detailed installation instructions, please look at [The Detailed Setup Instructions](./SETUP.md).\n\nIf you use `docker`, please see the debian based [Dockerfile](./Dockerfile).\n\nAdditionally, the following tools are required to run or debug the firmware:\n\nTool        | Recommended Version   | Link/Install\n---         | ---                   | ---\nSoftDevice  | S132-v4.0.2           | [Nordic Download](http://www.nordicsemi.com/eng/nordic/Products/nRF52832/S132-SD-v4/58803)\nJLink       | v6.16                 | [JLink Download](https://www.segger.com/downloads/jlink)\n\n## Building\n\n```text\ngit clone --recursive https://github.com/jamesmunns/nrf52dk-sys\ncd nrf52dk-sys\nxargo build --example blinky\n      Compiling core v0.0.0 (file:///root/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/src/libcore)\n       Finished release [optimized] target(s) in 14.8 secs\n       Updating registry `https://github.com/rust-lang/crates.io-index`\n    Downloading r0 v0.2.1\n    Downloading cortex-m v0.1.6\n    Downloading volatile-register v0.1.2\n      Compiling volatile-register v0.1.2\n      Compiling gcc v0.3.50\n      Compiling r0 v0.2.1\n      Compiling cortex-m v0.1.6\n      Compiling smooth_blue v0.1.0 (file:///nrf52dk-sys)\n       Finished dev [unoptimized + debuginfo] target(s) in 15.66 secs\n```\n\n**NOTE:** This crate does not work with incremental compilation. In our\n**.cargo/config**, we set `incremental = false`, which corresponds to setting\n`CARGO_INCREMENTAL=0` in your environment.\n\n## Flashing, Debugging, and Running\n\n### Flashing the SoftDevice\n\nFirst, you must flash the Nordic Soft Device. This should only be necessary to do once (the rest of the firmware will not overwrite this section).\n\n```text\ncd path/to/softdevice\nJLinkExe -device NRF52832_XXAA -if SWD -speed 4000 -autoconnect 1\n    Device \"NRF52832_XXAA\" selected.\n    Found SWD-DP with ID 0x2BA01477\n    Found SWD-DP with ID 0x2BA01477\n    AP-IDR: 0x24770011, Type: AHB-AP\n    AHB-AP ROM: 0xE00FF000 (Base addr. of first ROM table)\n    Found Cortex-M4 r0p1, Little endian.\n    FPUnit: 6 code (BP) slots and 2 literal slots\n    CoreSight components:\n    ROMTbl 0 @ E00FF000\n    ROMTbl 0 [0]: FFF0F000, CID: B105E00D, PID: 000BB00C SCS\n    ROMTbl 0 [1]: FFF02000, CID: B105E00D, PID: 003BB002 DWT\n    ROMTbl 0 [2]: FFF03000, CID: B105E00D, PID: 002BB003 FPB\n    ROMTbl 0 [3]: FFF01000, CID: B105E00D, PID: 003BB001 ITM\n    ROMTbl 0 [4]: FFF41000, CID: B105900D, PID: 000BB9A1 TPIU\n    ROMTbl 0 [5]: FFF42000, CID: B105900D, PID: 000BB925 ETM\n    Cortex-M4 identified.\nJ-Link>loadfile s132_nrf52_4.0.2_softdevice.hex\n    Downloading file [s132_nrf52_4.0.2_softdevice.hex]...\n    Comparing flash   [100%] Done.\n    Verifying flash   [100%] Done.\n    O.K.\n```\n\n### Flashing to device\n\nThis only flashes the firmware built above. If you would like to flash and debug, skip forward.\n\n```text\ncd nrf52dk-sys\narm-none-eabi-objcopy -O ihex target/thumbv7em-none-eabihf/debug/examples/blinky target.hex\nJLinkExe -device NRF52832_XXAA -if SWD -speed 4000 -autoconnect 1\nJ-Link>loadfile target.hex\n    Downloading file [target.hex]...\n    Comparing flash   [100%] Done.\n    Verifying flash   [100%] Done.\n    O.K.\n```\n\nIt may be necessary to reset the device after closing JLink with `CTRL-c`.\n\n### Flash and Debug\n\nFirst, create a GDB server on one terminal:\n\n```text\nJLinkGDBServer -device NRF52832_XXAA -if SWD -speed 4000\n# ...\n# Connecting to target...Connected to target\n# Waiting for GDB connection...\n```\n\n\nThen, in another terminal:\n\n```text\ncd nrf52dk-sys\narm-none-eabi-gdb -tui target/thumbv7em-none-eabihf/debug/examples/blinky\n(gdb) target remote :2331\n# ...\n(gdb) monitor reset\n# ...\n(gdb) load\n# ...\n(gdb) monitor reset\n# ...\n(gdb) continue\n# ...\n```\n\n## Docker\n\nI wrote the `Dockerfile` in order to support CI. This is not done yet. For now if you would like to build it to verify the master branch builds:\n\n```text\ncd nrf52dk-sys\n\n# Build the image\ndocker build -t nrf52dk .\n\n# Run the image\ndocker run -t nrf52dk\n```\n\nIf the docker container ran successfully, you should see something like this at the end:\n\n```text\n Compiling smooth_blue v0.1.0 (file:///nrf52dk-sys)\n  Finished dev [unoptimized + debuginfo] target(s) in 15.66 secs\n```\n\n## License\n\nAll Rust components are provided under the [MIT License](./LICENSE). Additional components provided by the Nordic nRF5-sdk contain additional licenses.\n"
 },
 {
  "repo": "alextyner/wasm-canvas",
  "language": "C",
  "readme_contents": "[![Build Status](https://travis-ci.com/alextyner/wasm-canvas.svg?branch=master)](https://travis-ci.com/alextyner/wasm-canvas)\n\n# wasm-canvas\n\nwasm-canvas is a **C99-compliant** layer of abstraction for interacting with the **HTML Canvas API**. \n\nThis library is intended for use in projects compiled with [Emscripten](https://emscripten.org/) targeting [WebAssembly](https://webassembly.org/).\n\n## Purpose\n\n### Familiarity\n\nIf you've worked with the Canvas API from JavaScript, the syntax for drawing is simple and friendly, as JavaScript tends to be. \n\nAlthough JavaScript excels in readability, it often lacks in optimizability. Web applications with high-performance requirements may turn to WebAssembly compiled from C.\n\nFor applications with visual components, interacting with the Canvas API from C feels unfriendly. This library provides an interface for doing so which more closely resembles the syntax used in JavaScript.\n\n### \"Object-Oriented\"\n\nThe library is written in C, not an object-oriented programming language. However, the design is patterned less like idiomatic C and more like a JavaScript programmer might expect.\n\nSome C idioms still shine through. With no garbage collector or constructors, `create...` and `free...` functions are provided to allocate, set up, and free the object-like structs. The structs are populated primarily by function pointers to make calls as similar to JavaScript as possible. Nearly all member functions require a pointer to their parent struct as the first parameter as there is no implicit `this` present in C.\n\n### Why not C++?\n\nThere are a lot of reasons.\n\n## Using the Library\n\n### Include the Headers\n\nCurrently, this is not a header-only library. Be sure that for each header included at compile time you introduce the corresponding source file at link time. See *Hello World* below for an example\n\nFor the Canvas API and drawing context functionality:\n\n```C\n#include \"canvas.h\"\n```\n\nFor access to the DOM Window object:\n\n```C\n#include \"window.h\"\n```\n\n### Creating a New Canvas\n\nUse the `createCanvas()` function to insert a new canvas element into the HTML.\n\n```C\nHTMLCanvasElement *myCanvas = createCanvas(\"testCanvas\");\n```\n\nA new `<canvas>` element has dimensions 300Wx150H by default.\n\n### Using an Existing Canvas\n\nIf your HTML already has a canvas element you'd like to bind to...\n\n```HTML\n<canvas id=\"someCanvas\"></canvas>\n```\n\n... use `createCanvas()` with the existing element's id.\n\n```C\nHTMLCanvasElement *myCanvas = createCanvas(\"someCanvas\");\n```\n\n### Drawing\n\nMost, but not all of the functions available with the 2D rendering context in JavaScript are available.\n\nSome functionality varies from JavaScript. Setting a field of the rendering context like `font`, for example, can be accomplished with a setter `setFont()` function. There is a corresponding getter `getFont()` function to read from the field.\n\nFor a full list of drawing functions available, see the [CanvasRenderingContext2D Struct Reference](https://alextyner.github.io/wasm-canvas/documentation/structCanvasRenderingContext2D.html).\n\n### Window()\n\n`#include \"window.h\"`\n\nThe DOM Window object, accessible via the global `window` in JavaScript, provides some useful functionality when used in conjunction with a canvas.\n\nFor example, a canvas can be expanded in size to fill the current window.\n\n```C\nmyCanvas->setHeight(myCanvas, Window()->getInnerHeight());\nmyCanvas->setWidth(myCanvas, Window()->getInnerWidth());\n```\n\nNote that window functions do not require a pointer to the struct as the first parameter. It is assumed that there is only one window, and you are referring to that one.\n\n### Cleaning Up\n\nSome memory is dynamically allocated for each `HTMLCanvasElement` created. Memory is only allocated for the `HTMLWindow` if it is used at least once in your program.\n\nAny function return values that needed to be dynamically allocated will also be freed here.\n\nFree the memory for each canvas using `freeCanvas()`.\n\n```C\nfreeCanvas(myCanvas);\n```\n\nAnd, if you've used the `Window()` function at least once, free it using `freeWindow()`.\n\n```C\nfreeWindow(Window());\n```\n\n## Documentation\n\n#### wasm-canvas\n\n[View the Doxygen](https://alextyner.github.io/wasm-canvas/documentation/)\n\n#### Emscripten\n\n[Installation Instructions](https://emscripten.org/docs/getting_started/downloads.html)\n\n[Emscripten API](https://emscripten.org/docs/api_reference/)\n\n## Examples\n\n### Hello World\n\nThis is how you might draw something simple, like the text \"Hello World\" on a new canvas.\n\nAlthough not required, I recommend giving Emscripten at least a basic HTML template to clearly see your canvas. If you choose not to provide a template, the default template does include a canvas element that you can bind to (see *Using the Library* above).\n\n**Directory Listing**\n\n- build/\n- canvas.c [wasm-canvas]\n- canvas.h [wasm-canvas]\n- template.html (optional)\n- hello.c\n\nfile: **template.html**\n\n```HTML\n<!DOCTYPE html>\n<html>\n    <body>\n        {{{ SCRIPT }}}\n    </body>\n</html>\n```\n\nfile: **hello.c**\n\n```C\n#include \"canvas.h\"\n\nint main(void) {\n    HTMLCanvasElement *canvas = createCanvas(\"myCanvas\");\n    CanvasRenderingContext2D *ctx = canvas->getContext(canvas, \"2d\");\n    ctx->setFont(ctx, \"48px serif\");\n    ctx->fillText(ctx, \"Hello World\", 0, 150, -1);\n    freeCanvas(canvas);\n    return 0;\n}\n```\n\n**Compiling & Linking**\n\n```bash\nemcc -Wall hello.c canvas.c -o hello.o\nemcc --shell-file template.html hello.o -o build/index.html\n```\n\nOr, more verbosely:\n\n```bash\nemcc -Wall canvas.c -o canvas.o\nemcc -Wall -I canvas.h hello.c -o hello.o\nemcc --shell-file template.html hello.o canvas.o -o build/index.html\n```\n\nOr, less verbosely:\n\n```bash\nemcc --shell-file template.html hello.c canvas.c -o build/index.html\n```\n\n"
 },
 {
  "repo": "psobot/pressure",
  "language": "C",
  "readme_contents": "# `pressure`\n*A reimplementation of Python's synchronized and bounded Queue.Queue on a Redis backend.*\n\nby Peter Sobot ([@psobot](http://twitter.com/psobot), [psobot.com](http://psobot.com)). Licensed under MIT.\n\n---\n\n`pressure` implements everybody's favourite Python data structure, the\ntrusty built-in `Queue.Queue`, on top of everybody's favourite distributed\ndata store, Redis. Nearly all of the original Queue's API is replicated.\n\n`pressure` allows for **synchronized**, **blocking** and **distributed**\n(a.k.a.: multi-process) queues to be shared between processes, between \nmachines, and even between data centers.\n\n`pressure` is also **multilingual** - the protocol is well-defined (see [`protocol.md`](https://github.com/psobot/pressure/blob/master/protocol.md)) and there are libraries already for different languages, all included\nas submodules of this repo:\n\n  - [`pressure-python`](http://github.com/psobot/pressure-python/)\n  - [`pressure-c`](http://github.com/psobot/pressure-c/) (not submoduled just yet)\n\n\n<!--  - [`pressure-go`](http://github.com/psobot/pressure-go/) -->\n\n\n## Examples\n\n    import pressure\n\n    #   Create two unique handles to the 'test' queue.\n    q1 = pressure.PressureQueue('test')\n    q2 = pressure.PressureQueue('test')\n\n    #   Create the 'test' queue, giving a bound of 5.\n    q1.create(bound=5)\n    \n    #   Put the strings into one handle.\n    for string in [\"hello\", \"goodbye\"]:\n        q1.put(string)\n\n    #   Close the queue - assert that no more data will be produced.\n    q1.close()\n\n    #   Receive the strings from the other handle!\n    for result in q2:\n        print result\n\n    # Prints:\n    #   \"hello\"\n    #   \"goodbye\"\n\n    #   Delete the queue from the database once its data has been consumed.\n    #   This should be done on the consumer's end, as the consumer knows\n    #   once all of the data is gone.\n    q2.delete()\n\n## Get Started\n\n    > sudo pip install pressure\n\n    # for quick local testing, make sure you have redis-server running\n    > redis-server &\n\n    # muck around with ipython?\n    > cd python\n    > ipython\n\n    Python 2.7.1 (r271:86832, Jun 16 2011, 16:59:05) \n    Type \"copyright\", \"credits\" or \"license\" for more information.\n\n    IPython 0.13 -- An enhanced Interactive Python.\n    ?         -> Introduction and overview of IPython's features.\n    %quickref -> Quick reference.\n    help      -> Python's own help system.\n    object?   -> Details about 'object', use 'object??' for extra details.\n\n    In [1]: import pressure\n    In [2]: q1 = pressure.PressureQueue('test_queue')\n    In [3]: q1.create(5)\n    In [4]: q1.put('hello springfield!')\n\n## TODOs\n\n    - Ensure that this library has 100% API coverage with the original Queue.Queue.\n    - Clean up and document the internals.\n\n## Questions/Comments/Feedback?\n\nThis *is* GitHub after all, so please feel free to open an issue if you discover\na bug. Better yet, feel free to fork this repo, add a test to cover the bug,\nmaybe even *fix* the bug, and send me back a pull request. You'll get a gold star\nand a very happy tweet from me.\n\n## LICENSE and all that jazz\n\n    Copyright (c) 2013 Peter Sobot\n\n    Permission is hereby granted, free of charge, to any person obtaining a\n    copy of this software and associated documentation files (the \"Software\"),\n    to deal in the Software without restriction, including without limitation\n    the rights to use, copy, modify, merge, publish, distribute, sublicense,\n    and/or sell copies of the Software, and to permit persons to whom the\n    Software is furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in\n    all copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n    THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n    DEALINGS IN THE SOFTWARE.\n\n"
 },
 {
  "repo": "cdown/psi-notify",
  "language": "C",
  "readme_contents": "# psi-notify | [![Tests](https://img.shields.io/travis/com/cdown/psi-notify/master.svg)](https://travis-ci.com/cdown/psi-notify) [![Fuzzer](https://img.shields.io/travis/com/cdown/psi-notify/master.svg?label=fuzz)](https://travis-ci.com/cdown/psi-notify) [![LGTM](https://img.shields.io/lgtm/grade/cpp/github/cdown/psi-notify.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/cdown/psi-notify/alerts/?mode=list)\n\n**tl;dr: psi-notify can alert you when resources on your machine are becoming\noversaturated, and allow you to take action *before* your system slows to a\ncrawl.**\n\n![](demo.gif)\n\npsi-notify is a minimal unprivileged notifier for system-wide resource pressure\nusing [PSI](https://facebookmicrosites.github.io/psi/). This can help you to\nidentify misbehaving applications on your machine before they start to severely\nimpact system responsiveness, in a way which `MemAvailable`, CPU graphs, I/O\nutilisation graphs and other metrics cannot.\n\n## Features\n\n- Runs unprivileged\n- Minimal resource usage\n- Works with any notifier using [Desktop\n  Notifications](https://specifications.freedesktop.org/notification-spec/latest/)\n\n## Requirements\n\n- Linux 4.20+ with `CONFIG_PSI` (enabled by default in most distributions)\n- libnotify\n\n## Installation\n\nOn Arch, the [psi-notify AUR\npackage](https://aur.archlinux.org/packages/psi-notify/) is available.\n\nOn Debian (bookworm and sid), the [psi-notify package\n](https://packages.debian.org/search?keywords=psi-notify) is available.\n\nOn Fedora and RHEL/CentOS 8, the [psi-notify\npackage](https://src.fedoraproject.org/rpms/psi-notify) is available in\nFedora/EPEL.\n\nOn Ubuntu, the [psi-notify package\n](https://packages.ubuntu.com/search?keywords=psi-notify) is in kinetic. For\nolder releases, please use the [psi-notify PPA\n](https://launchpad.net/~michel-slm/+archive/ubuntu/psi-notify).\n\nYou can also find packages in [Repology\n](https://repology.org/project/psi-notify/versions).\n\nOtherwise, manual installation is as simple as running `make` and putting the\nresulting `psi-notify` binary in your PATH. You will need `libnotify`\ninstalled.\n\nAfter that, you just start psi-notify. A systemd user service is packaged and\ncan be used like so:\n\n    systemctl --user start psi-notify\n\n## Config\n\nPut your configuration in `~/.config/psi-notify`. Here's an example that will\ncheck roughly every 5 seconds\u207a, and pop up a notification when the values are\nexceeded:\n\n```\nupdate 5\nlog_pressures false\n\nthreshold cpu some avg10 50.00\nthreshold memory some avg10 10.00\nthreshold io full avg10 15.00\n```\n\nThe above is the default configuration if no config file exists. You may have\nto tweak these depending on your hardware, machine behaviour, and normal\nworkloads.\n\nYou can reload the config without restarting by sending `SIGHUP` to psi-notify.\n\nLook at the \"config format\" section below to find out more about what a valid\nconfig looks like.\n\n\u207a PSI has poll() support, but it's not currently available to unprivileged\nusers. See [this\ndiscussion](https://lore.kernel.org/lkml/20200424153859.GA1481119@chrisdown.name).\n\n## Comparison with oomd\n\n[oomd](https://github.com/facebookincubator/oomd) and psi-notify are two\ncompatible and complementary projects -- they're not in opposition to each\nother. oomd also uses PSI metrics, but it requires a policy about \"what to\nkill\" in high-pressure scenarios. For example, on a web server we obviously\ndon't want to kill the web server if we can avoid that, so we should prioritise\nother applications. On the desktop though, it's hard to say: should we kill\nChrome, or some IDE, or maybe something playing a movie? It's extremely\ndifficult (although perhaps possible) to produce a single configuration that\nwill do the right thing in even the majority of cases, so we opt to alert early\ninstead and have the user make the decision about what's high priority in their\nuser session. When integrating oomd for the desktop, most distributions will\nlikely end up having to make it less aggressive than would be ideal, so they\ncan still interoperate.\n\nIt's hard to produce a good policy for, say, one's working day, where at one\ntime my terminal is the most critical thing, at another my browser is, and at\nanother it's my mail client. At other times maybe I'm ok with the slowdown and\nam willing to ride it out without killing anything. psi-notify sidesteps this\nproblem by simply notifying, rather than taking action.\n\n## Config format\n\n### update\n\nThe update interval in seconds is specified with `update [int]`. The default is\n`update 5` if unspecified.\n\n### log_pressures\n\nIf you'd like messages like this at every update interval, you can set\n`log_pressures true` (the default is `false`):\n\n```\nINFO: Current CPU pressures: some avg10=0.00 avg60=0.02 avg300=0.01\nINFO: Current memory pressures: some avg10=0.00 avg60=0.00 avg300=0.00\nINFO: Current memory pressures: full avg10=0.00 avg60=0.00 avg300=0.00\nINFO: Current I/O pressures: some avg10=0.00 avg60=0.00 avg300=0.00\nINFO: Current I/O pressures: full avg10=0.00 avg60=0.00 avg300=0.00\n```\n\n### threshold\n\nThresholds are specified with fields in the following format:\n\n1. The word `threshold`.\n2. The resource name, as shown in `cgroup.controllers`. `cpu`, `memory`, and\n   `io` are currently supported.\n3. Whether to use the `some` or `full` metric. See the definition\n   [here](https://facebookmicrosites.github.io/psi/docs/overview#pressure-metric-definitions).\n4. The PSI time period. `avg10`, `avg60`, and `avg300` are currently supported.\n5. The threshold, as a real number between 0 and 100. Decimals are ok.\n\n## Contributing\n\nIssues and pull requests are welcome! Please feel free to file them [on\nGitHub](https://github.com/cdown/psi-notify).\n\n[sd_notify]: https://www.freedesktop.org/software/systemd/man/sd_notify.html\n"
 },
 {
  "repo": "ttdennis/fpicker",
  "language": "C",
  "readme_contents": "# fpicker\n<img align=\"right\" src=\"./assets/fpicker_logo.png\" alt=\"Fpicker logo\" width=\"30%\"/>\n\nfpicker is a Frida-based fuzzing suite that offers a variety of fuzzing modes for in-process\nfuzzing, such as an AFL++ mode or a passive tracing mode. It should run on all platforms that\nare supported by Frida.\n\n* [Installation Instructions](#requirements-and-installation)\n* [Building and Running](#building-and-running)\n* [Creating a Fuzzing Harness](#creating-a-fuzzing-harness)\n* [Modes and Configuration](#modes-and-configuration)\n\nSome background information and the thoughts and ideas behind fpicker can be found in [a\nblogpost](https://insinuator.net/2021/03/fpicker-fuzzing-with-frida/) I wrote.\n\nFpicker is based on previous efforts on [ToothPicker](https://github.com/seemoo-lab/toothpicker),\nwhich was developed during my master thesis. Most of fpicker was developed during working hours at\nmy employer ([ERNW](https://ernw.de)).\n\n## Requirements and Installation\nRequired for running fpicker:\n- [frida_compile](https://github.com/frida/frida-compile) to compile the harness script into one JS file\n- The `frida-core-devkit` for the respective platform found at [Frida releases on GitHub](https://github.com/frida/frida/releases).\n    - Depending on the platform you want to target store the library as `frida-core-ios.a`, `frida-core-macos.a`, or `frida-core-linux.a`.\n    - The same goes for the header files (`frida-core.h`). Store them as `frida-core-linux.h` or `frida-core-ios.h` depending on the platform.\n    - The makefile was built this way so you can build for different systems on the same system (usually this is for building on macOS, where\n      you can compile for both iOS and macOs). (There is probably a better way do to this?)\n\nRequired only when running in AFL++ mode:\n- [AFL++](https://github.com/AFLplusplus/AFLplusplus/)\n    - on macOS:\n        - Compile with `CFLAGS=\"-DUSEMMAP=1\"`.\n    - on iOS:\n        - Apply the aflpp-ios.patch. This changes the shared mem and out file mode to 666 instead of\n          600. Fpicker needs to be run as root on iOS. If the target is not running as root, it will\n          not be able to read and write shared memory.\n        - Compile with `CFLAGS=\"-DUSEMMAP=1\"`.\n\n\n## Building and Running\nFpicker can be built for `macOS`, `iOS` or `Linux`. The Makefile currently only supports building\nfor iOS on macOS but it should be totally possible to build fpicker using an iOS toolchain on\nLinux.\n\nDepending on the desired target run:\n\n```bash\nmake fpicker-macos\nmake fpicker-ios\nmake fpicker-linux\n```\n\nto build fpicker.\n\nOnce fpicker is built, the fuzzing harness needs to be built next:\n\nSee the [examples folder](./examples) for different sample fuzzing cases. The general approach is as follows:\n\n- Create a custom harness for the target (e.g. `examples/test/test.js`) (see\n  [here](#creating-a-fuzzing-harness) for more information on harnesses)\n- Compile the custom harness using frida-compile `frida-compile test.js -o harness.js`\n\nNow fpicker can start fuzzing. The exact command highly depends on the configuration and setup. In\nthe following, a few example cases are given. These mostly correspond to the examples in the\n[examples folder](./examples).\n\n- Run fpicker as AFL++ proxy attaching to a target process fuzzing a specific function in process:\n```bash\nafl-fuzz -i examples/test-network/in -o ./examples/test-network/out -- \\\\\n    ./fpicker --fuzzer-mode afl -e attach -p test-network -f ./examples/test-network/harness.js\n```\n\n- Run fpicker in standalone mode attaching to a server and running a client program to send the fuzzing input:\n```bash\n./fpicker --fuzzer-mode standalone -e attach -p server-process -f harness.js --input-mode cmd \\\\\n    --command \"./client-send @@\" -i indir -o outdir\n```\n\n- Run fpicker in standalone mode attaching to a server, fuzzing in-process with a custom mutator cmd:\n```bash\n./fpicker --fuzzer-mode active --communication-mode shm -e attach -p server-process -f harness.js \\\\\n    -i indir -o outdir --standalone-mutator cmd --mutator-command \"radamsa\"\n```\n\n- Run fpicker in passive mode attaching to a server collecting coverage and payloads:\n```bash\n./fpicker --fuzzer-mode passive --communication-mode send -e attach -p server-process -o outdir -f harness.js\n```\n\n- Run fpicker in standalone mode attaching to a running process on a remote device, fuzzing in-process\nwith a custom mutator cmd:\n```bash\n./fpicker --fuzzer-mode active -e attach -p test -D remote -o examples/test/out/ -i examples/test/in/ \\\\\n    -f fuzzer-agent.js --standalone-mutator cmd --mutator-command \"radamsa\"\n```\n\n## Creating a Fuzzing Harness\nEach target requires its own fuzzing harness. The most important part of this harness is defining\nthe entry function of Frida's Stalker, which effectively determines at which point the\ninstrumentation is inserted. In the `in-process` mode this is simple. The function would usually\nbe the one that is called on each fuzzing iteration. However, it could also be a different one.\n\nA minimalist harness implementation (in `command` mode) could be this:\n```javascript\n// Import the fuzzer base class\nconst Fuzzer = require(\"harness/fuzzer.js\");\n\n// The custom fuzzer needs to subclass the Fuzzer class to work properly\nclass TestFuzzer extends Fuzzer.Fuzzer {\n    constructor() {\n        // The constructor needs to specify the address of the targeted function and a NativeFunction\n        // object that can later be called by the fuzzer.\n\n        const FUZZ_FUNCTION_ADDR = Module.getExportByName(null, \"FUZZ_FUNCTION\");\n        const FUZZ_FUNCTION = new NativeFunction(\n            FUZZ_FUNCTION_ADDR,\n            \"void\", [\"pointer\", \"int64\"], {\n        });\n\n        super(\"test\", FUZZ_FUNCTION_ADDR, FUZZ_FUNCTION);\n    }\n}\n\nconst f = new TestFuzzer();\nexports.fuzzer = f;\n```\nThis harness configures the instrumentation to follow the function `FUZZ_FUNCTION`. The\ninstrumentation will start when this function is entered and stops when the function returns.\nThis function should be chosen carefully as it is expensive and the more (potentially\nunimportant) parts of the process are instrumented, the slower the fuzzer gets. Of course, this is\na consideration between speed and intended coverage. Additionally, the fuzzer currently only\nsupports functions that are only entered once during one fuzzing iteration, i.e., the function\nshould not be called more than once during one fuzz case, otherwise the coverage information\nmight become unreliable.\n\nWhen the `in-process` mode is used, another function is required in the fuzzer script. The `fuzz`\nmethod. It will get called on each iteration. It will be called with two parameters, a pointer\nto a buffer and the length of the buffer. Our exemplary target function takes two parameters, a\npointer to a buffer and its length. Thus, we can just pass the parameters were getting in the\n`fuzz` method.\n\n```javascript\nfuzz(payload, len) {\n    this.target_function(payload, parseInt(len));\n}\n```\n\nIn `passive` mode, a callback needs to be specified that processes the required data. The fuzzer\nexpects to receive a payload buffer and its length. Depending on the target function that is\nfuzzed, this data needs to be extracted. In the following example, we again have a function that\nhas two parameters: a pointer to a buffer and its length. The `args` parameter contains all\npotential parameters the target function receives, so the length parameter (which is the second\none in our case) can be accessed with `args[1]`. We then read the buffer as `Uint8Array` and send\nit back to the fuzzer using the `sendPassiveCorpus` method.\n\n```javascript\npassiveCallback(args) {\n    const len = args[1];\n    const data = new Uint8Array(Memory.readByteArray(args[0], parseInt(len)));\n\n    // this encodes the data and sends it back to the fuzzer\n    this.sendPassiveCorpus(data, len);\n}\n```\n\nIn case the target needs some sort of preparation before the fuzzer can start, fpicker provides a\n`prepare` method that is called during the initialization of the fuzzer. Preparation could be the\nestablishment of state, e.g., by instantiating an object. Such a preparation function could look\nlike the following:\n```javascript\nprepare() {\n  // the object can be attached to the fuzzer instance so that it can be used within the\n  // fuzz() method later on.\n  this.required_object = call_native_function_that_creates_object();\n}\n```\n\n## Modes and Configuration\npficker offers a large set of modes and configurations that are explained in the following. Most of\nthese modes can be combined in different ways. At the end of this section is a table that shows\nwhich options can be combined and what their implementation status is.\n\n### Fuzzer Mode\nFpicker has three different *fuzzing modes*: AFL++ Mode, Standalone Active Mode and Standalone\nPassive Mode:\n\n- **AFL++ Mode:** In AFL++ mode, fpicker acts as a proxy between AFL++ and the target process. Using\n  Frida's instrumentation capabilities, AFL's coverage bitmap is populated while the target is\n  fuzzed with input data generated by AFL++.\n\n- **Standalone Active Mode:** In standalone active mode, the fuzzer uses Frida's [Stalker call\n  summaries](https://frida.re/docs/javascript-api/#stalker) to gather coverage in form of basic\n  blocks that are executed during an iteration. This is nothing new and has been implemented in\n  various forms before. However, in combination with some of the other fuzzer settings this can have\n  various benefits. It is also a good alternative if AFL++ is not applicable or desired in a given\n  environment or case.\n\n- **Standalone Passive Mode:** Passive mode is less of a fuzzer and more of a tracer. Essentially,\n  it does the same as standalone active mode. However, it does *not* send its own inputs. It just\n  attaches to a certain function and collects coverage. Once new coverage is observed, both the\n  coverage and the input is stored.\n\n### Input Mode\nWhile fpicker is largely designed as an in-process fuzzer, it also supports fuzzing via an external\ncommand. For this fpicker offers two input modes.\n\n- **Input Mode In-Process:** In in-process input mode, the harness directly calls a specified\n  function in the target process. The fuzzer sends the payload to the harness and the harness\n  prepares the payload in such a way that it can call the targeted function.\n\n- **Input Mode CMD:** In command input mode, the payload is redirected to an external command. This\n  is useful it is too complex to prepare the parameters other other state when directly calling the\n  target function. The coverage collection still needs to be attached to a certain function. Maybe\n  there is a client that can be supplied with a payload which then triggers the target function.\n\n### Communication Mode\nCommunication mode determines how the injected harness communicates with the fuzzer. This largely\ndepends on the target application. Frida offers an API to send and receive messages from the\ninjected agent script. This type of communication is quite costly. One of the factors is that the\ntransported message needs to be encoded in JSON. So sending binary data is straight-forward.\nTherefore, fpicker offers a second communicateion mode over shared memory. However, this only works\nif it is possible to establish shared memory between the fuzzer and the target application, which\nmeans that this mode cannot be used when the target is attached to the fuzzer host via USB. In CMD\ninput mode, the communication mode only refers to how the coverage information is communicated back\nto the fuzzer, not how the payload is sent, as this is deferred to an external command.\n\n- **Communication Mode Send:** In send communication mode the payload is sent by using Frida's RPC\n  calling mechanism. This lets the fuzzer execute a JavaScript function within the injected harness\n  script. This function inside the harness can then do all the necessary preparations to call the\n  target function. Once the target function is returned from, coverage collection will stop and the\n  harness can signal the fuzzer that the iteration is finished. This is done by sending the coverage\n  information back to the fuzzer using Frida's send API.\n\n- **Communication Mode SHM:** In SHM communication mode the fuzzer and the harness script\n  communicate via shared memory and semaphores. A buffer in shared memory is used to send the\n  payload and receive the coverage information. Instead of sending and receiving, the two components\n  use waiting and posting to the semaphore. Depending on the system and the target, this introduces\n  quite some perfomance gains.  Especially, because the binary payload is written to memory once and\n  does not have to be encoded and decoded or copied into other memory locations. Unfortunately, this\n  mode sometimes leads to a low stability when running with AFL++. Not sure why, yet.\n\n### Exec Mode\nExec mode can be either *spawn* or *attach*. This is pretty self-explanatory. fpicker can either\nattach to a runnning process or spawn a process. One thing that is a major difference between the\ntwo modes is that, should the attached target crash, fpicker will not try to respawn.\n\n### Standalone Mutator\nIn standalone mode fpicker offers three different input mutation strategies. Nicely put, input\nmutation certainly has lots of room for improvement.\n\n- **Standalone Mutator NULL:** This mutator does not mutate the payload and just returns a copy of\n  the same payload. Mostly for testing purposes. Otherwise not really useful.\n\n- **Standalone Mutator Rand:** A very bad random mutator. All it does is randomly replace values at\n  random locations in the original payload. It does not change the payload length.\n\n- **Standalone Mutator Custom:** This mutator can call an external command to mutate payloads. It\n  writes the payload to stdin and receives the mutated payload from stdout. Due to its shallow\n  implementation it has quite a performance impact.\n\n### Network devices\nWith option `-D remote` it is possible to fuzz a process running on a network device. For this, the\nremote device must be running `frida-server`. As a sample configuration, use SSH with port\nforwarding to bind the `frida-server` default listening port `27042` on the remote device to a\nsocket on the local client.\n```bash\nssh -N user@network.device -L 127.0.0.1:27042:127.0.0.1:27042\n```\nThen use `frida-ps` to validate the configuration by listing processes on the remote device:\n```bash\nfrida-ps -R\n```\n"
 },
 {
  "repo": "cosinekitty/astronomy",
  "language": "C",
  "readme_contents": "<img src=\"https://raw.githubusercontent.com/cosinekitty/astronomy/master/astronomy_engine_logo.png\" width=\"640\">\n\n### Supported Programming Languages\n\n<table style=\"border-width: 0px;\" cellspacing=\"0\" cellpadding=\"10\">\n    <tr>\n        <td style=\"text-align: center;\">\n            <div>C</div>\n            <div><img src=\"source/c/c_language.svg\" width=\"100\" height=\"100\" alt=\"C\" /></div>\n        </td>\n        <td style=\"text-align: center;\"><a href=\"demo/c/\">Examples</a></td>\n        <td style=\"text-align: center;\">\n            <div><a href=\"source/c/\"><img src=\"https://img.shields.io/badge/C%20%2f%20C%2b%2b-v2.1.5-blue\" /></a></div>\n            <div><a href=\"source/c/\">Code &amp; Docs</a></div>\n        </td>\n    </tr>\n    <tr>\n        <td style=\"text-align: center;\">\n            <div>C#</div>\n            <div><img src=\"source/csharp/csharp_language.svg\" width=\"100\" height=\"100\" alt=\"C#\" /></div>\n        </td>\n        <td style=\"text-align: center;\"><a href=\"demo/csharp/\">Examples</a></td>\n        <td style=\"text-align: center;\">\n            <div><a href=\"https://www.nuget.org/packages/CosineKitty.AstronomyEngine/\" target=\"_blank\"><img alt=\"NuGet\" src=\"https://img.shields.io/nuget/v/CosineKitty.AstronomyEngine\"></a></div>\n            <div><a href=\"source/csharp/\">Code &amp; Docs</a></div>\n        </td>\n    </tr>\n    <tr>\n        <td style=\"text-align: center;\">\n            <div>JavaScript</div>\n            <div><img src=\"source/js/nodejs.svg\" width=\"100\" height=\"100\" alt=\"Node.js\" /></div>\n        </td>\n        <td>\n            <div style=\"text-align: center;\">Examples</div>\n            <ul>\n                <li><a href=\"demo/browser/\">Browser</a></li>\n                <li><a href=\"demo/nodejs/\">Node.js</a></li>\n            </ul>\n        </td>\n        <td style=\"text-align: center;\">\n            <div><a href=\"https://www.npmjs.com/package/astronomy-engine\" target=\"_blank\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/astronomy-engine.svg\"></a></div>\n            <div><a href=\"source/js/\">Code &amp; Docs</a></div>\n        </td>\n    </tr>\n    <tr>\n        <td style=\"text-align: center;\">\n            <div>Python</div>\n            <div><img src=\"source/python/python_language.svg\" width=\"100\" height=\"100\" alt=\"Python\" /></div>\n        </td>\n        <td style=\"text-align: center;\"><a href=\"demo/python/\">Examples</a></td>\n        <td style=\"text-align: center;\">\n            <div><a href=\"https://pypi.org/project/astronomy-engine/\" target=\"_blank\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/astronomy-engine\"></a></div>\n            <div><a href=\"source/python/\">Code &amp; Docs</a></div>\n        </td>\n    </tr>\n    <tr>\n        <td style=\"text-align: center;\">\n            <div>Kotlin / JVM</div>\n            <div><img src=\"source/kotlin/kotlin_language.svg\" width=\"100\" height=\"100\" alt=\"Kotlin\" /></div>\n        </td>\n        <td>\n            <div style=\"text-align: center;\">Examples</div>\n            <ul>\n                <li><a href=\"demo/kotlin/\">Kotlin</a></li>\n                <li><a href=\"demo/java/\">Java</a></li>\n            </ul>\n        </td>\n        <td style=\"text-align: center;\">\n            <div><a href=\"https://jitpack.io/#cosinekitty/astronomy\" target=\"_blank\"><img alt=\"JitPack\" src=\"https://img.shields.io/jitpack/v/github/cosinekitty/astronomy\" /></a></div>\n            <div><a href=\"source/kotlin/\">Code &amp; Docs</a></div>\n        </td>\n    </tr>\n</table>\n\n### Overview\n\nAstronomy Engine is a suite of open source libraries for calculating positions of\nthe Sun, Moon, and planets, and for predicting interesting events like oppositions,\nconjunctions, rise and set times, lunar phases, eclipses, transits, and more.\n\nIt supports several popular programming langauges with a consistent API.\nFunction and type names are mostly consistent across all the supported languages.\n\nAstronomy Engine is designed to be small, fast, and accurate to within &plusmn;1 arcminute.\nIt is based on the authoritative and well-tested models\n[VSOP87](https://en.wikipedia.org/wiki/VSOP_(planets))\nand\n[NOVAS C 3.1](https://aa.usno.navy.mil/software/novas/novas_c/novasc_info.php).\n\nThese libraries are rigorously unit-tested against NOVAS,\n[JPL Horizons](https://ssd.jpl.nasa.gov/horizons.cgi),\nand other reliable sources of ephemeris data.\nCalculations are also verified to be identical among all the supported programming languages.\n\n### Features\n\n- Provides calculations for the Sun, Moon, Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto.\n\n- Calculates all supported objects for any calendar date and time for millennia\n  before or after the present.\n\n- Provides heliocentric and geocentric Cartesian vectors of all the above bodies.\n\n- Determines apparent horizon-based positions for an observer anywhere on the Earth,\n  given that observer's latitude, longitude, and elevation in meters.\n  Optionally corrects for atmospheric refraction.\n\n- Calculates rise, set, and culmination times of Sun, Moon, and planets.\n\n- Finds civil, nautical, and astronomical twilight times (dusk and dawn).\n\n- Finds date and time of Moon phases: new, first quarter, full, third quarter\n  (or anywhere in between as expressed in degrees of ecliptic longitude).\n\n- Predicts lunar and solar eclipses.\n\n- Predicts transits of Mercury and Venus.\n\n- Predicts lunar apogee and perigee dates, times, and distances.\n\n- Predicts date and time of equinoxes and solstices for a given calendar year.\n\n- Determines apparent visual magnitudes of all the supported celestial bodies.\n\n- Predicts dates of planetary conjunctions, oppositions, and apsides.\n\n- Predicts dates of Venus' peak visual magnitude.\n\n- Predicts dates of maximum elongation for Mercury and Venus.\n\n- Calculates the positions of Jupiter's four largest moons: Io, Europa, Ganymede, and Callisto.\n\n- Allows custom simulation of the movements of user-defined small bodies,\n  such as asteroids and comets, through the Solar System.\n\n- Converts angular and vector coordinates among the following orientations:\n  - Equatorial J2000\n  - Equatorial equator-of-date\n  - Ecliptic J2000\n  - Topocentric Horizontal\n  - Galactic (IAU 1958)\n\n- Determines which constellation contains a given point in the sky.\n\n- Calculates libration of the Moon.\n\n- Calculates axis orientation and rotation angles for the Sun, Moon, and planets.\n\n### Why I Created This Thing\n\nI have been an amateur astronomer since childhood. I still remember the amazement\nI felt when I saw Saturn through a backyard telescope for the first time.\nAs a software developer, I naturally became interested in combining my love of\nastronomy with my computer programming skills.\n\nIn 2008, I started to learn about formulas for calculating positions\nof the Moon and planets. I discovered many wonderful resources, including\n\n- Paul Schlyter's lucid and educational page\n[How to compute planetary positions](http://www.stjarnhimlen.se/comp/ppcomp.html).\n- [Practical Astronomy with your Calculator](https://www.amazon.com/Practical-Astronomy-Calculator-Peter-Duffett-Smith/dp/0521356997), third edition, by Peter Duffett-Smith, Cambridge University Press. ISBN&nbsp;0&nbsp;521&nbsp;35629&nbsp;6.\n- [Astronomy on the Personal Computer](https://www.amazon.com/Astronomy-Personal-Computer-Oliver-Montenbruck/dp/3540672214/) by Oliver Montenbruck and Thomas Pfleger. ISBN-13:&nbsp;978-3540672210.\n\nI implemented algorithms based on these resources. Over time, however, I noticed that they were not quite\nas accurate as I would like. Their calculated positions differed from those reported by online tools\nlike [JPL Horizons](https://ssd.jpl.nasa.gov/horizons.cgi) and [Heavens Above](https://www.heavens-above.com/)\nby large fractions of a degree in many cases.\n\nIn 2019 I renewed my interest in astronomy calculations, with the goal of creating something more accurate\nthat could be written in JavaScript to run inside a browser. I studied how professional\nastronomers and space agencies did their calculations. First I looked the United States Naval Observatory's\n[NOVAS C 3.1](https://github.com/indigo-astronomy/novas) library. I quickly realized it could not be\nported to the browser environment, because it required very large (hundreds of megabytes)\nprecomputed ephemeris files.\n\nThis led in turn to studying the French *Bureau des Longitudes* model known as\n[VSOP87](https://en.wikipedia.org/wiki/VSOP_(planets)). It requires more computation\nbut the data is much smaller, consisting of trigonometric power series coefficients.\nHowever, it was still too large to fit in a practical web page.\n\nFurthermore, these models were extremely complicated, and far more accurate than what I needed.\nNOVAS, for example, performs relativistic calculations to correct for the bending\nof light through the gravitational fields of planets, and time dilation due to different\nnon-intertial frames of reference! My humble needs did not require this herculean level\nof complexity. So I decided to create Astronomy Engine with the following engineering goals:\n\n- Support JavaScript, C, C#, and Python with the same algorithms, and verify them to produce identical results.\n  (Kotlin support was added in 2022.)\n- No external dependencies! The code must not require anything outside the standard library for each language.\n- Minified JavaScript code less than 120K. (The current size is <!--MINIFIED_SIZE-->116970 bytes.)\n- Accuracy always within 1 arcminute of results from NOVAS.\n- It would be well documented, relatively easy to use, and support a wide variety of common use cases.\n\nThe solution I settled on was to truncate the VSOP87 series to make it as small\nas possible without exceeding the 1 arcminute error threshold.\nI created a code generator that converts the truncated tables into C, C#, JavaScript,\nand Python source code. Then I built unit tests that compare the calculations\nagainst the NOVAS C 3.1 code operating on the DE405 ephemeris and other authoritative\nsources, including the JPL Horizons tool. Basing on VSOP87 and verifying\nagainst independent trusted sources provides extra confidence that everything is correct.\n\nPluto was a special case, because VSOP87 does not include a model for it. I ended up writing\na custom gravitation simulator for the major outer planets to model Pluto's orbit.\nThe results are verified against NOVAS and the model\n[TOP2013](https://www.aanda.org/articles/aa/abs/2013/09/aa21843-13/aa21843-13.html).\n\nAs far as I know, Astronomy Engine is the only open source solution in existence that\ncombines very compact code for four major programming languages with such rigorous\nvalidation and testing at a reasonable accuracy threshold.\nThe 1-arcminute accuracy is not good enough for spacecraft navigation,\nbut it is good enough for most amateur uses, and allows the code to be much\nsimpler, faster, and smaller.\n\nI am committed to maintaining this project for the long term, and I am happy to\nanswer questions about how to solve various astronomy calculation problems\nusing Astronomy Engine. Feel free to reach out on the\n[discussions page](https://github.com/cosinekitty/astronomy/discussions) or\n[submit a new issue](https://github.com/cosinekitty/astronomy/issues).\n\n### Acknowledgements\n\n[![Deploys by Netlify](https://www.netlify.com/img/global/badges/netlify-color-accent.svg)](https://www.netlify.com)\n"
 },
 {
  "repo": "wheybags/wcp",
  "language": "C",
  "readme_contents": "# wcp\nwcp is an experiment in re-implementing something like the standard cp file copy tool. The goal is to be as fast as possible, and provide the best possible progress bar, by counting up the total copy size in parallel with running the copy.\n\nLinux only, for now. It should work on any kernel >= 5.6, but I've only tested on 5.8.\n\n## Status\nwcp can copy files very fast, and show a pretty nice progress bar. It's probably not robust enough for daily use just yet.\n![](wcp_demo.gif)\n\n## How fast?\nUp to 70% faster than cp, depending on the size of the files being copied. The smaller the files the more dramatic the speedup. It seems to be a good bit faster even for large files. Here's some test data from my machine (64gb RAM, with a Samsung NVME SSD):\n\n20 512MiB files:\n```\nwcp                                 3.97s,  2579.59 MiB/s, 5.28 files/s\ncp -r                               8.44s,  1213.38 MiB/s, 2.48 files/s\nrsync -r --inplace -W --no-compress 17.26s, 593.33 MiB/s,  1.21 files/s\n```\n\n7,000 1MiB files:\n```\nwcp                                 2.64s,  2651.89 MiB/s, 2651.89 files/s\ncp -r                               8.29s,  844.51  MiB/s, 844.51 files/s\nrsync -r --inplace -W --no-compress 13.22s, 529.57  MiB/s, 529.57 files/s\n```\n\n200,000 1KiB files:\n```\nwcp                                 3.92s 200.51, MiB/s, 51020.66 files/s\nrsync -r --inplace -W --no-compress 8.09s 97.15,  MiB/s, 24722.00 files/s\ncp -r                               9.23s 85.15,  MiB/s, 21668.58 files/s\n```\n\n## How is it so fast?\nI'm using [io_uring](https://kernel.dk/io_uring.pdf), a relatively new IO / syscall system in the linux kernel. It allows you to run system calls asynchronously via a ring buffer in memory shared by user process and kernel, instead of using a full syscall with all of its overhead. I also use a lot of CPU, allocate as much RAM as I can get away with, and the implementation is 100% non-portable. Tradeoffs ;)\n\n## Build instructions\n\n```bash\nmkdir build\ncd build\ncmake .. -DCMAKE_BUILD_TYPE=Release\nmake\n```\n\nThen use `./wcp` to run\n"
 },
 {
  "repo": "paulross/PythonExtensionPatterns",
  "language": "C",
  "readme_contents": "# PythonExtensionPatterns\n\nExamples of reliable coding of Python 'C' extensions by Paul Ross.\n\nThe full documentation is on [Read the Docs](http://pythonextensionpatterns.readthedocs.org/en/latest/index.html).\n\nCode examples and documentation source are right here on GitHub.\n\n\n## TODO\n\n* Reorganinse source code.\n* More here...\n"
 },
 {
  "repo": "kaniini/slabbed-or-not",
  "language": "C",
  "readme_contents": "# slabbed-or-not\n\nDetect if your container/VPS is running under a specific hypervisor.\n\nWhy would you want this?\n\nSome hosting providers run their container hosting services (OpenVZ,\nLXC VPS, Docker-as-a-Service, etc) under a parent hypervisor, without\ndisclosing that practice.  Worse yet, a few providers have been caught\ndoing this while explicitly claiming not to.\n\nThere are also of course, providers which do rightfully disclose their\nusage of a parent hypervisor for their services, usually as part of a\nhigh availability scheme.\n\nIn essence, the point of this tool is simply to disclose whether or not\nyour container is running under a hypervisor, and what information can be\ncollected about the hypervisor from inside the container.\n\nIt also detects common container types (i.e. OpenVZ), to clarify to the\nuser whether or not their VPS environment is inside a container, whether it's\nin a hypervisor, or whether both aspects apply (container inside hypervisor).\n\n## usage\n\nJust download the source and go, basically.  You'll need a compiler installed, of course.\n\n```shell\nkaniini@localhost ~/slabbed-or-not> make\ncc -o slabbed-or-not slabbed-or-not.c\nkaniini@localhost ~/slabbed-or-not> ./slabbed-or-not\nContainer: OpenVZ\nHypervisor: Xen PV\nVersion: 4.3\n```\n\n## what it can detect\n\n* Xen\n  * PV or HVM mode\n  * Hypervisor version\n* VMware\n  * Hypervisor type (Workstation, GSX/ESX)\n  * Enumeratable virtual devices\n  * VMware Tools backdoor protocol version (usually 6)\n  * Virtualized hardware model version\n* Hyper-V\n* KVM\n* bhyve\n"
 },
 {
  "repo": "Martazza/Huawei-Bootloader-Unlocker",
  "language": "C",
  "readme_contents": "# Huawei Bootloader Unlocker\nThis tool let you unlock the bootloader of your Huawei / Honor Device even if the code can't be found.\n\nAfter closing the official EMUI website, which allowed to retrieve the code to unlock the bootloader of Huawei/Honor phones, here's the fastest free way to retrieve it by yourself.\n\n\n### How To use it\n1. Compile the .C file\n    1. `gcc source.c -o exec`\n    2. `chmod +x exec`\n2. Install Android Developer Tools\n3. Enable USB Debug on your device\n4. Enable OEM Unlock\n3. Connect your device to your computer\n4. Use ```adb reboot bootloader``` to reboot your device in bootloader mode\n5. Run the program\n    1. `./exec`\n6. Wait\n\n## FAQ & Troubleshooting\n**The app on Windows doesn't detect my device. What could be the issue?**\nWindows most likely doesn't recognise your device in ADB mode. \nInstall the universal ADB drivers from [here](http://dl.adbdriver.com/upload/adbdriver.zip) then\nreboot your PC and try again.\n\n# License\n```\nMIT License\n\nCopyright (c) 2019 Martazza\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n"
 },
 {
  "repo": "touqir14/Microdict",
  "language": "C",
  "readme_contents": "# Microdict\nA high performance python hash table library that is generally faster and consumes significantly less memory than Python Dictionaries. It currently supports Python 3.5+.\n___\n### Why Microdict? Why create another Hash table library when there is the builtin Python Dictionary?\nPython Dictionaries are fast but their memory consumption can also be high at the same time. This is partly due to the nature of Python keeping data, within RAM, in the form of PyObjects, which consume far more memory than native types such as Integers and Character Arrays. As a result, Python Dictionaries can be prohibitive in many cases while building memory intensive python applications. This motivated me to develop a typed python hash table library that consumes significantly (upto 7 times) less memory compared to python dictionaries. It is also faster than python dictionaries. Moreover, it's underlying C implementation can also outperform Google's highly optimized [Swiss Table](https://abseil.io/blog/20180927-swisstables) and Facebook's [F14](https://engineering.fb.com/2019/04/25/developer-tools/f14/) hash tables. See the [Performance Section](https://github.com/touqir14/Microdict/blob/main/README.md#performance).\n____\n### Installation and Building\nYou can install Microdict using pip : ```pip install microdict```.\n\nMicrodict is absolutely built using C extensions and as such building it will require Python C api header files. Build and install the package using \n```python setup.py install``` from the terminal after cloning the repository. Microdict is tested to work on Linux, Mac OSX, and Windows systems. You will need GCC 7+ on linux/mac osx systems and Visual C++ 14+ compiler on Windows systems to build the package. For the best performance use on a 64 bit system.\n____\n### Run tests\nOnce installed, type the following code snippet in your python interpreter to run the tests:\n```python\nfrom microdict import run_tests\nrun_tests.run()\n```\n____\n### Usage\nThe following code snippet shows common uses of the library.\n\n```python\nfrom microdict import mdict\n\ndict_i32 = mdict.create(\"i32:i32\") # Generates a dictionary with key and value type of signed 32 bit integer.\ndict_i32[1] = 2 # Just like python dictionaries, setting a key value pair.\nprint(4 in dict_i32) # prints False.\n\nprint(dict_i32[1]) # prints 2\ntry:\n   print(dict_i32.pop(4)) # prints None.\nexcept KeyError:\n   pass\nprint(dict_i32.pop(1)) # Removes [1,2] from the hashtable and prints 2.\n\ndict_i32[10] = 0\ndict_i32[5] = 1\ndict_i32[6] = 8\n\nfor k in dict_i32:\n   print(k) # Will print 10, 5, 6\n  \nfor v in dict_i32.values():\n   print(v) # Will print 0, 1, 8\n  \nfor k,v in dict_i32.items():\n   print(k,v) # Will print all the items.\n\nd2 = dict_i32.copy() # Creates a deep copy of dict_i32.\ndict_i32.clear([10,6]) # Removes the pairs [10,0] and [6,8]\ndict_i32.clear() # Makes the dictionary empty.\n\npydict = d2.to_Pydict() # Returns a python dictionary containing all the items in d2\npydict[120] = 5\npydict[42] = 9\n\nd2.update(pydict) # d2 now will additionally have the pairs [120, 5] and [42, 9]\n\ndict_i32[111] = 89\ndict_i32[123] = 1\nd2.update(dict_i32) # d2 now will additionally have the pairs [111, 89] and [123, 1].\n\nprint(list(d2.items())) # prints all d2 items\n\"\"\"\nFaster approach shown below. d2.get_items() creates and returns a list of all items. \nSo if you don't need a list container, iterate using d2.items() for memory efficiency. \nSame applies for other d2.get_* methods shown below.\n\"\"\"\nprint(d2.get_items()) # \nprint(list(d2.values())) # prints all d2 values\nprint(d2.get_values()) # Same but faster approach\nprint(list(d2)) # prints all d2 keys\nprint(d2.get_keys()) # Same but faster approach\n\n```\n___\n#### Hash Table types\nCurrently, Microdict includes 5 types of dictionaries:\n* ```\"i32:i32\"``` -> 32 bit signed keys and 32 bit signed values\n* ```\"i32:i64\"``` -> 32 bit signed keys and 64 bit signed values\n* ```\"i64:i32\"``` -> 64 bit signed keys and 32 bit signed values\n* ```\"i64:i64\"``` -> 64 bit signed keys and 64 bit signed values\n* ```\"str:str\"``` -> string keys and string values.\n\n___\n#### Method Documentations\n* **microdict.mdict.create** (*dtype, key_len=None, val_len=None*)\n\n   : Returns a Microdict hash table of any of the types given [above](#hash-table-types).\n   \n   **Parameters:**\n   \n   * *dtype:*  A python string type (```str```) that sets the hash table type to be created. It can be any one of the above [types](#hash-table-types).\n   * *key_len:*  A python Integer type (```int```). It sets the maximum number of bytes the characters of a key (UTF-8 string) requires. Passing a UTF-8 encoded string key which consumes more bytes than *key_len* will not be accepted. This argument is only applicable when ```dtype=\"str:str\"```. It only accepts a value of at most 65355 and a larger value will raise a ```TypeError```.\n   * *val_len:* A python Integer type(```int```). It sets the maximum number of bytes the characters of a value (UTF-8 string) requires. Passing a UTF-8 encoded string value which consumes more bytes than *val_len* will not be accepted. This argument is only applicable when ```dtype=\"str:str\"```. It only accepts a value of at most 65355 and a larger value will raise a ```TypeError```.\n   \n* **microdict.mdict.listDictionaryTypes** ()\n\n   : Prints a series of lines of the form : ```Key Type: key_t . Value Type: val_t```, where ```key_t:val_t``` forms a type given [above](#hash-table-types).\n   \nThe following are the methods that are common to all hash table types returned by ```mdict.create```.\n* **clear** (*key_list = None*)\n\n   : Returns None. If *key_list* is not provided, the **clear** method will delete all items from the hash table.\n   \n   **Parameters:**\n   \n   * *key_list:* It is an optional argument but not a keyword optional argument (keyword must not be provided). So, it suffices: ```keys = [1,2]; d.clear(keys)```. If provided, it must be of type ```list```. The entries within *key_list* are the keys that will be removed from the hash table. By default, any entry that is not present in the hash table will be skipped. For any of the integer hash table types, any non python ```int``` entry will be skipped. It is upto the programmer to pass the proper sized integer to prevent overflows. For ```str:str``` type, the entries must be python UTF-8 strings with UTF-8 character bytes upto *key_len* as set by ```mdict.create``` and otherwise, that entry will be skipped.\n   \n* **copy** ()\n\n   : Returns a deep copy of the Microdict Hash table of the same type as the caller Hash table object.\n   \n* **get_items** ()\n\n   : Creates and returns a python ```list``` containing all the items (key, value) in the hash table.\n   \n* **get_keys** ()\n   \n   : Creates and returns a python ```list``` containing all the keys in the hash table.\n   \n* **get_values** ()\n\n   : Creates and returns a python ```list``` containing all the values in the hash table.\n   \n* **items** ()\n\n   : Used to iterate over items using a ```for``` loop. Example : ```for k,v in d.items() : print(k, v)```\n   \n* **pop** (*key*)\n\n   : Deletes a *key* from the hash table and returns its corresponding value. If the *key* is not present, then a ```KeyError``` is raised.\n   \n   **Parameters:**\n   \n   * *key:* For any of the integer hash table types, any non python ```int``` entry will raise a ```TypeError```. It is upto the programmer to pass the proper sized integer to prevent overflows. For ```str:str``` type, the entries must be python UTF-8 strings with UTF-8 character bytes upto *key_len* as set by ```mdict.create``` and otherwise, a ```TypeError``` will be raised.\n   \n* **to_Pydict** ()\n\n   : Creates and returns a python dictionary containing all items present in the Microdict hash table.\n   \n* **update** (*dictionary*)\n\n   : Inserts all the items present in the dictionary into the Microdict hash table.\n   \n   **Parameters:**\n   \n   * *dictionary:* Must be either a python dictionary or a Microdict hash table. If it is a python dictionary, then all its items that are of the same type as the Microdict hash table will be inserted. The rest will be skipped by default. The conditions given in the method documentation of **clear** regarding type constraints apply here too.   \n   \n* **values** ()\n\n   : Used to iterate over values using a ```for``` loop. Example : ```for v in d.values() : print(v)```.\n\n___\n\n### Performance\n#### Competing with Python Dictionary\nEach of the cells in the tables below are of the format (Speed Gain, Space Gain). \n\n* Speed Gain is defined as : <img src=\"https://render.githubusercontent.com/render/math?math=\\dfrac{\\text{Average execution time for competing hash table}}{\\text{Average execution time for Microdict hash table}}\">. \n\n* Similarly, Space Gain : <img src=\"https://render.githubusercontent.com/render/math?math=\\dfrac{\\text{Average memory consumed by competing hash table}}{\\text{Average memory consumed by Microdict hash table}}\">\n\nExperiments were carried out for the types ```\"i32:i32\"```, ```\"i64:64\"```, ```\"str:str\"``` (key and value length kept to 8 characters). Speed Gain and Space Gain are computed by averaging over the results of 30 experiments carried out using (unique) randomly generated data. Space consumption was computed using the [psutil](https://github.com/giampaolo/psutil) library. Time consumption was computed using the ```time.perf_counter``` method. All the experiments were conducted with Python 3.8.2 on a 64 bit Ubuntu 20.04 LTS system. The following table shows the benchmarks against Python Dictionary for retrieving all values using the full set of keys and the ```[]``` operator.\n\n| #Items     | ```i32:i32``` | ```i64:i64```| ```str:str```|\n|------------|---------------|--------------|--------------|\n| 100000     | 1.48x, 7.13x  | 1.29x, 4.67x | 1.43x, 5.19x |\n| 1000000    | 1.47x, 4.23x  | 1.48x, 2.64x | 1.46x, 4.46x |\n| 10000000   | 1.44x, 4.77x  | 1.48x, 3.02x | 1.53x, 4.97x |\n| 3x10000000 | 1.55x, 4.19x  | 1.3x , 2.57x | 1.36x, 3.93x |\n\n___\n#### Competing with Google's Swiss Table and Facebook's F14\nMicrodict's underlying C implementation was benchmarked against Swiss Table```->```[abseil::flat_hash_map](https://abseil.io/docs/cpp/guides/container) and F14```->```[folly::F14FastMap](https://github.com/facebook/folly/blob/master/folly/container/F14.md) to further test its capabilities. The data types were set as above using the same settings. The following tables show the results for retrieving all values using the keys.\n\n<table>\n<tr><th>abseil::flat_hash_map </th><th>folly::F14FastMap </th></tr>\n<tr><td>\n\n| #Items     | ```i32:i32``` | ```i64:i64```| ```str:str```|   \n|------------|---------------|--------------|--------------|\n| 1000000    | 1.22x, 1.56x  | 1.02x, 1.08x | 1.33x, 1.82x |\n| 10000000   | 1.14x, 1x     | 1.09x, 1.33x | 1.73x, 1.67x |\n| 3x10000000 | 1.11x, 1.27x  | 1.15x, 1.03x | 1.68x, 1.65x |\n\n</td><td>\n\n| #Items     | ```i32:i32``` | ```i64:i64```| ```str:str```|   \n|------------|---------------|--------------|--------------|\n| 1000000    | 2.96x, 1.56x  | 2.74x, 1x    | 3.12x, 1.39x |\n| 10000000   | 2.11x, 1x     | 2.12x, 1.29x | 3.45x, 1.4x  |\n| 3x10000000 | 1.84x, 1.22x  | 1.97x, 1x    | 3.36x, 1.21x |\n\n</td></tr> </table>\n\n"
 },
 {
  "repo": "horsicq/stringsx64dbg",
  "language": "C",
  "readme_contents": "[![GitHub tag (latest SemVer)](https://img.shields.io/github/tag/horsicq/stringsx64dbg.svg)](https://github.com/horsicq/stringsx64dbg/releases)\r\n[![GitHub All Releases](https://img.shields.io/github/downloads/horsicq/stringsx64dbg/total.svg)](https://github.com/horsicq/stringsx64dbg/releases)\r\n\r\nPlugin for x64dbg.\r\n\r\n![alt text](https://github.com/horsicq/stringsx64dbg/blob/master/screenshot.jpg \"Screenshot\")\r\n\r\nMore info: http://n10info.blogspot.com/2019/03/strings-plugin-for-x64dbg.html\r\n\r\n## Special Thanks\r\n\r\n- [PELock Software Protection & Reverse Engineering](https://www.pelock.com)\r\n"
 },
 {
  "repo": "tinylcy/vino",
  "language": "C",
  "readme_contents": "# Vino\n\nVino is a lightweight and efficient web server written in the C programming language.\n\n## Compile and run\n\nFor now Vino only support Linux 2.6 or later.\n\nPlease make sure you have [cmake](https://cmake.org) installed.\n\n```cmake\nmkdir build && cd build\ncmake .. && make\n./vino\n```\n\nIf you want to build for debug and print debug information when compiling, use `cmake -DCMAKE_BUILD_TYPE=Debug <path>`. If you want to enable profiling, you can use `cmake -DCMAKE_C_FLAGS=-pg <path>`.\n\nBy default the server accepts connections on port 8080, if you want to assign other port for the server, run it as `./vino -p|--port <port>`. For more options, please type `./vino -h|--help` into terminal.\n\n## Features\n\n- [x] Single-threaded, non-blocking I/O based on event-driven model\n- [x] HTTP persistent connection (HTTP Keep-Alive)\n- [x] A timer for executing the handler after having waited the specified number of milliseconds\n- [x] A parser for extracting request line and headers from HTTP request message\n- [x] A unified memory pool\n- [x] HTTP GET method\n\n## Performance \n\nI have made a small performance test with [Nginx](https://nginx.org/en/), a high-performance web server and reverse proxy (I have learned a lot from it). In a way it is not a fair comparison, for the processing logic in Vino is relatively simpler than Nginx.\n\n```markdown\nab -n 10000 -c 500 http://<IP>:<PORT>/index.html\n```\n\n```markdown\ncpu: Intel(R) Xeon(R) CPU L5630  @ 2.13GHz, 8 cores.\nmem: 32GB.\n```\n\n**Nginx**\n\n```markdown\nServer Software:        nginx/1.0.14\nServer Hostname:        scut.xxxx.com\nServer Port:            80\n\nDocument Path:          /index.html\nDocument Length:        151 bytes\n\nConcurrency Level:      500\nTime taken for tests:   5.193 seconds\nComplete requests:      10000\nFailed requests:        0\nTotal transferred:      3620000 bytes\nHTML transferred:       1510000 bytes\nRequests per second:    1925.85 [#/sec] (mean)\nTime per request:       259.626 [ms] (mean)\nTime per request:       0.519 [ms] (mean, across all concurrent requests)\nTransfer rate:          680.82 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        7   93 292.0     20    2281\nProcessing:     4  104 257.5     20    4206\nWaiting:        4  100 255.2     20    4206\nTotal:         20  197 416.3     41    4250\n\nPercentage of the requests served within a certain time (ms)\n  50%     41\n  66%     47\n  75%     57\n  80%    250\n  90%    673\n  95%   1053\n  98%   1504\n  99%   2158\n 100%   4250 (longest request)\n \n```\n\n**Vino**\n\n```markdown\n\nServer Software:        Vino\nServer Hostname:        scut.xxxx.com\nServer Port:            8080\n\nDocument Path:          /index.html\nDocument Length:        151 bytes\n\nConcurrency Level:      500\nTime taken for tests:   4.855 seconds\nComplete requests:      10000\nFailed requests:        0\nTotal transferred:      2490000 bytes\nHTML transferred:       1510000 bytes\nRequests per second:    2059.81 [#/sec] (mean)\nTime per request:       242.741 [ms] (mean)\nTime per request:       0.485 [ms] (mean, across all concurrent requests)\nTransfer rate:          500.87 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        6  108 287.3     21    2305\nProcessing:     3   91 180.7     21    1987\nWaiting:        3   87 178.3     20    1987\nTotal:         15  199 360.5     43    3165\n\nPercentage of the requests served within a certain time (ms)\n  50%     43\n  66%     51\n  75%    200\n  80%    269\n  90%    724\n  95%   1060\n  98%   1289\n  99%   1644\n 100%   3165 (longest request)\n\n```\n## TODO\n\n- [ ] FastCGI\n- [ ] HTTP POST and other methods\n- [ ] More HTTP/1.1 features"
 },
 {
  "repo": "timonwong/SublimeAStyleFormatter",
  "language": "Python",
  "readme_contents": "Sublime Text 2 & 3 AStyle Formatter Plugin\n==========================================\n\n[![Travis-CI Build Status](https://travis-ci.org/timonwong/SublimeAStyleFormatter.svg?branch=master)](https://travis-ci.org/timonwong/SublimeAStyleFormatter)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/timonwong/SublimeAStyleFormatter?branch=master&svg=true)](https://ci.appveyor.com/project/timonwong/SublimeAStyleFormatter)\n\nDescription\n-----------\n\nSublimeAStyleFormatter is a simple code formatter plugin for Sublime Text.\nIt provides ability to format C, C++, Cuda-C++, OpenCL, Arduino, C#, and Java files.\n\n**NOTE**: Syntax files required to be installed separately for Cuda-C++ and OpenCL.\n\n### Donation\n\nIf you find my work useful, please consider buying me a cup of coffee, all\ndonations are much appreciated :)\n\n[![Donate via PayPal](http://dl.dropbox.com/u/2451120/donate-with-paypal.png)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=GGVE2BPUP7KEC)\n\nInstallation\n------------\n\n### With the Package Control plugin\n\nThe easiest way to install SublimeAStyleFormatter is through [Package Control].\n\n[Package Control]: http://wbond.net/sublime_packages/package_control\n\nOnce you have Package Control installed, restart Sublime Text.\n\n1. Bring up the Command Palette (<kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>P</kbd>\non Windows and Linux. <kbd>\u2318</kbd>+<kbd>\u21e7</kbd>+<kbd>P</kbd> on OS X).\n2. Type \"Install\" and select \"Package Control: Install Package\".\n3. Select \"SublimeAStyleFormatter\" from list.\n\nThe advantage of using Package Control is that it will keep SublimeAStyleFormatter up to date.\n\n### Manual Install\n\n**Without Git:**\n\n[Download](https://github.com/timonwong/SublimeAStyleFormatter) the latest source code,\nand extract it to the Packages directory.\n\n**With Git:**\n\nType the following command in your Sublime Text 2 or Sublime Text 3 Packages directory:\n\n`git clone git://github.com/timonwong/SublimeAStyleFormatter.git`\n\nThe \"Packages\" directory is located at:\n\n**Sublime Text 2**\n\n* **Windows**: `%APPDATA%\\Sublime Text 2\\Packages`\n* **Linux**: `~/.config/sublime-text-2/Packages/`\n* **OS X**: `~/Library/Application Support/Sublime Text 2/Packages/`\n\n**Sublime Text 3**\n\n* **Windows**: `%APPDATA%\\Sublime Text 3\\Packages`\n* **Linux**: `~/.config/sublime-text-3/Packages/`\n* **OS X**: `~/Library/Application Support/Sublime Text 3/Packages/`\n\nUsage\n-----\n\n### Key Bindings\n\nThe default key bindings for this plugin:\n\n**Windows, Linux:**\n\n* <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>F</kbd>: Format current file.\n* <kbd>Ctrl</kbd>+<kbd>K</kbd>, <kbd>Ctrl</kbd>+<kbd>F</kbd>: Format current selection.\n\n**OSX:**\n\n* <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>F</kbd>: Format current file.\n* <kbd>\u2318</kbd>+<kbd>K</kbd>, <kbd>\u2318</kbd>+<kbd>F</kbd>: Format current selection.\n\n### Command Palette\n\nOpen the command palette, it appears as `SublimeAStyleFormatter: Format Current File` and\n`SublimeAStyleFormatter Format Current Selection`.\n\nSettings\n--------\n\n### Per-project Settings\n\nBefore starting, you may want to have a look at SublimeAStyleFormatter.sublime-settings.\n\nTo edit your project setting, select `Project/Edit Project` from main menu. A project setting contains\nper-project settings for SublimeAStyleFormatter should look like this:\n\n```javascript\n{\n    \"settings\":\n    {\n        \"AStyleFormatter\":\n        {\n        }\n    }\n}\n```\n\nFor example, if you don't want to inherit the default settings, instead, use your own astylerc file for\nC and C++ individually, then your project setting might look like this:\n\n```javascript\n{\n    // project folders, etc\n    // ...\n    // project settings\n    \"settings\":\n    {\n        \"AStyleFormatter\":\n        {\n            \"options_default\":\n            {\n                // Use 2 spaces for indentation\n                \"indent\": \"spaces\",\n                \"indent-spaces\": 2\n            },\n            \"options_c\":\n            {\n                \"use_only_additional_options\": true,\n                \"additional_options_file\": \"/path/to/your/astylerc/for/c\"\n            },\n            \"options_c++\":\n            {\n                \"use_only_additional_options\": true,\n                \"additional_options_file\": \"/path/to/your/astylerc/for/c++\"\n            }\n        }\n    }\n}\n```\n\n\nWhat's New\n-------------\n\n[CHANGELOG.md](./CHANGELOG.md)\n\n\nLicense\n------\n\nThis plugin is using MIT License:\n\n    Copyright (c) 2012-2015 Timon Wong\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy of\n    this software and associated documentation files (the \"Software\"), to deal in\n    the Software without restriction, including without limitation the rights to\n    use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n    of the Software, and to permit persons to whom the Software is furnished to do\n    so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE.\n\nCredits\n-------\n\n**[Artistic Style]** - A Free, Fast and Small Automatic Formatter for C, C++, C#,\nand Java Source Code.\n\nLicensed under [GNU Lesser General Public License version 3.0]\n\n[Artistic Style]: http://sourceforge.net/projects/astyle/\n[GNU Lesser General Public License version 3.0]: http://astyle.sourceforge.net/license.html\n\nDonors\n------\n\n[DONORS.md](./DONORS.md)\n"
 },
 {
  "repo": "tow/sunburnt",
  "language": "Python",
  "readme_contents": "Sunburnt\n========\n\nSunburnt is a Python-based interface for working with the `Apache Solr\n<http://lucene.apache.org/solr/>`_ search engine.\n\nIt was written by Toby White <toby@timetric.com> for use in the `Timetric\nplatform <http://timetric.com>`_.\n\nPlease send queries/comments/suggestions to the `mailing list\n<http://groups.google.com/group/python-sunburnt>`_.\n\nBugs can be filed on the `issue tracker <https://github.com/tow/sunburnt/issues>`_.\n\nIt's tested with Solr 1.4.1 and 3.1; previous versions were known to work\nwith 1.3 and 1.4 as well.\n\nFull documentation can be found at http://opensource.timetric.com/sunburnt/index.html.\n\nDependencies\n============\n\n- Requirements:\n\n  * `httplib2 <http://code.google.com/p/httplib2/>`_ **or** `requests <http://requests.readthedocs.org/>`_\n  * `lxml <http://lxml.de>`_\n\n- Strongly recommended:\n\n  * `mx.DateTime <http://www.egenix.com/products/python/mxBase/mxDateTime/>`_\n\n    Sunburnt will happily deal with dates stored either as Python datetime\n    objects, or as mx.DateTime objects. The latter are preferable,\n    having better semantics and a wider representation range. They will\n    be used if present, otherwise sunburnt will fall back to Python\n    datetime objects.\n\n  * `pytz <http://pytz.sourceforge.net>`_\n\n    If you're using native Python datetime objects with Solr (rather than\n    mx.DateTime objects) you should also have pytz installed to guarantee\n    correct timezone handling.\n\n- Optional (only to run the tests)\n\n  * `nose <http://somethingaboutorange.com/mrl/projects/nose/>`_\n"
 },
 {
  "repo": "gorilla-co/s3pypi",
  "language": "Python",
  "readme_contents": "# S3PyPI\n\nS3PyPI is a CLI for creating a Python Package Repository in an S3 bucket.\n\n\n## Alternatives\n\n- [AWS CodeArtifact](https://aws.amazon.com/codeartifact/) is a fully managed\n  service that integrates with IAM.\n\n\n## Getting started\n\n### Installation\n\nInstall s3pypi using pip:\n\n```console\n$ pip install s3pypi\n```\n\n\n### Setting up S3 and CloudFront\n\nBefore you can start using `s3pypi`, you must set up an S3 bucket for storing\npackages, and a CloudFront distribution for serving files over HTTPS. Both of\nthese can be created using the [Terraform](https://www.terraform.io/)\nconfiguration provided in the `terraform/` directory:\n\n```console\n$ git clone https://github.com/gorilla-co/s3pypi.git\n$ cd s3pypi/terraform/\n\n$ terraform init\n$ terraform apply\n```\n\nYou will be asked to enter your desired AWS region, S3 bucket name, and domain\nname for CloudFront. You can also enter these in a file named\n`config.auto.tfvars`:\n\n```terraform\nregion = \"eu-west-1\"\nbucket = \"example-bucket\"\ndomain = \"pypi.example.com\"\n```\n\n#### DNS and HTTPS\n\nThe Terraform configuration assumes that a [Route 53 hosted zone] exists for\nyour domain, with a matching (wildcard) certificate in [AWS Certificate\nManager]. If your certificate is a wildcard certificate, add\n`use_wildcard_certificate = true` to `config.auto.tfvars`.\n\n#### Distributed locking with DynamoDB\n\nTo ensure that concurrent invocations of `s3pypi` do not overwrite each other's\nchanges, the objects in S3 can be locked via an optional DynamoDB table (using\nthe `--lock-indexes` option). To create this table, add `enable_dynamodb_locking\n= true` to `config.auto.tfvars`.\n\n#### Basic authentication\n\nTo enable basic authentication, add `enable_basic_auth = true` to\n`config.auto.tfvars`. This will attach a [Lambda@Edge] function to your\nCloudFront distribution that reads user passwords from [AWS Systems Manager\nParameter Store]. Users and passwords can be configured using the `put_user.py`\nscript:\n\n```console\n$ basic_auth/put_user.py pypi.example.com alice\nPassword:\n```\n\nThis creates a parameter named `/s3pypi/pypi.example.com/users/alice`. Passwords\nare hashed with a random salt, and stored as JSON objects:\n\n```json\n{\n  \"password_hash\": \"7364151acc6229ec1468f54986a7614a8b215c26\",\n  \"password_salt\": \"RRoCSRzvYJ1xRra2TWzhqS70wn84Sb_ElKxpl49o3Y0\"\n}\n```\n\n#### Terraform module\n\nThe Terraform configuration can also be included in your own project as a\nmodule:\n\n```terraform\nprovider \"aws\" {\n  region = \"eu-west-1\"\n}\n\nprovider \"aws\" {\n  alias  = \"us_east_1\"\n  region = \"us-east-1\"\n}\n\nmodule \"s3pypi\" {\n  source = \"github.com/gorilla-co/s3pypi//terraform/modules/s3pypi\"\n\n  bucket = \"example-bucket\"\n  domain = \"pypi.example.com\"\n\n  use_wildcard_certificate = true\n  enable_dynamodb_locking  = true\n  enable_basic_auth        = true\n\n  providers = {\n    aws.us_east_1 = aws.us_east_1\n  }\n}\n```\n\n#### Migrating from s3pypi 0.x to 1.x\n\nExisting resources created using the CloudFormation templates from s3pypi 0.x\ncan be [imported into Terraform] and [removed from CloudFormation]. For example:\n\n```console\n$ terraform init\n$ terraform import module.s3pypi.aws_s3_bucket.pypi example-bucket\n$ terraform import module.s3pypi.aws_cloudfront_distribution.cdn EDFDVBD6EXAMPLE\n$ terraform apply\n```\n\n[imported into Terraform]: https://www.terraform.io/docs/import/index.html\n[removed from CloudFormation]: https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/\n\nIn this new configuration, CloudFront uses the S3 REST API endpoint as its\norigin, not the S3 website endpoint. This allows the bucket to remain private,\nwith CloudFront accessing it through an [Origin Access Identity (OAI)]. To make\nthis work with your existing S3 bucket, all `<package>/index.html` objects must\nbe renamed to `<package>/`. You can do so using the provided script:\n\n```console\n$ scripts/migrate-s3-index.py example-bucket\n```\n\n[Origin Access Identity (OAI)]: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\n\n## Usage\n\n### Distributing packages\n\nYou can now use `s3pypi` to upload packages to S3:\n\n```console\n$ cd /path/to/your-project/\n$ python setup.py sdist bdist_wheel\n\n$ s3pypi dist/* --bucket example-bucket [--prefix PREFIX] [--acl ACL]\n```\n\nSee `s3pypi --help` for a description of all options.\n\n\n### Installing packages\n\nInstall your packages using `pip` by pointing the `--extra-index-url` to your\nCloudFront domain. If you used `--prefix` while uploading, then add the prefix\nhere as well:\n\n```console\n$ pip install your-project --extra-index-url https://pypi.example.com/PREFIX/\n```\n\nAlternatively, you can configure the index URL in `~/.pip/pip.conf`:\n\n```\n[global]\nextra-index-url = https://pypi.example.com/PREFIX/\n```\n\n\n## Roadmap\n\nCurrently there are no plans to add new features to s3pypi. If you have any\nideas for new features, check out our [contributing guidelines](CONTRIBUTING.md)\non how to get these on our roadmap.\n\n\n## Contact\n\nGot any questions or ideas? We'd love to hear from you. Check out our\n[contributing guidelines](CONTRIBUTING.md) for ways to offer feedback and\ncontribute.\n\n\n## License\n\nCopyright (c) [Gorillini NV](https://gorilla.co).\nAll rights reserved.\n\nLicensed under the [MIT](LICENSE) License.\n\n\n[Route 53 hosted zone]: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/AboutHZWorkingWith.html\n[AWS Certificate Manager]: https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-public.html\n[Lambda@Edge]: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html\n[AWS Systems Manager Parameter Store]: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\n"
 },
 {
  "repo": "msleal/asciivmssdashboard",
  "language": "Python",
  "readme_contents": "# ASCii VMSS Dashboard v2.0\nDashboard to show and configure Azure VM Scale Set status, properties...\nVersion 2.0 New Features:\n 1. New Azure regions (GA), now the dashboard shows 46 Azure Regions!\n 2. UniCurses included in the tool and a pip package for easy of installation (just execute: pip install asciivmssdashboard).\n 3. PDCurses library included as well (32bits and 64bits versions) for Windows machines (ps.: 64bits version needs a fix as it compiled OK but is crashing in runtime).\n 4. Linux and Windows, Python 2 and 3 as well as 32bits and 64bits packages ready to install! Just a note above for the Windows 64bits version of PDCurses. \n 5. New DEMO Execution Mode where you can just install the tool and execute it for a quick and nice demo (no need to have a real VMSS setup). Ideal to quickly see the tool in action and Azure/VMSS demos.\n\n![Image of ASCii VMSS Dashboard](https://raw.githubusercontent.com/msleal/msleal.github.com/master/screencast-win_animation.gif)\n\nSimple and easy to start VMs...\n![Image of ASCii VMSS Dashboard](https://raw.githubusercontent.com/msleal/msleal.github.com/master/ascii-screencast.gif)\n\nWatch live graphs of Insights Telemetry data, logs and more!\n![Image of ASCii VMSS Dashboard](https://raw.githubusercontent.com/msleal/msleal.github.com/master/asciidash-v1-4.png)\n\nFear nothing, the global map is still here...\n![Image of ASCii VMSS Dashboard](https://raw.githubusercontent.com/msleal/msleal.github.com/master/asciidash-v1-4-map.png)\n\n\n## Installation\n\nUSING DOCKER (Ubuntu):\n- If you don't have docker installed, first execute:\n  sudo apt-get install -y docker.io\n\n- With docker installed:\n  1. sudo docker pull msleal/asciivmssdashboard\n  2. sudo docker run -i -t e28007e9030b /bin/bash\n  3. su - architect\n  4. cd asciivmssdashboard\n  5. git pull origin master\n  6. cp asciivmssdashboard.json.tmpl /home/your-username/.asciivmssdashboard/asciivmssdashboard.json\n- Fill in the asciivmssdashboard.json config file with your custom values...\n  8. export TERM=screen; export PYTHONDONTWRITEBYTECODE=1; ./console.py \n\nOR:\n\n  1. Install Python 2.x or 3.x.\n  2. Install the ASCii VMSS Dashboard: pip install asciivmssdashboard.\n  3. This application uses Unicurses: https://pypi.org/project/UniCurses, and a version of UniCurses (+pdcurses for Windows platform), already comes with asciivmssdashboard.\n\nAfter you have the application installed, you will need:\n  _For DEMO (No real vmss config)_\n  1. Just run: asciivmssdashboard\n\n  _For use it in a Real VMSS Dashboard_\n  1. Register an Azure application, create a service principal, and get your tenant id. See \"Using ASCiiVMSSDashboard\".\n  2. Put in values for your application along with your resource group name and VM Scale Set name in /home/your-username/asciivmssdashboard.json file.\n  3. If you want to watch the Insights Telemetry Graphs (for the Azure Service), you will need to configure Azure Insights. See \"Using Application Insights Telemetry Data\".\n\n     You can use a custom URL (e.g.: Your own Telemetry API). For that, you will need to have an Telemetry API that provides the metrics (e.g.: JSON), similar to Azure Application Insights.\n\n     NOTE: If you want to use custom telemetry APIs Services, leave the 'insightsUrl' config option 'empty'. If the insightsUrl is configured, it will have precedence and ASCiiVMSSDashboard will use it\n           in conjuction with the metric (e.g.: insightsOneMetric) configured. If it is not configured, and (for example): insightsOne is enabled, the insightsOneUrl will be used to get the first metric.\n\n  4. Run (on Linux): asciivmssdashboard or (on Windows): path_to_python path_to_asciivmssdashboard_script.\n  5. To Exit the Console, just hit: Ctrl+C or use the command: 'quit' (for a 'clean' exit, we will wait for the update threads to finish).\n     In the Windows Platform you can just use the command: 'quit' for now (See \"CAVEATS\" bellow).\n\n## WATCH THE CONSOLE IN ACTION:\nSubtitle/Captions should be enabled by default, but if not, enable them to follow the action (English and Portuguese BR available).\nDon't forget to subscribe to the channel for updates...\n\n[ASCiiVMSSDashboard (v1.4) - Screencast](https://www.youtube.com/watch?v=PUagvJ6fTd0)\n\n[ASCiiVMSSDashboard (v1.4) - running natively on Android (Youtube Version)](https://www.youtube.com/watch?v=pK-xQNnyZfY)\n\n[ASCiiVMSSDashboard (v1.0) - Screencast](https://www.youtube.com/watch?v=MomiZ9rU9NU)\n\nEnjoy some code and loud music!\n\n## FEATURES:\nRelease Version 1.8 has animations for the Windows (Continents), Azure Regions updated,, and change of the color for the regions on map\nfor better visibility. Other than that, I changed the character for representing the regions to be a 'block' and not just a space, so I \ncould add bright color and have it highlighted (just works for foreground, so space was not working).\n\nVersion 1.6 features Telemetry data on graphs integrated on the ASCiiVMSSDashboard now is fully integrated and flexible.\n\nThe Version 1.4 brings many new features and commands for the ASCiiVMSSDashboard utility. \nThis version has an option to create a log file with many useful information that can be watched live directly in the ASCiiVMSSDashboard.\nIt's very simple to display the log on the screen, just issue the command: 'log'. Using the console config file, you can set the log level \n(e.g.: INFO, DEBUG, and etc).\n\n![Image of ASCii VMSS Dashboard Log](https://raw.githubusercontent.com/msleal/msleal.github.com/master/asciidash-v1-4-log.png)\n\nOne of the key features of this new version of the ASCiiVMSSDashboard is the ability to see in real time two instrumentation graphs.\nYou can eanble this feature on the asciivmssdashboard.json file, and set the title of the graph and the URL that the console will use to gather the \nmetrics. The console will just do a HTTP GET on each URL and expect to receive an 'number' to plot on the graphs.\n\n![Image of ASCii VMSS Dashboard Insights 1](https://raw.githubusercontent.com/msleal/msleal.github.com/master/asciidash-v1-4-insights1.png)\n\nThere is an option to enable one, two or disable both graphs. In the future I plan to integrate with the Azure Application Insights, \nand so we should be able to select any of the metrics configured on the platform. Stay tuned!\n\n![Image of ASCii VMSS Dashboard Insights 1](https://raw.githubusercontent.com/msleal/msleal.github.com/master/asciidash-v1-4-insights2.png)\n\n\nThese are the PROMPT, General Info, and System Info windows:\n![Image of ASCii VMSS Dashboard General Info](https://raw.githubusercontent.com/msleal/msleal.github.com/master/general.png)\n\nMultiple pages for visualization of all your Virtual Machines (each page shows 100 VM's).\nHere you can see a screenshot showing the first page:\n\n![Image of ASCii VMSS Dashboard Multiple Pages](https://raw.githubusercontent.com/msleal/msleal.github.com/master/virtualmachines.png)\n\nNext, we can see the second page:\n\n![Image of ASCii VMSS Dashboard Second page](https://raw.githubusercontent.com/msleal/msleal.github.com/master/virtualmachines2.png)\n\nThe console will show you your subscription/region usage and limits:\n\n![Image of ASCii VMSS Dashboard Usage](https://raw.githubusercontent.com/msleal/msleal.github.com/master/usage.png)\n\nThe ASCiiVMSSDashboard can run on Python 2.x or 3.x versions, and you will see the version it is running at the top title:\n![Image of ASCii VMSS Dashboard Python Version](https://raw.githubusercontent.com/msleal/msleal.github.com/master/version.png)\n\nIn case of any errors, you will be able to see the messages on the LOG Window, and also as a specific message in the main window:\n![Image of ASCii VMSS Dashboard Error Message](https://raw.githubusercontent.com/msleal/msleal.github.com/master/error.png)\n\n###TIP #1: \nTo not create the .pyc files, I use the following (on Linux): export TERM=screen; export PYTHONDONTWRITEBYTECODE=1; ./console.py.\nIMPORTANT: I use 'screen' terminal and it was the best emulation I found to run curses application. You can try other terminals\nthat you have the best experience or compatibility in your system (e.g.: TERM=xterm, TERM=xterm-color, etc).\n\n###TIP #2:\nI have used the console with no issues using a refresh interval of 60 seconds. If you use a more 'agressive'\nupdate interval, keep one eye at the last-update registered at the top-left of the dashboard window and/or in the log\nwindow (e.g.: 'log'), to see if the console is stil running.  If you notice it stopped, you should see the 'ERROR' window\ndescribed above, and the console should resume in 30 seconds. if the log has information about AZURE API 'throttling\", \nyou will need to restart the ASCiiVMSSDashboard (with a bigger inteval)...\n \n###TIP #3:\nAs we wait for the threads to finish as you hit 'Ctrl+C' or 'quit' (to exit), the time you will wait to get your prompt\nback will be proportional to you refresh interval (e.g.: max='INTERVAL'). You can change the update interval in the \n'asciivmssdashboard.json' file.\n\n## ASCiiVMSSDashboard Commands\n  To issue commands for the Azure Resource Manager API to add and/or delete virtual machines from the Scale Set,\nyou just need to type ':'. After that, the cursor will appear at (PROMPT window), and you will be able to enter commands.\nTo see a help window just type ':' (to activate the command PROMPT), and 'help'. To hide the help window, just type \n'help' again.\n\nREMEMBER: To activate the prompt window, type: ':'\n\nCommands (v1.4):\n- add vm 'nr': Use this command to add virtual machines to your VMSS deployment.\n- del vm 'nr': Use this command to delete virtual machines to your VMSS deployment.\n- select vm 'nr': Use this command to select a specific virtual machine on your VMSS deployment and see detailed info.\nYou can select any Virtual Machine, and get specific details about it:\n\n![Image of ASCii VMSS Dashboard VM Details](https://raw.githubusercontent.com/msleal/msleal.github.com/master/vmdetails.png)\n\n- deselect: Use this command to clear the selection of any specific virtual machine (and hide the VM details window).\n- rg 'resourcegroupname' vmss 'vmscalesetname': Use this command to switch the visualization to another VM Scale Set.\n- log: Use this command to show/hide a live log about the console executiong (e.g.: http requests, errors, and etc).\n- insights 1: This command will show/hide the graph window for the first metric configured.\n- insights 2: This command will show/hide the graph window for the second metric configured.\n- debug: Use this command to show/hide all three windows (log, insights 1 and insights 2).\n- quit 'or' exit: Use this command to exit the console (Any platform).\n- Ctrl+c: Use this key combination to exit the ASCiiVMSSDashboard (Not working on windows for now).\n- help: Use this command to get help about the dashboard commands (inside the ASCiiVMSSDashboard).\n\n## CAVEATS\n- The 'stdscr.nodelay(1)' seems not to be multiplatform (at least does not work on Windows), and we are using it \nas a non-block function when reading user commands. For now, use the command 'quit' or 'exit' to end the dashboard on Windows. \nI'm looking for an alternative non-block call to use on windows and fix this.\n- It would be nice to have any feedback of this program running on MacOS or any other platform... \n\n##PDCURSES\n- This application uses Unicurses: https://pypi.org/project/UniCurses, and a version of UniCurses (+pdcurses for Windows platform), already comes with asciivmssdashboard.\n- If you have problems installing pdcurses on windows (not able to load uniCurses: import error), you can just add the DLL directly\non the current directory of the ASCiiVMSSDashboard installation. To run the ASCiiVMSSDashboard on Windows, I have tested it \njust cloning the repo on Windows 10, and copying the 'pdcurses.dll' file to the cloned folder, and it runs without any issues (you\nstill needs to have the UniCurses installed, but you have the Windows Installer link at the top of this README file). \n\n## Using ASCiiVMSSDashboard \nTo use this app (and in general to access Azure Resource Manager from a program without going through 2 factor authentication), \nyou need to register your application with Azure and create a \"Service Principal\" (an application equivalent of a user). \nOnce you've done this you'll have 3 pieces of information: A 'tenant ID', an 'application ID', and an 'application secret'.\nYou will use these to populate the 'asciivmssdashboard.json' file. \n\nFor more information on how to get this information go here: [Authenticating a service principal with Azure Resource Manager][service-principle].\nSee also: [Azure Resource Manager REST calls from Python][python-auth].\n\n## Using Application Insights Telemetry Data\nIf you want to have integrated graphs on the ASCiiVMSSDashboard directly from Azure Application Insights, you should follow the steps\ndescribed here: [App Insights I][create-resource] and [App Insights II][api-key].\n\nExample JSON:\n```json\n    {\n      \"value\": {\n          \"request/totalCount\": {\n              \"values\": \"30\"\n          }\n      }\n    }\n```\n\n## Example VMSS ARM Template:\n[Ubuntu Linux VM Scale Set integrated with Azure autoscale][arm-template].\n\n[service-principle]: https://azure.microsoft.com/en-us/documentation/articles/resource-group-authenticate-service-principal/\n[python-auth]: https://msftstack.wordpress.com/2016/01/05/azure-resource-manager-authentication-with-python\n[create-resource]: https://azure.microsoft.com/en-us/documentation/articles/app-insights-create-new-resource/\n[api-key]: https://github.com/Microsoft/ApplicationInsights-Home/wiki\n[arm-template]: https://github.com/Azure/azure-quickstart-templates/tree/master/201-vmss-ubuntu-autoscale\n"
 },
 {
  "repo": "soeaver/Parsing-R-CNN",
  "language": "Python",
  "readme_contents": "# Parsing-R-CNN\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/parsing-r-cnn-for-instance-level-human/human-part-segmentation-on-cihp)](https://paperswithcode.com/sota/human-part-segmentation-on-cihp?p=parsing-r-cnn-for-instance-level-human)\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/parsing-r-cnn-for-instance-level-human/pose-estimation-on-densepose-coco)](https://paperswithcode.com/sota/pose-estimation-on-densepose-coco?p=parsing-r-cnn-for-instance-level-human)\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/parsing-r-cnn-for-instance-level-human/human-part-segmentation-on-mhp-v20)](https://paperswithcode.com/sota/human-part-segmentation-on-mhp-v20?p=parsing-r-cnn-for-instance-level-human)\n\n**(New!)** Official implementation of **Parsing R-CNN for Instance-Level Human Analysis (CVPR 2019)**\n\n## Citing Parsing R-CNN\n\nIf you use Parsing R-CNN, please use the following BibTeX entry.\n\n```BibTeX\n@inproceedings{yang2019cvpr,\n  title = {Parsing R-CNN for Instance-Level Human Analysis},\n  author = {Lu Yang and Qing Song and Zhihui Wang and Ming Jiang},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2019}\n}\n\n```\n\nIn this repository, we release the Parsing R-CNN code in Pytorch.\n\n- Parsing R-CNN architecture:\n<p align=\"center\"><img width=\"90%\" src=\"data/parsing_rcnn.png\" /></p>\n\n- Parsing R-CNN output:\n<p align=\"center\"><img width=\"90%\" src=\"data/output.png\" /></p>\n\n\n## Installation\n- 8 x TITAN RTX GPU\n- pytorch1.1\n- python3.6.8\n\nInstall Parsing R-CNN following [INSTALL.md](https://github.com/soeaver/Parsing-R-CNN/blob/master/INSTALL.md#install).\n\n\n## Dataset\n\nYou need to download the datasets and annotations following this repo's formate. As:\n\n- [CIHP](https://drive.google.com/open?id=1OLBd23ufm6CU8CZmLEYMdF-x2b8mRgxV)\n\n- [MHP-v2](coming soon)\n\n- [DensePoseData](https://drive.google.com/open?id=1WiTLYVIgMyCDENXHPVEWW7qbZ-3EBjbt)(using original [MSCOCO2017](http://cocodataset.org/#download) images)\n\nAnd following [data structure](https://github.com/soeaver/Parsing-R-CNN/blob/master/INSTALL.md#data-and-pre-train-weights) to train or evaluate Parsing R-CNN models.\n\n\n## Results and Models\n\n**On CIHP val**\n\n|  Backbone  |  LR  | Det AP | mIoU |Parsing (APp50/APvol/PCP50) | DOWNLOAD |\n|------------|:----:|:------:|:----:|:--------------------------:| :-------:|\n|  R-50-FPN  |  1x  | 65.8   | 52.8 |      57.2/51.2/55.4        |          |\n|  R-50-FPN  |  3x  | 68.7   | 56.0 |      64.1/54.1/60.7        | [GoogleDrive](https://drive.google.com/open?id=16bASrD7AoCADKzXynIgmdyzmbuzCfAUL)|\n\n\n**On MHP-v2 val**\n\n|  Backbone  |  LR  | Det AP | mIoU |Parsing (APp50/APvol/PCP50) | DOWNLOAD |\n|------------|:----:|:------:|:----:|:--------------------------:| :-------:|\n|  R-50-FPN  |  1x  | 66.5   | 34.0 |      19.9/36.7/32.4        |          |\n|  R-50-FPN  |  3x  | 69.0   | 36.1 |      27.4/40.5/38.3        | [GoogleDrive](https://drive.google.com/open?id=1rbSNP4_DoJdNK4l6KHrthO0x4WOFgHGy)|\n\n\n**On DensePose_COCO val**\n\n|  Backbone  |  LR  | Det AP |UV AP (AP/AP50/AP75/APm/APl)| DOWNLOAD |\n|------------|:----:|:------:|:--------------------------:| :-------:|\n|  R-50-FPN  |  s1x | 57.4   |  59.3/90.5/68.7/56.2/60.8  | [GoogleDrive](https://drive.google.com/open?id=1YQygKoOb5SbZWYnF7f9vEpC_NenpMhH5)|\n\n\n- New metric GPSm is adopted for evaluating UV\n\n\n**ImageNet pretrained weight**\n\n- [R-50](https://drive.google.com/open?id=1EtqFhrFTdBJNbp67effArVrTNx4q_ELr)\n- [R-50-GN](https://drive.google.com/open?id=1LzcVD7aADhXXY32DdtKhaY9hTXaduhlg)\n- [X-101-32x8d](https://drive.google.com/open?id=1c4OSVZIZtDT49B0DTC0tK3vcRgJpzR9n)\n\n\n## Visualization\n\ncoming soon.\n\n\n## Training\n\nTo train a model with 8 GPUs run:\n```\npython -m torch.distributed.launch --nproc_per_node=8 tools/train_net.py --cfg cfgs/CIHP/e2e_rp_rcnn_R-50-FPN_3x_ms.yaml\n```\n\n\n## Evaluation\n\n### multi-gpu evaluation,\n```\npython tools/test_net.py --cfg ckpts/CIHP/e2e_rp_rcnn_R-50-FPN_3x_ms/e2e_rp_rcnn_R-50-FPN_3x_ms.yaml --gpu_id 0,1,2,3,4,5,6,7\n```\n\n### single-gpu evaluation,\n```\npython tools/test_net.py --cfg ckpts/CIHP/e2e_rp_rcnn_R-50-FPN_3x_ms/e2e_rp_rcnn_R-50-FPN_3x_ms.yaml --gpu_id 0\n```\n\n\n## License\nParsing-R-CNN is released under the [MIT license](https://github.com/soeaver/Parsing-R-CNN/blob/master/LICENSE).\n"
 },
 {
  "repo": "meetps/tf-3dgan",
  "language": "Python",
  "readme_contents": "# tf-3dgan\n\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/meetshah1995/tf-3dgan/blob/master/LICENSE)\n[![arXiv Tag](https://img.shields.io/badge/arXiv-1610.07584-brightgreen.svg)](https://arxiv.org/abs/1610.07584)\n\n## Tensorflow implementation of 3D Generative Adversarial Network.\n\nThis is a tensorflow implementation of the paper \"Learning a Probabilistic Latent Space of Object Shapes \nvia 3D Generative-Adversarial Modeling\"\n\n![](http://3dgan.csail.mit.edu/images/model.jpg)\n\n[Blog Post with interactive volume plots](https://meetshah1995.github.io/gan/deep-learning/tensorflow/visdom/2017/04/01/3d-generative-adverserial-networks-for-volume-classification-and-generation.html)\n\n### Requirements\n\n* tensorflow>=1.0\n* visdom>=1.0.1 (for mesh visualization)\n* scipy\n* scikit-image\n* stl (optional)\n\n\n#### One-line installation\n    \n`pip install scipy scikit-image stl visdom`\n\n### Data\n\n* Download the training data from the 3D Shapenet [website](http://3dshapenets.cs.princeton.edu/3DShapeNetsCode.zip)\n* Extract the zip and modify the path appropriately in `dataIO.py`\n\n### Usage\n\nLaunch [visdom](https://github.com/facebookresearch/visdom#launch) by running\n\n```\npython -m visdom.server\n```\n\nTo train the model (visdom will show generated chairs after every 200 minibatches)\n\n```\npython 3dgan_mit_biasfree.py 0 <path_to_model_checkpoint>\n```\n\nTo generate chairs\n\n```\npython 3dgan_mit_biasfree.py 1 <path_to_trained_model>\n```\n\nSome sample generated chairs\n\n|            |              |            |          |          |\n|------------|--------------|------------|----------|----------|\n|![](http://homepages.iitb.ac.in/~meetshah1995/files/2_.png) | ![](http://homepages.iitb.ac.in/~meetshah1995/files/4_.png) |  ![](http://homepages.iitb.ac.in/~meetshah1995/files/1_.png) |  ![](http://homepages.iitb.ac.in/~meetshah1995/files/3_.png) |  ![](http://homepages.iitb.ac.in/~meetshah1995/files/5_.png) |\n\n\n### Source code files\n\n| File      | Description                                                                   |\n|-----------|-------------------------------------------------------------------------------|\n|3dgan_mit_biasfree.py      | 3dgan as mentioned in the paper, with same hyperparams. \n|3dgan.py                   | baseline 3dgan with fully connected layer at end of discriminator.\n|3dgan_mit.py               | 3dgan as mentioned in the paper with bias in convolutional layers.\n|3dgan_autoencoder.py       | 3dgan with support for autoencoder based pre-training.\n|3dgan_feature_matching.py  | 3dgan with additional loss of feature mathcing of last layers. \n|dataIO.py                  | data input output and plotting utilities.\n|utils.py                   | tensorflow utils like leaky_relu and batch_norm layer.\n\n\n### Todo\n\n* Host the trained models\n* Add argparser based interface\n* Add threaded dataloader\n* Release the pytorch and keras versions of the GAN.\n* Train for longer number of epochs to improve quality of generated chairs.\n\n### Contributors\n\n* @meetshah1995\n* @khushhallchandra\n"
 },
 {
  "repo": "jaxony/ShuffleNet",
  "language": "Python",
  "readme_contents": "# ShuffleNet in PyTorch\nAn implementation of `ShuffleNet` in PyTorch. `ShuffleNet` is an efficient convolutional neural network architecture for mobile devices. According to the paper, it outperforms Google's MobileNet by a small percentage.\n\n## What is ShuffleNet?\nIn one sentence, `ShuffleNet` is a ResNet-like model that uses residual blocks (called `ShuffleUnits`), with the main innovation being the use of pointwise, or 1x1, *group* convolutions as opposed to normal pointwise convolutions.\n\n## Usage\nClone the repo:\n```bash\ngit clone https://github.com/jaxony/ShuffleNet.git\n```\n\nUse the model defined in `model.py`:\n```python\nfrom model import ShuffleNet\n\n# running on MNIST\nnet = ShuffleNet(num_classes=10, in_channels=1)\n```\n\n## Performance\n\nTrained on ImageNet (using the [PyTorch ImageNet example][imagenet]) with\n`groups=3` and no channel multiplier. On the test set, got 62.2% top 1 and\n84.2% top 5. Unfortunately, this isn't comparable to Table 5 of the paper,\nbecause they don't run a network with these settings, but it is somewhere\nbetween the network with `groups=3` and half the number of channels (42.8%\ntop 1) and the network with the same number of channels but `groups=8`\n(32.4% top 1). The pretrained state dictionary can be found [here][tar], in\nthe [following\nformat](://github.com/pytorch/examples/blob/master/imagenet/main.py#L165-L171):\n\n```\n{\n    'epoch': epoch + 1,\n    'arch': args.arch,\n    'state_dict': model.state_dict(),\n    'best_prec1': best_prec1,\n    'optimizer' : optimizer.state_dict()\n}\n```\n\nNote: trained with the default ImageNet settings, which are actually\ndifferent from the training regime described in the paper. Pending running\nagain with those settings (and `groups=8`).\n\n[tar]: https://drive.google.com/file/d/12oGJsyDgp51LhQ7FOzKxF9nBsutLkE6V/view?usp=sharing\n[imagenet]: https://github.com/pytorch/examples/tree/master/imagenet\n"
 },
 {
  "repo": "ENCODE-DCC/atac-seq-pipeline",
  "language": "Python",
  "readme_contents": "# ENCODE ATAC-seq pipeline\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.156534.svg)](https://doi.org/10.5281/zenodo.156534)[![CircleCI](https://circleci.com/gh/ENCODE-DCC/atac-seq-pipeline/tree/master.svg?style=svg)](https://circleci.com/gh/ENCODE-DCC/atac-seq-pipeline/tree/master)\n\n\n## Conda environment name change (since v2.2.0 or 6/13/2022)\n\nPipeline's Conda environment's names have been shortened to work around the following error:\n```\nPaddingError: Placeholder of length '80' too short in package /XXXXXXXXXXX/miniconda3/envs/\n```\n\nYou need to reinstall pipeline's Conda environment. It's recommended to do this for every version update.\n```bash\n$ bash scripts/uninstall_conda_env.sh\n$ bash scripts/install_conda_env.sh\n```\n\n## Introduction\n\nThis pipeline is designed for automated end-to-end quality control and processing of ATAC-seq and DNase-seq data. The pipeline can be run on compute clusters with job submission engines as well as on stand alone machines. It inherently makes uses of parallelized/distributed computing. Pipeline installation is also easy as most dependencies are automatically installed. The pipeline can be run end-to-end, starting from raw FASTQ files all the way to peak calling and signal track generation using a single caper submit command. One can also start the pipeline from intermediate stages (for example, using alignment files as input). The pipeline supports both single-end and paired-end data as well as replicated or non-replicated datasets. The outputs produced by the pipeline include 1) formatted HTML reports that include quality control measures specifically designed for ATAC-seq and DNase-seq data, 2) analysis of reproducibility, 3) stringent and relaxed thresholding of peaks, 4) fold-enrichment and pvalue signal tracks. The pipeline also supports detailed error reporting and allows for easy resumption of interrupted runs. It has been tested on some human, mouse and yeast ATAC-seq datasets as well as on human and mouse DNase-seq datasets.\n\nThe ATAC-seq pipeline protocol specification is [here](https://docs.google.com/document/d/1f0Cm4vRyDQDu0bMehHD7P7KOMxTOP-HiNoIvL1VcBt8/edit?usp=sharing). Some parts of the ATAC-seq pipeline were developed in collaboration with Jason Buenrostro, Alicia Schep and Will Greenleaf at Stanford.\n\n### Features\n\n* **Portability**: The pipeline run can be performed across different cloud platforms such as Google, AWS and DNAnexus, as well as on cluster engines such as SLURM, SGE and PBS.\n* **User-friendly HTML report**: In addition to the standard outputs, the pipeline generates an HTML report that consists of a tabular representation of quality metrics including alignment/peak statistics and FRiP along with many useful plots (IDR/TSS enrichment). An example of the [HTML report](https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR889WQX/example_output/qc.html). The [json file](https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR889WQX/example_output/qc.json) used in generating this report.\n* **Supported genomes**: Pipeline needs genome specific data such as aligner indices, chromosome sizes file and blacklist. We provide a genome database downloader/builder for hg38, hg19, mm10, mm9. You can also use this [builder](docs/build_genome_database.md) to build genome database from FASTA for your custom genome.\n\n## Installation\n\n1) Make sure that you have Python>=3.6. Caper does not work with Python2. Install Caper and check its version >=2.0.\n\t```bash\n\t$ pip install caper\n\n\t# use caper version >= 2.3.0 for a new HPC feature (caper hpc submit/list/abort).\n\t$ caper -v\n\t```\n2) Read Caper's [README](https://github.com/ENCODE-DCC/caper/blob/master/README.md) carefully to choose a backend for your system. Follow the instruction in the configuration file.\n\t```bash\n\t# this will overwrite the existing conf file ~/.caper/default.conf\n\t# make a backup of it first if needed\n\t$ caper init [YOUR_BACKEND]\n\n\t# edit the conf file\n\t$ vi ~/.caper/default.conf\n\t```\n\n3) Git clone this pipeline.\n\t```bash\n\t$ cd\n\t$ git clone https://github.com/ENCODE-DCC/atac-seq-pipeline\n\t```\n\n4) (Optional for Conda) **DO NOT USE A SHARED CONDA. INSTALL YOUR OWN [MINICONDA3](https://docs.conda.io/en/latest/miniconda.html) AND USE IT.** Install pipeline's Conda environments if you don't have Singularity or Docker installed on your system. We recommend to use Singularity instead of Conda.\n\t```bash\n\t# check if you have Singularity on your system, if so then it's not recommended to use Conda\n\t$ singularity --version\n\n\t# check if you are not using a shared conda, if so then delete it or remove it from your PATH\n\t$ which conda\n\n\t# change directory to pipeline's git repo\n\t$ cd atac-seq-pipeline\n\n\t# uninstall old environments\n\t$ bash scripts/uninstall_conda_env.sh\n\n\t# install new envs, you need to run this for every pipeline version update.\n\t# it may be killed if you run this command line on a login node.\n\t# it's recommended to make an interactive node and run it there.\n\t$ bash scripts/install_conda_env.sh\n\t```\n\n\n## Input JSON file specification\n\n> **IMPORTANT**: DO NOT BLINDLY USE A TEMPLATE/EXAMPLE INPUT JSON. READ THROUGH THE FOLLOWING GUIDE TO MAKE A CORRECT INPUT JSON FILE. ESPECIALLY FOR AUTODETECTING/DEFINING ADAPTERS.\n\nAn input JSON file specifies all the input parameters and files that are necessary for successfully running this pipeline. This includes a specification of the path to the genome reference files and the raw data fastq file. Please make sure to specify absolute paths rather than relative paths in your input JSON files.\n\n1) [Input JSON file specification (short)](docs/input_short.md)\n2) [Input JSON file specification (long)](docs/input.md)\n\n\n## Running on local computer/HPCs\n\nYou can use URIs(`s3://`, `gs://` and `http(s)://`) in Caper's command lines and input JSON file then Caper will automatically download/localize such files. Input JSON file example: https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled.json\n\nAccording to your chosen platform of Caper, run Caper or submit Caper command line to the cluster. You can choose other environments like `--singularity` or `--docker` instead of `--conda`. But you must define one of the environments.\n\nPLEASE READ [CAPER'S README](https://github.com/ENCODE-DCC/caper) VERY CAREFULLY BEFORE RUNNING ANY PIPELINES. YOU WILL NEED TO CORRECTLY CONFIGURE CAPER FIRST. These are just example command lines.\n\n    ```bash\n    # Run it locally with Conda (DO NOT ACTIVATE PIPELINE'S CONDA ENVIRONEMT)\n    $ caper run atac.wdl -i https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled.json --conda\n\n    # On HPC, submit it as a leader job to SLURM with Singularity\n    $ caper hpc submit atac.wdl -i https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled.json --singularity --leader-job-name ANY_GOOD_LEADER_JOB_NAME\n\n    # Check job ID and status of your leader jobs\n    $ caper hpc list\n\n    # Cancel the leader node to close all of its children jobs\n    # If you directly use cluster command like scancel or qdel then\n    # child jobs will not be terminated\n    $ caper hpc abort [JOB_ID]\n\t```\n\n## Running and sharing on Truwl\n\nYou can run this pipeline on [truwl.com](https://truwl.com/). This provides a web interface that allows you to define inputs and parameters, run the job on GCP, and monitor progress. To run it you will need to create an account on the platform then request early access by emailing [info@truwl.com](mailto:info@truwl.com) to get the right permissions. You can see the example case from this repo at [https://truwl.com/workflows/instance/WF_e85df4.f10.8880/command](https://truwl.com/workflows/instance/WF_e85df4.f10.8880/command). The example job (or other jobs) can be forked to pre-populate the inputs for your own job.\n\nIf you do not run the pipeline on Truwl, you can still share your use-case/job on the platform by getting in touch at [info@truwl.com](mailto:info@truwl.com) and providing your inputs.json file.\n\n\n## Running on Terra/Anvil (using Dockstore)\n\nVisit our pipeline repo on [Dockstore](https://dockstore.org/workflows/github.com/ENCODE-DCC/atac-seq-pipeline). Click on `Terra` or `Anvil`. Follow Terra's instruction to create a workspace on Terra and add Terra's billing bot to your Google Cloud account.\n\nDownload this [test input JSON for Terra](https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled.terra.json) and upload it to Terra's UI and then run analysis.\n\nIf you want to use your own input JSON file, then make sure that all files in the input JSON are on a Google Cloud Storage bucket (`gs://`). URLs will not work.\n\n\n## Running on DNAnexus (using Dockstore)\n\nSign up for a new account on [DNAnexus](https://platform.dnanexus.com/) and create a new project on either AWS or Azure. Visit our pipeline repo on [Dockstore](https://dockstore.org/workflows/github.com/ENCODE-DCC/atac-seq-pipeline). Click on `DNAnexus`. Choose a destination directory on your DNAnexus project. Click on `Submit` and visit DNAnexus. This will submit a conversion job so that you can check status of it on `Monitor` on DNAnexus UI.\n\nOnce conversion is done download one of the following input JSON files according to your chosen platform (AWS or Azure) for your DNAnexus project:\n- AWS: https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled_dx.json\n- Azure: https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled_dx_azure.json\n\nYou cannot use these input JSON files directly. Go to the destination directory on DNAnexus and click on the converted workflow `atac`. You will see input file boxes in the left-hand side of the task graph. Expand it and define FASTQs (`fastq_repX_R1` and also `fastq_repX_R2` if it's paired-ended) and `genome_tsv` as in the downloaded input JSON file. Click on the `common` task box and define other non-file pipeline parameters. e.g. `auto_detect_adapters` and `paired_end`.\n\nWe have a separate project on DNANexus to provide example FASTQs and `genome_tsv` for `hg38` and `mm10`. We recommend to make copies of these directories on your own project.\n\n`genome_tsv`\n- AWS: https://platform.dnanexus.com/projects/BKpvFg00VBPV975PgJ6Q03v6/data/pipeline-genome-data/genome_tsv/v4\n- Azure: https://platform.dnanexus.com/projects/F6K911Q9xyfgJ36JFzv03Z5J/data/pipeline-genome-data/genome_tsv/v4\n\nExample FASTQs\n- AWS: https://platform.dnanexus.com/projects/BKpvFg00VBPV975PgJ6Q03v6/data/pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ/fastq_subsampled\n- Azure: https://platform.dnanexus.com/projects/F6K911Q9xyfgJ36JFzv03Z5J/data/pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ/fastq_subsampled\n\n\n## Running on DNAnexus (using our pre-built workflows)\n\nSee [this](docs/tutorial_dx_web.md) for details.\n\n\n## How to organize outputs\n\nInstall [Croo](https://github.com/ENCODE-DCC/croo#installation). **You can skip this installation if you have installed pipeline's Conda environment and activated it**. Make sure that you have python3(> 3.4.1) installed on your system. Find a `metadata.json` on Caper's output directory.\n\n```bash\n$ pip install croo\n$ croo [METADATA_JSON_FILE]\n```\n\n## How to make a spreadsheet of QC metrics\n\nInstall [qc2tsv](https://github.com/ENCODE-DCC/qc2tsv#installation). Make sure that you have python3(> 3.4.1) installed on your system. \n\nOnce you have [organized output with Croo](#how-to-organize-outputs), you will be able to find pipeline's final output file `qc/qc.json` which has all QC metrics in it. Simply feed `qc2tsv` with multiple `qc.json` files. It can take various URIs like local path, `gs://` and `s3://`.\n\n```bash\n$ pip install qc2tsv\n$ qc2tsv /sample1/qc.json gs://sample2/qc.json s3://sample3/qc.json ... > spreadsheet.tsv\n```\n\nQC metrics for each experiment (`qc.json`) will be split into multiple rows (1 for overall experiment + 1 for each bio replicate) in a spreadsheet.\n"
 },
 {
  "repo": "ramonsaraiva/timy",
  "language": "Python",
  "readme_contents": "# timy\n\n![Python 3.4](https://img.shields.io/badge/python-3.4-blue.svg)\n![Python 3.5](https://img.shields.io/badge/python-3.5-blue.svg)\n![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)\n![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)\n\n![CircleCI](https://img.shields.io/circleci/project/github/ramonsaraiva/timy/master.svg)\n![Codecov](https://img.shields.io/codecov/c/github/ramonsaraiva/timy/master.svg)\n\nMinimalist measurement of python code time\n> **timy** comes with a different idea of the built-in module [timeit](https://docs.python.org/2.7/library/timeit.html). It adds flexibility and different ways of measuring code time, using simple context managers and function decorators.\n\n## Installing\n```\npip install timy\n```\n\n## Usage\n\n### Decorating a function\nLet's say you have a `calculate` function and you want to keep track of its execution time\n```python\nimport timy\n\n@timy.timer()\ndef calculate(n, r):\n    \"\"\"\n    Divide, multiply and sum 'n' to every number in range 'r'\n    returning the result list\n    \"\"\"\n    return [i / n * n + n for i in range(r)]\n```\n\nWhenever you call that function, the execution time will be tracked\n\n```python\ncalculate(5, 10000000)\n>> Timy executed (calculate) for 1 time(s) in 1.529540 seconds\n>> Timy best time was 1.529540 seconds\n```\n\nChanging the **ident** and adding **loops** to the execution\n\n```python\nimport timy\n\n@timy.timer(ident='My calculation', loops=10)\ndef calculate(n, r):\n    return [i / n * n + n for i in range(r)]\n    \ncalculate(5, 10000000)\n>> My calculation executed (calculate) for 10 time(s) in 15.165313 seconds\n>> My calculation best time was 1.414186 seconds\n```\n\n### Tracking **specific points** along your code\nThe `with` statement can also be used to measure code time\n> Named tracking points can be added with the `track` function\n\n```python\nimport timy\n\nwith timy.Timer() as timer:\n    N = 10000000\n    for i in range(N):\n        if i == N/2:\n            timer.track('Half way')\n            \n>> Timy (Half way) 0.557577 seconds\n>> Timy 0.988087 seconds            \n```\n\nAnother usage of tracking in a prime factors function\n\n```python\ndef prime_factors(n):\n    with timy.Timer('Factors') as timer:\n        i = 2\n        factors = []\n        def add_factor(n):\n            factors.append(n)\n            timer.track('Found a factor')\n\n        while i * i <= n:\n            if n % i == 0:\n                add_factor(i)\n                n //= i\n            else:\n                i += 1\n        return factors + [n]\n\nfactors = prime_factors(600851475143)\nprint(factors)\n\n>> Factors (Found a factor) 0.000017 seconds\n>> Factors (Found a factor) 0.000376 seconds\n>> Factors (Found a factor) 0.001547 seconds\n>> Factors 0.001754 seconds\n>> [71, 839, 1471, 6857]\n```\n\n### Configuring\n\n#### Importing timy config\n\n```python\nfrom timy.settings import timy_config\n```\n\n#### Enable or disable timy trackings\nYou can enable or disable timy trackings with the `tracking` value.\n> The default value of `tracking` is `True`\n\n```python\ntimy_config.tracking = False\n```\n\n#### Changing the way timy outputs information\nYou can choose between print or logging for all timy outputs by setting the\nvalue of `tracking_mode`.\n> The default value of `tracking_mode` is `TrackingMode.PRINTING`.\n\n```python\nfrom timy.settings import (\n    timy_config,\n    TrackingMode\n)\n\ntimy_config.tracking_mode = TrackingMode.LOGGING\n```\n\ntimy logs at the INFO level, which is not printed or stored by default. To\nconfigure the logging system to print all INFO messages do\n```\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\nor to configure the logging system to print only timy's INFO messages do\n```\nimport logging\nlogging.basicConfig()\nlogging.getLogger('timy').level=logging.INFO\n```\n\n## Contribute\nContributions are **always** welcome, but keep it simple and small.\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n"
 },
 {
  "repo": "hiidef/oauth2app",
  "language": "Python",
  "readme_contents": "Other projects have been able to devote the time and energy necessary to maintain oauth apps that we cannot, so while this library is not exactly unmaintained, you should probably be using something else. We highly recommend `djoauth2 <https://github.com/Locu/djoauth2/>`_ and `Django OAuth Toolkit <https://github.com/evonove/django-oauth-toolkit/>`_\n\n\n\n\n* See http://hiidef.github.com/oauth2app for documentation. \n* See https://github.com/hiidef/oauth2app for source code.\n* Based on http://code.google.com/p/django-oauth2\n* Support for OAuth 2.0 draft 16, http://tools.ietf.org/html/draft-ietf-oauth-v2-16\n\nInstallation\n------------\n\nIf easy_install is available, you can use: ::\n\n    easy_install https://github.com/hiidef/oauth2app/tarball/master\n\nIntroduction\n------------\n\nThe oauth2app module helps Django site operators provide an OAuth 2.0 interface. The module\nis registered as an application.\n\nIn settings.py, add 'oauth2app' to INSTALLED_APPS. ::\n\n\n    INSTALLED_APPS = (\n        ...,\n        'oauth2app' \n    )\n\nSync the DB models. ::\n\n    python manage.py syncdb\n\nIn urls.py, add /oauth2/authorize and /oauth2/token views to a new or existing app. ::\n\n    urlpatterns += patterns('',\n        (r'^oauth2/missing_redirect_uri/?$',   'mysite.oauth2.views.missing_redirect_uri'),\n        (r'^oauth2/authorize/?$',                'mysite.oauth2.views.authorize'),\n        (r'^oauth2/token/?$',                    'oauth2app.token.handler'),\n    )\n    \nCreate client models. ::\n\n    from oauth2app.models import Client\n\n    Client.objects.create(\n        name=\"My Sample OAuth 2.0 Client\",\n        user=user)\n\nCreate authorize and missing_redirect_uri handlers. ::\n\n    from django.shortcuts import render_to_response\n    from django.http import HttpResponseRedirect\n    from django.template import RequestContext\n    from django.contrib.auth.decorators import login_required\n    from oauth2app.authorize import Authorizer, MissingRedirectURI, AuthorizationException\n    from django import forms\n\n    class AuthorizeForm(forms.Form):\n        pass\n\n    @login_required\n    def missing_redirect_uri(request):\n        return render_to_response(\n            'oauth2/missing_redirect_uri.html', \n            {}, \n            RequestContext(request))\n\n    @login_required\n    def authorize(request):\n        authorizer = Authorizer()\n        try:\n            authorizer.validate(request)\n        except MissingRedirectURI, e:\n            return HttpResponseRedirect(\"/oauth2/missing_redirect_uri\")\n        except AuthorizationException, e:\n            # The request is malformed or invalid. Automatically \n            # redirects to the provided redirect URL.\n            return authorizer.error_redirect()\n        if request.method == 'GET':\n            template = {}\n            # Use any form, make sure it has CSRF protections.\n            template[\"form\"] = AuthorizeForm()\n            # Appends the original OAuth2 parameters.\n            template[\"form_action\"] = '/oauth2/authorize?%s' % authorizer.query_string\n            return render_to_response(\n                'oauth2/authorize.html', \n                template, \n                RequestContext(request))\n        elif request.method == 'POST':\n            form = AuthorizeForm(request.POST)\n            if form.is_valid():\n                if request.POST.get(\"connect\") == \"Yes\":\n                    # User agrees. Redirect to redirect_uri with success params.\n                    return authorizer.grant_redirect()\n                else:\n                    # User refuses. Redirect to redirect_uri with error params.\n                    return authorizer.error_redirect()\n        return HttpResponseRedirect(\"/\")\n\nAuthenticate requests. ::\n\n    from oauth2app.authenticate import Authenticator, AuthenticationException\n    from django.http import HttpResponse\n    \n    def test(request):\n        authenticator = Authenticator()\n        try:\n            # Validate the request.\n            authenticator.validate(request)\n        except AuthenticationException:\n            # Return an error response.\n            return authenticator.error_response(content=\"You didn't authenticate.\")\n        username = authenticator.user.username\n        return HttpResponse(content=\"Hi %s, You authenticated!\" % username)\n\nIf you want to authenticate JSON requests try the JSONAuthenticator. ::\n\n    from oauth2app.authenticate import JSONAuthenticator, AuthenticationException\n\n    def test(request):\n        authenticator = JSONAuthenticator()\n        try:\n            # Validate the request.\n            authenticator.validate(request)\n        except AuthenticationException:\n            # Return a JSON encoded error response.\n            return authenticator.error_response()\n        username = authenticator.user.userame\n        # Return a JSON encoded response.\n        return authenticator.response({\"username\":username})\n\nExamples\n--------\n\nAn `example Django project <https://github.com/hiidef/oauth2app/tree/develop/examples/mysite>`_ demonstrating client and server functionality is available in the repository.\n\nhttps://github.com/hiidef/oauth2app/tree/develop/examples/mysite\n"
 },
 {
  "repo": "pcevikogullari/AndroidShortcuts",
  "language": "Java",
  "readme_contents": "# Android Shortcuts\n\nExample app for shortcuts in design library v25\n\n### Demo\n![Demo 1](https://raw.githubusercontent.com/pcevikogullari/AndroidShortcuts/master/shortcut1.gif) ![Demo 1](https://raw.githubusercontent.com/pcevikogullari/AndroidShortcuts/master/shortcut2.gif)\n\n### Manifest\nAdd meta-data before ```</activity>``` tag in Manifest.xml\n```\n<meta-data android:name=\"android.app.shortcuts\"\n    android:resource=\"@xml/shortcuts\" />\n```\n\n### Add Shortcut\nTo add or edit a new shotcut, go to /res/xml/shortcuts.xml :\n```sh\n<shortcuts xmlns:android=\"http://schemas.android.com/apk/res/android\">\n    <shortcut\n        android:shortcutId=\"shortcut1\"\n        android:enabled=\"true\"\n        android:icon=\"@drawable/ic_directions_run_black_24dp\"\n        android:shortcutShortLabel=\"@string/shortcut1\"\n        android:shortcutLongLabel=\"@string/shortcut1_long\"\n        android:shortcutDisabledMessage=\"@string/shortcut1_disabled\">\n        <intent\n            android:action=\"custom_action\"\n            android:targetPackage=\"com.pamir.shortcuts\"\n            android:targetClass=\"com.pamir.shortcuts.MainActivity\" />\n    </shortcut>\n</shortcuts>\n```\n\n### Handle Actions\n\nTo handle shortcuts, just add new constant:\n```\nprivate final static String CUSTOM_ACTION = \"custom_action\";\n```\n\nand check the intent for custom action :\n```\nswitch (getIntent().getAction()){\n    case CUSTOM_ACTION:\n        textView.setText(CUSTOM_ACTION);\n        break;\n    default:\n        break;\n}\n```\n\n"
 },
 {
  "repo": "zhaozhentao/MaterialImageView",
  "language": "Java",
  "readme_contents": "# MaterialImageView\n\n\u5c0f\u800c\u7f8e\u7684MaterialImageView\n\n\u9634\u5f71 \u5706\u89d2 \u65cb\u8f6c\u6297\u952f\u9f7f\n\n# ScreenShot\n\n![image](https://github.com/zhaozhentao/MaterialImageView/blob/master/screenshot/screenshot.jpg)\n\n\n\n"
 },
 {
  "repo": "KilianB/JImageHash",
  "language": "Java",
  "readme_contents": "<img align=left src = \"https://user-images.githubusercontent.com/9025925/48595271-388ba280-e954-11e8-8bc6-8b8afe108682.png\" />\n\n# JImageHash\n\n[![Travis](https://travis-ci.org/KilianB/JImageHash.svg?branch=master)](https://travis-ci.org/KilianB/JImageHash)\n[![GitHub license](https://img.shields.io/github/license/KilianB/JImageHash.svg)](https://github.com/KilianB/JImageHash/blob/master/LICENSE)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/3c7db745b9ff4dd9b89484a6aa46ad2f)](https://www.codacy.com/app/KilianB/JImageHash?utm_source=github.com&utm_medium=referral&utm_content=KilianB/JImageHash&utm_campaign=Badge_Grade)\n\nJImageHash is a performant perceptual image fingerprinting library entirely written in Java. The library returns a similarity score aiming to identify entities which are likely modifications of the original source while being robust various attack vectors ie. color, rotation and scale transformation.\n\n> A perceptual hash is a fingerprint of a multimedia file derived from various features from its content. Unlike cryptographic hash functions which rely on the avalanche effect of small changes in input leading to drastic changes in the output, perceptual hashes are \"close\" to one another if the features are similar.\n\nThis library was inspired by _Dr. Neal Krawetz_ blog post \"<a href=\"http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html\">kind of like that</a>\" and incorporates several improvements. A comprehensive overview of perceptual image hashing can be found in this <a href=\"https://www.phash.org/docs/pubs/thesis_zauner.pdf\">paper</a> by Christoph Zauner.\n\n\n\n## Maven\n\nThe project is hosted on maven central\n\n```XML\n<dependency>\n\t<groupId>dev.brachtendorf</groupId>\n\t<artifactId>JImageHash</artifactId>\n\t<version>1.0.0</version>\n</dependency>\n\n<!-- If you want to use the database image matcher you need to add h2 as well -->\n<dependency>\n\t<groupId>com.h2database</groupId>\n\t<artifactId>h2</artifactId>\n\t<version>1.4.200</version>\n</dependency>\n```\n\n### Breaking Changes: migration guide to version 1.0.0\n\n**Please be aware that migrating from one major version to another usually invalidates created hashes in order to retain validity when persistently storing the hashes.**\nThe algorithm id of hashes is adjusted in order for the jvm to throw an error if the possibility exist that hashes generated for the same input image are not consistent throughout the compared versions.\n\n Hashes generated with the following 2 algorithm have to be regenerated:\n\n- RotPAverage hash was fixed to correctly return hashes when the algorithm is used multiple times.\n- KernelAverageHash algorithm id changed due to JVMs internal hashcode calculation and the package name update. Hashes generated with this algorithm have to be regenerated.\n\nThe package is now published to maven central under a new group id. The internal package structure has been adjusted from `com.github.kilianB` to `dev.brachtendorf.jimagehash`. Adjust your imports accordingly.\n\n\n## Hello World\n\n```Java\nFile img0 = new File(\"path/to/file.png\");\nFile img1 = new File(\"path/to/secondFile.jpg\");\n\nHashingAlgorithm hasher = new PerceptiveHash(32);\n\nHash hash0 = hasher.hash(img0);\nHash hash1 = hasher.hash(img1);\n\ndouble similarityScore = hash0.normalizedHammingDistance(hash1);\n\nif(similarityScore < .2) {\n    //Considered a duplicate in this particular case\n}\n\n//Chaining multiple matcher for single image comparison\n\nSingleImageMatcher matcher = new SingleImageMatcher();\nmatcher.addHashingAlgorithm(new AverageHash(64),.3);\nmatcher.addHashingAlgorithm(new PerceptiveHash(32),.2);\n\nif(matcher.checkSimilarity(img0,img1)) {\n    //Considered a duplicate in this particular case\n}\n```\n\n## Examples\n\nExamples and convenience methods can be found in the [examples repository](https://github.com/KilianB/JImageHash-Examples)\n\n## Transparent image support\n\nSupport for transparent images has to be enabled specifically due to backwards compatibility and force users of the libraries to understand the implication of this setting.\n\nThe `setOpaqueHandling(Color? replacementColor, int alphaThreshold)` will replace transparent pixels with the specified color before calculating the hash.\n\n### Be aware of the following culprits: \n\n- the replacement color must be consistent throughout hash calculation for the entire sample space to ensure robustness against color transformations of the images.\n- the replacement color should be a color that does not appear often in the input space to avoid masking out available information.\n- when not specified `Orange` will be used as replacement. This choice was arbitrary and ideally, a default color should be chosen which results in 0 and 1 bits being computed in 50% of the time in respect to all other pixels and hashing algorithms.\n- supplying a replacement value of null will attempt to either use black or white as a replacement color conflicting with the advice given above. Computing the contrast color will fail if the transparent area of an image covers a large space and comes with a steep performance penalty.\n\n```java\nHashingAlgorithm hasher = new PerceptiveHash(32);\n\n//Replace all pixels with alpha values smaller than 0-255. The alpha value cutoff is taken into account after down scaling the image, therefore choose a reasonable value.  \nint alphaThreshold = 253;\nhasher.setOpaqueHandling(alphaThreshold)\n\n```\n\n## Multiple types image matchers are available for each situation\n\nThe `persistent` package allows hashes and matchers to be saved to disk. In turn the images are not kept in memory and are only referenced by file path allowing to handle a great deal of images\nat the same time.\nThe `cached` version keeps the BufferedImage image objects in memory allowing to change hashing algorithms on the fly and a direct retrieval of the buffered image objects of matching images.\nThe `categorize` package contains image clustering matchers. KMeans and Categorical as well as weighted matchers.\nThe `exotic` package features BloomFilter, and the SingleImageMatcher used to match 2 images without any fancy additions.\n\n<table>\n<tr> <th>Image</th>  <th></th> <th>High</th> <th>Low</th> <th>Copyright</th> <th>Thumbnail</th> <th>Ballon</th> </tr>\n\n<tr> <td>High Quality</td>  <td><img width= 75% src=\"https://user-images.githubusercontent.com/9025925/36542413-046d8116-17e1-11e8-93ed-210f65293d51.jpg\"></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td> \n</tr> \n<tr> <td>Low Quality</td>  <td><img width= 75% src=\"https://user-images.githubusercontent.com/9025925/36542414-0498079c-17e1-11e8-9224-a9852797b96f.jpg\"></td> \n<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td>\n</tr>\n\n <tr> <td>Altered Copyright</td>  <td><img width= 75% src=\"https://user-images.githubusercontent.com/9025925/36542411-0438eb36-17e1-11e8-9a59-2c69937560bf.jpg\"> </td> \n<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td>\n</tr>\n\n<tr> <td>Thumbnail</td>  <td><img src=\"https://user-images.githubusercontent.com/9025925/36542415-04ca8078-17e1-11e8-9be4-9a90b08c404b.jpg\"></td> \n<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td>\n</tr> \n\t\n<tr> <td>Ballon</td>  <td><img width= 75% src=\"https://user-images.githubusercontent.com/9025925/36542417-04f3e6a2-17e1-11e8-91b2-50f9961524b4.jpg\"></td> \n<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/DC143C?text=+\"/></p></td> \n\t<td><p align=\"center\"><image src=\"https://via.placeholder.com/30/228B22?text=+\"/></p></td>\n</tr> \n\t\n</table>\n\n## Hashing algorithm\n\nImage matchers can be configured using different algorithm. Each comes with individual properties\n\n<table>\n  <tr><th>Algorithm</th>  <th>Feature</th><th>Notes</th> </tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#averagehash-averagekernelhash-medianhash-averagecolorhash\">AverageHash</a></td>  <td>Average Luminosity</td> <td>Fast and good all purpose algorithm</td> </tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#averagehash-averagekernelhash-medianhash-averagecolorhash\">AverageColorHash</a></td>  <td>Average Color</td> <td>Version 1.x.x AHash. Usually worse off than AverageHash. Not robust against color changes</td> </tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#differencehash\">DifferenceHash</a></td> <td>Gradient/Edge detection</td> <td>A bit more robust against hue/sat changes compared to AColorHash </td> </tr>\n  <tr><td>Wavelet Hash</td> <td>Frequency & Location</td> <td>Feature extracting by applying haar wavlets multiple times to the input image. Detection quality better than inbetween aHash and pHash.</td> </tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#perceptive-hash\">PerceptiveHash</a></td> <td>Frequency</td> <td>Hash based on Discrete Cosine Transformation. Smaller hash distribution but best accuracy / bitResolution.</td> </tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#averagehash-averagekernelhash-medianhash-averagecolorhash\">MedianHash</a></td> <td>Median Luminosity</td> <td>Identical to AHash but takes the median value into account. A bit better to detect watermarks but worse at scale transformation</td> </tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#averagehash-averagekernelhash-medianhash-averagecolorhash\">AverageKernelHash</a></td>  <td>Average luminosity </td> <td>Same as AHash with kernel preprocessing. So far usually performs worse, but testing is not done yet.</td> </tr>\n  <tr><td colspan=3 align=center><b>Rotational Invariant</b></td></tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#rotphash\">RotAverageHash</a></td>  <td>Average Luminosity</td> <td>Rotational robust version of AHash. Performs well but performance scales disastrous with higher bit resolutions . Conceptual issue: pixels further away from the center are weightend less.</td> </tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#rotphash\">RotPHash</a></td> <td>Frequency</td> <td> Rotational invariant version of pHash using ring partition to map pixels in a circular fashion. Lower complexity for high bit sizes but due to sorting pixel values usually maps to a lower normalized distance. Usually bit res of >= 64bits are preferable</td> </tr>  \n   <tr><td colspan=3 align=\"center\"><i><b>Experimental.</b> Hashes available but not well tuned and subject to changes</i></td></tr>\n  <tr><td><a href=\"https://github.com/KilianB/JImageHash/wiki/Hashing-Algorithms#hoghash\">HogHash</a></td> <td>Angular Gradient based (detection of shapes?) </td> <td>A hashing algorithm based on hog feature detection which extracts gradients and pools them by angles. Usually used in support vector machine/NNs human outline detection. It's not entirely set how the feature vectors should be encoded. Currently average, but not great results, expensive to compute and requires a rather high bit resolution</td> </tr>  \n</table>\n\n### Version 3.0.0 Image clustering\n\nImage clustering with fuzzy hashes allowing to represent hashes with probability bits instead of simple 0's and 1's\n\n![1_fxpw79yoon8xo3slqsvmta](https://user-images.githubusercontent.com/9025925/51272388-439d9600-19ca-11e9-8220-fe3539ed6061.png)\n\n### Algorithm benchmarking\n\nSee the wiki page on how to test different hashing algorithms with your set of images\nCode available at the example repo: https://github.com/KilianB/JImageHash-Examples/tree/main/src/main/java/com/github/kilianB/benchmark\n\n<img src=\"https://user-images.githubusercontent.com/9025925/49185669-c14a0b80-f362-11e8-92fa-d51a20476937.jpg\" />\n"
 },
 {
  "repo": "etsy/statsd-jvm-profiler",
  "language": "Java",
  "readme_contents": "# statsd-jvm-profiler [![Build Status](https://travis-ci.org/etsy/statsd-jvm-profiler.svg)](https://travis-ci.org/etsy/statsd-jvm-profiler)\n\nstatsd-jvm-profiler is a JVM agent profiler that sends profiling data to StatsD.  Inspired by [riemann-jvm-profiler](https://github.com/riemann/riemann-jvm-profiler), it was primarily built for profiling Hadoop jobs, but can be used with any JVM process.\n\nRead [the blog post](https://codeascraft.com/2015/01/14/introducing-statsd-jvm-profiler-a-jvm-profiler-for-hadoop/) that introduced statsd-jvm-profiler on [Code as Craft](https://codeascraft.com/), Etsy's engineering blog.\n\nAlso check out [the blog post](https://codeascraft.com/2015/05/12/four-months-of-statsd-jvm-profiler-a-retrospective/) reflecting on the experience of open-sourcing the project.\n\n## Mailing List\nThere is a mailing list for this project at https://groups.google.com/forum/#!forum/statsd-jvm-profiler.  If you have questions or suggestions for the project send them here!\n\n## Installation\n\nYou will need the statsd-jvm-profiler JAR on the machine where the JVM will be running.  If you are profiling Hadoop jobs, that means the JAR will need to be on all of the datanodes.\n\nThe JAR can be built with `mvn package`.  You will need a relatively recent Maven (at least Maven 3).\n\nstatsd-jvm-profiler is available in Maven Central:\n```xml\n<dependency>\n  <groupId>com.etsy</groupId>\n  <artifactId>statsd-jvm-profiler</artifactId>\n  <version>2.0.0</version>\n</dependency>\n```\n\nIf you would like an uberjar containing all of the dependencies instead of the standard JAR, use the `jar-with-dependencies` classifier:\n```xml\n<dependency>\n  <groupId>com.etsy</groupId>\n  <artifactId>statsd-jvm-profiler</artifactId>\n  <version>2.0.0</version>\n  <classifier>jar-with-dependencies</classifier>\n</dependency>\n```\n\n## Usage\n\nThe profiler is enabled using the JVM's `-javaagent` argument.  You are required to specify at least the StatsD host and port number to use.  You can also specify the prefix for metrics and a whitelist of packages to be included in the CPU profiling.  Arguments can be specified like so:\n```\n-javaagent:/usr/etsy/statsd-jvm-profiler/statsd-jvm-profiler.jar=server=hostname,port=num\n```\n\nYou should use the uberjar when starting the profiler in this manner so that all the profiler's dependencies are available.\n\nThe profiler can also be loaded dynamically (after the JVM has already started), but this technique requires relying on Sun's `tools.jar`, meaning it's an implementation-specific solution that might not work for all JVMs. For more information see the [Dynamic Loading section](#dynamic-loading-of-agent). \n\nAn example of setting up Cascading/Scalding jobs to use the profiler can be found in the `example` directory.\n\n### Global Options\n\nName             | Meaning\n---------------- | -------\nserver           | The hostname to which the reporter should send data (required)\nport             | The port number for the server to which the reporter should send data (required)\nprefix           | The prefix for metrics (optional, defaults to statsd-jvm-profiler)\npackageWhitelist | Colon-delimited whitelist for packages to include (optional, defaults to include everything)\npackageBlacklist | Colon-delimited whitelist for packages to exclude (optional, defaults to exclude nothing)\nprofilers        | Colon-delimited list of profiler class names (optional, defaults to `CPUTracingProfiler` and `MemoryProfiler`)\nreporter         | Class name of the reporter to use (optional, defaults to StatsDReporter)\nhttpServerEnabled| Determines if the embedded HTTP server should be started. (optional, defaults to `true`)\nhttpPort         | The port on which to bind the embedded HTTP server (optional, defaults to 5005). If this port is already in use, the next free port will be taken.\n\n### Embedded HTTP Server\nstatsd-jvm-profiler embeds an HTTP server to support simple interactions with the profiler while it is in operation.\nYou can configure the port on which this server runs with the `httpPort` option.\nYou can disable it altogether using the `httpServerEnabled=false` argument.\n \nEndpoint                    | Usage\n---------------             | -----\n/profilers                  | List the currently enabled profilers\n/isRunning                  | List the running profilers. This should be the same as /profilers.\n/disable/:profiler          | Disable the profiler specified by `:profiler`. The name must match what is returned by `/profilers`.\n/errors                     | List the past 10 errors from the running profilers and reporters.\n/status/profiler/:profiler  | Displays a status message with the number of recorded stats for the requested profiler.\n\n### Reporters\nstatsd-jvm-profiler supports multiple backends.  StatsD is the default, but InfluxDB is also supported.  You can select the backend to use by passing the `reporter` argument to the profiler; `StatsDReporter` and `InfluxDBReporter` are the supported values.\n\nSome reporters may require additional arguments.\n\n#### StatsDReporter\nThis reporter does not have any additional arguments.\n\n#### InfluxDBReporter\n\nName        | Meaning\n----------- | -------\nusername    | The username with which to connect to InfluxDB (required)\npassword    | The password with which to connect to InfluxDB (required)\ndatabase    | The database to which to write metrics (required)\ntagMapping  | A mapping of tag names from the metric prefix (optional, defaults to no mapping)\nuseHttps    | A flag indicating if https connecition should be used (optional, defaults to false)\n\n##### Tag Mapping\nInfluxDB 0.9 supports tagging measurements and querying based on those tags.  statsd-jvm-profilers uses these tags to support richer querying of the produced data.  For compatibility with other metric backends, the tags are extracted from the metric prefix.\n\nIf the `tagMapping` argument is not defined, only the `prefix` tag will be added, with the value of the entire prefix.\n\n`tagMapping` should be a period-delimited set of tag names.  It must have the same number of components as `prefix`, or else an exception would be thrown.  Each component of `tagMapping` is the name of the tag.  The component in the corresponding position of `prefix` will be the value.\n\nIf you do not want to include a component of `prefix` as a tag, use the special name `SKIP` in `tagMapping` for that position.\n\n## Profilers\n\n`statsd-jvm-profiler` offers 3 profilers: `MemoryProfiler`, `CPUTracingProfiler` and `CPULoadProfiler`.\n\nThe metrics for all these profilers will prefixed with the value from the `prefix` argument or it's default value: `statsd-jvm-profiler`.\n\nYou can enable specific profilers through the `profilers` argument like so:\n1. Memory metrics only: `profilers=MemoryProfiler`\n2. CPU Tracing metrics only: `profilers=CPUTracingProfiler`\n3. JVM/System CPU load metrics only: `profilers=CPULoadProfiler`\n\nDefault value: `profilers=MemoryProfiler:CPUTracingProfiler`\n\n### Garbage Collector and Memory Profiler: `MemoryProfiler`\nThis profiler will record:\n\n1. Heap and non-heap memory usage\n2. Number of GC pauses and GC time\n\nAssuming you use the default prefix of `statsd-jvm-profiler`,\nthe memory usage metrics will be under `statsd-jvm-profiler.heap` and `statsd-jvm-profiler.nonheap`,\nthe GC metrics will be under `statsd-jvm-profiler.gc`.\n\nMemory and GC metrics are reported once every 10 seconds.\n\n### CPU Tracing Profiler: `CPUTracingProfiler`\nThis profiler records the time spent in each function across all Threads.\n\nAssuming you use the default prefix of `statsd-jvm-profiler`, the the CPU time metrics will be under `statsd-jvm-profiler.cpu.trace`.\n\nThe CPU time is sampled every millisecond, but only reported every 10 seconds.\nThe CPU time metrics represent the total time spent in that function.\n\nProfiling a long-running process or a lot of processes simultaneously will produce a lot of data, so be careful with the\ncapacity of your StatsD instance.  The `packageWhitelist` and `packageBlacklist` arguments can be used to limit the number\nof functions that are reported. Any function whose stack trace contains a function in one of the whitelisted packages will be included.\n\nThe `visualization` directory contains some utilities for visualizing the output of this profiler.\n\n### JVM And System CPU Load Profiler: `CPULoadProfiler`\n\nThis profiler will record the JVM's and the overall system's CPU load, if the JVM is capable of providing this information.\n\nAssuming you use the default prefix of `statsd-jvm-profiler`, the JVM CPU load metrics will be under `statsd-jvm-profiler.cpu.jvm`,\nand the System CPU load wil be under `statsd-jvm-profiler.cpu.system`.\n\nThe reported metrics will be percentages in the range of [0, 100] with 1 decimal precision.\n\nCPU load metrics are sampled and reported once every 10 seconds.\n\nImportant notes:\n* This Profiler is not enabled by default. To enable use the argument `profilers=CPULoadProfiler`\n* This Profiler relies on Sun/Oracle-specific JVM implementations that offer a JMX bean that might not be available in other JVMs.\n  Even if you are using the right JVM, there's no guarantee this JMX bean will remain there in the future.\n* The minimum required JVM version that offers support for this is for Java 7.\n* See [com.sun.management.OperatingSystemMXBean](https://docs.oracle.com/javase/7/docs/jre/api/management/extension/com/sun/management/OperatingSystemMXBean.html#getProcessCpuLoad())\n  for more information.\n* If the JVM doesn't support the required operations, the metrics above won't be reported at all.\n\n## Dynamic Loading of Agent\n\n1. Make sure you have the `tools.jar` available in your classpath during compilation and runtime. This JAR is usually found in the JAVA_HOME directory under the `/lib` folder for Oracle Java installations.\n2. Make sure the `jvm-profiler` JAR is available during runtime. \n3. During your application boostrap process, do the following:\n\n```scala\n  val jarPath: String = s\"$ABSOLUTE_PATH_TO/com.etsy.statsd-jvm-profiler-$VERSION.jar\"\n  val agentArgs: String = s\"server=$SERVER,port=$PORT\"\n  attachJvmAgent(jarPath, agentArgs)\n\n  def attachJvmAgent(profilerJarPath: String, agentArgs: String): Unit = {\n    val nameOfRunningVM: String = java.lang.management.ManagementFactory.getRuntimeMXBean.getName\n    val p: Integer = nameOfRunningVM.indexOf('@')\n    val pid: String = nameOfRunningVM.substring(0, p)\n\n    try {\n      val vm: com.sun.tools.attach.VirtualMachine = com.sun.tools.attach.VirtualMachine.attach(pid)\n      vm.loadAgent(profilerJarPath, agentArgs)\n      vm.detach()\n      LOGGER.info(\"Dynamically loaded StatsD JVM Profiler Agent...\");\n    } catch {\n      case e: Exception => LOGGER.warn(s\"Could not dynamically load StatsD JVM Profiler Agent ($profilerJarPath)\", e);\n    }\n  }\n```\n\n\n## Contributing\nContributions are highly encouraged!  Check out [the contribution guidlines](https://github.com/etsy/statsd-jvm-profiler/blob/master/CONTRIBUTING.md).\n\nAny ideas you have are welcome,  but check out [some ideas](https://github.com/etsy/statsd-jvm-profiler/wiki/Contribution-Ideas) for contributions.\n"
 },
 {
  "repo": "hanlyjiang/AndroidDocumentViewer",
  "language": "Java",
  "readme_contents": "\n## \u5728 Android \u4e0a\u67e5\u770bword\uff0cexcel\uff0cpowerpoint\uff0cpdf\n\n\u793a\u4f8bapp\u6548\u679c\uff1a\n\n![\u793a\u4f8bapp\u6548\u679c](doc/fileviewer-demo-screenshots.gif)\n\n## \u5982\u4f55\u4f7f\u7528\uff1f\n\u53c2\u8003: [\u793a\u4f8b](app)\n* \u590d\u5236`lib_fileviewer`\u6a21\u5757\uff0c\u52a0\u5165\u5230project\n* App\u4e2dbuild.gradle\u4e2d\u52a0\u5165\u4ee5\u4e0b\u914d\u7f6e\uff1a\n```groovy\nndk {\n            // \u6b64\u5904\u5fc5\u987b\u8bbe\u7f6e\u4e3a armeabi \uff0cTBS \u6587\u4ef6\u6d4f\u89c8\u4e0d\u652f\u6301\u5176\u4ed6\u7c7b\u578b\n            abiFilters \"armeabi\"\n    }\n```\n* TBS\u521d\u59cb\u5316(\u5728Application\u4e2d)\uff1a\n```java\nQbSdk.initX5Environment(getApplicationContext(), new QbSdk.PreInitCallback() {\n            @Override\n            public void onCoreInitFinished() {\n                Log.d(TAG, \"onCoreInitFinished\");\n            }\n\n            @Override\n            public void onViewInitFinished(boolean initResult) {\n                Log.e(TAG, \"onViewInitFinished\" + initResult);\n            }\n        });\n```\n\n* **\u67e5\u770b\u6587\u4ef6\u7edf\u4e00\u5165\u53e3\uff1a**\n```\nUri uri = Uri.fromFile(new File(filePath));\nFileViewer.viewFile(context,uri)\n```\n* **\u76f4\u63a5\u4f7f\u7528mupdf\u67e5\u770b\uff1a**\n```java\nFileViewer.viewPDFWithMuPDFByPath(Context context, String filePath)\n```\n\u6216\uff1a\n```java\nFileViewer.startMuPDFActivityByUri(Context context, Uri documentUri)\n```\n\n* **\u76f4\u63a5\u4f7f\u7528TBS\u67e5\u770bword\u6587\u6863\uff1a**\n```\nTBSFileViewActivity.viewFile(context, filePath);\n```\n\n## \u6ce8\u610f\u4e8b\u9879\n1. office\u6587\u4ef6\u65e0\u6cd5\u67e5\u770b\uff08TBS\u521d\u59cb\u5316\u5931\u8d25\uff09\uff0c\u53ef\u4ee5\u67e5\u770b\u8fd9\u4e2a\u6587\u6863\uff1a[\u65e0\u6cd5\u52a0\u8f7dx5\u5185\u6838\u7684\u89e3\u51b3\u65b9\u6848.doc](doc/\u65e0\u6cd5\u52a0\u8f7dx5\u5185\u6838\u7684\u89e3\u51b3\u65b9\u6848.doc)\n\n\n## \u4f7f\u7528\u5230\u7684\u5e93\uff1a\n### PDF\u67e5\u770b\uff1a mupdf\n\n\u7248\u672c\uff1a v1.11.1\n\n\n> \u4ecb\u7ecd\uff1a\n> MuPDF is an open source software framework for viewing and converting PDF, XPS, and E-book documents. There are viewers for various platforms, several command line tools, and a software library for building tools and applications.\n\n\u9879\u76ee\u5730\u5740:\nhttps://mupdf.com/docs/\n\nAndroid \u6587\u6863:\nhttps://mupdf.com/docs/android-sdk.html\n\n### word\u7b49\u6587\u4ef6\u67e5\u770b \uff1a TBS\uff08\u817e\u8baf\u6d4f\u89c8\u670d\u52a1\uff09\n> **\u7b80\u4ecb:**\n> web\u5185\u6838\n\n\n[\u5b98\u65b9\u9875\u9762](http://x5.tencent.com/)\n"
 },
 {
  "repo": "glung/redux-java",
  "language": "Java",
  "readme_contents": "// This project is unmaintained //\n\n# \u25b2\u25b2\u25b2 Redux-java \u25b2\u25b2\u25b2\n\nA java implementation of [jvm-redux-api](https://github.com/jvm-redux/jvm-redux-api)\n\n# Integration\n\n## Gradle \n\n```\nallprojects {\n  repositories {\n    ...\n    maven { url 'https://jitpack.io' }\n  }\n}\n```\n\n```\ndependencies {\n  compile 'com.github.glung:redux-java:1.0'\n}\n```\n\n## Maven\n```\n<repositories>\n  <repository>\n      <id>jitpack.io</id>\n      <url>https://jitpack.io</url>\n  </repository>\n</repositories>\n```\n\n```\n<dependency>\n  <groupId>com.github.glung</groupId>\n  <artifactId>redux-java</artifactId>\n  <version>1.0</version>\n</dependency>\n```\n"
 },
 {
  "repo": "kaczmarkiewiczp/rcloneExplorer",
  "language": "Java",
  "readme_contents": "# RcloneExplorer\n[![Packagist](https://img.shields.io/packagist/l/doctrine/orm.svg)](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/LICENSE) [![Github Releases](https://img.shields.io/github/downloads/kaczmarkiewiczp/rcloneExplorer/total.svg)](https://github.com/kaczmarkiewiczp/rcloneExplorer/edit/master/releases) [![GitHub release](https://img.shields.io/github/release/kaczmarkiewiczp/rcloneExplorer.svg)](https://github.com/kaczmarkiewiczp/rcloneExplorer/releases/latest)\n\nrclone explorer for Android\n\nFeatures\n--------\n- Allows to browse rclone remotes, including encrypted ones\n- Import configuration file from rclone\n- Create new remotes from the app\n- Download and upload files\n- Move, rename, and delete files and folders\n- Create new folders\n- Streaming media files\n- Serving directories over HTTP or Webdav\n- Dark theme\n- Customizable primary and accent colors\n- Supports ARM and x86 devices\n- Supports SDK 21+ (Lollipop 5.0)\n\nTODO\n------------\n- [X] Creating new remotes\n  - [ ] Creating Team Drive remotes\n- [X] Deleting existing remotes\n\nScreenshots\n-----------\n\nRemotes|Encrypted Config|File Explorer|File Upload\n:-----:|:--------------:|:-----------:|:---------:|\n![screenshot1](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_1.png?raw=true)|![screenshot11](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_11.png?raw=true)|![screenshot2](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_2.png?raw=true)|![screenshot3](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_3.png?raw=true)\n\nFile Editing|Empty Folder|Dark Theme|Encrypted Config\n:----------:|:----------:|:--------:|:-------------:|\n![screenshot4](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_4.png?raw=true)|![screenshot5](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_5.png?raw=true)|![screenshot6](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_6.png?raw=true)|![screenshot10](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_10.png?raw=true)\n**File Explorer**|**File Editing**|**Empty Folder**|\n![screenshot7](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_7.png?raw=true)|![screenshot8](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_8.png?raw=true)|![screenshot9](https://github.com/kaczmarkiewiczp/rcloneExplorer/blob/master/screenshots/screenshot_9.png?raw=true)|\n\nInstallation\n------------\nGrab the [latest version](https://github.com/kaczmarkiewiczp/rcloneExplorer/releases/latest) of the signed APK and install it on your phone. Only devices running Android Lollipop 5.0 and up are supported.\n\n- For ARM 32-bit devices download RcloneExplorer-ARM_32.apk\n- For ARM 64-bit devices download RcloneExplorer-ARM_64.apk\n- For x86 devices download RcloneExplorer-x86.apk\n- Ultimately, RcloneExplorer.apk will work with both ARM and x86 devices.\n\nCredits/Libraries\n-----------------\n- [Android Support Libraries](https://developer.android.com/topic/libraries/support-library)\n- [Floating Action Button SpeedDial](https://github.com/leinardi/FloatingActionButtonSpeedDial) - A Floating Action Button Speed Dial implementation for Android that follows the Material Design specification.\n- [Glide](https://github.com/bumptech/glide) - An image loading and caching library for Android focused on smooth scrolling.\n- [Markdown View](https://github.com/falnatsheh/MarkdownView) - MarkdownView is an Android webview with the capablity of loading Markdown text or file and display it as HTML, it uses MarkdownJ and extends Android webview.\n- [Material Design Icons](https://github.com/Templarian/MaterialDesign) - 2200+ Material Design Icons from the Community.\n- [Recyclerview Animators](https://github.com/wasabeef/recyclerview-animators) - An Android Animation library which easily add itemanimator to RecyclerView items.\n- [rclone](https://github.com/ncw/rclone) - \"rsync for cloud storage\"\n- [Toasty](https://github.com/GrenderG/Toasty) - The usual Toast, but with steroids.\n- Icon made by [Smashicons](https://www.flaticon.com/authors/smashicons) from [Flaticon](https://www.flaticon.com)\n"
 },
 {
  "repo": "jenkinsci/dingtalk-plugin",
  "language": "Java",
  "readme_contents": "# DingTalk \u673a\u5668\u4eba\u901a\u77e5\n\n\n![\u673a\u5668\u4eba\u5934\u50cf](jenkins-logo.png)\n\n\n#    [\ud83d\udcaf\u3000\u80fd\u770b\u5230\u5417\uff1f\u8fd9\u662f\u6587\u6863\uff01\uff01\uff01](https://jenkinsci.github.io/dingtalk-plugin/)\n\n## Contributing\n\n [\ud83c\udf7b\u3000Contributing](./CONTRIBUTING.md)\n"
 },
 {
  "repo": "rongi/rotate-layout",
  "language": "Java",
  "readme_contents": "Rotate Layout\n=============\n\nA custom layout that can rotate it's view\n\n[![Example](https://github.com/rongi/rotate-layout/raw/master/docs/screenshot5.png)](#Example)\n\nUsage\n=====\n\nIn your layout file add\n\n```xml \n<com.github.rongi.rotate_layout.layout.RotateLayout\n\txmlns:app=\"http://schemas.android.com/apk/res-auto\"\n\tandroid:layout_width=\"wrap_content\"\n\tandroid:layout_height=\"wrap_content\"\n\tapp:angle=\"90\">\t<!-- Specify rotate angle here -->\n\n\t<YourLayoutHere\n\t\tandroid:layout_width=\"wrap_content\"\n\t\tandroid:layout_height=\"wrap_content\">\n\t</YourLayoutHere>\n</com.github.rongi.rotate_layout.layout.RotateLayout>\n```\n\nVoila! Your layout will be rotated 90 degrees.\n\nDownload\n========\n\n```groovy\nimplementation 'rongi.rotate-layout:rotate-layout:3.0.0'\n```\n\nFeatures\n========\n\n1. The rotated view receives correct touch events.\n2. The bounding box is also rotated. This means that if the view was 100x50px before the rotation, then after 90 degrees rotation it will be 50x100px and can fit into another layout with this dimensions.\n\n"
 },
 {
  "repo": "anupcowkur/Here-Be-Dragons",
  "language": "Java",
  "readme_contents": "# Deprecated\nThis project is no longer maintained. No new issues or pull requests will be accepted. You can still use the source or fork the project to suit your needs.\n\n# Here Be Dragons\nHere be dragons is an Intellij/Android Studio plugin that let's you annotate your impure Java methods with the `@SideEffect` annotation and shows a little dragon icon in the gutter when you call them.\n\n![How the plugin works](here-be-dragons.gif)\n\n# Why would I want to do this?\nWhen writing functional style code, isolating impure and pure functions becomes very important. This helps you visually identify which methods are impure when a bunch of methods are being called in a code block.\n\nFor a beginner friendly introduction to functional programming in Android, please checkout out my [blog series](https://medium.freecodecamp.com/functional-programming-for-android-developers-part-1-a58d40d6e742#.z2eewm52o).\n\n# Installation\n## Install the plugin\n[Download](https://github.com/anupcowkur/here-be-dragons/releases/download/v1.0.1/here-be-dragons-1.0.1.jar) the plugin jar and select \"Install Plugin From Disk\" in IntelliJ's plugin preferences.\n\n## Add the annotation library\nAdd the jcenter repository to your gradle build file if it's not already present:\n\n```groovy\nrepositories {\n    jcenter()\n}\n```\n\nNext, add the lib containing the `@SideEffect` annotation as a dependency:\n\n```groovy\ndependencies {\n    compile 'com.anupcowkur:here-be-dragons-annotation:1.0.1'\n}\n```\n\n# Usage\nSimpy add the `@SideEffect` annotation to any method you want like this:\n\n```java\nimport com.anupcowkur.herebedragons.SideEffect;\n\npublic class Test {\n    @SideEffect\n    public void foo() {\n        // do some impure things\n    }\n}\n```\n\nAnd when you call the method anywhere, you'll see the dragon icon show up in the IDE gutter.\n\n# License\nThis project is licensed under the [MIT License](https://github.com/anupcowkur/here-be-dragons/blob/master/License.txt)\n\n"
 },
 {
  "repo": "Yuriy-Svetlov/compress-images",
  "language": "C",
  "readme_contents": "# Compress-images\n\n\n[![Build Status](https://travis-ci.org/semiromid/compress-images.svg?branch=master)](https://travis-ci.org/semiromid/compress-images)\n\n> [compress-images](https://github.com/semiromid/compress-images) **Minify size your images**. Image compression with extension: jpg/jpeg, svg, png, gif.\n\n**Minify** size your images. **Image compression** with extension: **`jpg/jpeg`**, **`svg`**, **`png`**, **`gif`**. \n\n![Image](https://raw.githubusercontent.com/semiromid/compress-images/master/screenshots/1.png)\n\n\nYou can also use:\n* [imagemin](https://www.npmjs.com/package/imagemin)\n\n\n### Features\n\n\nYou can use different algorithms and methods for compressing images with many options.\n\n* For **JPG**: `jpegtran`, `mozjpeg`, `webp`, `guetzli`, `jpegRecompress`, `jpegoptim`, `tinify`;\n* For **PNG**: `pngquant`, `optipng`, `pngout`, `webp`, `pngcrush`, `tinify`;\n* For **SVG**: `svgo`;\n* For **GIF**: `gifsicle`, `giflossy`, `gif2webp`;\n\n##### Combine compression \n\n> You can even minify images by using a **combination of compression algorithms**. As an example - `mozjpeg` + `jpegoptim` or `jpegtran` + `mozjpeg` or any other algorithm.\n\n\n##### Saving error log\n\n> If you get an error, the error log will be saved. Default path `./log/compress-images`.\n\n\n##### Alternative configuration/algorithm for compressing images\n\n> If you get an error, alternative algorithms for compressing images can be used. As an example: you want to compress images in `jpegRecompress`, but you get the error  **Unsupported color conversion request**, so an alternative algorithm to compress the images can be used, like `mozjpeg`.\n\n\n##### Detect path for saving images\nYou can specify the path to source images folder and all images in the folder will be compressed and moved to output folder.\n\n**As an example, one of many**:\n\n        INPUT ['src/img/source/**/*.{jpg,JPG,jpeg,JPEG,gif,png,svg}']\n        OUTPUT ['build/img/']\n\n##### Note\nYou should have in your path slash: `/`.\nIf you have slash `\\` it may be to replaced: `input.replace(/\\\\/g, '/')`;\n\n![Image](https://raw.githubusercontent.com/semiromid/compress-images/master/screenshots/img_structure_forder.png)\n\n\nOther useful plugins: \n* [Live Reload Browser Page](https://live-reload-browser-page.com/)\n* [Live Alert Browser Page](https://live-alert-browser-page.com/)\n* [Live HTML Validator](https://live-html-validator.com/)\n\n\n# Get started\n\n## Install\n```shell\nnpm install compress-images --save-dev\n```\n\n## Examples of how to use it\n\n#### Base example\nhttps://github.com/semiromid/compress-images/tree/master/example\n* Read the [Manual](https://github.com/semiromid/compress-images/blob/master/example/Manual.txt)\n\n\n\n#### Example 1\n```javascript\n\nconst compress_images = require(\"compress-images\"),\n  INPUT_path_to_your_images,\n  OUTPUT_path;\n\nINPUT_path_to_your_images = \"src/img/**/*.{jpg,JPG,jpeg,JPEG,png,svg,gif}\";\nOUTPUT_path = \"build/img/\";\n\ncompress_images(INPUT_path_to_your_images, OUTPUT_path, { compress_force: false, statistic: true, autoupdate: true }, false,\n                { jpg: { engine: \"mozjpeg\", command: [\"-quality\", \"60\"] } },\n                { png: { engine: \"pngquant\", command: [\"--quality=20-50\", \"-o\"] } },\n                { svg: { engine: \"svgo\", command: \"--multipass\" } },\n                { gif: { engine: \"gifsicle\", command: [\"--colors\", \"64\", \"--use-col=web\"] } },\n  function (error, completed, statistic) {\n    console.log(\"-------------\");\n    console.log(error);\n    console.log(completed);\n    console.log(statistic);\n    console.log(\"-------------\");\n  }\n);\n```\n\n#### Example 2\n```javascript\nconst compress_images = require(\"compress-images\");\n\nfunction MyFun() {\n  compress_images(\n    \"src/img/**/*.{jpg,JPG,jpeg,JPEG,png,svg,gif}\",\n    \"build/img/\",\n    { compress_force: false, statistic: true, autoupdate: true },\n    false,\n    { jpg: { engine: \"mozjpeg\", command: [\"-quality\", \"60\"] } },\n    { png: { engine: \"pngquant\", command: [\"--quality=20-50\", \"-o\"] } },\n    { svg: { engine: \"svgo\", command: \"--multipass\" } },\n    {\n      gif: { engine: \"gifsicle\", command: [\"--colors\", \"64\", \"--use-col=web\"] },\n    },\n    function (err, completed) {\n      if (completed === true) {\n        // Doing something.\n      }\n    }\n  );\n}\n```\n\n\n#### Example 3\n```javascript\nconst compress_images = require('compress-images');\n\n// We will be compressing images [jpg] with two algorithms, [webp] and [jpg];\n\n//[jpg] ---to---> [webp]\ncompress_images(\n  \"src/img/**/*.{jpg,JPG,jpeg,JPEG}\",\n  \"build/img/\",\n  { compress_force: false, statistic: true, autoupdate: true },\n  false,\n  { jpg: { engine: \"webp\", command: false } },\n  { png: { engine: false, command: false } },\n  { svg: { engine: false, command: false } },\n  { gif: { engine: false, command: false } },\n  function (err) {\n    if (err === null) {\n      //[jpg] ---to---> [jpg(jpegtran)] WARNING!!! autoupdate  - recommended to turn this off, it's not needed here - autoupdate: false\n      compress_images(\n        \"src/img/**/*.{jpg,JPG,jpeg,JPEG}\",\n        \"build/img/\",\n        { compress_force: false, statistic: true, autoupdate: false },\n        false,\n        { jpg: { engine: \"jpegtran\", command: false } },\n        { png: { engine: false, command: false } },\n        { svg: { engine: false, command: false } },\n        { gif: { engine: false, command: false } },\n        function () {}\n      );\n    } else {\n      console.error(err);\n    }\n  }\n);\n```\n\n\n\n#### Example 4\n```javascript\nconst compress_images = require('compress-images');\n\n// Combine compressing images [jpg] with two different algorithms, [jpegtran] and [mozjpeg];\n//[jpg] ---to---> [jpg(jpegtran)]\ncompress_images(\n  \"src/img/source/**/*.{jpg,JPG,jpeg,JPEG}\",\n  \"src/img/combination/\",\n  { compress_force: false, statistic: true, autoupdate: true },\n  false,\n  {\n    jpg: {\n      engine: \"jpegtran\",\n      command: [\"-trim\", \"-progressive\", \"-copy\", \"none\", \"-optimize\"],\n    },\n  },\n  { png: { engine: false, command: false } },\n  { svg: { engine: false, command: false } },\n  { gif: { engine: false, command: false } },\n  function () {\n    //[jpg(jpegtran)] ---to---> [jpg(mozjpeg)] WARNING!!! autoupdate  - recommended to turn this off, it's not needed here - autoupdate: false\n    //----------------\n    compress_images(\n      \"src/img/combination/**/*.{jpg,JPG,jpeg,JPEG}\",\n      \"build/img/\",\n      { compress_force: false, statistic: true, autoupdate: false },\n      false,\n      { jpg: { engine: \"mozjpeg\", command: [\"-quality\", \"75\"] } },\n      { png: { engine: false, command: false } },\n      { svg: { engine: false, command: false } },\n      { gif: { engine: false, command: false } },\n      function () {}\n    );\n    //----------------\n  }\n);\n```\n\n\n\n#### Example 5\n```javascript\nconst compress_images = require('compress-images');\n\n//[jpg+gif+png+svg] ---to---> [jpg(webp)+gif(gifsicle)+png(webp)+svg(svgo)]\ncompress_images('src/img/source/**/*.{jpg,JPG,jpeg,JPEG,gif,png,svg}', 'build/img/', {compress_force: false, statistic: true, autoupdate: true}, false,\n                                            {jpg: {engine: 'webp', command: false}},\n                                            {png: {engine: 'webp', command: false}},\n                                            {svg: {engine: 'svgo', command: false}},\n                                            {gif: {engine: 'gifsicle', command: ['--colors', '64', '--use-col=web']}}, function(){\n      //-------------------------------------------------                                    \n      //[jpg] ---to---> [jpg(jpegtran)] WARNING!!! autoupdate  - recommended to turn this off, it's not needed here - autoupdate: false\n      compress_images('src/img/source/**/*.{jpg,JPG,jpeg,JPEG}', 'src/img/combine/', {compress_force: false, statistic: true, autoupdate: false}, false,\n                                                      {jpg: {engine: 'jpegtran', command: ['-trim', '-progressive', '-copy', 'none', '-optimize']}},\n                                                      {png: {engine: false, command: false}},\n                                                      {svg: {engine: false, command: false}},\n                                                      {gif: {engine: false, command: false}}, function(){\n            //[jpg(jpegtran)] ---to---> [jpg(mozjpeg)] WARNING!!! autoupdate  - recommended to turn this off, it's not needed here - autoupdate: false\n            compress_images('src/img/combine/**/*.{jpg,JPG,jpeg,JPEG}', 'build/img/', {compress_force: false, statistic: true, autoupdate: false}, false,\n                                                            {jpg: {engine: 'mozjpeg', command: ['-quality', '75']}},\n                                                            {png: {engine: false, command: false}},\n                                                            {svg: {engine: false, command: false}},\n                                                            {gif: {engine: false, command: false}}, function(){\n                  //[png] ---to---> [png(pngquant)] WARNING!!! autoupdate  - recommended to turn this off, it's not needed here - autoupdate: false\n                  compress_images('src/img/source/**/*.png', 'build/img/', {compress_force: false, statistic: true, autoupdate: false}, false,\n                                                                  {jpg: {engine: false, command: false}},\n                                                                  {png: {engine: 'pngquant', command: ['--quality=30-60', '-o']}},\n                                                                  {svg: {engine: false, command: false}},\n                                                                  {gif: {engine: false, command: false}}, function(){                                                      \n                  }); \n            });                                      \n      });\n      //-------------------------------------------------\n});\n```\n\n\n\n#### Example 6\nSometimes you could get errors, and then use alternative configuration \"compress-images\".\nAs an example, one of many:\n\n1. If you get an error from 'jpegRecompress', for example, the error \"Unsupported color conversion request\". In this case, an alternative image compression algorithm will be used.\n\n2. An error log will be created at path './log/lib/compress-images'.\n\n3. The algorithm 'mozjpeg' will attempt to be used instead.\n\n```javascript\n    const \n    compress_images = require('compress-images'),\n    INPUT_path_to_your_images = 'src/**/*.{jpg,JPG,jpeg,JPEG,png,svg,gif}',\n    OUTPUT_path = 'build/';\n    \n    compress_images(INPUT_path_to_your_images, OUTPUT_path, {compress_force: false, statistic: true, autoupdate: true, pathLog: './log/lib/compress-images'}, false,\n                                                {jpg: {engine: 'jpegRecompress', command: ['--quality', 'high', '--min', '60']}},\n                                                {png: {engine: 'pngquant', command: ['--quality=20-50', '-o']}},\n                                                {svg: {engine: 'svgo', command: '--multipass'}},\n                                                {gif: {engine: 'gifsicle', command: ['--colors', '64', '--use-col=web']}}, function(err, completed){\n            if(err !== null){\n                //---------------------------------------\n                //if you get an ERROR from 'jpegRecompress' ---> We can use alternate config of compression\n                //---------------------------------------\n                if(err.engine === 'jpegRecompress'){\n                    compress_images(err.input, err.output, {compress_force: false, statistic: true, autoupdate: true}, false,\n                                                                {jpg: {engine: 'mozjpeg', command: ['-quality', '60']}},\n                                                                {png: {engine: false, command: false}},\n                                                                {svg: {engine: false, command: false}},\n                                                                {gif: {engine: false, command: false}}, function(err){\n                            if(err !== null){\n                                //Alternative config of compression\n\n                            }                                       \n                    });\n                }\n                //---------------------------------------\n\n            }                                       \n\n    });\n```\n\n#### Example 7\n\nCompressing an image in the same folder (currently this only works with 'pngquant')\n\n```javascript\n    const \n    compress_images = require('compress-images'),\n    fs = require('fs'),\n    INPUT_path_to_your_images = 'src/img/**/!(*-min).png',\n    OUTPUT_path = 'src/img/';\n    \n    compress_images(INPUT_path_to_your_images, OUTPUT_path, {compress_force: true, statistic: false, autoupdate: true}, false,\n                                                {jpg: {engine: false, command: false}},\n                                                {png: {engine: 'pngquant', command: ['--quality=20-50', '--ext=-min.png', '--force']}},\n                                                {svg: {engine: false, command: false}},\n                                                {gif: {engine: false, command: false}}, function(err, completed, statistic){\n        if(err === null){\n            fs.unlink(statistic.input, (err) => {\n                if (err) throw err;\n                console.log('successfully compressed and deleted '+statistic.input);\n            });\n        }\n    });\n```\n\n```html\n    <picture>\n        <source type=\"image/webp\" srcset=\"//hostname/build/img/art/1/chat.webp\">\n        <img width=\"700\" height=\"922\" alt=\"test\" src=\"//hostname/build/img/art/1/chat.jpg\">\n    </picture>\n```\n\n## API\n\n**`compress_images`**(*`input`*, *`output`*, *`option`*, *`globoption`*, *`enginejpg`*, *`enginepng`*, *`enginesvg`*, *`enginegif`*, *`callback`*)\n+ **input** (type:string): Path to source image or images;  <br />\n        Example:    <br />\n        1. `'src/img/**/*.{jpg,JPG,jpeg,JPEG,png,svg,gif}'`;  <br />\n        2. `'src/img/**/*.jpg'`;  <br />\n        3. `'src/img/*.jpg'`;  <br />\n        4. `'src/img/myimagename.jpg'`;  \n\n+ **output** (type:string): Path to compress images;  <br />\n            Example:   <br />\n            1. `'build/img/'`;  <br />\n\n+ **option** (type:plainObject): Options module\\`s \u00abcompress-images\u00bb; \n    + **compress_force** (type:boolean): Force compress images already compressed images *`true`* or *`false`*;\n    + **statistic** (type:boolean): show image compression statistics *`true`* or *`false`*;\n    + **pathLog** (type:string): Path to log file. Default is `./log/compress-images`;\n    + **autoupdate** (type:boolean): Auto-update module \u00abcompress_images\u00bb to the latest version *`true`* or *`false`*;  <br />\n            Example:   <br />\n            1. `{compress_force: false, statistic: true, autoupdate: true}`;  \n\n+  **globoption** (type:boolean|other): Options  module\\`s [glob](https://www.npmjs.com/package/glob). Also you can set `false`;\n\n+  **enginejpg** (type:plainObject): Engine for compressing **jpeg** and options compress. Key to be `jpg`;\n    + **engine** (type:string): Engine for compressing jpeg. Possible values:\n*`jpegtran`*,*`mozjpeg`*, *`webp`*, *`guetzli`*, *`jpegRecompress`*, *`jpegoptim`*, *`tinify`*;\n    + **command** (type:boolean|array): Options for compression. Can be `false` or commands array.\n        + For **jpegtran** - `['-trim', '-progressive', '-copy', 'none', '-optimize']` in details; [jpegtran](https://libjpeg-turbo.org/);\n        + For **mozjpeg** - `['-quality', '10']` in details [mozjpeg](https://github.com/mozilla/mozjpeg/);\n        + For **webp** - `['-q', '60']` in details [webp](https://developers.google.com/speed/webp/);\n        + For **guetzli** - `['--quality', '84']` (Very long compresses on Win 8.1 [https://github.com/google/guetzli/issues/238](https://github.com/google/guetzli/issues/238)) in details [guetzli](https://github.com/google/guetzli/);\n        To use guetzli you must `npm install guetzli --save`, this library does not work properly on some OS and platforms.\n        + For **jpegRecompress** - `['--quality', 'high', '--min', '60']` in details [jpegRecompress](https://github.com/danielgtaylor/jpeg-archive/);\n        + For **jpegoptim** - `['--all-progressive', '-d']` \n        To use jpegoptim you must `npm install jpegoptim-bin --save`, this library does not work properly on some OS and platforms.\n        from https://github.com/imagemin/jpegoptim-bin\n        **Issues!**\n        May be a problems with installation and use on Win 7 x32 and maybe other OS: \n        [compress-images - issues/21](https://github.com/semiromid/compress-images/issues/21)\n        **Caution!** if do not specify `'-d'` all images will be compressed in the source folder and will be replaced. \n        For Windows x32 and x63 also, you can use [https://github.com/vikas5914/jpegoptim-win](https://github.com/vikas5914/jpegoptim-win). Copy jpegoptim-32.exe and replace and rename in \"node_modules\\jpegoptim-bin\\vendor\\jpegoptim.exe\"\n          \n        + For **tinify** - `['copyright', 'creation', 'location']` In details [tinify](https://tinypng.com/developers/reference/nodejs/);\n    + **key** (type:string): Key used for engine **tinify**.  In details; [tinify](https://tinypng.com/developers/reference/nodejs/);  <br />\n            Example:  <br /> \n            1. `{jpg: {engine: 'mozjpeg', command: ['-quality', '60']}`;  <br />\n            2. `{jpg: {engine: 'tinify', key: \"sefdfdcv335fxgfe3qw\", command: ['copyright', 'creation', 'location']}}`;  <br />\n            3. `{jpg: {engine: 'tinify', key: \"sefdfdcv335fxgfe3qw\", command: false}}`;    \n    \n+  **enginepng** (type:plainObject): Engine for compressing **png** and options for compression. Key to be `png`;\n    + **engine** (type:string): Engine for compressing png. Possible values:\n*`pngquant`*,*`optipng`*, *`pngout`*, *`webp`*, *`pngcrush`*, *`tinify`*;\n    + **command** (type:boolean|array): Options for compression. Can be `false` or commands array.\n        + For **pngquant** - `['--quality=20-50', '-o']` If you want to compress in the same folder, as example: ['--quality=20-50', '--ext=.png', '--force']. To use this library you need to install it manually. It does not work properly on some OS (Win 7 x32 and maybe other). `npm install pngquant-bin --save` \n        Quality should be in format min-max where min and max are numbers in range 0-100. Can be problems with cyrillic filename [issues/317](https://github.com/kornelski/pngquant/issues/317) \n        In details: \n        [pngquant](https://pngquant.org/) and \n        [pngquant-bin - wrapper](https://github.com/imagemin/pngquant-bin)\n        + For **optipng** - To use this library you need to install it manually. \n        It does not work properly on some OS (Win 7 x32 and maybe other). `npm install --save optipng-bin` in details [optipng-bin - wrapper](https://github.com/imagemin/optipng-bin)\n        and [optipng](http://optipng.sourceforge.net/);\n        + For **pngout** - in details [pngout](http://advsys.net/ken/util/pngout.htm);\n        + For **webp** - `['-q', '60']` in details [webp](https://developers.google.com/speed/webp/);\n        + For **pngcrush** (It does not work properly on some OS) - `['-reduce', '-brute']` in details [pngcrush](https://pmt.sourceforge.io/pngcrush/);\n        + For **tinify** - `['copyright', 'creation', 'location']` in details [tinify](https://tinypng.com/developers/reference/nodejs/);\n    + **key** (type:string): Key used for engine **tinify**.  In details; [tinify](https://tinypng.com/developers/reference/nodejs/);  <br />\n            Example:  <br /> \n            1. `{png: {engine: 'webp', command: ['-q', '100']}`;  <br />\n            2. `{png: {engine: 'tinify', key: \"sefdfdcv335fxgfe3qw\", command: ['copyright', 'creation', 'location']}}`;  <br />\n            3. `{png: {engine: 'optipng', command: false}}`;\n\n\n+  **enginesvg** (type:plainObject): Engine for compressing **svg** and options for compression. Key to be `svg`;\n    + **engine** (type:string): Engine for compressing svg. Possible values:\n*`svgo`*;    \n    + **command** (type:string): Options for compression. Can be `false` or commands type string.\n        + For **svgo** - `'--multipass'` in details [svgo](https://www.npmjs.com/package/svgo/);  <br />\n                Example:  <br />\n                1. `{svg: {engine: 'svgo', command: '--multipass'}`;  <br />\n                2. `{svg: {engine: 'svgo', command: false}}`;\n\n\n+  **enginegif** (type:plainObject): Engine for compressing **gif** and options for compression. Key to be `gif`;\n    + **engine** (type:string): Engine for compressing gif. Possible values:\n*`gifsicle`*, *`giflossy`*, *`gif2webp`*;  \n    + **command** (type:boolean|array): Options for compression. Can be `false` or commands type array.\n        + For **gifsicle** - To use this library you need to install it manually.\nIt does not work properly on some OS. `npm install gifsicle --save`. \n        Example options:  \n        `['--colors', '64', '--use-col=web']` or `['--optimize']` In details [gifsicle](http://www.lcdf.org/gifsicle/);\n        + For **giflossy** - (For Linux x64 and Mac OS X) `['--lossy=80']` In details [giflossy](http://www.lcdf.org/gifsicle/);\n        + For **gif2webp** - `['-f', '80', '-mixed', '-q', '30', '-m', '2']` in details [gif2webp](https://developers.google.com/speed/webp/docs/gif2webp);    <br />\n                Example:  <br />\n                1. `{gif: {engine: 'gifsicle', command: ['--colors', '64', '--use-col=web', '--scale', ' 0.8']}}`;  <br />\n                2. `{gif: {engine: 'giflossy', command: false}}`;  <br />\n                3. `{gif: {engine: 'gif2webp', command: ['-f', '80', '-mixed', '-q', '30', '-m', '2']}}`;\n                \n+ **callback** (err, completed, statistic): \nreturns: \n    +  **err** (type:json object|null)  \n        + engine - The name of the algorithm engine \n        + input - The path to the input image \n        + output - The path to the output image\n    + **completed** (type:boolean)\n        + `true` - result completed.\n        + `false` - result not completed.\n    + **statistic** (type:json object)\n        + `input`\n        + `path_out_new`\n        + `algorithm`\n        + `size_in`\n        + `size_output`\n        + `percent`\n        + `err`        \n<br />\n\n\n## How to use promise API \n\n\n#### Example 1\n\n\n```javascript\n    const { compress } = require('compress-images/promise');\n    const INPUT_path_to_your_images = 'src/img/**/*.{jpg,JPG,jpeg,JPEG,png}';\n    const OUTPUT_path = 'build/img/';\n\n    const processImages = async () => {\n        const result = await compress({\n            source: INPUT_path_to_your_images,\n            destination: OUTPUT_path,\n            enginesSetup: {\n                jpg: { engine: 'mozjpeg', command: ['-quality', '60']},\n                png: { engine: 'pngquant', command: ['--quality=20-50', '-o']},\n            }\n        });\n\n        const { statistics, errors } = result;\n        // statistics - all processed images list\n        // errors - all errros happened list\n    };\n\n    processImages();\n```\n\n#### Example 2\nUsing `onProgress`\n\n```javascript\n    const { compress } = require('compress-images/promise');\n    const INPUT_path_to_your_images = 'src/img/**/*.{jpg,JPG,jpeg,JPEG,png}';\n    const OUTPUT_path = 'build/img/';\n\n    const processImages = async (onProgress) => {\n        const result = await compress({\n            source: INPUT_path_to_your_images,\n            destination: OUTPUT_path,\n            onProgress,\n            enginesSetup: {\n                jpg: { engine: 'mozjpeg', command: ['-quality', '60']},\n                png: { engine: 'pngquant', command: ['--quality=20-50', '-o']},\n            }\n        });\n\n        const { statistics, errors } = result;\n        // statistics - all processed images list\n        // errors - all errros happened list\n    };\n\n    processImages((error, statistic, completed) => {\n        if (error) {\n            console.log('Error happen while processing file');\n            console.log(error);\n            return;\n        }\n\n        console.log('Sucefully processed file');\n\n        console.log(statistic)\n    });\n```\n\n## Promised API\n\n**`promise/compress`**(*`params`*)\n\n+ **params** (type:plainObject): Module options;\n    + **source** (type:string): **input**, see above;\n    + **destination** (type:string): **output**, see above;\n    + **enginesSetup** (type:plainObject):  Engines setup mapping, only needed ones, for example: `{ jpg: <enginejpg>, png: <enginepng> }`, see details above;\n    + (optional) **params** (type:plainObject): Options module\\`s \u00abcompress-images\u00bb, see **option** above;\n    + (optional) **globOptions** (type:boolean|other): see **globoption** above;\n    + (optional) **onProgress** (err, statistic, completed): see **callback** above\n\n+ returns **Promise** with object:\n    + **statistics** (type:**statistic[]**), see above;\n    + **errors** (type:**err[]**), see above;\n\n_______________________\n\n### Donate\n![Image](https://raw.githubusercontent.com/semiromid/compress-images/master/screenshots/health-care.png)\nIf this is a useful thing for you, support the project.\n\n **PayPal** | [https://www.paypal.com/myaccount/transfer/send](https://www.paypal.com/myaccount/transfer/send) **`startpascal1@mail.ru`**\n \n **Visa Card** | **`4731 1856 1426 6432`** First name and Last name: `SEMINA TAMARA` or `SEMINA TAMARA PETROVNA`\n \n **Payeer** | [payeer.com](payeer.com) No.[**`P77135727`**]\n \n **PaYoneer** | [https://www.payoneer.com](https://www.payoneer.com) **`startpascal1@mail.ru`**\n \n_______________________\n\n\n## Related\ngif2webp [https://developers.google.com/speed/webp/docs/gif2webp](https://developers.google.com/speed/webp/docs/gif2webp) author is Google;\n\nNode package giflossy [https://www.npmjs.com/package/giflossy](https://www.npmjs.com/package/giflossy) Author is Jihchi;\n\ngifsicle and giflossy [http://www.lcdf.org/gifsicle/](http://www.lcdf.org/gifsicle/) author is Eddie Kohler;\n\ngifsicle-bin [https://github.com/imagemin/gifsicle-bin](https://github.com/imagemin/gifsicle-bin) author is Kevva;\n\nsvgo [https://www.npmjs.com/package/svgo](https://www.npmjs.com/package/svgo) author is Greli;\n\npngcrush [https://pmt.sourceforge.io/pngcrush/](https://pmt.sourceforge.io/pngcrush/) author is Glenn Randers-Pehrson;\n\npngcrush-bin [https://github.com/imagemin/pngcrush-bin](https://github.com/imagemin/pngcrush-bin) author is Kevva;\n\nwebp [https://developers.google.com/speed/webp/](https://developers.google.com/speed/webp/) author is Google;\n\npngout [http://advsys.net/ken/util/pngout.htm](http://advsys.net/ken/util/pngout.htm) author is Kerry Watson, with updates by Ken Silverman and Matthew Fearnley;\n\npngout-bin [https://github.com/imagemin/pngout-bin](https://github.com/imagemin/pngout-bin) author is 1000ch;\n\npngquant [https://pngquant.org/](https://pngquant.org/) author is  Kornel Lesi\u0144ski and contributors. It's based on code by Greg Roelofs and Jef Poskanzer;\n\npngquant-bin [https://github.com/imagemin/pngquant-bin](https://github.com/imagemin/pngquant-bin) author is Kevva;\n\ntinypng [https://tinypng.com/developers/reference/nodejs](https://tinypng.com/developers/reference/nodejs) author is Voormedia;\n\ntinyjpg [https://tinyjpg.com/](https://tinyjpg.com/) author is Voormedia;\n\njpegoptim [https://github.com/tjko/jpegoptim](https://github.com/tjko/jpegoptim) author is Tjko;\n\njpegoptim-bin [https://github.com/imagemin/jpegoptim-bin](https://github.com/imagemin/jpegoptim-bin) author is 1000ch;\n\njpeg-archive [https://github.com/danielgtaylor/jpeg-archive](https://github.com/danielgtaylor/jpeg-archive) author is Danielgtaylor;\n\njpeg-recompress-bin [https://github.com/imagemin/jpeg-recompress-bin](https://github.com/imagemin/jpeg-recompress-bin) author is 1000ch;\n\nguetzli [https://github.com/google/guetzli](https://github.com/google/guetzli) author is Google;\n\nguetzli-bin [https://github.com/imagemin/guetzli-bin](https://github.com/imagemin/guetzli-bin) author is 1000ch;\n\nmozjpeg-bin [https://github.com/imagemin/mozjpeg-bin](https://github.com/imagemin/mozjpeg-bin) author is 1000ch;\n\nmozjpeg [https://github.com/mozilla/mozjpeg](https://github.com/mozilla/mozjpeg) author is Pornel;\n\njpegtran-bin [https://github.com/imagemin/jpegtran-bin](https://github.com/imagemin/jpegtran-bin) author is 1000ch;\n\nlibjpeg-turbo [https://libjpeg-turbo.org/](https://libjpeg-turbo.org/) author is Dcommander;\n\nVectors [https://www.flaticon.com/authors/vectors-market](https://www.flaticon.com/authors/vectors-market) author is Vectors Market;\n\ncolors [https://www.npmjs.com/package/colors](https://www.npmjs.com/package/colors) author is Marak;\n\nglob [https://www.npmjs.com/package/glob](https://www.npmjs.com/package/glob) author is Isaacs;\n\n\nmkdirp [https://www.npmjs.com/package/mkdirp](https://www.npmjs.com/package/mkdirp) author is Substack;\n\n\nbytes [https://www.npmjs.com/package/bytes](https://www.npmjs.com/package/bytes) author is Dougwilson;\n\n\n\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0441\u0430\u0439\u0442\u043e\u0432 [\u0412\u0435\u0431-\u0441\u0442\u0443\u0434\u0438\u044f \u0425\u0430\u0440\u044c\u043a\u043e\u0432](https://web-studio.kh.ua)\n\n\n## Bugs\n  * github - [https://github.com/semiromid/compress-images/issues](https://github.com/semiromid/compress-images/issues) \n\n## Author\nSEMINA TAMARA\n\n\n## License\nMIT License\n\nCopyright (c) 2017 TAMARA SEMINA\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
 },
 {
  "repo": "hasherezade/process_ghosting",
  "language": "C",
  "readme_contents": "Process Ghosting\n==========\n\n[![Build status](https://ci.appveyor.com/api/projects/status/2nabj2ukws4ees0w?svg=true)](https://ci.appveyor.com/project/hasherezade/process-ghosting)\n\nThis is my implementation of the technique presented by [Gabriel Landau](https://twitter.com/GabrielLandau):<br/>\nhttps://www.elastic.co/blog/process-ghosting-a-new-executable-image-tampering-attack\n\n![](img/proc_ghost.png)\n\nCharacteristics:\n-\n+ Memory artifacts as in [Process Doppelg\u00e4nging](https://github.com/hasherezade/process_doppelganging)\n+ Payload mapped as `MEM_IMAGE` (unnamed: not linked to any file)\n+ Sections mapped with original access rights (no `RWX`)\n+ Payload connected to PEB as the main module\n+ Remote injection supported (but only into a newly created process)\n+ Process is created from an unnamed module (`GetProcessImageFileName` returns empty string)\n\n<hr/>\n<b>WARNING:</b> <br/>\nThe 32bit version works on 32bit system only. \n"
 },
 {
  "repo": "zakirullin/tiny-compiler",
  "language": "C",
  "readme_contents": "# A tiny compiler for a simple synthetic language featuring [LL(2) grammar](https://en.wikipedia.org/wiki/LL_grammar), written in pure C \n## The compiler consist of typical parts, known as:\n* [Lexer](https://en.wikipedia.org/wiki/Lexical_analysis) ([`lex.c`](./src/lex.c))\n* [Parser](https://en.wikipedia.org/wiki/Parsing) ([`parser.c`](./src/parser.c))\n* Assembler like [code generator](https://en.wikipedia.org/wiki/Code_generation_(compiler)) ([`gen.c`](./src/gen.c))\n* [Virtual machine](https://en.wikipedia.org/wiki/Virtual_machine) ([`vm.c`](./src/vm.c))\n* [Symbol table](https://en.wikipedia.org/wiki/Symbol_table) ([`sym.c`](./src/sym.c))\n* [Abstract syntax tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree) ([`ast.c`](./src/ast.c))\n## It is by no means a complete industry standard implementation. Some parts are simplified for the sake of better understanding\n## Build\n```$ make```\n## Usage\n```$ ./compiler <source>```\n## An example program for Pythagorean theorem:\n```\ncath1 = 3;\ncath2 = 4;\nhypsquare = cath1 * cath1 + cath2 * cath2;\n```\nExecution result:\n```\nhypsquare = 25\n```\nGenerated ASM:\n```asm\nPUSH 3\nWRITE cath1\nPUSH 4\nWRITE cath2\nREAD cath1\nREAD cath1\nMUL POP, POP\nREAD cath2\nREAD cath2\nMUL POP, POP\nADD POP, POP\nWRITE hypsquare\n```\n## The language description in [EBNF](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form):\n```\nprogram = expr, \";\", { program } ;\nexpr = id, \"=\", expr | term, { (\"+\"|\"-\"), term } ;\nterm = factor, { (\"*\"|\"/\"), factor } ;\nfactor = \"id\" | \"num\" | \"(\", expr, \")\" ;\n```\n"
 },
 {
  "repo": "schwittlick/ofxDarknet",
  "language": "C",
  "readme_contents": "# ofxDarknet\n\nofxDarknet is a openFrameworks wrapper for darknet.\n\nDarknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation. http://pjreddie.com/darknet/\n\n## Features\n\n### YOLO: Real-Time Object Detection (http://pjreddie.com/darknet/yolo/)\n\nDarknet comes with two pre-trained models for this task. Additionally each has a smaller (and faster) but therefore less accurate version:\n\nMS COCO dataset (80 different classes)\n* yolo.cfg & [yolo.weights](http://pjreddie.com/media/files/yolo.weights) (256 MB COCO-model)\n* tiny-yolo.cfg & [tiny-yolo.weights](http://pjreddie.com/media/files/tiny-yolo.weights) (60 MB COCO-model)\n\n```\n\tstd::string cfgfile = ofToDataPath( \"cfg/tiny-yolo.cfg\" );\n\tstd::string weightfile = ofToDataPath( \"tiny-yolo.weights\" );\n\tstd::string nameslist = ofToDataPath( \"cfg/names.list\" );\n\tdarknet.init( cfgfile, weightfile, nameslist );\n```\n\nPascal VOC dataset (20 different classes)\n* yolo-voc.cfg & [yolo-voc.weights](http://pjreddie.com/media/files/yolo-voc.weights) (256 MB VOC-model)\n* tiny-yolo-voc.cfg & [tiny-yolo-voc.weights](http://pjreddie.com/media/files/tiny-yolo-voc.weights) (60 MB VOC-model)\n\n```\n\tstd::string cfgfile = ofToDataPath( \"cfg/tiny-yolo-voc.cfg\" );\n\tstd::string weightfile = ofToDataPath( \"tiny-yolo-voc.weights\" );\n\tstd::string nameslist = ofToDataPath( \"cfg/voc.names\" );\n\tdarknet.init( cfgfile, weightfile, nameslist );\n```\n\nYOLO2 with 9000 classes\n\n* yolo9000.cfg & [yolo9000.weights](http://pjreddie.com/media/files/yolo9000.weights) (190MB COCO-model)\n\n```\n\tstd::string datacfg = ofToDataPath( \"cfg/combine9k.data\" );\n\tstd::string cfgfile = ofToDataPath( \"cfg/yolo9000.cfg\" );\n\tstd::string weightfile = ofToDataPath( \"yolo9000.weights\" );\n\tdarknet.init( cfgfile, weightfile, datacfg );\n```\n\n```\n\tfloat thresh = 0.25;\n\tstd::vector< detected_object > detections = darknet.yolo( image.getPixelsRef(), thresh );\n\n\tfor( detected_object d : detections )\n\t{\n\t\tofSetColor( d.color );\n\t\tglLineWidth( ofMap( d.probability, 0, 1, 0, 8 ) );\n\t\tofNoFill();\n\t\tofDrawRectangle( d.rect );\n\t\tofDrawBitmapStringHighlight( d.label + \": \" + ofToString(d.probability), d.rect.x, d.rect.y + 20 );\n\t}\n```\n\n![YOLO2](https://raw.githubusercontent.com/mrzl/ofxDarknet/master/images/yolo2.jpg)\n\n### Imagenet Classification (http://pjreddie.com/darknet/imagenet/)\n\nIn order to classify an image with more classes, this is the spot. This classifies an image according to the 1000-class [ImageNet Challenge](http://image-net.org/challenges/LSVRC/2015/index).\n\n* [AlexNet](http://pjreddie.com/darknet/imagenet/#alexnet)\ncfg: [alexnet.cfg](https://github.com/mrzl/ofxDarknet/blob/master/example-imagenet/bin/data/cfg/alexnet.cfg) weights: [alexnet.weights](http://pjreddie.com/media/files/alexnet.weights)\n* [Darknet Reference](http://pjreddie.com/darknet/imagenet/#reference)\ncfg: [darknet.cfg](https://github.com/mrzl/ofxDarknet/blob/master/example-imagenet/bin/data/cfg/darknet.cfg) weights: [darknet.cfg](http://pjreddie.com/media/files/darknet.weights)\n* [VGG-16](http://pjreddie.com/darknet/imagenet/#vgg)\ncfg: [vgg-16.cfg](https://github.com/mrzl/ofxDarknet/blob/master/example-imagenet/bin/data/cfg/vgg-16.cfg) weights: [vgg-16.weights](http://pjreddie.com/media/files/vgg-16.weights)\n* [Extraction](http://pjreddie.com/darknet/imagenet/#extraction)\ncfg: [extraction.cfg](https://github.com/mrzl/ofxDarknet/blob/master/example-imagenet/bin/data/cfg/extraction.cfg) weights: [extraction.weights](http://pjreddie.com/media/files/extraction.weights)\n* [Darknet19](http://pjreddie.com/darknet/imagenet/#darknet19)\ncfg: [darknet16.cfg](https://github.com/mrzl/ofxDarknet/blob/master/example-imagenet/bin/data/cfg/darknet19.cfg) weights: [darknet19.weights](http://pjreddie.com/media/files/darknet19.weights)\n* [Darknet19 448x448](http://pjreddie.com/darknet/imagenet/#darknet19_448)\ncfg: [darknet19_488.cfg](https://github.com/mrzl/ofxDarknet/blob/master/example-imagenet/src/bin/data/cfg/darknet19_448.cfg) weights: [darknet19_488.weights](http://pjreddie.com/media/files/darknet19_448.weights)\n\n```\n\tstd::string cfgfile = ofToDataPath( \"cfg/darknet.cfg\" );\n\tstd::string weightfile = ofToDataPath( \"darknet.weights\" );\n\tstd::string nameslist = ofToDataPath( \"cfg/imagenet.shortnames.list\" );\n\tdarknet.init( cfgfile, weightfile, nameslist );\n\n\tclassifications = darknet.classify( image.getPixelsRef() );\n\tint offset = 20;\n\tfor( classification c : classifications )\n\t{\n\t\tstd::stringstream ss;\n\t\tss << c.label << \" : \" << ofToString( c.probability );\n\t\tofDrawBitmapStringHighlight( ss.str(), 20, offset );\n\t\toffset += 20;\n\t}\n```\n\n![Classification](https://raw.githubusercontent.com/mrzl/ofxDarknet/master/images/imagenet_classification.jpg)\n\n### Deep Dream (http://pjreddie.com/darknet/nightmare/)\n\n[vgg-conv.cfg](https://github.com/mrzl/ofxDarknet/blob/master/example-deepdream/bin/data/cfg/vgg-conv.cfg) & [vgg-conv.weights](http://pjreddie.com/media/files/vgg-conv.weights)\n\n```\n\tstd::string cfgfile = ofToDataPath( \"cfg/vgg-conv.cfg\" );\n\tstd::string weightfile = ofToDataPath( \"vgg-conv.weights\" );\n\tdarknet.init( cfgfile, weightfile );\n\t\n\tint max_layer = 13;\n\tint range = 3;\n\tint norm = 1;\n\tint rounds = 4;\n\tint iters = 20;\n\tint octaves = 4;\n\tfloat rate = 0.01;\n\tfloat thresh = 1.0;\n\tnightmare = darknet.nightmate( image.getPixelsRef(), max_layer, range, norm, rounds, iters, octaves, rate, thresh );\n```\n\n![DeepDream](https://raw.githubusercontent.com/mrzl/ofxDarknet/master/images/deep_dream.jpg)\n\n### Recurrent Neural Network (http://pjreddie.com/darknet/rnns-in-darknet/)\n\nDarknet pre-trained weights files:\n* [Shakespeare](http://pjreddie.com/media/files/grrm.weights)\n* [George R.R. Martin](http://pjreddie.com/media/files/shakespeare.weights)\n* [Leo Tolstoy](http://pjreddie.com/media/files/tolstoy.weights)\n* [Immanuel Kant](http://pjreddie.com/media/files/kant.weights)\n\nofxDarknet custom pre-trained weight files (each trained for 20h on NVidia TitanX):\n* [Anonymous - Hypersphere](http://mrzl.net/ofxdarknet/anonymous-hypersphere.weights)\nHypersphere, written by Anonymous with the help of the 4chan board /lit/ (of The Legacy of Totalitarianism in a Tundra fame) is an epic tale spanning over 700 pages.\nA postmodern collaborative writing effort containing Slavoj \ufffdi\ufffdek erotica, top secret Donald Trump emails, poetry, repair instructions for future cars, a history of bottles in the Ottoman empire; actually, it contains everything since it takes place in the Hypersphere, and the Hypersphere is a big place; really big in fact.\n* [Books on art history & aesthetics](http://mrzl.net/ofxdarknet/arts_arthistory_aesthetics.weights)\n* [Books on digital culture](http://mrzl.net/ofxdarknet/digital_and_internet_theory.weights)\n\n\n```\n\tstd::string cfgfile = ofToDataPath( \"cfg/rnn.cfg\" );\n\tstd::string weightfile = ofToDataPath( \"shakespeare.weights\" );\n\tdarknet.init( cfgfile, weightfile );\n\n\tint character_count = 100;\n\tfloat temperature = 0.8;\n\tstd::string seed_text = \"openframeworks is \";\n\tstd::string generated_text = darknet.rnn( character_count, seed_text, temperature );\n```\n\n![RNN](https://raw.githubusercontent.com/mrzl/ofxDarknet/master/images/rnn.jpg)\n\nYou can train your own RNN models with darknet\n\n```\n\t// no need to init\n\tdarknet.train_rnn( ofToDataPath( \"training_text.txt\" ), \"cfg/rnn.cfg\" );\n```\n\n### Go\n\nDarknet has a policy network for Go. Read the [original doc here](https://pjreddie.com/darknet/darkgo-go-in-darknet/).\n\nIn the example `example-go` is a 2-player game where darknet gives recommendations. To play, click on the square you wish to move a piece onto. More doc on this soon.\n\n![Go](https://raw.githubusercontent.com/mrzl/ofxDarknet/master/images/go.jpg)\n\n## Setup\n\n### Windows\n\nInstall the dependencies for building darknet on Windows 10:\n* [Visual Studio 2015 (Community)](https://www.microsoft.com/download/details.aspx?id=48146)\n* [CUDA 8.0 64bit](https://developer.nvidia.com/cuda-downloads)\n\nThere are some more necessary steps that don't work with the OF project generator:\n\n* Compile as Debug or Release in x64 mode\n* Within VS2015 Solution Explorer, rightclick on the generated project -> Build Dependencies -> Build Customizations -> Tick CUDA 8.0\n* Copy pthreadVC2.dll from ofxDarknet\\libs\\3rdparty\\dll\\x64 to your applications bin folder\n\n### OSX\n\nFirst make sure to install [CUDA 8.0 64bit](https://developer.nvidia.com/cuda-downloads) (Driver & Toolkit). CUDA requires an NVIDIA graphics card and a reasonably recent Mac OS.\n\nAfter that, projects should compile fine from the Project Generator. Make sure to download the necessary weights (links can be found [here](http://pjreddie.com/darknet/yolo/) and include the required cfg files (found in the examples) in any app that opens them.\n\n## Building the library from source\n\nIf you want to make changes to the darknet lib, you can build it from source with cmake. `cd` into `libs/darknet/cMake/` and then run:\n\n    cmake .\n    make\n\nNote, you need to have CUDA and OpenCV installed on your system first.\n\n## Training your own models\n\n### YOLO\n\ntcb\n\n## Credits\n\n* Original Code: https://github.com/pjreddie/darknet\n* Help to compile on Windows: https://github.com/AlexeyAB/darknet/\n* Help to call from C++: https://github.com/prabindh/darknet\n\n## Reading\n\n * [tutorial](https://timebutt.github.io/static/how-to-train-yolov2-to-detect-custom-objects/) on training YoloV2 to detect custom objects"
 },
 {
  "repo": "breakstring/eInkCalendarOfToxicSoul",
  "language": "C",
  "readme_contents": "# \u7f18\u7531\n\u524d\u4e24\u4e2a\u661f\u671f\uff0c\u65e0\u610f\u4e2d\u5728\u5c0f\u7c73\u6709\u54c1\u7684\u5546\u5e97\u4e2d\u770b\u5230\u4e00\u6b3e[\u7535\u5b50\u58a8\u6c34\u5c4f\u7684\u65e5\u5386](https://www.xiaomiyoupin.com/detail?gid=120143&spmref=YouPinPC.$SearchFilter$1.search_list.1.66578030)\uff0c\u5982\u4e0b\u56fe\u3002\u89c9\u5f97\u633a\u6709\u610f\u601d\u3002\u521a\u597d\u65b0\u51a0\u75ab\u60c5\u524d\u671f\u65f6\uff0c\u5728\u5bb6\u95f2\u5f97\u614c\u4e5f\u7528\u5fae\u96ea\u7684\u58a8\u6c34\u5c4f\u505a\u8fc7\u4e00\u4e2a[\u5c55\u793aCOVID\u6570\u636e\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u5916\u6302](https://github.com/breakstring/covid19_e-paper)\u3002\n\u7b80\u5355\u8bc4\u4f30\u4e86\u4e0b\u89c9\u5f97\u8fd9\u73a9\u610f\u513f\u4e5f\u4e0d\u96be\uff0c\u90a3\u5c31\u81ea\u5df1\u505a\u4e00\u4e2a\u6bd2\u9e21\u6c64\u7535\u5b50\u58a8\u6c34\u5c4f\u65e5\u5386\u5427\u3002\n\n<img src=\"images/miaomiaoce.jpg\" width=\"400\" alt=\"\u79d2\u79d2\u6d4b\u7535\u5b50\u58a8\u6c34\u5c4f\u65e5\u5386\" />\n\n## \u65b9\u6848\u9009\u578b\n### \u786c\u4ef6\n- **\u5c4f\u5e55**\uff1a\u5728\u8fd9\u6837\u7684\u7535\u5b50\u4ea7\u54c1\u4e2d\uff0c\u5c4f\u5e55\u53ef\u80fd\u4f1a\u662f\u786c\u4ef6\u6210\u672c\u6700\u5927\u7684\u4e00\u5757\u4e86\u3002\u4ece\u67d0\u4e9b\u8bc4\u6d4b\u89c6\u9891\u4e2d\u770b\u5230\uff0c\u79d2\u79d2\u6d4b\u7684\u8fd9\u6b3e\u7535\u5b50\u65e5\u5386\u4e3a\u4e86\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u5e94\u8be5\u662f\u6ca1\u6709\u91c7\u7528\u5e76\u53e3\u7684\u58a8\u6c34\u5c4f\u65b9\u6848\uff0c\u53ea\u80fd\u5168\u5237\u5c4f\u5e55\u4e14\u5237\u65b0\u7387\u8f83\u4f4e\uff0c\u5c3a\u5bf8\u4e3a5.83\u5bf8\u3002\u6070\u597d\u5fae\u96ea\u4e5f\u6709\u4e00\u6b3e[\u540c\u5c3a\u5bf8\u7684\u5c4f\u5e55\u5982\u4e0b\u56fe](https://www.waveshare.net/shop/5.83inch-e-Paper.htm)\u5206\u8fa8\u7387\u4e3a648*480\uff0c\u57fa\u4e8eSPI\u63a5\u53e3\uff0c\u6211\u731c\u6d4b\u548c\u79d2\u79d2\u6d4b\u5e94\u8be5\u662f\u540c\u7c7b\u578b/\u578b\u53f7\u7684\u9762\u677f\uff0c\u4e8e\u662f\u6beb\u4e0d\u72b9\u8c6b\u7684\u91c7\u7528\u4e86\u8fd9\u6b3e\u3002\u6b64\u5c4f\u5e55\u96f6\u552e\u4ef7\u683c\u4e3a231\u5143\u3002\u5982\u679c\u60a8\u5e0c\u671b\u672a\u6765\u901a\u8fc7\u6811\u8393\u6d3e\u6216\u8005\u5176\u4ed6Arduino\u8bbe\u5907\u6765\u9a71\u52a8\u4ed6\uff0c\u53ef\u4ee5\u9009\u62e9\u5e26\u6709\u9002\u914d\u677f\u7684\u5957\u88c5\u4ef7\u683c\u4e3a283.5\u5143\u3002\u8fd8\u6709\u5c31\u662f\uff0c\u73b0\u5728\u968f\u7740\u7535\u5b50\u58a8\u6c34\u5c4f\u6280\u672f\u7684\u53d1\u5c55\u5176\u5b9e\u4e5f\u6709\u4e86\u5f69\u8272\u7684\uff0c\u4f8b\u5982\u5fae\u96ea\u8fd9\u91cc\u4e5f\u6709\u53ef\u989d\u5916\u663e\u793a\u7ea2\u8272\u6216\u8005\u9ec4\u8272\u7684\u7248\u672c\u3002\u4e0d\u8fc7\u6210\u672c\u81ea\u7136\u5c31\u4e0a\u53bb\u4e86\uff0c\u540c\u65f6\u5982\u679c\u60a8\u60f3\u8981\u590d\u7528\u6211\u7684\u4ee3\u7801\uff0c\u53ef\u80fd\u4e5f\u6216\u591a\u6216\u5c11\u9700\u8981\u505a\u4e9b\u5c11\u91cf\u6539\u52a8\u3002\n\n<img src=\"images/waveshare5.83.jpg\" alt=\"\u5fae\u96ea5.83\u5bf8\u7535\u5b50\u58a8\u6c34\u5c4f\u6a21\u5757\" width=400 />\n<img src=\"images/waveshare-epaper-driver.jpg\" alt=\"\u5fae\u96ea\u7535\u5b50\u58a8\u6c34\u5c4f\u9002\u914d\u677f\" width=400 />\n\n- **\u8ba1\u7b97\u5355\u5143**\uff1a\u76ee\u524d\u4f7f\u7528\u4e50\u946b\u7684ESP32\u82af\u7247\u53ef\u80fd\u662f\u4e2a\u6700\u4f73\u9009\u62e9\uff0c\u81ea\u5e262.4G\u65e0\u7ebf\u7f51\u7edc\u548c\u84dd\u7259\uff0c\u5177\u6709\u5145\u8db3\u7684\u5f15\u811a\u3002\u52a8\u624b\u80fd\u529b\u5f3a\u7684\u53ef\u4ee5\u81ea\u5df1\u4e70\u57fa\u4e8eESP32\u7684\u6a21\u7ec4\u7136\u540e\u6574\u5408\u5230\u81ea\u5df1\u7684\u677f\u5b50\u91cc\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u91c7\u7528\u5e02\u9762\u4e0a\u5404\u79cd\u5404\u6837\u57fa\u4e8eESP32\u7684\u5f00\u53d1\u677f\uff08\u5f53\u7136\uff0c\u524d\u9762\u8fd9\u4e24\u79cd\u65b9\u5f0f\u90fd\u9700\u8981\u4e70\u5c4f\u5e55\u65f6\u987a\u4fbf\u4e70\u4e0a\u9002\u914d\u677f\uff0c\u597d\u5c06\u5c4f\u5e55\u7684\u8f6f\u6392\u7ebf\u8f6c\u6362\u4e3aSPI\u63a5\u53e3\uff09\u3002\u4f5c\u4e3a\u6211\u8fd9\u79cd\u786c\u4ef6\u624b\u6b8b\u515a\uff0c\u52a0\u4e0a\u4e3a\u4e86\u5feb\u901f\u51fa\u4e1c\u897f\uff0c\u81ea\u7136\u8fd8\u662f\u9009\u7528\u5fae\u96ea\u7684\u81ea\u5e26\u4e86\u8f6f\u6392\u7ebf\u63a5\u53e3\u7684\u57fa\u4e8eESP32\u7684[\u58a8\u6c34\u5c4f\u5f00\u53d1\u677f](https://www.waveshare.net/shop/e-Paper-ESP32-Driver-Board.htm)\u4e86\uff0c\u4ef7\u683c78.75\u5143\u3002\n\n<img src=\"images/ESP32-WROOM-32E.png\" alt=\"ESP32\u6a21\u7ec4\" width=300 />\n<img src=\"images/ESP32-DevKitC.png\" alt=\"ESP32 DevKitC\u5f00\u53d1\u677f\" width=300 />\n<img src=\"images/e-Paper-ESP32-Driver-Board-intro.jpg\" alt=\"\u5fae\u96eaESP32\u7535\u5b50\u58a8\u6c34\u5c4f\u5f00\u53d1\u677f\" width=400 />\n\n- **\u7535\u6e90**\uff1a\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u7535\u8111\u4e0a\u7684USB\u63a5\u53e3\u7528micro USB\u7ebf\u76f4\u63a5\u901a\u8fc7\u5f00\u53d1\u677f\u6765\u8fdb\u884c\u4f9b\u7535\u548c\u8c03\u8bd5\u5de5\u4f5c\u3002~~\u5982\u679c\u60a8\u60f3\u8981\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u6446\u8131\u5f00\u7535\u6e90\u7ebf\u7684\u8bdd\uff0c\u6211\u5efa\u8bae\u60a8\u4ece\u4f1f\u5927\u7684\u67d0\u5b9d\u4e0a\u627e\u4e00\u5757\u81ea\u5e26micro USB\u5145\u653e\u63a5\u53e3\u7684\u9502\u7535\u6c60\u3002\u6839\u636e\u7535\u6c60\u7684\u5bb9\u91cf\u4e0d\u540c\u4ef7\u683c\u53ef\u80fd\u4e0d\u4e00\uff0c\u4e0d\u8fc7\u4e00\u822c\u4e5f\u90fd\u662f\u572850\u5143\u4ee5\u4e0b\uff0c\u53ef\u9009\u62e9\u7684\u592a\u591a\u8fd9\u91cc\u6211\u5c31\u4e0d\u653e\u94fe\u63a5\u4e86\u3002~~ \u56e0\u4e3a\u7f3a\u5c11\u7535\u538b/\u7535\u91cf\u68c0\u6d4b\uff0c\u5f88\u5bb9\u6613\u9020\u6210\u9502\u7535\u8fc7\u653e\uff0c\u6240\u4ee5\u4e0d\u5efa\u8bae\u7528\u6211\u4e0b\u56fe\u8fd9\u79cd\u7535\u6c60\uff08\u9664\u975e\u60a8\u4e70\u7684\u7535\u6c60\u662f\u672c\u8eab\u6709\u7535\u91cf\u663e\u793a\u7684\uff09\u3002\u3002\u3002\u800c\u4e14\u6211\u624b\u91cc\u7684\u8fd9\u5757\u5fae\u96ea\u7684\u677f\u5b50\u7684\u9488\u811a\u90fd\u88ab\u6211\u526a\u65ad\u5e76\u7528\u70ed\u7194\u80f6\u7c98\u5230\u4e86\u955c\u6846\u80cc\u9762\uff0c\u6240\u4ee5\u77ed\u671f\u5185\u53ef\u80fd\u65e0\u6cd5\u8fdb\u884c\u7535\u6c60\u7535\u538b\u68c0\u6d4b\u76f8\u5173\u7684\u5c1d\u8bd5\uff0c\u4e3b\u8981\u7684\u662f\uff0c\u5927\u5bb6\u7684\u7535\u6e90\u53ef\u80fd\u4e0d\u4e00\u6837\uff0c\u800c\u4e14\u63a5\u5230\u4e3b\u677f\u4e0a\u7684\u65b9\u5f0f\u4e5f\u4e0d\u4e00\u6837\u6bd4\u8f83\u96be\u505a\u4e00\u4e2a\u7edf\u4e00\u7684\u7535\u6c60\u7535\u6e90\u7535\u91cf\u68c0\u6d4b\u65b9\u6848\u3002\u6240\u4ee5\uff0c**\u76ee\u524d\u5efa\u8bae\u5927\u5bb6\u8981\u4e48\u4e70\u5e26\u6709\u7535\u91cf\u663e\u793a\u7684\u9502\u7535\u6c60\uff0c\u8981\u4e48\u7528\u5176\u4ed6USB\u7535\u6e90\u76f4\u63a5\u4f9b\u7535\u3002**\n\n<img src=\"images/battery.jpg\" width=300 />\n\n- \u6700\u540e\u3002\u3002\u3002\u522b\u5fd8\u4e86\u3002\u3002\u3002\u6846\u67b6\uff1a \u6839\u636e\u60a8\u7684\u5b9e\u9645\u60c5\u51b5\uff0c\u627e\u4e2a\u5408\u9002\u7684\u955c\u6846/\u76d2\u5b50\u4ec0\u4e48\u7684\uff0c\u5f53\u7136\u60a8\u613f\u610f\u7684\u8bdd\u88f8\u677f\u6302\u5728\u5899\u4e0a\u4e5f\u4e0d\u662f\u4e0d\u53ef\u4ee5\u3002\n\n\n### \u8f6f\u4ef6\n- **\u6846\u67b6**\uff1aESP32\u4e0a\u7684\u5e94\u7528\u7a0b\u5e8f\u53ef\u4ee5\u91c7\u7528\u591a\u79cd\u8f6f\u4ef6\u6846\u67b6\u548c\u8bed\u8a00\u6765\u5f00\u53d1\uff0c\u4f8b\u5982Espressif\u81ea\u5df1\u7684EDF-IDF\u6846\u67b6\uff0c\u6216\u8005\u5927\u5bb6\u719f\u6089\u7684Arduino\u3002\u5bf9\u6211\u6765\u8bf4\uff0c\u8fd8\u662f\u7528Arduino\u6bd4\u8f83\u65b9\u4fbf\u70b9\uff0c\u6bd5\u7adf\u91cc\u9762\u6709\u8bb8\u591a\u5927\u91cf\u7684\u73b0\u6210\u7684\u7b2c\u4e09\u65b9\u5e93\u53ef\u4ee5\u4f7f\u7528\u3002\n- **\u5f00\u53d1\u5de5\u5177**\uff1a\u867d\u7136\u4f7f\u7528\u4e86Arduino\u6846\u67b6\uff0c\u4f46\u662f\u6211\u8fd8\u662f\u653e\u5f03\u4e86Arduino\u81ea\u6709\u7684IDE\uff0c\u90a3\u73a9\u610f\u513f\u592a\u96be\u7528\u4e86\uff0c\u8c01\u7528\u8c01\u77e5\u9053\u3002\u90fd2020\u5e74\uff08\u8bef\uff0c\u5199\u6587\u6863\u7684\u4eca\u5929\u5df2\u7ecf\u662f2021\u5e74\u4e86\uff09\u6211\u81ea\u7136\u662f\u63a8\u8350\u4f7f\u7528[Visual Studio Code](https://code.visualstudio.com/)\u4e86\uff0c\u548cGit\u7684\u6574\u5408\uff0c\u5404\u79cd\u8bed\u6cd5\u9ad8\u4eae\u5feb\u6377\u8df3\u8f6c\u3002\u3002\u3002\u76f8\u6bd4\u4e4b\u4e0bArduino IDE\u7b80\u76f4\u662f\u5c0f\u76c6\u53cb\u7684\u73a9\u5177\u3002\u4f7f\u7528VSCode\u4f60\u6765\u5f00\u53d1Arduino\u5e94\u7528\u7684\u8bdd\u4e5f\u8fd8\u6709\u4e24\u4e2a\u9009\u62e9\uff0c\u4f7f\u7528[\u5fae\u8f6f\u81ea\u5df1\u7684Arduino\u63d2\u4ef6](https://github.com/Microsoft/vscode-arduino)\uff0c\u6216\u8005\u4f7f\u7528[PlatformIO](https://platformio.org/)\u3002\u5b83\u4e5f\u662fVSCode\u7684\u63d2\u4ef6\uff0c\u4f46\u662f\u5185\u7f6e\u4e86\u5404\u79cd\u5d4c\u5165\u5f0f\u786c\u4ef6\u5e73\u53f0\u548c\u6846\u67b6\u548c\u5e93\u7684\u6574\u5408\u3002\u7528\u5b83\u6765\u505aArduino\u5f00\u53d1\u7b80\u76f4\u8981\u723d\u5230\u98de\u8d77\u3002\u4e0d\u8fc7PlatformIO\u548c\u5fae\u8f6f\u7684Arduino\u63d2\u4ef6\u6709\u51b2\u7a81\u53ea\u80fd\u4e8c\u9009\u4e00\uff08\u6211\u4e2a\u4eba\u8fd8\u662f\u63a8\u8350PlatformIO\uff09\u3002\u5c24\u5176\u662f\u672cRepo\u4e2d\u5de5\u7a0b\u4e3aPlatformIO\u5de5\u7a0b\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u5728VSC\u4e2d\u6700\u5c0f\u6539\u52a8\u8fdb\u884c\u90e8\u7f72\u7684\u8bdd\uff0c\u90a3\u4e48\u60a8\u4e5f\u4f7f\u7528PaltformIO\u662f\u6700\u597d\u7684\u9009\u62e9\u3002\u5f53\u7136\uff0c\u4f7f\u7528\u539f\u751f Arduino IDE \u9700\u8981\u6709\u4e00\u70b9\u70b9\u5c0f\u7684\u6539\u52a8\u3002<img src=\"images/platformio.png\" alt=\"PlatformIO IDE\" />\n- **\u76f8\u5173\u7c7b\u5e93**: \u5728\u672c\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u4e3b\u8981\u7528\u5230\u4e86\u5982\u4e0b\u7b2c\u4e09\u65b9\u5e93\n    - [Adafruit GFX](https://github.com/adafruit/Adafruit-GFX-Library): \u7531\u8457\u540d\u7684\u7535\u5b50\u786c\u4ef6\u793e\u533aAdafruit\u63d0\u4f9b\u7684\u4e00\u5957\u56fe\u5f62\u56fe\u50cf\u5f15\u64ce\u3002\u505a\u5404\u79cd\u9700\u8981\u663e\u793a\u8f93\u51fa\u7684Arduino\u5e94\u7528\u4e00\u822c\u90fd\u5c11\u4e0d\u4e86\u5b83\u4e86\u3002\n    - [GxEPD2](https://github.com/ZinggJM/GxEPD2):\u57fa\u4e8e[Adafruit_GFX](https://github.com/adafruit/Adafruit-GFX-Library)\u5e93\u6765\u9a71\u52a8\u5404\u79cd\u7535\u5b50\u58a8\u6c34\u5c4f\u3002\n    - [U8g2 for Adafruit GFX](https://github.com/olikraus/U8g2_for_Adafruit_GFX)\uff1a\u4e00\u5957\u57fa\u4e8e[U8g2](https://github.com/olikraus/U8g2)\u5b57\u4f53\u5f15\u64ce\u6765\u901a\u8fc7Adafruit GFX\u6765\u663e\u793a\u6587\u5b57\u7684\u7b2c\u4e09\u65b9\u5e93\u3002\n    - \u5bf9\u4e8e\u4e2d\u6587\u5b57\u5e93\u7684\u751f\u6210\u6211\u4f7f\u7528\u4e86\u524d\u51e0\u5929\u64b8\u7684\u4e00\u4e2a[\u61d2\u4eba\u5de5\u5177](https://github.com/breakstring/u8g2_fontmaker)\u6765\u914d\u5408U8g2 for Adafruit GFX.\n    - [ArduinoJSON](https://arduinojson.org/):\u8981\u5904\u7406JSON\u5b57\u7b26\u4e32\u73b0\u5728\u5c11\u4e0d\u4e86\u5b83\u4e86\u3002\n- **\u76f8\u5173\u670d\u52a1**\uff1a\n    - \u6bd2\u9e21\u6c64\uff1a\u6bd2\u9e21\u6c64\u7684\u5185\u5bb9\u5e76\u6ca1\u6709\u91c7\u7528\u7f51\u7edc\u4e0a\u73b0\u6709\u7684\u67d0\u4e2aAPI\u6765\u5904\u7406\uff0c\u800c\u662f\u76f4\u63a5\u786c\u7f16\u7801\u5230\u4e86[src/toxicsoul.h](src/toxicsoul.h)\u91cc\u3002\u4e00\u662f\u89c9\u5f97\u53c8\u8981\u7533\u8bf7\u4ec0\u4e48API Key\u4e4b\u7c7b\u7684\u592a\u9ebb\u70e6\uff0c\u670d\u52a1\u8fd8\u4e0d\u4e00\u5b9a\u957f\u671f\u6709\u6548\uff0c\u4e8c\u662f\u89c9\u5f97\u5185\u5bb9\u4e0d\u53ef\u63a7\u6709\u70b9\u4e0d\u653e\u5fc3\u3002\u6240\u4ee5\u53ea\u4e0d\u8fc7\u968f\u624b\u4ece\u7f51\u4e0a\u641c\u4e86\u641c\u4e00\u4e9b\u522b\u4eba\u63d0\u4f9b\u7684\u6bd2\u9e21\u6c64\u5185\u5bb9\uff0c\u7136\u540e\u7efc\u5408\u6574\u7406\u53bb\u91cd\uff0c\u7b80\u5355\u7684\u6e05\u6d17\u6389\u8fc7\u957f\u6216\u8005\u8fc7\u65f6\uff0c\u6216\u8005\u4e00\u773c\u770b\u4e0a\u53bb\u5c31\u6bd4\u8f83\u4e09\u4fd7\u7684\u5185\u5bb9\u3002\u5f53\u7136\uff0c\u6211\u4e5f\u6ca1\u6709\u8010\u5fc3\u6328\u4e2a\u53bb\u770b\u8fd9\u597d\u51e0\u5343\u6761\uff0c\u6240\u4ee5\u6bd5\u7adf\u4f1a\u6709\u758f\u5ffd\u7684\u5730\u65b9\u3002\u5982\u679c\u60a8\u6709\u66f4\u597d\u7684\u5185\u5bb9\u6216\u8005\u89c9\u5f97\u73b0\u6709\u5185\u5bb9\u6709\u4e0d\u5408\u9002\u7684\u5730\u65b9\u6216\u8005\u8bf4\u60a8\u89c9\u5f97\u54ea\u4e9b\u5185\u5bb9\u4fb5\u72af\u4e86\u60a8\u7684\u6743\u5229\uff0c\u6b22\u8fce\u63d0\u51faPR\u3002\n    - [IP\u5730\u5740\u67e5\u8be2](https://www.myip.la/)\uff1a\u7528\u6765\u901a\u8fc7\u5f53\u524d\u8bbe\u5907\u7684IP\u5730\u5740\u67e5\u8be2\u5f97\u77e5\u5f53\u524d\u4f4d\u7f6e\u3002\u5177\u4f53\u53ef\u89c1 [src/MyIP.h](src/MyIP.h) \u548c [src/MyIP.cpp](src/MyIP.cpp)\n    - \u5b57\u4f53\uff1a\u9879\u76ee\u4e2d\u7684\u5b57\u4f53\u4f7f\u7528\u4e86[\u9020\u5b57\u5de5\u623f](https://www.makefont.com/)\u7684\u90e8\u5206\u975e\u5546\u7528\u5b57\u4f53\u6765\u751f\u6210\u3002\u5982\u60a8\u8981\u4f7f\u7528\uff0c\u8bf7\u786e\u4fdd\u5728\u5176[\u6388\u6743\u8303\u56f4](https://www.makefont.com/authorization.html)\u5185\u4f7f\u7528\u3002\n    - \u5929\u6c14\u670d\u52a1\uff1a\u8fd9\u91cc\u7528\u4e86[\u548c\u98ce\u5929\u6c14\u5f00\u53d1\u5e73\u53f0](https://dev.qweather.com/)\u7684\u670d\u52a1\u3002\u6240\u4ee5\u9700\u8981\u60a8\u524d\u5f80\u6ce8\u518c\u8d26\u53f7\u5e76\u83b7\u53d6\u5230\u81ea\u5df1\u7684\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8fKey\u6765\u66ff\u6362[src/config.h](src/config.h)\u4e2d\u7684\u5360\u4f4d\u7b26\u3002\u5177\u4f53\u76f8\u5173\u4ee3\u7801\u53ef\u4ee5\u53c2\u89c1 [src/QWeather.h](src/QWeather.h) \u548c [src/QWeather.cpp](src/QWeather.cpp)\n```cpp\nconst String QWEATHER_API_KEY = \"********************\";\n```\n## \u88c5\u914d\n### \u786c\u4ef6\n\u8fde\u63a5\u60a8\u7684\u7535\u5b50\u58a8\u6c34\u5c4f\u5230\u5fae\u96ea\u7684ESP32\u5f00\u53d1\u677f\u4e0a\uff0c\u5176\u5b9e\u5f88\u7b80\u5355\uff0c\u5c31\u4e00\u4e2a\u8f6f\u6392\u7ebf\u63a5\u53e3\uff0c\u63d2\u4e0a\u53e3\u6309\u4e0b\u5361\u6263\u5c31\u597d\u3002\u5c31\u4e0d\u591a\u8bf4\u4e86\u3002\n### \u8f6f\u4ef6\nVSCode\u548cPlatformIO IDE\u7684\u5b89\u88c5\u6211\u4e5f\u4e0d\u5728\u7d2f\u8ff0\uff0c\u8bf7\u81ea\u884c\u5b8c\u6210\u3002\u9700\u8981\u6ce8\u610f\u4ee5\u4e0b\u4e24\u70b9\uff1a\n- \u4fee\u6539\u548c\u98ce\u5929\u6c14API\u7684Key\uff0c**\u522b\u5fd8\u4e86**\uff01\n<img src=\"images/step1.png\" width=500 />\n- \u4eceVSCode\u4e2d\u5206\u522b\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u4ecePlatformIO\u7684\u63d2\u4ef6\u91cc\u5206\u522bBuild\u548c\u4e0a\u4f20\u6587\u4ef6\u5206\u533a\u955c\u50cf\uff08\u5c31\u662f\u90a3\u4e9b\u5929\u6c14\u56fe\u6807\u4ec0\u4e48\u7684\uff09\u548c\u7a0b\u5e8f\u56fa\u4ef6\u3002\n<img src=\"images/step2.png\" width=500 /> \n\n\u5982\u679c\u4e00\u5207\u987a\u5229\uff0c\u5728VSCode\u91ccPlatformIO\u7684\u4e32\u53e3\u76d1\u89c6\u5668\u91cc\u5c31\u80fd\u770b\u5230\u8f93\u51fa\u7684\u4fe1\u606f\u4e86\u3002\n\n## \u6210\u54c1\n\u53ea\u9700\u8981\u7b80\u5355\u7684\u66ff\u6362\u6e90\u4ee3\u7801\u4e2d\u4f60\u7684\u548c\u98ce\u5929\u6c14API\u7684Key\uff0c\u7136\u540e\u628a\u7a0b\u5e8f\u70e7\u5f55\u5230\u5f00\u53d1\u677f\u91cc\uff0c\u7528\u70ed\u7194\u80f6\u548c\u5176\u4ed6\u5de5\u5177\u628a\u5b83\u56fa\u5b9a\u5230\u955c\u6846\u91cc\uff0c\u8fde\u63a5\u4e0a\u7535\u6e90\u3002\u4e00\u4e2a**\u6bd2\u9e21\u6c64\u7535\u5b50\u58a8\u6c34\u5c4f\u65e5\u5386**\u5c31\u5b8c\u5de5\u4e86\u3002\n<img src=\"images/finish.jpg\" alt=\"\u6bd2\u9e21\u6c64\u7535\u5b50\u58a8\u6c34\u5c4f\u65e5\u5386\" width=640 />\n<img src=\"images/finish_back.jpg\" alt=\"\u6210\u54c1\u80cc\u9762\" width=400 />\n<img src=\"images/finish_1.jpg\" alt=\"\u6210\u54c1\" width=400 />\n<img src=\"images/finish_2.jpg\" alt=\"\u6210\u54c1\" width=400 />\n\n## \u5f85\u4f18\u5316\n- ~~\u52a0\u5165ESP32\u7684\u7761\u7720\u6a21\u5f0f\u5904\u7406\u6765\u8282\u7535~~\uff0c\uff08\u5df2\u7ecf\u5b8c\u6210\uff09\n- \u589e\u52a0\u66f4\u591a\u5185\u5bb9\u3002\u3002\u3002\u6570\u636e\u90fd\u6709\u5c31\u770b\u600e\u4e48\u753b\u597d\u770b\u4e86\u3002\n- u8g2Fonts\u5f15\u64ce\u548cGxEPD2\u539f\u751f\u7ed8\u56fe\u4e4b\u95f4\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u76ee\u524d\u53ea\u80fd\u5148\u5199\u5b8c\u6587\u5b57\u5237\u65b0\u540e\u518d\u91cd\u65b0\u753b\u56fe\u7136\u540e\u518d\u5237\u65b0\u3002\u5b58\u5728\u4e8c\u6b21\u5237\u65b0\u95ee\u9898\u3002\n- ~~\u6539\u5584\u7f51\u7edc\u7684\u5237\u65b0\u673a\u5236\uff0c\u76ee\u524d\u6bd4\u8f83\u61d2\uff0c\u6240\u4ee5\u662f\u6bcf\u6b21\u8bf7\u6c42\u65f6\u90fd\u4f1a\u5237\u65b0\u3002~~(\u4f7f\u7528Deep Sleep\u673a\u5236\uff0c\u6240\u4ee5\u8fd8\u662f\u6bcf\u6b21\u90fd\u5237\u65b0\u4e00\u4e0b\u597d)\n- \u6e05\u7406\u4e0d\u5fc5\u8981\u7684\u5b57\u4f53/\u7f29\u51cf\u5b57\u4f53\u6587\u4ef6\u5927\u5c0f\u3002\u76ee\u524d\u4e3a\u4e86\u7701\u4e8b\u513f\uff0c\u6240\u4ee5\u6253\u5305\u8fdb\u4e86\u592a\u591a\u5b57\u4f53\u5360\u7528\u4e86\u592a\u591a\u7a7a\u95f4\uff0c\u56de\u5934\u6709\u65f6\u95f4\u5f97\u8981\u6e05\u7406\u4e0b\u3002\n"
 },
 {
  "repo": "lemon-lang/lemon",
  "language": "C",
  "readme_contents": "The Lemon Programming Language\n==============================\n\nOverview\n--------\n\n* `src` source code of Lemon compiler and virtual machine\n* `lib` source code of core Lemon library\n* `doc` documentations of source code\n* `test` test code\n\nGetting Source\n--------------\n\n```\n$ git clone https://github.com/lemon-lang/lemon.git\n```\n\nBuild Instructions\n------------------\n```\nmake\n```\nor\n\n```\nmake DEBUG=0 STATIC=0 USE_MALLOC=0 MODULE_OS=1 MODULE_SOCKET=1\n```\n\n* `DEBUG`, debug compiler flags, 0 is off.\n* `STATIC`, 0 build with dynamic-linked library, 1 build with static-linked.\n* `USE_MALLOC`, stdlib's `malloc` ensure return aligned pointer\n* `MODULE_OS`, POSIX builtin os library\n* `MODULE_SOCKET`, BSD Socket builtin library\n\nWindows Platform\n----------------\n\nLemon can build on Windows via [TDM-GCC](http://tdm-gcc.tdragon.net/download),\ngetting source code and use command\n\n```\nmingw32-make\n```\n\nPorting\n-------\n\n`lib/os.c` and `lib/socket.c` are support POSIX and Windows environment.\n\nContributing\n------------\n\n1. Accept [Developer Certificate of Origin](https://developercertificate.org/) by adding `Signed-off-by: Your Name <email@example.org>` to commit log.\n2. Check Code Style.\n3. Send patch to commiter.\n\n\nLicense\n-------\nCopyright (c) 2017 Zhicheng Wei\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
 },
 {
  "repo": "dosgo/ngrok-c",
  "language": "C",
  "readme_contents": "## ngrok-c\r\n\r\n- the client of ngrok in language C\r\n- Before compiling, you need to generate the libpolarssl.a static library(via polarssl official website).\r\n- contact me via email: dosgo@qq.com\r\n\r\n\r\n\u4e0d\u4f1a\u7f16\u8bd1\u7684\u4f19\u4f34\u53ef\u4ee5\u7528\uff0chauntek\u5927\u795e\u5199\u7684python\u7248\u672c\uff0chttps://github.com/hauntek/python-ngrok\r\n\r\n# openwrt \u7f16\u8bd1\u65b9\u6cd5\u3002\r\n- \u770b\u6559\u7a0bhttp://www.jianshu.com/p/8428949d946c\r\n- \u53e6\u5916\u8fd8\u9700\u8981\u5728mbedtls-lib\u5b50\u76ee\u5f55\u4e2d\u4ee5\u540c\u6837\u65b9\u6cd5\u4fee\u6539\u5e76\u6267\u884cbuild.sh\uff0c\u5e76\u5c06\u7f16\u8bd1\u751f\u6210\u7684\u4e09\u4e2a.a\u9759\u6001\u5e93\u6587\u4ef6\u62f7\u8d1d\u81f3\u4e0a\u5c42\u76ee\u5f55\u4e2d\r\n\r\n\r\n## build  \r\n\r\nddns.cpp and https.cpp are abandoned \r\n\r\n\r\n# windows\r\nrun Build.bat in CLI.\r\n\r\n\r\n# linux\r\nrun build.sh instead of makefile\r\n\r\n\r\n# openwrt\r\nrun openwrtbuild.sh\r\n\r\n# tomatoware\r\n### openssl\r\n cp Makefile.openssl.static Makefile\r\n\r\n make\r\n### mbedTLS\r\n\r\nCompile and install mbedTLS into tomatoware first, then:\r\n\r\n cp Makefile.mbedtls.static Makefile\r\n\r\n make\r\n\r\n\r\n## \u4f7f\u7528\u8bf4\u660e\r\n\r\n\u547d\u4ee4\r\n\r\nngrok-polarssl -SER[Shost:ngrokd.ngrok.com,Sport:443,Atoken:xxx] -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Sdname:xxdosgo]\r\n\r\n\r\nShost   -Server host.   //\u670d\u52a1\u5668host\r\n\r\nSport   -server port.   //\u670d\u52a1\u5668\u7aef\u53e3\r\n\r\nAtoken  -ngrok authtoken. //\u670d\u52a1\u5668\u8ba4\u8bc1\u4e32\r\n\r\n\r\ntype    -tcp or http or https.   //\u8981\u6620\u5c04\u7684\u7c7b\u578b\uff0ctcp,http,https\r\n\r\nLhost   -local address.     //\u672c\u5730\u5730\u5740\uff0c\u5982\u679c\u662f\u672c\u673a\u76f4\u63a5127.0.0.1\r\n\r\nLport   -local port.     //\u672c\u5730\u7aef\u53e3\r\n\r\nsdname  -Subdomain.     //\u5b50\u57df\u540d\r\n\r\nHostname -hostname      //\u81ea\u5b9a\u4e49\u57df\u540d\u6620\u5c04      \r\n\r\nRport    -remote port  //\u8fdc\u7a0b\u7aef\u53e3\uff0ctcp\u6620\u5c04\u7684\u65f6\u5019\uff0c\u5236\u5b9a\u7aef\u53e3\u4f7f\u7528\u3002\r\n\r\n\r\n## Example\r\n- ngrokc -SER[Shost:ngrokd.ngrok.com,Sport:443] -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Sdname:Example]  \r\n- ngrokc -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Sdname:Example]\r\n\r\nYou can also register multiple Tunnel, but can only have one of each type.  \r\n- ngrokc -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Sdname:Example] -AddTun[Type:https,Lhost:127.0.0.1,Lport:81,Sdname:Example1]\r\n\r\n\r\n## 2015/7/10\u66f4\u65b0\u589e\u52a0\r\n\r\n\r\n- \u5982\uff0c\u4f60\u53ea\u8981\u628axx.xxx.org\u89e3\u6790\u5230tunnel.mobi\uff0c\u90a3\u4e48\u8bbf\u95eexx.xxx.org\u5c31\u53ef\u4ee5\u4e86..\u6bd4Sdname\u597d\u3002\u3002\r\n\r\nngrokc.exe -SER[Shost:tunnel.mobi,Sport:44433] -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Hostname:xx.xxx.org] -AddTun[Type:tcp,Lhost:127.0.0.1,Lport:80,Rport:55556] \r\n- \u6709\u7a7a\u7684\u8bdd\u589e\u52a0openssl\u7684\u652f\u6301\r\n\r\n\r\n- \u611f\u8c22981213\u7684makefile\u6587\u4ef6\r\n- \u611f\u8c22maz-1 \u4fee\u590dubuntu\u4e0b\u7f16\u8bd1bug\r\n\r\n\r\n\r\n## 2015/7/28\u66f4\u65b0\r\n- \u589e\u52a0openssl\u652f\u6301\uff0c\r\n- \u5982\u679c\u7f16\u8bd1openssl\u7248\u672c\uff0c\u4fee\u6539config.h\u6587\u4ef6\u628adefine OPENSSL \u6539\u4e3a1,\r\n- \u5982\u679c\u7f16\u8bd1polarssl\u7248\u672c\uff0c\u4fee\u6539config.h\u6587\u4ef6\u628adefine OPENSSL \u6539\u4e3a0,\r\n\r\n## 2015/8/4\r\n- make -f Makefile.openssl\u7f16\u8bd1openssl\u7248\u672c\r\n- make -f Makefile.polarssl\u7f16\u8bd1polarssl\u7248\u672c\r\n- openssl\u7248\u672c\u6709bug\uff0c\u8fd0\u884c\u4e45\u4e86\u4f1a\u5d29\u6e83...\u614e\u7528\u3002\u3002\r\n\r\n## 2015/10/7\r\n- \u589e\u52a0PolarSSL v2.0.0++\u7248\u672c\u517c\u5bb9\r\n\r\n### \u6ce8\u610f\u5982\u679c\u7f16\u8bd1polarssl\u7248\u672c\r\n\u5982\u679c\u4f60\u4f7f\u7528\u7684PolarSSL \u7248\u672c\u662fv2.0.0\u53ca\u5176\u4ee5\u4e0a\u7684\u7248\u672c\u7f16\u8bd1\uff0c\u8bf7\u628aconfig.h\u6587\u4ef6\u7684define ISMBEDTLS \u6539\u4e3a1\r\n\r\n\r\n## 2015/10/17\r\n - \u4fee\u590d\u5185\u5b58\u6cc4\u6f0f\r\n - \u589e\u52a0\u7f16\u8bd1\u8be6\u7ec6\u8bf4\u660e\r\n - \u53d1\u73b0bug\uff0c\u8def\u7531\u4e0aCPU\u5076\u5c149%,\u6682\u672a\u4fee\u590d\u3002\r\n\r\n## 2015/10/20\r\n - \u5927\u5e45\u51cf\u5c11\u5185\u5b58\u5360\u7528\r\n - \u5982\u65e0\u91cd\u5927bug\uff0c\u4e0d\u66f4\u65b0\u4e86\u3002\u3002\r\n - \r\n\r\n## 2015/11/5\r\n- \u4fee\u590d\u6ca1\u7f51\u5bfc\u81f4\u7684\u5185\u5b58\u6cc4\u6f0f\u95ee\u9898\u3002\u4f1a\u5bfc\u81f4\u8def\u7531\u4e0d\u65ad\u91cd\u542f\r\n- \u589e\u52a0\u7248\u672c\u53f7\u3002\r\n\r\n## 2015/12/8\r\n- \u505a\u4e9b\u5c0f\u4f18\u5316\uff0c\u907f\u514d\u5d4c\u5165\u5f0f\u8bbe\u5907\uff0c\u6808\u6ea2\u51fa\uff0c\u4e3b\u8981\u662f\u5d4c\u5165\u5f0f\u8bbe\u5907\u6808\u5185\u5b58\u592a\u5c0f\u3002\u3002\u591a\u4e2a\u8fde\u63a5\u5c31\u4f1a\u5bfc\u81f4\u5d29\u6e83\u3002\r\n- \u5bf9\u4e86\uff0c\u8bb0\u5f97\u7f16\u8bd1\u7684\u65f6\u5019\u7528-O2\u53c2\u6570\uff0c\u4e0d\u7136\u8fd9\u4e9b\uff0c\u4f18\u5316\u5b8c\u5168\u65e0\u6548\u3002\u3002\u7535\u8111\u53ef\u4ee5\u5ffd\u7565\uff0c\u4f46\u662f\u5d4c\u5165\u5f0f\u8bbe\u5907\uff0c\u8bb0\u5f97\u7528\uff0c\u6211\u76848M\uff0c20\u4e2a\u8fde\u63a5\u5c31\u6ea2\u51fa\u4e86\u3002\u4f18\u5316\u4ee5\u540e\u6ca1\u6d4b\u8bd5\uff0c\u611f\u89c9\u6ca1\u90a3\u4e48\u8106\u5f31\u4e86\u3002\u3002\r\n\r\n## 2015/12/13\r\n- \u4fee\u590d\u4e00\u4e2a\u53e4\u8001\u7684bug,10\u6708\u4efd\u4ee5\u540e\u7684\u7248\u672c\u90fd\u6709\u8fd9\u95ee\u9898,\u5982\u679c\u662f\uff0c\u8bf7\u52a1\u5fc5\u5347\u7ea7\u5230\u6700\u65b0\u7248\u672c\uff0c\u52a0\u8f7d\u901f\u5ea6\u5927\u5e45\u63d0\u5347\u3002\r\n\r\n## 2015/12/20\r\n- \u5173\u95ed\u3002\u3002tcp\u7f13\u5b58\u533a\u3002\u3002\u4ee5\u4fee\u590dhttp\u8fde\u63a5\uff0c\u4fdd\u5b58\u5931\u8d25\u95ee\u9898\u3002\r\n- \u6253\u5f00tcp keepalive\u907f\u514d\u6b7b\u94fe\u95ee\u9898\u3002\r\n- \u8fd9\u4e2a\u7248\u672c\u76f8\u5bf9\u5b8c\u5584\uff0c\u6ca1\u5565\uff0c\u5927\u95ee\u9898\u4e86\u3002\u3002\u4e0d\u66f4\u65b0\u4e86\u3002\r\n\r\n## 2015/12/30\r\n- \u4fee\u590d\u4e00\u4e2a\u8d85\u957furl\u5728\u5927\u7aef\u4e0b,\u65e0\u6cd5\u4f7f\u7528\u7684bug\u3002\r\n\r\n## 2016/3/12\r\n- \u589e\u52a0http://www.ngrok.cc/\u670d\u52a1\u5668\u652f\u6301\r\n\r\n## 2016/3/29\r\n- \u589e\u52a0openssl\u5e93\u52a8\u6001\u52a0\u8f7d\uff0c\u4ee5\u65b9\u4fbf\u7f16\u8bd1openwrt\u7248\u672c\u3002\r\n- \u589e\u52a0openwrt\u50bb\u74dc\u7f16\u8bd1\u6559\u7a0b\u3002\r\n- \u5173\u4e8e\u9700\u8981luci\u754c\u9762\u7684\uff0c\u6069\u5c71\u6709\u5927\u795e\u5199\u4e86\u3002\u6211\u63d0\u4ea4\u4e86\u3002\u53ebluci-app-ngrokc_git-15.290.16504-8c2fd44-1_all.ipk,\u4e0b\u8f7d\u5b89\u88c5\u597d\u4ee5\u540e\u628a\u7f16\u8bd1\u597d\u7684ngrokc\u66ff\u6362/usr/bin/\u91cc\u9762\u7684ngrokc\uff0c\u6539\u6743\u9650\uff0c\u91cd\u542f\u5c31\u53ef\u4ee5\u4e86\u3002\u3002\r\n\r\n## 2016/3/31\r\n- \u4fee\u590dopenssl\u7248\u672c\u8fde\u63a5\u4e0d\u4e86sslv2\u670d\u52a1\u5668\u95ee\u9898\u3002\r\n\r\n\r\n## 2016/4/4\r\n- \u53bb\u6389\u7ebf\u7a0b\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002\r\n- \u4fee\u6539\u53d1\u9001\u63a5\u6536\u7f13\u5b58\u533a\u5927\u5c0f\uff0c\u907f\u514d\u963b\u585e\u3002\r\n\r\n## 2016/04/19\r\n- \u4fee\u590d\u4e00\u4e2abug\uff0c\u589e\u5927\u7f13\u5b58\u533a\u3002\r\n- \u5927\u5e45\u51cf\u5c11CPU\u6d88\u8017\uff0c\u4e00\u822c10%\u4ee5\u4e0b\r\n\r\n\r\n## 2016/04/20\r\n- \u4fee\u590d1.22\u7248\u672c\u7684polarssl\u4e0b\u7684bug\uff0c\u5982\u679c\u4f60\u4f7f\u7528\u7684\u662fngrokc1.22\u7248\u672c\uff0c\u8bf7\u66f4\u65b0\uff0c\u90a3\u4e2a\u7248\u672c\u6709\u91cd\u5927bug\uff0copenssl\u7248\u672c\u6ca1\u95ee\u9898\u3002\r\n- \u5168\u90e8\u5efa\u8bae\u52301.24\u7a33\u5b9a\u7248\u672c\u3002\r\n\r\n## 2016/04/25\r\n-1.25 \u7248\u672c\uff08\u6d4b\u8bd5\uff09\u589e\u52a0\u591a\u4e2a\u901a\u9053\u529f\u80fd\uff0c\u907f\u514d\u591a\u4e2a\u8fdb\u7a0b\uff0c\u53ef\u4ee5\u6ce8\u518cN\u4e2a\u901a\u9053\uff0c\u5982- ngrokc -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Sdname:Example] -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Sdname:Example1] -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Sdname:Example2]\r\n- \u8fd9\u662f\u4e00\u4e2a\u652f\u6301ngrok\u5b8c\u5168\u534f\u8bae\u7684\u7248\u672c\u3002go\u8bed\u8a00\u4ee5\u540e\u53ef\u80fd\u5458\u751f\u652f\u6301mips\uff0c\u8fd9\u4e2a\u5c31\u6ca1\u4eba\u7528\u4e86\u3002\u53ef\u80fd\u4e0d\u66f4\u65b0\u4e86\u3002\r\n\r\n## 2016/05/2\r\n-1.32\u7a33\u5b9a\u7248\u672c\uff0c\u4fee\u590d1.25\u7248\u672ctcp\u6620\u5c04\u51fa\u9519\u95ee\u9898\u3002\u4fee\u590d1.25\u7684\u5d29\u6e83bug\u3002\r\n\r\n## 2016/05/11\r\n-1.33\u7248\u672c \u4fee\u590d\u91cd\u8fde\u5b50\u57df\u540dbug\u3002\r\n\r\n## 2016/05/25\r\n-1.37\u7248\u672c \u4fee\u590d\u4e00\u4e2a\u5d29\u6e83bug\uff0c\u5efa\u8bae\u6240\u6709\u7684\u90fd\u5347\u7ea7\u5230\u8fd9\u4e2a\u7248\u672c\u3002\r\n\r\n## 2016/07/26\r\n-1.39\u7248\u672c \u4e3a\u4e86\u589e\u52a0\u81ea\u5b9a\u4e49cid\u3002\u5176\u5b83\u6ca1\u7528\r\n\r\n## 2016/10/27\r\n-1.40\u7248\u672c \u589e\u52a0password\u529f\u80fd\u652f\u6301\u3002\r\n\r\n## 2016/11/26\r\n  \u51c6\u5907\u65b0\u529f\u80fd\u5f00\u53d1\uff0cUDP\u7aef\u53e3\u6620\u5c04\u529f\u80fd\uff0c\u6709\u6ca1\u6709\u6709\u5174\u8da3\u7684\uff0c\u4e00\u8d77\u5f00\u53d1\uff0c\u76ee\u524d\u6700\u5927\u7684\u95ee\u9898\u5728\u4e8e\uff0c\u6211\u7684go\u8bed\u8a00\u5f88\u70c2\uff0c\u522b\u6307\u671b\u5b98\u65b9\u4f1a\u52a0\u8fd9\u529f\u80fd\uff0c\u81ea\u5df1\u81ea\u5df1\u52a8\u624b\uff0c\u53c8\u5174\u8da3\u7684\u52a0\u7fa4192182463\uff08QQ\u7fa4\uff09\r\n\r\n## 2017/1/9\r\n  -1.41\u7248\u672c \u4fee\u590d\u4e00\u4e2a\u7f51\u7edc\u4e0d\u597d\u53ef\u80fd\u5bfc\u81f4\u7684\u5185\u5b58\u6cc4\u6f0f\u95ee\u9898\u3002\r\n\r\n## 2018/5/7\r\n  -1.45\u7248\u672c \u589e\u52a0\u81ea\u5b9a\u4e49\u8f6c\u53d1Hostheader\u529f\u80fd\u3002\u7528\u4e8e\u90a3\u4e9b\u672c\u5730\u4f1a\u5224\u65adhost\u7684\u7f51\u7ad9\r\n \u00a0\u4f7f\u7528\u65b9\u6cd5-AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Hostheader:localhost]\r\n## 2019/07/28\r\n  -1.53  add support openssl 1.1\u3002\r\n## 2019/11/2\r\n  -1.54  add support http basic auth\u3002\r\n    Example  -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Httpauth:\"test:test\"]\r\n    \r\n ## 2019/11/2\r\n  -1.54  add support http basic auth\u3002\r\n    Example  -AddTun[Type:http,Lhost:127.0.0.1,Lport:80,Httpauth:\"test:test\"]\r\n ## 2021/8/31\r\n   -1.55  add support local tls\u3002\r\n    Example  -AddTun[Type:https,Lhost:127.0.0.1,Lport:443,Ltls:1]\r\n\r\n### \u5173\u4e8e\u7f16\u8bd1\u5bf9\u5e94\u8def\u7531\u7684\u7248\u672c\u7684ngrokc\u3002\r\n## \u4e00\u3002\u53bbhttp://downloads.openwrt.org/\u4e0b\u8f7d\u4f60\u8def\u7531\u5bf9\u5e94\u7684SDK\u7248\u672c \uff0c\u5982OpenWrt-SDK-ar71xx-for-linux-x86_64-gcc-4.8-linaro_uClibc-0.9.33.2.tar.bz2\uff0c\u5e76\u4e14\u89e3\u538b\u3002\r\n## \u4e8c.\u9700\u8981\u5148\u7f16\u8bd1polarssl\u6216\u8005openssl\u5e93\uff08\u53d6\u51b3\u4f60\u60f3\u7528\u5565\u5e93,2\u90091\uff09\u3002\r\n- 1.\u7f16\u8bd1polarssl\u5e93\r\n    - \u53bbhttps://tls.mbed.org/download-archive\u4e0b\u8f7d\uff0cpolarssl\u7248\u672c\uff0c\u7136\u540e\u89e3\u538b\r\n    - \u518d\u628abuildlib.sh\u590d\u5236\u8fdb\u53bb\uff0c\u5e76\u4e14\u4fee\u6539export STAGING_DIR export PATH,\u628a\u91cc\u9762\u7684\u8def\u5f84\u6539\u6210\u4f60\u4e0b\u8f7d\u7684SDK\uff0c\u6240\u5728\u7684\u76ee\u5f55\uff0c\u6ce8\u610f\u662fstaging_dir\u76ee\u5f55\u5bf9\u5e94STAGING_DIR\uff0cbin\u76ee\u5f55\u5bf9\u5e94PATH\u3002\r\n    - \u8fd8\u5f97\u4fee\u6539CC=mips-openwrt-linux-gcc CXX=mips-openwrt-linux-g++ AR=mips-openwrt-linux-ar RANLIB=mips-openwrt-linux-ranlib,\u628a\u8fd9\u4e9b\u53c2\u6570\u5206\u522b\u5bf9\u5e94\u4f60\u7684\uff0c\u7f16\u8bd1\u5668\u3002\u540d\u79f0\u3002\u3002\r\n    - \u7136\u540e\u6267\u884c\uff0cbuildlib.sh\r\n    - \u5982\u679c\u4e00\u5207\u987a\u5229\u7684\u8bdd\uff0c\u5c31\u4f1a\u5728library\u76ee\u5f55\u4e0b\u751f\u6210\uff0c2.0\u7248\u672c\uff08libmbedtls.a libmbedcrypto.a libmbedx5.9.a\uff091.3\u7248\u672c\uff08libpolarssl.a\uff09\r\n    - \u8fd9\u5c31\u7f16\u8bd1\u597d\u4e86polarssl\u5e93\u3002\r\n- 2.\u7f16\u8bd1 openssl\u5e93\r\n    - \u53bbhttps://www.openssl.org/source/ \u4e0b\u8f7dopenssl\uff0c\u7136\u540e\u89e3\u538b\u3002\r\n    - \u518d\u628abuildlib.sh\u590d\u5236\u8fdb\u53bb\uff0c\u5e76\u4e14\u4fee\u6539export STAGING_DIR export PATH,\u628a\u91cc\u9762\u7684\u8def\u5f84\u6539\u6210\u4f60\u4e0b\u8f7d\u7684SDK\uff0c\u6240\u5728\u7684\u76ee\u5f55\uff0c\u6ce8\u610f\u662fstaging_dir\u76ee\u5f55\u5bf9\u5e94STAGING_DIR\uff0cbin\u76ee\u5f55\u5bf9\u5e94PATH\u3002\r\n    - \u7136\u540e\u6267\u884c\uff0cbuildlib.sh\r\n    - \u5982\u679c\u4e00\u5207\u987a\u5229\u7684\u8bdd\uff0c\u5c31\u4f1a\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u751f\u6210\uff0clibssl.a,libcrypto.a\r\n    - \u8fd9\u5c31\u7f16\u8bd1\u597d\u4e86openssl\u5e93\u5e93\u3002\r\n    \r\n\r\n## \u4e09.\u7f16\u8bd1ngrokc\r\n - 1.openssl\u7248\u672c\r\n      - \u628a\u4e0b\u8f7d\u7684openssl\u91cc\u9762\u7684include/openssl\uff0c\u6587\u4ef6\u5939\u590d\u5236\u5230\uff0c\u4f60SDK\u91cc\u9762\u7684staging_dir/toolchain-mips_r2_gcc-4.6-linaro_uClibc-0.9.33.2/include\u91cc\u9762\uff0c\uff08\u53ef\u80fd\u6839\u636eSDK\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff09\u3002\r\n      - \u7136\u540e\u628a\u7b2c\u4e8c\u6b65\u751f\u6210\u7684\uff0clibssl.a,libcrypto.a,\u653e\u5230ngrok-c\u76ee\u5f55\u3002\r\n      - \u518d\u4fee\u6539\uff0cngrok-c\u91cc\u9762\u7684config.h\uff0c#define OPENSSL 0\uff0c\u6539\u6210#define OPENSSL 1;\r\n      - \u7ee7\u7eed\u4fee\u6539openwrtbuild.sh\u6587\u4ef6export STAGING_DIR export PATH,\u628a\u91cc\u9762\u7684\u8def\u5f84\u6539\u6210\u4f60\u4e0b\u8f7d\u7684SDK\uff0c\u6240\u5728\u7684\u76ee\u5f55\uff0c\u6ce8\u610f\u662fstaging_dir\u76ee\u5f55\u5bf9\u5e94STAGING_DIR\uff0cbin\u76ee\u5f55\u5bf9\u5e94PATH\u3002\r\n      - \u8fd8\u6709\uff0c\u6700\u540e\u4e00\u884c\u7684libpolarssl-mips.a\u6539\u6210libssl.a,libcrypto.a.\r\n      - \u6267\u884copenwrtbuild.sh\uff0c\u5c31\u884c\u4e86\u3002\u3002\r\n      - \u5c31\u4f1a\u5728build-mips\u751f\u6210ngrokc\u6587\u4ef6\u3002\u3002\u4f60\u7528ssh\uff0c\u4e0a\u4f20\u5230\u8def\u7531\u7684/bin\u76ee\u5f55\uff0c\u5e76\u4e14\u52a0\u5165\u6267\u884c\u6743\u9650\u3002\u3002\u5c31\u53ef\u4ee5\u4e86\u3002\u3002\u8dd1\u4e86\u3002\u3002\r\n\r\n\r\n- 2.polarssl\u7248\u672c\r\n      - \u628a\u4e0b\u8f7d\u7684polarssl\u91cc\u9762\u7684include/polarssl\uff0c\u6216\u8005include/mbedtls \u6587\u4ef6\u5939\u590d\u5236\u5230\uff0c\u4f60SDK\u91cc\u9762\u7684staging_dir/toolchain-mips_r2_gcc-4.6-linaro_uClibc-0.9.33.2/include\u91cc\u9762\uff0c\uff08\u53ef\u80fd\u6839\u636eSDK\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff09\u3002\r\n      - \u7136\u540e\u628a\u7b2c\u4e8c\u6b65\u751f\u6210\u7684\uff0c2.0\u7248\u672c\uff08libmbedtls.a libmbedcrypto.a libmbedx5.9.a\uff091.3\u7248\u672c\uff08libpolarssl.a\uff09\u653e\u5230ngrok-c\u76ee\u5f55\u3002\r\n      - \u518d\u4fee\u6539\uff0cngrok-c\u91cc\u9762\u7684config.h\uff0c#define OPENSSL 1\uff0c\u6539\u6210#define OPENSSL 0\uff0c\u5982\u4f60\u7684\u662f2.0\u7248\u672c\uff0c\u8bf7ISMBEDTLS 0\u6539\u6210ISMBEDTLS 1\uff0c\u5982\u679c\u662f1.3\uff0cISMBEDTLS 0\r\n      - \u7ee7\u7eed\u4fee\u6539openwrtbuild.sh\u6587\u4ef6export STAGING_DIR export PATH,\u628a\u91cc\u9762\u7684\u8def\u5f84\u6539\u6210\u4f60\u4e0b\u8f7d\u7684SDK\uff0c\u6240\u5728\u7684\u76ee\u5f55\uff0c\u6ce8\u610f\u662fstaging_dir\u76ee\u5f55\u5bf9\u5e94STAGING_DIR\uff0cbin\u76ee\u5f55\u5bf9\u5e94PATH\u3002\r\n      - \u8fd8\u6709\uff0c\u6700\u540e\u4e00\u884c\u7684libpolarssl-mips.a\u6539\u62102.0\u7248\u672c\uff08libmbedtls.a libmbedcrypto.a libmbedx5.9.a\uff091.3\u7248\u672c\uff08libpolarssl.a\uff09.\r\n      - \u6267\u884copenwrtbuild.sh\uff0c\u5c31\u884c\u4e86\u3002\u3002\r\n      - \u5c31\u4f1a\u5728build-mips\u751f\u6210ngrokc\u6587\u4ef6\u3002\u3002\u4f60\u7528ssh\uff0c\u4e0a\u4f20\u5230\u8def\u7531\u7684/bin\u76ee\u5f55\uff0c\u5e76\u4e14\u52a0\u5165\u6267\u884c\u6743\u9650\u3002\u3002\u5c31\u53ef\u4ee5\u4e86\u3002\u3002\u8dd1\u4e86\u3002\u3002\r\n\t\r\n- 3.\u8fd9\u91cc\u7528Centos7 \u7f16\u8bd1\u6590\u8bafK2\u8def\u7531(polarssl\u5e93)\u505a\u4e2a(\u7eaf\u5c0f\u767d)\u6559\u7a0b\r\n\r\n\t\t###\u5b89\u88c5\u76f8\u5e94\u8f6f\u4ef6\u5305\r\n\t\tyum install bzip2 gzip git vim wget -y\r\n\t\r\n\t\t###\u4e0b\u8f7d\u5e76\u89e3\u538bSDK(\u4ee5\u4e0b\u6ca1\u8bf4\u660e\u7684\u8bdd\u5747\u9ed8\u8ba4\u5728/root\u76ee\u5f55\u4e0b\u64cd\u4f5c)\r\n\t\twget http://archive.openwrt.org/chaos_calmer/15.05/ramips/mt7621/OpenWrt-SDK-15.05-ramips-mt7621_gcc-4.8-linaro_uClibc-0.9.33.2.Linux-x86_64.tar.bz2\r\n\t\ttar jxvf OpenWrt-SDK-15.05.1-ramips-mt7621_gcc-4.8-linaro_uClibc-0.9.33.2.Linux-x86_64.tar.bz2\r\n\t\r\n\t\t###\u6587\u4ef6\u5939\u540d\u5b57\u592a\u957f\uff0c\u6539\u4e2a\u540d\u5b57....\r\n\t\tmv OpenWrt-SDK-15.05.1-ramips-mt7621_gcc-4.8-linaro_uClibc-0.9.33.2.Linux-x86_64 op\r\n\t\r\n\t\t###\u62c9\u53d6ngrok\u6e90\u7801\r\n\t\tgit clone https://github.com/dosgo/ngrok-c.git\r\n\r\n\r\n\t\t###\u5f00\u59cb\u7f16\u8bd1polarsshl\u5e93###\r\n\t\t###\u4e0b\u8f7d\u5e76\u89e3\u538bpolarssl\r\n\t\twget https://tls.mbed.org/download/mbedtls-2.14.1-gpl.tgz&&tar zxvf mbedtls-2.14.1-gpl.tgz\r\n\t\r\n\t\t###\u590d\u5236buildlib.sh\u6587\u4ef6\u5230polarssl\u6587\u4ef6\u5939\r\n\t\tcp /root/ngrok-c/buildlib.sh /root/mbedtls-2.14.1/buildlib.sh\r\n\t\r\n\t\t###\u4fee\u6539buildlib.sh\r\n\t\tvim /root/mbedtls-2.14.1/buildlib.sh\r\n\t\r\n\t\t###\u6211\u8fd9\u91ccSDK\u662f\u8fd9\u4e2a\u6240\u4ee5\u628aPATH\u6539\u6210\u8fd9\u4e2a\r\n\t\t###export PATH=$PATH:'/root/op/staging_dir/toolchain-mipsel_1004kc+dsp_gcc-4.8-linaro_uClibc-0.9.33.2/bin'\r\n\t\t###export STAGING_DIR=\"/root/op/staging_dir\"\r\n\t\r\n\t\t###\u4e0b\u9762\u7684SDK\u7f16\u8bd1\u5668\u540d\u5b57\u53ef\u4ee5\u5728SDK/bin \u4e0b\u627e\u5230\uff0c\u4f8b\u5982:\r\n\t\t### ls /root/op/staging_dir/toolchain-mipsel_1004kc+dsp_gcc-4.8-linaro_uClibc-0.9.33.2/bin\r\n\t\t###\u770b\u5230\u4e00\u4e9bgcc\u548cg++\u4e4b\u7c7b\u7684\uff0c\u628a\u540d\u5b57\u590d\u5236\u66ff\u6362\u6389\u5c31\u884c\u4e86\r\n\t\r\n\t\t###\u8fd0\u884cbuildlib.sh\r\n\t\tcd /root/mbedtls-2.14.1&&chmod 777 buildlib.sh&&./buildlib.sh\r\n\t\t###\u4e0d\u51fa\u610f\u5916\u7684\u8bdd\u4f1a\u5728library\u6587\u4ef6\u5939\u751f\u6210\u4e09\u4e2a\u6587\u4ef6libmbedcrypto.a,libmbedtls.a,libmbedx509.a\r\n\t\r\n\t\t###\u628a\u751f\u6210\u7684\u6587\u4ef6\u590d\u5236\u5230ngrok\u76ee\u5f55\u4e0b\r\n\t\tcd /root/mbedtls-2.14.1/library\r\n\t\tcp {libmbedcrypto.a,libmbedtls.a,libmbedx509.a} /root/ngrok-c/\r\n\t\r\n\t\t###\u590d\u5236mbedtls\u6587\u4ef6\u5939\u5230SDK/include\u4e0b\r\n\t\tcp -r /root/mbedtls-2.14.1/include/mbedtls /root/op/staging_dir/toolchain-mipsel_1004kc+dsp_gcc-4.8-linaro_uClibc-0.9.33.2/include/\r\n\t\r\n\t\t###\u4fee\u6539config.h\u6587\u4ef6\uff0c\u6211\u4eec\u7528\u7684\u662fpolarssl2.0 \u6240\u4ee5\u628a #define OPENSSL 1\uff0c\u6539\u6210#define OPENSSL 0\uff1b#define ISMBEDTLS 0\u6539\u6210#define ISMBEDTLS 1(\u5176\u5b9e\u9ed8\u8ba4\u5c31\u662f1.....)\r\n\t\tvim /root/ngrok-c/config.h\r\n\t\r\n\t\t###\u4fee\u6539openwrtbuild.sh\u91cc\u9762 SDK\u8def\u5f84\u548c\u7f16\u8bd1\u5668\uff0cSDK\u8def\u5f84\u4e0a\u8fb9\u8bf4\u4e86\u5c31\u4e0d\u518d\u91cd\u590d\uff0c\u8fd9\u91cc\u7684\u7f16\u8bd1\u5668\u6539\u6210\u4e0a\u9762\u7f16\u8bd1polarssh\u65f6buildlib.sh\u91cc\u7684\u90a3\u4e2aCXX\u7684\u503c\r\n\t\t###\u6211\u8fd9\u91cc\u662fCXX=mipsel-openwrt-linux-g++\uff0c\u6240\u4ee5openwrtbuild.sh\u7684CC=mipsel-openwrt-linux-g++\r\n\t\tvim /root/ngrok-c/openwrtbuild.sh\r\n\t\t###\u4fee\u6539CC=mipsel-openwrt-linux-g++\uff0c\u4fdd\u5b58\u9000\u51fa\r\n\t\tcd /root/ngrok-c/\r\n\t\tchmod 777 openwrtbuild.sh&&./openwrtbuild.sh\r\n\t\r\n\t\t###\u6267\u884c\u5b8copenwrtbuild.sh\u5982\u679c\u6ca1\u6709\u63d0\u793aerroe\u7684\u8bdd\u5c31\u884c\u4e86\u3002\u3002\u3002\r\n\t\r\n\t\t###\u7136\u540e\u4f1a\u5728build-mips\u751f\u6210ngrokc\u6587\u4ef6\u3002\u3002\u4f60\u7528ssh\uff0c\u4e0a\u4f20\u5230\u8def\u7531\u7684/bin\u76ee\u5f55\uff0c\u5e76\u4e14\u52a0\u5165\u6267\u884c\u6743\u9650\r\n\t\t###\u4e0b\u9762\u5728SSH\u8fde\u63a5\u5230\u8def\u7531\u540e\u7684\u64cd\u4f5c\r\n\t\t###\u8bb0\u5f97\u5b89\u88c5\uff0clibstdcpp.ipk\uff0c\u5b98\u7f51\u6709\u4e0b\u8f7d\uff0c\u6211\u8fd9\u4e2aSDK\u5bf9\u5e94\u7684\u4e0b\u8f7d\u5730\u5740\u662f\r\n\t\twget http://archive.openwrt.org/chaos_calmer/15.05/ramips/mt7621/packages/base/libstdcpp_4.8-linaro-1_ramips_1004kc.ipk\r\n\t\t###\u5b89\u88c5\r\n\t\topkg install libstdcpp_4.8-linaro-1_ramips_1004kc.ipk\r\n\t\t\u5b89\u88c5\u5b8c\u5c31\u53ef\u4ee5\u7528\u4e86\u3002\u3002\u3002\u3002\u3002\r\n\t\t\uff08\u5c0f\u58f0BB:\u4f5c\u8005\u5f88\u6709\u8010\u5fc3\uff0c\u7eaf\u5c0f\u767d\u95ee\u4e00\u4e0b\u5f88\u611a\u8822\u7684\u95ee\u9898\u4ed6\u90fd\u4f1a\u8010\u5fc3\u89e3\u7b54\uff0c\u54c8\u54c8\uff0c\u518d\u6b21\u611f\u8c22dosgo\uff09\r\n\r\n\r\n\u200b      \r\n## \u56db.\u7b80\u5355\u7f16\u8bd1ngrokc\u3002\r\n      1. \u53bbhttp://downloads.openwrt.org/\u4e0b\u8f7d\u4f60\u8def\u7531\u5bf9\u5e94\u7684SDK\u7248\u672c \uff0c\u5982OpenWrt-SDK-ar71xx-for-linux-x86_64-gcc-4.8-linaro_uClibc-0.9.33.2.tar.bz2\uff0c\u5e76\u4e14\u89e3\u538b\u3002\r\n      2.\u7f16\u8f91openwrtbuildv2.sh \u4fee\u6539export STAGING_DIR export PATH,\u628a\u91cc\u9762\u7684\u8def\u5f84\u6539\u6210\u4f60\u4e0b\u8f7d\u7684SDK\u3002\r\n      3. \u628aopenssl.zip\u89e3\u538b\u5230\u4f60\u7684\u4e0b\u8f7d\u7684sdk,/xxx/OpenWrt-SDK/staging_dir/toolchain-mips_r2_gcc-4.6-linaro_uClibc-0.9.33.2/include\u76ee\u5f55\u4e0b\uff0c\r\n      4. \u8fd0\u884c\u7f16\u8f91openwrtbuildv2.sh,\u5c31\u4f1a\u5728build-mips\u751f\u6210\u4e00\u4e2a\u53ebngrokc\u7684\u6587\u4ef6\u3002\u6210\u529f\u4e86\u3002\u3002\u628a\u5b83\u590d\u5236\u5230bin\u76ee\u5f55\u3002\u5c31\u53ef\u4ee5\u8fd0\u884cngrokc\u4e86\u3002\r\n      5.\u8bb0\u5f97\u5b89\u88c5\uff0clibopenssl.ipk\uff0c\u5b98\u7f51\u6709\u4e0b\u8f7d\u3002\r\n\r\n## \u4e94.zig\u4ea4\u53c9\u7f16\u8bd1ngrokc\u3002\r\n      1. \u53bbhttps://ziglang.org/download/\u4e0b\u8f7dzig\u7f16\u8bd1\u5668\u5e76\u4e14\u5b89\u88c5\u597d\uff0c\r\n      2. \u4fee\u6539crossbuild.bat\u6587\u4ef6\u7684TARGET=mipsel-linux-musl\u6539\u6210\u4f60\u7684\u5e73\u53f0\u3002\r\n      3. \u8fd0\u884ccrossbuild.bat\u5c31\u4f1a\u5728\u5f53\u524d\u76ee\u5f55\u751f\u6210\u7f16\u8bd1\u6587\u4ef6\u3002\r\n      4. \u8bb0\u5f97\u5b89\u88c5\uff0clibopenssl.ipk\uff0c\u5b98\u7f51\u6709\u4e0b\u8f7d\u3002\r\n\r\n\r\n\r\n\u7f16\u8bd1\u5c31\u8fd9\u6837\u4e86\uff0c\u4ee5\u540e\u8bf7\u4e0d\u8981\u90ae\u4ef6\u95ee\u6211\u600e\u4e48\u7f16\u8bd1\u4e86\uff0c\u6709bug\u53ef\u4ee5\u8054\u7cfb\u3002\r\n\r\n\r\n\r\n\r\n\r\n\u73b0\u5728tunnel.mobi\u4e0d\u80fd\u7528\u4e86\u3002\u3002\uff0c\u54ea\u91cc\u8fd8\u6709\u56fd\u5185\u7684\u670d\u52a1\u5668\u7684\u3002\u3002\r\n\r\n\r\n"
 },
 {
  "repo": "kyleavery/AceLdr",
  "language": "C",
  "readme_contents": "# AceLdr - Avoid Memory Scanners\r\nA position-independent reflective loader for Cobalt Strike. Zero results from [Hunt-Sleeping-Beacons](https://github.com/thefLink/Hunt-Sleeping-Beacons), [BeaconHunter](https://github.com/3lp4tr0n/BeaconHunter), [BeaconEye](https://github.com/CCob/BeaconEye), [Patriot](https://github.com/joe-desimone/patriot), [Moneta](https://github.com/forrest-orr/moneta), [PE-sieve](https://github.com/hasherezade/pe-sieve), or [MalMemDetect](https://github.com/waldo-irc/MalMemDetect).\r\n![AceLdr Demo](demo.gif)\r\n\r\n## Features\r\n\r\n#### Easy to Use\r\nImport a single CNA script before generating shellcode.\r\n\r\n#### Dynamic Memory Encryption\r\nCreates a new heap for any allocations from Beacon and encrypts entries before sleep.\r\n\r\n#### Code Obfuscation and Encryption\r\nChanges the memory containing CS executable code to non-executable and encrypts it (FOLIAGE).\r\n\r\n#### Return Address Spoofing at Execution\r\nCertain WinAPI calls are executed with a spoofed return address (InternetConnectA, NtWaitForSingleObject, RtlAllocateHeap).\r\n\r\n#### Sleep Without Sleep\r\nDelayed execution using WaitForSingleObjectEx.\r\n\r\n#### RC4 Encryption\r\nAll encryption performed with SystemFunction032.\r\n\r\n## Known Issues\r\n- Not compatible with loaders that rely on the shellcode thread staying alive.\r\n\r\n## References\r\nThis project would not have been possible without the following:\r\n- [FOLIAGE](https://github.com/secidiot/FOLIAGE)\r\n- [x64 return address spoofing (source + explanation)](https://www.unknowncheats.me/forum/anti-cheat-bypass/268039-x64-return-address-spoofing-source-explanation.html)\r\n\r\nOther features and inspiration were taken from the following:\r\n- [https://www.arashparsa.com/bypassing-pesieve-and-moneta-the-easiest-way-i-could-find/](https://www.arashparsa.com/bypassing-pesieve-and-moneta-the-easiest-way-i-could-find/)\r\n- [https://github.com/secidiot/TitanLdr](https://github.com/secidiot/TitanLdr)\r\n- [https://github.com/JLospinoso/gargoyle](https://github.com/JLospinoso/gargoyle)\r\n- [https://www.forrest-orr.net/post/masking-malicious-memory-artifacts-part-ii-insights-from-moneta](https://www.forrest-orr.net/post/masking-malicious-memory-artifacts-part-ii-insights-from-moneta)\r\n- [https://www.arashparsa.com/hook-heaps-and-live-free/](https://www.arashparsa.com/hook-heaps-and-live-free/)\r\n- [https://blog.f-secure.com/hunting-for-gargoyle-memory-scanning-evasion/](https://blog.f-secure.com/hunting-for-gargoyle-memory-scanning-evasion/)\r\n- [https://www.elastic.co/blog/detecting-cobalt-strike-with-memory-signatures](https://www.elastic.co/blog/detecting-cobalt-strike-with-memory-signatures)\r\n"
 },
 {
  "repo": "fogleman/hmm",
  "language": "C",
  "readme_contents": "# hmm\n\n`hmm` is a <b>h</b>eight<b>m</b>ap <b>m</b>eshing utility.\n\nIf you've done any 3D game development, 3D printing, or other such things,\nyou've likely wanted to convert a grayscale heightmap image into a 3D mesh. The\nnaive way is pretty simple but generates huge meshes with millions of\ntriangles. After hacking my way through various solutions over the years, I\nfinally decided I needed to write a good tool for this purpose.\n\n`hmm` is a modern implementation of a nice algorithm from the 1995 paper\n[Fast Polygonal Approximation of Terrains and Height Fields](http://mgarland.org/files/papers/scape.pdf)\nby Garland and Heckbert. The meshes produced by `hmm` satisfy the Delaunay\ncondition and can satisfy a specified maximal error or maximal number of\ntriangles or vertices. It's also very fast.\n\n![Example](https://i.imgur.com/xLGcmWS.png)\n\n### Dependencies\n\n- C++11 or higher\n- [glm](https://glm.g-truc.net/0.9.9/index.html)\n\n### Installation\n\n```bash\nbrew install glm # on macOS\nsudo apt-get install libglm-dev # on Ubuntu / Debian\n\ngit clone https://github.com/fogleman/hmm.git\ncd hmm\nmake\nmake install\n```\n\n### Usage\n\n```\nheightmap meshing utility\nusage: hmm --zscale=float [options] ... infile outfile.stl\noptions:\n  -z, --zscale           z scale relative to x & y (float)\n  -x, --zexagg           z exaggeration (float [=1])\n  -e, --error            maximum triangulation error (float [=0.001])\n  -t, --triangles        maximum number of triangles (int [=0])\n  -p, --points           maximum number of vertices (int [=0])\n  -b, --base             solid base height (float [=0])\n      --level            auto level input to full grayscale range\n      --invert           invert heightmap\n      --blur             gaussian blur sigma (int [=0])\n      --gamma            gamma curve exponent (float [=0])\n      --border-size      border size in pixels (int [=0])\n      --border-height    border z height (float [=1])\n      --normal-map       path to write normal map png (string [=])\n      --shade-path       path to write hillshade png (string [=])\n      --shade-alt        hillshade light altitude (float [=45])\n      --shade-az         hillshade light azimuth (float [=0])\n  -q, --quiet            suppress console output\n  -?, --help             print this message\n```\n\n`hmm` supports a variety of file formats like PNG, JPG, etc. for the input\nheightmap. The output is always a binary STL file. The only other required\nparameter is `-z`, which specifies how much to scale the Z axis in the output\nmesh.\n\n```bash\n$ hmm input.png output.stl -z ZSCALE\n```\n\nYou can also provide a maximal allowed error, number of triangles, or number of\nvertices. (If multiple are specified, the first one reached is used.)\n\n```bash\n$ hmm input.png output.stl -z 100 -e 0.001 -t 1000000\n```\n\n### Visual Guide\n\nClick on the image below to see examples of various command line arguments. You\ncan try these examples yourself with this heightmap: [gale.png](https://www.michaelfogleman.com/static/hmm/guide/gale.png).\n\n![Visual Guide](https://www.michaelfogleman.com/static/hmm/guide/all.png)\n\n### Z Scale\n\nThe required `-z` parameter defines the distance between a fully black pixel\nand a fully white pixel in the vertical Z axis, with units equal to one pixel\nwidth or height. For example, if each pixel in the heightmap represented a 1x1\nmeter square area, and the vertical range of the heightmap was 100 meters, then\n`-z 100` should be used.\n\n### Z Exaggeration\n\nThe `-x` parameter is simply an extra multiplier on top of the provided Z\nscale. It is provided as a convenience so you don't have to do multiplication\nin your head just to exaggerate by, e.g. 2x, since Z scales are often derived\nfrom real world data and can have strange values like 142.2378.\n\n### Max Error\n\nThe `-e` parameter defines the maximum allowed error in the output mesh, as a\npercentage of the total mesh height. For example, if `-e 0.01` is used, then no\npixel will have an error of more than 1% of the distance between a fully black\npixel and a fully white pixel. This means that for an 8-bit input image, an\nerror of `e = 1 / 256 ~= 0.0039` will ensure that no pixel has an error greater\nthan one full grayscale unit. (It may still be desirable to use a lower value\nlike `0.5 / 256`.)\n\n### Base Height\n\nWhen the `-b` option is used to create a solid mesh, it defines the height of\nthe base before the lowest part of the heightmesh appears, as a percentage of\nthe heightmap's height. For example, if `-z 100 -b 0.5` were used, then the\nfinal mesh would be about 150 units tall (if a fully white pixel exists in the\ninput).\n\n### Border\n\nA border can be added to the mesh with the `--border-size` and\n`--border-height` flags. The heightmap will be padded by `border-size` pixels\nbefore triangulating. The (pre-scaled) Z value of the border can be set with\n`border-height` which defaults to 1.\n\n### Filters\n\nA Gaussian blur can be applied with the `--blur` flag. This is particularly\nuseful for noisy images.\n\nThe heightmap can be inverted with the `--invert` flag. This is useful for\n[lithophanes](https://en.wikipedia.org/wiki/Lithophane).\n\nThe heightmap can be auto-leveled with the `--level` flag. This will stretch\nthe grayscale values to use the entire black => white range.\n\nA gamma curve can be applied to the heightmap with the `--gamma` flag. This\napplies `x = x ^ gamma` to each pixel, where `x` is in [0, 1].\n\n### Normal Maps\n\nA full resolution [normal map](https://en.wikipedia.org/wiki/Normal_mapping)\ncan be generated with the `--normal-map` argument. This will save a normal map\nas an RGB PNG to the specified path. This is useful for rendering higher\nresolution bumps and details while using a lower resolution triangle mesh.\n\n### Hillshade Images\n\nA grayscale hillshade image can be generated with the `--shade-path` argument.\nThe altitude and azimuth of the light source can be changed with the\n`--shade-alt` and `--shade-az` arguments, which default to 45 degrees in\naltitude and 0 degrees from north (up).\n\n### Performance\n\nPerformance depends a lot on the amount of detail in the heightmap, but here\nare some figures for an example heightmap of a [40x40 kilometer area centered\non Mount Everest](https://i.imgur.com/1i9djJ0.jpg). Various heightmap\nresolutions and permitted max errors are shown. Times computed on a 2018 13\"\nMacBook Pro (2.7 GHz Intel Core i7).\n\n#### Runtime in Seconds\n\n| Image Size / Error | e=0.01 | e=0.001 | e=0.0005 | e=0.0001 |\n| ---: | ---: | ---: | ---: | ---: |\n| 9490 x 9490 px (90.0 MP) | 6.535 | 13.102 | 19.394 | 58.949 |\n| 4745 x 4745 px (22.5 MP) | 1.867 |  4.903 |  8.886 | 33.327 |\n| 2373 x 2373 px  (5.6 MP) | 0.559 |  2.353 |  4.930 | 14.243 |\n| 1187 x 1187 px  (1.4 MP) | 0.168 |  1.021 |  1.961 |  3.709 |\n\n#### Number of Triangles Output\n\n| Image Size / Error | e=0.01 | e=0.001 | e=0.0005 | e=0.0001 |\n| ---: | ---: | ---: | ---: | ---: |\n| 9490 x 9490 px (90.0 MP) | 33,869 | 1,084,972 | 2,467,831 | 14,488,022 |\n| 4745 x 4745 px (22.5 MP) | 33,148 | 1,032,263 | 2,323,772 | 11,719,491 |\n| 2373 x 2373 px  (5.6 MP) | 31,724 |   935,787 | 1,979,227 |  6,561,070 |\n| 1187 x 1187 px  (1.4 MP) | 27,275 |   629,352 | 1,160,079 |  2,347,713 |\n\n### TODO\n\n- reconstruct grayscale image?\n- better error handling, especially for file I/O\n- better overflow handling - what's the largest supported heightmap?\n- mesh validation?\n"
 },
 {
  "repo": "burrito-elixir/burrito",
  "language": "C",
  "readme_contents": "# Burrito \ud83c\udf2f\n## Cross-Platform Elixir Deployments\n\n* [What Is It?](#what-is-it)\n  * [Background](#background)\n  * [Feature Overview](#feature-overview)\n  * [Technical Component Overview](#technical-component-overview)\n  * [End-To-End Overview](#end-to-end-overview)\n* [Quick Start](#quick-start)\n  * [Experimental Disclaimer](#disclaimer)\n  * [Preparation and Requirements](#preparation-and-requirements)\n  * [Mix Project Setup](#mix-project-setup)\n  * [Mix Release Config Options](#mix-release-config-options)\n  * [Build-Time Environment Variables](#build-time-environment-variables)\n  * [Application Entry Point](#application-entry-point)\n  * [Maintenance Commands](#maintenance-commands)\n* [Advanced Build Configuration](#advanced-build-configuration)\n  * [Build Steps and Phases](#build-steps-and-phases)\n  * [Build Targets and Qualifiers](#build-targets-and-qualifiers)\n  * [Using custom ERTS builds](#using-custom-erts-builds)\n* [Known Limitations and Issues](#known-limitations-and-issues)\n  * [Runtime Requirements](#runtime-requirements)\n* [Contributing](#contributing)\n  * [Welcome!](#welcome)\n\n## What Is It?\n\n#### Background\nBurrito is our answer to the problem of distributing Elixir CLI applications across varied environments, where we cannot guarantee that the Erlang runtime is installed, and where we lack the permissions to install it ourselves. In particular, we have CLI tooling that must be deployed on-premise, by consultants, into customer environments that may be running MacOS, Linux, or Windows.\n\nFurthermore, these tools depend on NIFs that we need to cross-compile for any of the environments that we support, from one common build server, running in our CI environment.\n\nWe were heavily inspired by [Bakeware](https://github.com/bake-bake-bake/bakeware), which lays a lot of the ground work for our approach. Ultimately we implemented and expanded upon many of Bakeware's ideas using [Zig](https://ziglang.org/).\n\n#### Feature Overview\n* Builds a self-extracting archive for a Mix project, targeting Windows, MacOS, and Linux, containing:\n  * Your compiled BEAM code\n  * The required ERTS for your project\n  * Compilation artifacts for any [elixir-make](https://github.com/elixir-lang/elixir_make) based NIFs used by the project\n* Provides a \"plugin\" interface for injecting Zig code into your application's boot sequence\n  * We use this to perform automatic updates and licensing checks (see `lib/versions/release_file.ex` for details)\n* Automatically uninstalls old versions of the payload if a new version is run.\n\n#### Technical Component Overview\nBurrito is composed of a few different components:\n* **Mix Release Module** - A module that is executed as a Mix release step. This module takes care of packing up the files, downloading and copying in different ERTS runtimes, and launching the Zig Archiver and Wrapper.\n* **Zig Archiver** - A small Zig library that packs up an entire directory into a tar-like blob. This is known as the \"payload\" -- which will contain all the compiled BEAM code for your release, and the ERTS for the target platform. This is Gzip compressed and then embedded directly into the wrapper program.\n* **Zig Wrapper** - This is portable cross-platform Zig code that wraps around the payload generated during the Mix release process.\n\n```\n      Burrito Produced Binary\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                \u2502\n\u2502       Zig Wrapper Binary       \u2502 <---- Compiled from `wrapper.zig`\n\u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        Payload Archive         \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502                            \u2502 \u2502\n\u2502 \u2502    ERTS Native Binaries    \u2502 <------ If cross-compiling, this is downloaded from a build server\n\u2502 \u2502                            \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                \u2502 <---- This bottom payload portion is generated by `archiver.zig`\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502                            \u2502 \u2502\n\u2502 \u2502   Application BEAM Code    \u2502 \u2502\n\u2502 \u2502                            \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n#### End To End Overview\n\n 1. You build a Burrito wrapped binary of your application and send it to an end-user\n 2. The end-user launches your binary like any other native application on their system\n 3. In the background (first-run only) the payload is extracted into a well defined location on the system. (AppData, Application Support, etc.)\n 4. The wrapper executes the Erlang runtime in the background, and transparently launches your application within the same process\n 5. Subsequent runs of the same version of that application will use the previously extracted payload\n\n## Quick Start\n#### Disclaimer\nBurrito was built with our specific use case in mind, and while we've found success with deploying applications packaged using Burrito to a number of production environments, the approach we're taking is still experimental.\n\nThat being said, we're excited by our early use of the tooling, and are eager to accept community contributions that improve the reliability of Burrito, or that add support for additional platforms.\n\n#### Preparation and Requirements\n\n**NOTE:** Due to current limitations of Zig, some platforms are less suited as build machines than others: we've found the most success building from Linux and MacOS. The matrix below outlines which build targets are currently supported by each host.\n\n|Target| Host | Host | Host | Host |\n|--|--|--|--|--|\n| | Windows x64 | Linux | MacOS (x86_64) | MacOS (Apple Silicon)** |\n| Windows x64 |\u274c|\u2705|\u2705|\u274c|\n| Linux |\u274c|\u2705|\u2705|\u274c|\n| MacOS (x86_64) |\u274c|\u26a0\ufe0f*|\u2705|\u274c|\n| MacOS (Apple Silicon)** |\u274c|\u26a0\ufe0f*|\u2705|\u2705|\n\n\\* NIFs implemented using `elixir-make` cannot be cross-compiled from Linux to MacOS, pending a [proposed linker change in Zig](https://github.com/ziglang/zig/issues/8180)\n\n** Automated testing and building of Apple silicon Erlang builds is blocked on [support for Apple Silicon in Github Actions](https://github.com/actions/virtual-environments/issues/2187).\n\n----\n\nYou must have the following installed and in your PATH:\n\n* Zig (0.9.0) -- `zig`\n* XZ -- `xz`\n* 7z -- `7z` (For Windows Targets)\n\n----\n\n#### Mix Project Setup\n\n1. Add `burrito` to your list of dependencies:\n\n```elixir\ndefp deps() do\n  [{:burrito, github: \"burrito-elixir/burrito\"}]\nend\n```\n\n2. Create a `releases` function in your `mix.exs`, add and configure the following for your project:\n\n```elixir\n  def releases do\n  [\n    example_cli_app: [\n      steps: [:assemble, &Burrito.wrap/1],\n      burrito: [\n        targets: [\n          macos: [os: :darwin, cpu: :x86_64],\n          linux: [os: :linux, cpu: :x86_64],\n          windows: [os: :windows, cpu: :x86_64]\n        ],\n      ]\n    ]\n  ]\n  end\n```\n\n(See the [Mix Release Config Options](#mix-release-config-options) for additional options)\n\n3. To build a release for all the targets defined in your `mix.exs` file: `MIX_ENV=prod mix release`\n4. You can also build a single target by setting the `BURRITO_TARGET` environment variable to the alias for that target (e.g. Setting `BURRITO_TARGET=macos` builds only the `macos` target defined above.)\n\nNOTE: In order to speed up iteration times during development, if the Mix environment is not set to `prod`, the binary will always extract its payload, even if that version of the application has already been unpacked on the target machine.\n\n#### Mix Release Config Options\n\n* `targets` - A list of atoms, the targets you want to build for (`:darwin`, `:win64`, `:linux`, `:linux_musl`) whenever you run a `mix release` command -- if not defined, defaults to native host platform only.\n* `debug` - Boolean, will produce a debug build if set to true. (Default: `false`)\n* `no_clean` - Boolean, will not clean up after building if set to true. (Default: `false`)\n* `plugin` - String, a path to a Zig file that contains a function `burrito_plugin_entry()` which will be called before unpacking the payload at runtime. See [the example application for details.](example/test_plugin/plugin.zig)\n\n#### Build-Time Environment Variables\n\n* `BURRITO_TARGET` - Override the list of targets provided in your release configuration. (ex: `BURRITO_TARGET=win64`, `BURRITO_TARGET=linux,darwin`)\n\n#### Application Entry Point\nFor Burrito to work properly you must define a `:mod` in your project's Mix config:\n```elixir\n  def application do\n    [\n      mod: {MyEntryModule, []}\n    ]\n  end\n```\nThis module must implement the callbacks defined by the [`Application`](https://hexdocs.pm/elixir/1.12/Application.html) module, as stated in the Mix documentation:\n\n```elixir\ndefmodule MyEntryModule do\n  def start(_, _) do\n   # Returning `{:ok, pid}` will prevent the application from halting.\n   # Use System.halt(exit_code) to terminate the VM when required\n  end\nend\n\n```\n\nIf you wish you retrieve the argv passed to your program by Burrito use this snippet:\n```elixir\n args = Burrito.Util.Args.get_arguments() # this returns a list of strings\n ```\n\n#### Maintenance Commands\nBinaries built by Burrito include a built-in set of commands for performing maintenance operations against the included application:\n\n* `./my-binary maintenance uninstall` - Will prompt to uninstall the unpacked payload on the host machine.\n\n## Advanced Build Configuration\n\n#### Build Steps and Phases\n\nBurrito runs the mix release task in three \"Phases\". Each of these phases contains a number of \"Steps\", and a context struct containing the current state of the build, which is passed between each step.\n\nThe three phases of the Burrito build pipeline are:\n\n  * `Fetch` - This phase is responsible for downloading or copying in any replacement ERTS builds for cross-build targets.\n  * `Patch` - The patch phase injects custom scripts into the build directory, this phase is also where any custom files should be copied into the build directory before being archived.\n  * `Build` -  This is the final phase in the build flow, it produces the final wrapper binary with a payload embedded inside.\n\n  You can add your own steps before and after phases execute. Your custom steps will also receive the build context struct, and can return a modified one to customize a build to your liking.\n\n  An example of adding a step before the fetch phase, and after the build phase:\n\n  ```elixir\n  # ... mix.exs file\n  def releases do\n    [\n      my_app: [\n        steps: [:assemble, &Burrito.wrap/1],\n        burrito: [\n          # ... other Burrito configuration\n          extra_steps: [\n            fetch: [pre: [MyCustomStepModule, AnotherCustomStepModule]],\n            build: [post: [CustomStepAgain, YetAnotherCustomStepModule]]\n          ]\n        ]\n      ]\n    ]\n  end\n  ```\n\n  You can override default steps or phases by setting the `phases` option.\n\n  If your build phase's requirements are different to Burrito's, specify your own `assemble` step that calls `Burrito.Builder.build(release)`.\n\n  ```elixir\n    # ... mix.exs file\n    def releases do\n      [\n        my_app: [\n          steps: [:assemble, &MyProject.wrap/1],\n          burrito: [\n            # ... other Burrito configuration\n            phases: [\n              build: [MyProject.CustomBuildStep1, MyProject.CustomBuildStep2]\n            ]\n          ]\n        ]\n      ]\n    end\n  ```\n\n  ```elixir\n  defmodule MyProject\n    def wrap(%Mix.Release{} = release) do\n      pre_check(release)\n      Burrito.Builder.build(release)\n    end\n\n    defp pre_check(release) do\n      # checks specific to your build.\n    end\n  end\n  ```\n\n#### Build Targets and Qualifiers\n\nA Burrito build target is a keyword list that contains an operating system, a CPU architecture, and extra build options (called Qualifiers).\n\nHere's a definition for a build target configured for Linux x86-64:\n\n```elixir\ntargets: [\n  linux: [os: :linux, cpu: :x86_64]\n]\n```\n\nBuild targets can be further customized using build qualifiers. For example, a Linux build target can be configured to use `musl` instead of `glibc` using the following definition:\n\n```elixir\ntargets: [\n  linux_musl: [\n    os: :linux,\n    cpu: :x86_64,\n    libc: :musl\n  ]\n]\n```\n\nBuild qualifiers are a simple way to pass specific flags into the Burrito build pipeline. Currently, only the `libc` and `custom_erts` qualifiers have any affect on the standard Burrito build phases and steps.\n\nTip: You can use these qualifiers as a way to pass per-target information into your custom build steps.\n\n#### Using Custom ERTS Builds\n\nThe Burrito project provides precompiled builds of Erlang for the following platforms:\n\n```elixir\n[os: :darwin, cpu: :x86_64],\n[os: :linux, cpu: :x86_64, libc: :glibc], # or just [os: :linux, cpu: :x86_64]\n[os: :linux, cpu: :x86_64, libc: :musl],\n[os: :windows, cpu: :x86_64]\n```\n\nIf you require a custom build of ERTS, you're able to override the precompiled binaries on a per target basis by setting `custom_erts` to the path of your ERTS build:\n\n```elixir\ntargets: [\n  linux_arm: [\n    os: :linux,\n    cpu: :arm64,\n    custom_erts: \"/path/to/my_custom_erts.tar.gz\"\n  ]\n]\n```\n\nThe `custom_erts` value should be a path to a local `.tar.gz` of a release from the Erlang source tree. The structure inside the archive should mirror:\n\n```\n. (TAR Root)\n\u2514\u2500 otp-A.B.C-OS-ARCH\n  \u251c\u2500 erts-X.Y.Z/\n  \u251c\u2500 releases/\n  \u251c\u2500 lib/\n  \u251c\u2500 misc/\n  \u251c\u2500 usr/\n  \u2514\u2500 Install\n```\n\nYou can easily build an archive like this by doing the following commands inside the (official Erlang source code)[https://github.com/erlang/otp]:\n\n```bash\n# configure and build Erlang as you require...\n# ...\n\nexport RELEASE_ROOT=$(pwd)/release/otp-A.B.C-OS-ARCH\nmake release\ncd release\ntar czf my_custom_erts.tar.gz otp-A.B.C-OS-ARCH\n```\n\n## Known Limitations and Issues\n#### Runtime Requirements\nMinimizing the runtime dependencies of the package binaries is an explicit design goal, and the requirements for each platform are as follows:\n##### Windows\n* MSVC Runtime for the Erlang version you are shipping\n* Windows 10 Build 1511 or later (for ANSI color support)\n##### Linux\n* Any distribution with glibc (or musl libc)\n* libncurses-5\n##### MacOS\n* No runtime dependencies, however a security exemption must be set in MacOS Gatekeeper unless the binary undergoes codesigning\n\n## Contributing\n#### Welcome!\nWe are happy to review and accept pull requests to improve Burrito, and ask that you follow the established code formatting present in the repo!\n\nEverything in this repo is licensed under The MIT License, see `LICENSE` for the full license text.\n"
 },
 {
  "repo": "Cysu/open-reid",
  "language": "Python",
  "readme_contents": "# Open-ReID\n\nOpen-ReID is a lightweight library of person re-identification for research\npurpose. It aims to provide a uniform interface for different datasets, a full\nset of models and evaluation metrics, as well as examples to reproduce (near)\nstate-of-the-art results.\n\n## Installation\n\nInstall [PyTorch](http://pytorch.org/) (version >= 0.2.0). Although we support\nboth python2 and python3, we recommend python3 for better performance.\n\n```shell\ngit clone https://github.com/Cysu/open-reid.git\ncd open-reid\npython setup.py install\n```\n\n## Examples\n\n```shell\npython examples/softmax_loss.py -d viper -b 64 -j 2 -a resnet50 --logs-dir logs/softmax-loss/viper-resnet50\n```\n\nThis is just a quick example. VIPeR dataset may not be large enough to train a deep neural network.\n\nCheck about more [examples](https://cysu.github.io/open-reid/examples/training_id.html)\nand [benchmarks](https://cysu.github.io/open-reid/examples/benchmarks.html).\n"
 },
 {
  "repo": "una-dinosauria/3d-pose-baseline",
  "language": "Python",
  "readme_contents": "## 3d-pose-baseline\n\nThis is the code for the paper\n\nJulieta Martinez, Rayat Hossain, Javier Romero, James J. Little.\n_A simple yet effective baseline for 3d human pose estimation._\nIn ICCV, 2017. https://arxiv.org/pdf/1705.03098.pdf.\n\nThe code in this repository was mostly written by\n[Julieta Martinez](https://github.com/una-dinosauria),\n[Rayat Hossain](https://github.com/rayat137) and\n[Javier Romero](https://github.com/libicocco).\n\nWe provide a strong baseline for 3d human pose estimation that also sheds light\non the challenges of current approaches. Our model is lightweight and we strive\nto make our code transparent, compact, and easy-to-understand.\n\n### Dependencies\n\n* Python \u2265 3.5\n* [cdflib](https://github.com/MAVENSDC/cdflib)\n* [tensorflow](https://www.tensorflow.org/) 1.0 or later\n\n### First of all\n1. Watch our video: https://youtu.be/Hmi3Pd9x1BE\n\n2. Clone this repository\n\n```bash\ngit clone https://github.com/una-dinosauria/3d-pose-baseline.git\ncd 3d-pose-baseline\nmkdir -p data/h36m/\n```\n\n3. Get the data\n\nGo to http://vision.imar.ro/human3.6m/, log in, and download the `D3 Positions` files for subjects `[1, 5, 6, 7, 8, 9, 11]`,\nand put them under the folder `data/h36m`. Your directory structure should look like this\n```bash\nsrc/\nREADME.md\nLICENCE\n...\ndata/\n  \u2514\u2500\u2500 h36m/\n    \u251c\u2500\u2500 Poses_D3_Positions_S1.tgz\n    \u251c\u2500\u2500 Poses_D3_Positions_S11.tgz\n    \u251c\u2500\u2500 Poses_D3_Positions_S5.tgz\n    \u251c\u2500\u2500 Poses_D3_Positions_S6.tgz\n    \u251c\u2500\u2500 Poses_D3_Positions_S7.tgz\n    \u251c\u2500\u2500 Poses_D3_Positions_S8.tgz\n    \u2514\u2500\u2500 Poses_D3_Positions_S9.tgz\n```\n\nNow, move to the data folder, and uncompress all the data\n\n```bash\ncd data/h36m/\nfor file in *.tgz; do tar -xvzf $file; done\n```\n\nFinally, download the `code-v1.2.zip` file, unzip it, and copy the `metadata.xml` file under `data/h36m/`\n\nNow, your data directory should look like this:\n\n```bash\ndata/\n  \u2514\u2500\u2500 h36m/\n    \u251c\u2500\u2500 metadata.xml\n    \u251c\u2500\u2500 S1/\n    \u251c\u2500\u2500 S11/\n    \u251c\u2500\u2500 S5/\n    \u251c\u2500\u2500 S6/\n    \u251c\u2500\u2500 S7/\n    \u251c\u2500\u2500 S8/\n    \u2514\u2500\u2500 S9/\n\n```\n\nThere is one little fix we need to run for the data to have consistent names:\n\n```bash\nmv h36m/S1/MyPoseFeatures/D3_Positions/TakingPhoto.cdf \\\n   h36m/S1/MyPoseFeatures/D3_Positions/Photo.cdf\n\nmv h36m/S1/MyPoseFeatures/D3_Positions/TakingPhoto\\ 1.cdf \\\n   h36m/S1/MyPoseFeatures/D3_Positions/Photo\\ 1.cdf\n\nmv h36m/S1/MyPoseFeatures/D3_Positions/WalkingDog.cdf \\\n   h36m/S1/MyPoseFeatures/D3_Positions/WalkDog.cdf\n\nmv h36m/S1/MyPoseFeatures/D3_Positions/WalkingDog\\ 1.cdf \\\n   h36m/S1/MyPoseFeatures/D3_Positions/WalkDog\\ 1.cdf\n```\n\nAnd you are done!\n\nPlease note that we are currently not supporting SH detections anymore, only training from GT 2d detections is possible now.\n\n### Quick demo\n\nFor a quick demo, you can train for one epoch and visualize the results. To train, run\n\n`python src/predict_3dpose.py --camera_frame --residual --batch_norm --dropout 0.5 --max_norm --evaluateActionWise --epochs 1`\n\nThis should take about <5 minutes to complete on a GTX 1080, and give you around 56 mm of error on the test set.\n\nNow, to visualize the results, simply run\n\n`python src/predict_3dpose.py --camera_frame --residual --batch_norm --dropout 0.5 --max_norm --evaluateActionWise --epochs 1 --sample --load 24371`\n\nThis will produce a visualization similar to this:\n\n![Visualization example](/imgs/viz_example.png?raw=1)\n\n### Training\n\nTo train a model with clean 2d detections, run:\n\n<!-- `python src/predict_3dpose.py --camera_frame --residual` -->\n`python src/predict_3dpose.py --camera_frame --residual --batch_norm --dropout 0.5 --max_norm --evaluateActionWise`\n\nThis corresponds to Table 2, bottom row. `Ours (GT detections) (MA)`\n\n<!--\nTo train on Stacked Hourglass detections, run\n\n`python src/predict_3dpose.py --camera_frame --residual --batch_norm --dropout 0.5 --max_norm --evaluateActionWise --use_sh`\n\nThis corresponds to Table 2, next-to-last row. `Ours (SH detections) (MA)`\n\nOn a GTX 1080 GPU, this takes <8 ms for forward+backward computation, and\n<6 ms for forward-only computation per batch of 64.\n-->\n\n<!--\n### Pre-trained model\n\nWe also provide a model pre-trained on ground truth 2d detections, available through [google drive](https://drive.google.com/file/d/0BxWzojlLp259MF9qSFpiVjl0cU0/view?usp=sharing).\n\nTo test the model, decompress the file at the top level of this project, and call\n\n`python src/predict_3dpose.py --camera_frame --residual --batch_norm --dropout 0.5 --max_norm --evaluateActionWise --epochs 200 --sample --load 4874200`\n-->\n\n<!--\n### Fine-tuned stacked-hourglass detections\n\nYou can find the detections produced by Stacked Hourglass after fine-tuning on the H3.6M dataset on [google drive](https://drive.google.com/open?id=0BxWzojlLp259S2FuUXJ6aUNxZkE).\n-->\n\n### Citing\n\nIf you use our code, please cite our work\n\n```\n@inproceedings{martinez_2017_3dbaseline,\n  title={A simple yet effective baseline for 3d human pose estimation},\n  author={Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J.},\n  booktitle={ICCV},\n  year={2017}\n}\n```\n\n### Other implementations\n\n* [Pytorch](https://github.com/weigq/3d_pose_baseline_pytorch) by [@weigq](https://github.com/weigq)\n* [MXNet/Gluon](https://github.com/lck1201/simple-effective-3Dpose-baseline) by [@lck1201](https://github.com/lck1201)\n\n### Extensions\n\n* [@ArashHosseini](https://github.com/ArashHosseini) maintains [a fork](https://github.com/ArashHosseini/3d-pose-baseline) for estimating 3d human poses using the 2d poses estimated by either [OpenPose](https://github.com/ArashHosseini/openpose) or [tf-pose-estimation](https://github.com/ildoonet/tf-pose-estimation) as input.\n\n### License\nMIT\n"
 },
 {
  "repo": "dfm/emcee",
  "language": "Python",
  "readme_contents": "emcee\n=====\n\n**The Python ensemble sampling toolkit for affine-invariant MCMC**\n\n.. image:: https://img.shields.io/badge/GitHub-dfm%2Femcee-blue.svg?style=flat\n    :target: https://github.com/dfm/emcee\n.. image:: https://github.com/dfm/emcee/workflows/Tests/badge.svg\n    :target: https://github.com/dfm/emcee/actions?query=workflow%3ATests\n.. image:: http://img.shields.io/badge/license-MIT-blue.svg?style=flat\n    :target: https://github.com/dfm/emcee/blob/main/LICENSE\n.. image:: http://img.shields.io/badge/arXiv-1202.3665-orange.svg?style=flat\n    :target: https://arxiv.org/abs/1202.3665\n.. image:: https://coveralls.io/repos/github/dfm/emcee/badge.svg?branch=main&style=flat&v=2\n    :target: https://coveralls.io/github/dfm/emcee?branch=main\n.. image:: https://readthedocs.org/projects/emcee/badge/?version=latest\n    :target: http://emcee.readthedocs.io/en/latest/?badge=latest\n\n\nemcee is a stable, well tested Python implementation of the affine-invariant\nensemble sampler for Markov chain Monte Carlo (MCMC)\nproposed by\n`Goodman & Weare (2010) <http://cims.nyu.edu/~weare/papers/d13.pdf>`_.\nThe code is open source and has\nalready been used in several published projects in the Astrophysics\nliterature.\n\nDocumentation\n-------------\n\nRead the docs at `emcee.readthedocs.io <http://emcee.readthedocs.io/>`_.\n\nAttribution\n-----------\n\nPlease cite `Foreman-Mackey, Hogg, Lang & Goodman (2012)\n<https://arxiv.org/abs/1202.3665>`_ if you find this code useful in your\nresearch. The BibTeX entry for the paper is::\n\n    @article{emcee,\n       author = {{Foreman-Mackey}, D. and {Hogg}, D.~W. and {Lang}, D. and {Goodman}, J.},\n        title = {emcee: The MCMC Hammer},\n      journal = {PASP},\n         year = 2013,\n       volume = 125,\n        pages = {306-312},\n       eprint = {1202.3665},\n          doi = {10.1086/670067}\n    }\n\nLicense\n-------\n\nCopyright 2010-2021 Dan Foreman-Mackey and contributors.\n\nemcee is free software made available under the MIT License. For details see\nthe LICENSE file.\n"
 },
 {
  "repo": "swapagarwal/JARVIS-on-Messenger",
  "language": "Python",
  "readme_contents": "# JARVIS on Messenger\n\nJust A Rather Very Intelligent System, now on Messenger!\n\n[![Build Status](https://travis-ci.org/swapagarwal/JARVIS-on-Messenger.svg?branch=master)](https://travis-ci.org/swapagarwal/JARVIS-on-Messenger)\n![Python](https://img.shields.io/badge/python-2.7-blue.svg)\n[![PEP8](https://img.shields.io/badge/code%20style-pep8-orange.svg)](https://www.python.org/dev/peps/pep-0008/)\n[![Gitmoji](https://img.shields.io/badge/gitmoji-%20\ud83d\ude80%20\ud83d\udc33-FFDD67.svg)](https://gitmoji.carloscuesta.me)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/swapagarwal/JARVIS-on-Messenger/master/LICENSE)\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/swapagarwal/JARVIS-on-Messenger?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Contributors](https://img.shields.io/github/contributors/swapagarwal/JARVIS-on-Messenger.svg)](https://github.com/swapagarwal/JARVIS-on-Messenger/graphs/contributors)\n[![Beginner Issues](https://img.shields.io/github/issues/swapagarwal/JARVIS-on-Messenger/Low-Hanging%20Fruit.svg?label=low-hanging%20fruits)](https://github.com/swapagarwal/JARVIS-on-Messenger/labels/Low-Hanging%20Fruit)\n[![Pull Requests Closed](https://img.shields.io/github/issues-pr-closed/swapagarwal/JARVIS-on-Messenger.svg)](https://github.com/swapagarwal/JARVIS-on-Messenger/issues?q=is%3Apr+is%3Aclosed)\n\nMessenger is now used by 1.2 billion people every month. With the launch of Send/Receive API, bots are about to [take](http://time.com/4291214/facebook-messenger-bots/) [over](http://www.computerworld.com/article/3055588/social-media/an-army-of-chatbots-will-take-over-facebook-here-s-why.html).\n\n### Usage\n\nJARVIS is at your service [here](https://m.me/J.A.R.V.I.S.on.Messenger).\n\n<a href=\"https://www.buymeacoffee.com/swap\" target=\"_blank\"><img src=\"https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png\" alt=\"Buy Me A Coffee\" style=\"height: auto !important;width: auto !important;\" ></a>\n<a href=\"https://www.patreon.com/bePatron?u=7999565\" target=\"_blank\"><img src=\"https://c5.patreon.com/external/logo/become_a_patron_button.png\" alt=\"Become a Patron!\" height=\"41\"></a>\n\n### Demo (Vimeo)\n\n<a href=\"https://vimeo.com/226022581\" target=\"_blank\" title=\"Click to open Vimeo link\">\n  <img src=\"https://i.vimeocdn.com/video/645512677_640.jpg\" alt=\"JARVIS on Messenger Demo\" width=\"300\">\n</a>\n\n### Why?\n\nI created JARVIS with two goals in mind:\n\n1. It should have a lot of useful features (both fun and commonly used).\n1. Anyone can contribute to this project. (As this is module-based, anybody with a decent knowledge of Python can contribute.) One of the prime goals of this project is to lower the entry barrier into the world of open source.\n\nTake a look at the [contributing guidelines](https://github.com/swapagarwal/JARVIS-on-Messenger/blob/master/CONTRIBUTING.md) to see how easy it is to add your own code. I'll be waiting for your pull request! :wink:\n\nA massive Thank You to all [contributors](https://github.com/swapagarwal/JARVIS-on-Messenger/graphs/contributors), and congratulations to people who made their first open-source contribution! :tada:\n\n### Modules\n\nFeel free to add to this list by opening an Issue/Pull Request.\n\n| Name | Sample Query | Source (w/ Attribution) |\n|:-:|:-:|:-:|\n| anime | death note anime | Kitsu |\n| book | anything you want book | Powered by Goodreads |\n| bye | goodbye | --- |\n| coin | flip a coin | --- |\n| currency | usd to eur rate | Fixer.io |\n| dice | roll a die | --- |\n| dictionary | define comfort | Words API |\n| fact | tell me a fact | [JARVIS](https://github.com/swapagarwal/JARVIS-on-Messenger/blob/master/data/facts.json) |\n| hello | Hi, Jarvis! | --- |\n| help | What can you do? | --- |\n| joke | tell me a joke | [JARVIS](https://github.com/swapagarwal/JARVIS-on-Messenger/blob/master/data/jokes.json) |\n| lyrics | paradise lyrics | Powered by musiXmatch |\n| movie | iron man 2 movie plot | <img src=\"/images/powered_by_tmdb.png\"/> |\n| music | songs by linkin park | Spotify |\n| news | latest news | Powered by NewsAPI |\n| ping | ping google.com | Is it up? |\n| quote | random quote | [JARVIS](https://github.com/swapagarwal/JARVIS-on-Messenger/blob/master/data/quotes.json) |\n| request | report a bug <br> request a feature | --- |\n| thanks | Thank you! | --- |\n| time | time in seattle | TimeZoneDB API |\n| url | shorten google.com <br> expand http://goo.gl/7aqe | Google URL Shortener |\n| video | videos of sia | YouTube |\n| weather | weather in london | Info provided by OpenWeatherMap |\n| wiki | wiki html | MediaWiki API |\n| xkcd | show a random xkcd comic | [xkcd](https://xkcd.com/json.html) |\n\nMore sample queries can be found [here](https://github.com/swapagarwal/JARVIS-on-Messenger/tree/master/modules/tests).\n\n### Structure\n\n```sh\n\u251c\u2500\u2500 modules/         # home for various features\n\u251c\u2500\u2500 modules/src/     # code goes here\n\u251c\u2500\u2500 modules/tests/   # tests go here\n\u251c\u2500\u2500 data/            # home for shared data\n\u251c\u2500\u2500 templates/       # for sending structured messages\n\u251c\u2500\u2500 CONTRIBUTING.md  # contributing guidelines\n\u2514\u2500\u2500 jarvis.py        # the main bot\n```\n\n### Local Development / Testing\n\n1. Clone this repo.\n2. Linux:  \na) Debian (Ubuntu, Linux Mint, etc.): `sudo apt-get install python-dev libffi-dev libssl-dev`  \nb) Arch Linux: `sudo pacman -S python2 libffi openssl`  \nc) Fedora: `sudo yum install python-devel libffi-devel openssl-devel`  \nWindows: These should already be pre-installed in your Python bundle.  \nMac/OS X:  \na) If you install Python using brew, the relevant headers are already installed for you. In other words, you don't need python-devel.  \nb) `brew install pkg-config libffi`  \n`export PKG_CONFIG_PATH=/usr/local/Cellar/libffi/3.0.13/lib/pkgconfig/` # May change with libffi version  \n`pip install cffi`  \nc) `brew install libtins`  \n3. `pip install -r requirements.txt`\n4. `python jarvis.py`\n5. Visit the following URLs to see results:  \n`http://localhost:5000/process/?q=<<YOUR_QUERY>>` returns the intent of the query.  \n`http://localhost:5000/search/?q=<<YOUR_QUERY>>` returns the search result of the query.\n\n![result](/images/result_hello.png)\n\n![result](/images/result_joke.png)\n\n* The \"process\" endpoint returns what module the system classifies your query e.g. a dictionary query, a song search, etc. Visit the following URLs to understand the output format:  \n`http://localhost:5000/process/?q=tell%20me%20a%20joke`  \n`http://localhost:5000/process/?q=time%20in%20seattle`  \n`http://localhost:5000/process/?q=convert%2025%20usd%20to%20eur`  \n> You can mock the results for local testing by adding your queries [here](https://github.com/swapagarwal/JARVIS-on-Messenger/blob/master/local/wit.json).\n* The \"search\" endpoint returns the actual bot output, which you get when you interact with the bot using that query.\n\nNote that for the search query to work, you have to set your own key (of the module that you want to test) in config.py  \n\nIf you want a public endpoint, use the below button to deploy on Heroku and fill the relevant API keys that you would like to use:\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy)\n\n### TL;DR for Beginners\n\n1. J.A.R.V.I.S. runs on Python 2\n1. For the search query to work, you have to set your own key (of the module that you want to test) in config.py\n1. The best place to ask anything: https://gitter.im/swapagarwal/JARVIS-on-Messenger\n1. Some issues are reserved for you! https://github.com/swapagarwal/JARVIS-on-Messenger/labels/Low-Hanging%20Fruit\n1. If you're working on something, let everyone know by either creating an issue or commenting on an existing one so that work is not duplicated.\n1. Prefer using an IDE (Use [PyCharm](https://www.jetbrains.com/pycharm/download/) if you don't have any preference yet)\n\n### History\n\nI started out with a rule-based model, but it didn't scale well so now I've shifted to Natural Language Processing.\nRest assured, I'll strive to keep it as simple as possible so that you, yes you, can contribute!\n\nIf you'd like to contribute to the old model, you are welcome to do so as well.\nI've created a new branch [`legacy`](https://github.com/swapagarwal/JARVIS-on-Messenger/tree/legacy) for this purpose. I'll be accepting Pull Requests to this branch also. :smile:\n\nP.S. If you've come this far, you might as well contribute.\nLooking for a place to start? Take a look at some of the [low-hanging fruits](https://github.com/swapagarwal/JARVIS-on-Messenger/labels/Low-Hanging%20Fruit)!\n\n### References\n\n* https://github.com/toddmotto/public-apis\n"
 },
 {
  "repo": "jlsutherland/doc2text",
  "language": "Python",
  "readme_contents": "doc2text\n========\n\n.. image:: https://badge.fury.io/py/doc2text.svg\n    :target: https://badge.fury.io/py/doc2text\n\n|\n\n.. image:: docs/assets/images/news-button.png\n   :alt: Signup for Announcements\n   :target: http://eepurl.com/celDRz\n   :width: 500px\n\n.. image:: https://peachtree.ai/images/doc2text.png\n   :alt: doc2text Example\n   :target: https://peachtree.ai/images/doc2text.png\n   :width: 250px\n\n`doc2text` extracts higher quality text by fixing common scan errors\n--------------------------------------------------------------------\nDeveloping text corpora can be a massive pain in the butt. Much of the text data we are interested in as scientists are locked away in pdfs that are poorly scanned. These scans can be off kilter, poor resolution, have a hand in them... and if you OCR these scans without fixing these errors, the OCR doesn't turn out so well. `doc2text` was created to help researchers fix these errors and extract the highest quality text from\ntheir pdfs as possible.\n\n\n`doc2text` is super duper alpha atm\n-----------------------------------\n`doc2text` is developed and tested on Ubuntu 16.04 LTS Xenial Xerus. We do not pretend to serve all operating systems at the moment because that would be irresponsible. Please use this software with a huge grain of salt. We are currently working on:\n\n- Increasing the responsiveness of the text block identifier.\n- Optimizing the binarization for tesseract detection.\n- Identifying text in multiple columns (right now, treats as one big column).\n- Handling tables.\n- Many other optimizations.\n\nSupport and Contributions\n-------------------------\nIf you have feedback or would like to contribute, *please, please* submit a pull request or contact me at `joseph dot sutherland at columbia dot edu`.\n\n\nInstallation\n------------\nTo install the `doc2text` package, simply:\n\n.. code-block:: python\n\n   pip install doc2text\n\n`doc2text` relies on the `OpenCV <http://github.com/opencv/opencv>`_, `tesseract <http://github.com/tesseract-ocr/tesseract>`_, and `PythonMagick` libraries. To execute the quick-install script, which installs OpenCV, tesseract, and PythonMagick:\n\n.. code-block:: bash\n\n   curl https://raw.githubusercontent.com/jlsutherland/doc2text/master/install_deps.sh | bash\n\nManual installation\n~~~~~~~~~~~~~~~~~~~\nTo install OpenCV manually:\n\n.. code-block:: bash\n\n   sudo apt-get install -y build-essential\n   sudo apt-get install -y cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev\n   sudo apt-get install -y python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev\n   git clone https://github.com/opencv/opencv.git opencv\n   git clone https://github.com/opencv/opencv_contrib.git opencv_contrib\n   cd opencv\n   git checkout 3.1.0\n   cd ../opencv_contrib\n   git checkout 3.1.0\n   cd ../opencv\n   mkdir build\n   cd build\n   cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_C_EXAMPLES=OFF -D INSTALL_PYTHON_EXAMPLES=ON -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules -D BUILD_EXAMPLES=ON ..\n   make -j4\n   sudo make install\n   sudo ldconfig\n\nTo install tesseract manually:\n\n.. code-block:: bash\n\n   sudo apt-get install tesseract-ocr\n\nTo install PythonMagick manually:\n\n.. code-block:: bash\n\n   sudo apt-get install python-pythonmagick\n\nExample usage\n-------------\n\n.. code-block:: python\n\n   import doc2text\n\n   # Initialize the class.\n   doc = doc2text.Document()\n\n   # You can pass the lang (as 3 letters code) to the class to improve accuracy\n   # On ubuntu it requires the package tesseract-ocr-$lang$\n   # On other OS, see https://github.com/tesseract-ocr/langdata\n   doc = doc2text.Document(lang=\"eng\")\n\n   # Read the file in. Currently accepts pdf, png, jpg, bmp, tiff.\n   # If reading a PDF, doc2text will split the PDF into its component pages.\n   doc.read('./path/to/my/file')\n\n   # Crop the pages down to estimated text regions, deskew, and optimize for OCR.\n   doc.process()\n\n   # Extract text from the pages.\n   doc.extract_text()\n   text = doc.get_text()\n\nBig thanks\n----------\n\ndoc2text would be nothing without the open-source contributions of:\n\n- `@danvk <http://github.com/danvk>`_\n- `@jrosebr1 <http://github.com/jrosebr1>`_\n- Countless stackoverflow posts and comments.\n"
 },
 {
  "repo": "randy3k/Terminus",
  "language": "Python",
  "readme_contents": "# Bring a real terminal to Sublime Text\n\n<a href=\"https://packagecontrol.io/packages/Terminus\"><img src=\"https://packagecontrol.herokuapp.com/downloads/Terminus.svg\"></a>\n<a href=\"https://www.paypal.me/randy3k/5usd\" title=\"Donate to this project using Paypal\"><img src=\"https://img.shields.io/badge/paypal-donate-blue.svg\" /></a>\n\n\nThe first cross platform terminal for Sublime Text.\n\n<table>\n    <tr>\n        <th>Unix shell</th>\n        <th>Cmd.exe</th>\n    </tr>\n    <tr>\n        <td width=\"50%\">\n            <a href=\"https://user-images.githubusercontent.com/1690993/41784539-03534fdc-760e-11e8-845d-3d133a559df5.gif\">\n                <img src=\"https://user-images.githubusercontent.com/1690993/41784539-03534fdc-760e-11e8-845d-3d133a559df5.gif\" width=\"100%\">\n            </a>\n        </td>\n        <td width=\"50%\">\n            <a href=\"https://user-images.githubusercontent.com/1690993/41786131-a625d870-7612-11e8-882d-f1574184faba.gif\">\n                <img src=\"https://user-images.githubusercontent.com/1690993/41786131-a625d870-7612-11e8-882d-f1574184faba.gif\" width=\"100%\">\n            </a>\n        </td>\n    </tr>\n    <tr>\n        <th>Terminal in panel</th>\n        <th>Support <a href=\"https://www.iterm2.com/documentation-images.html\">showing images</a></th>\n    </tr>\n    <tr>\n        <td width=\"50%\">\n            <a href=\"https://user-images.githubusercontent.com/1690993/41784748-a7ed9d90-760e-11e8-8979-dd341933f1bb.gif\">\n                <img src=\"https://user-images.githubusercontent.com/1690993/41784748-a7ed9d90-760e-11e8-8979-dd341933f1bb.gif\" width=\"100%\">\n            </a>\n        </td>\n        <td width=\"50%\">\n            <img src=\"https://user-images.githubusercontent.com/1690993/51725223-1dfa3780-202f-11e9-9600-6e24b78d562d.png\" width=\"100%\">\n        </td>\n    </tr>\n</table>\n\nThis package is heavily inspired by [TerminalView](https://github.com/Wramberg/TerminalView). Compare with TerminalView, this has\n\n- Windows support\n- continuous history\n- easily customizable themes (see Terminus Utilities)\n- unicode support\n- 256 colors support\n- better xterm support\n- terminal panel\n- [imgcat](https://www.iterm2.com/documentation-images.html) support (PS: it also works on Linux / WSL)\n\n## Installation\n\nPackage Control.\n\n### Getting started\n\n- run `Terminus: Open Default Shell in Tab`\n\n- [OdatNurd](https://github.com/OdatNurd) has made several videos on Terminus. See, for examples,\n    - https://www.youtube.com/watch?v=etIJMVIvVgg (most up to date)\n    - https://www.youtube.com/watch?v=mV0ghkMwTQc\n\n\n## Shell configurations\n\nTerminus comes with several shell configurations. The settings file should be quite self explanatory. \n\n\n## User Key Bindings\n\nYou may find these key bindings useful. To edit, run `Preferences: Terminus Key Bindings`.\nCheck the details for the arguments of `terminus_open` below.\n\n\n- toggle terminal panel\n```json\n[\n    { \n        \"keys\": [\"alt+`\"], \"command\": \"toggle_terminus_panel\"\n    }\n]\n```\n\n- open a terminal view at current file directory\n```json\n[\n    { \n        \"keys\": [\"ctrl+alt+t\"], \"command\": \"terminus_open\", \"args\": {\n            \"cwd\": \"${file_path:${folder}}\"\n        }\n    }\n]\n```\nor by passing a custom `cmd`, say `ipython`\n```json\n[\n    { \n        \"keys\": [\"ctrl+alt+t\"], \"command\": \"terminus_open\", \"args\": {\n            \"cmd\": \"ipython\",\n            \"cwd\": \"${file_path:${folder}}\"\n        }\n    }\n]\n```\n\n- open terminal in a split view by using [Origami](https://github.com/SublimeText/Origami)'s `carry_file_to_pane`\n```json\n[\n    {\n        \"keys\": [\"ctrl+alt+t\"],\n        \"command\": \"terminus_open\",\n        \"args\": {\n            \"post_window_hooks\": [\n                [\"carry_file_to_pane\", {\"direction\": \"down\"}]\n            ]\n        }\n    }\n]\n```\n\n- <kbd>ctrl-w</kbd> to close terminal\n\nFollowing keybinding can be considered if one wants to use `ctrl+w` to close terminals.\n\n```json\n{ \n    \"keys\": [\"ctrl+w\"], \"command\": \"terminus_close\", \"context\": [{ \"key\": \"terminus_view\"}]\n}\n```\n\n## User Commands in Palette\n\n- run `Preferences: Terminus Command Palette`. Check the details for the arguments of `terminus_open` below\n\n```json\n[\n    {\n        \"caption\": \"Terminus: Open Default Shell at Current Location\",\n        \"command\": \"terminus_open\",\n        \"args\"   : {\n            \"cwd\": \"${file_path:${folder}}\"\n        }\n    }\n]\n```\nor by passing custom `cmd`, say `ipython`\n\n```json\n[\n    {\n        \"caption\": \"Terminus: Open iPython\",\n        \"command\": \"terminus_open\",\n        \"args\"   : {\n            \"cmd\": \"ipython\",\n            \"cwd\": \"${file_path:${folder}}\",\n            \"title\": \"iPython\"\n        }\n    }\n]\n```\n\n- open terminal in a split tab by using [Origami](https://github.com/SublimeText/Origami)'s `carry_file_to_pane`\n\n```json\n[\n    {\n        \"caption\": \"Terminus: Open Default Shell in Split Tab\",\n        \"command\": \"terminus_open\",\n        \"args\": {\n            \"post_window_hooks\": [\n                [\"carry_file_to_pane\", {\"direction\": \"down\"}]\n            ]\n        }\n    }\n]\n```\n\n## Terminus Build System\n\nIt is possible to use `Terminus` as a build system. The target `terminus_exec` is a drop in replacement of the default target `exec`. It takes exact same arguments as `terminus_open` except that their default values are set differently.\n\n`terminus_cancel_build` is used to cancel the build when user runs `cancel_build` triggered by <kbd>ctrl+c</kbd> (macOS) or <kbd>ctrl+break</kbd> (Windows / Linux).\n\nThe following is an example of build system define in project settings that run a python script\n\n```json\n{\n    \"build_systems\":\n    [\n        {\n            \"name\": \"Hello World\",\n            \"target\": \"terminus_exec\",\n            \"cancel\": \"terminus_cancel_build\",\n            \"cmd\": [\n                \"python\", \"helloworld.py\"\n            ],\n            \"working_dir\": \"$folder\"\n        }\n    ]\n}\n```\n\nThe same Hello World example could be specified via a `.sublime-build` file.\n\n```json\n{\n    \"target\": \"terminus_exec\",\n    \"cancel\": \"terminus_cancel_build\",\n    \"cmd\": [\n        \"python\", \"helloworld.py\"\n    ],\n    \"working_dir\": \"$folder\"\n}\n```\n\nInstead of `cmd`, user could also specify `shell_cmd`. In macOS and linux, a bash shell will be invoked; and in Windows, cmd.exe will be invoked.\n\n```json\n{\n    \"target\": \"terminus_exec\",\n    \"cancel\": \"terminus_cancel_build\",\n    \"shell_cmd\": \"python helloworld.py\",\n    // to directly invoke bash command\n    // \"shell_cmd\": \"echo helloworld\",\n    \"working_dir\": \"$folder\"\n}\n```\n\n## Alt-Left/Right to move between words (Unix)\n\n- Bash: add the following in `.bash_profile` or `.bashrc`\n\n    ```sh\n    if [ \"$TERM_PROGRAM\" == \"Terminus-Sublime\" ]; then\n        bind '\"\\e[1;3C\": forward-word'\n        bind '\"\\e[1;3D\": backward-word'\n    fi\n    ```\n\n- Zsh: add the following in `.zshrc`\n\n    ```sh\n    if [ \"$TERM_PROGRAM\" = \"Terminus-Sublime\" ]; then\n        bindkey \"\\e[1;3C\" forward-word\n        bindkey \"\\e[1;3D\" backward-word\n    fi\n    ```\n\nSome programs, such as julia, do not recognize the standard keycodes for `alt+left` and `alt+right`. You could\nbind them to `alt+b` and `alt+f` respectively\n```json\n[\n    { \"keys\": [\"alt+left\"], \"command\": \"terminus_keypress\", \"args\": {\"key\": \"b\", \"alt\": true}, \"context\": [{\"key\": \"terminus_view\"}] },\n    { \"keys\": [\"alt+right\"], \"command\": \"terminus_keypress\", \"args\": {\"key\": \"f\", \"alt\": true}, \"context\": [{\"key\": \"terminus_view\"}] }\n]\n```\n\n## Terminus API\n\n- A terminal could be opened using the command `terminus_open` with\n\n```py\nwindow.run_command(\n    \"terminus_open\", {\n        \"config_name\": None,     # the shell config name, use `None` for the default config\n        \"cmd\": None,             # the cmd to execute\n        \"shell_cmd\": None,       # a script to execute in a shell\n                                 # bash on Unix and cmd.exe on Windows\n        \"cwd\": None,             # the working directory\n        \"working_dir\": None,     # alias of \"cwd\"\n        \"env\": {},               # extra environmental variables\n        \"title\": None,           # title of the view, let terminal configures it if leave empty\n        \"panel_name\": None,      # the name of the panel if terminal should be opened in panel\n        \"focus\": True,           # focus to the panel\n        \"tag\": None,             # a tag to identify the terminal\n        \"file_regex\": None       # the `file_regex` pattern in sublime build system\n                                 # see https://www.sublimetext.com/docs/3/build_systems.html\n        \"line_regex\": None       # the `file_regex` pattern in sublime build system\n        \"pre_window_hooks\": [],  # a list of window hooks before opening terminal\n        \"post_window_hooks\": [], # a list of window hooks after opening terminal\n        \"post_view_hooks\": [],   # a list of view hooks after opening terminal\n        \"auto_close\": True,      # auto close terminal if process exits successfully\n        \"cancellable\": False,    # allow `cancel_build` command to terminate process, only relevent to panels\n        \"timeit\": False          # display elapsed time when the process terminates\n    }\n)\n```\n\nThe fields `cmd` and `cwd` understand Sublime Text build system [variables](https://www.sublimetext.com/docs/3/build_systems.html#variables).\n\n\n- the setting `view.settings().get(\"terminus_view.tag\")` can be used to identify the terminal and \n\n- keybind can be binded with specific tagged terminal\n\n```json\n    {\n        \"keys\": [\"ctrl+alt+w\"], \"command\": \"terminus_close\", \"context\": [\n            { \"key\": \"terminus_view.tag\", \"operator\": \"equal\", \"operand\": \"YOUR_TAG\"}\n        ]\n    }\n```\n\n- text can be sent to the terminal with\n\n```py\nwindow.run_command(\n    \"terminus_send_string\", \n    {\n        \"string\": \"ls\\n\",\n        \"tag\": \"<YOUR_TAG>\"        # ignore this or set it to None to send text to the first terminal found\n        \"visible_only\": False      # send to visible terminal only, default is `False`. Only relevent when `tag` is None\n    }\n)\n```\n\nIf `tag` is not provided or is `None`, the text will be sent to the first terminal found in the current window.\n\n\n## FAQ\n\n### Memory issue\n\nIt is known that Terminus sometimes consumes a lot of memory after extensive use. It is because Sublime Text keeps an infinite undo stack. There is virtually no fix unless upstream provides an API to work with the undo stack. Meanwhile, users could execute `Terminus: Reset` to release the memory.\n\nThis issue has been fixed in Sublime Text >= 4114 and Terminus v0.3.20.\n\n### Color issue when maximizing and minimizing terminal\n\nIt is known that the color of the scrollback history will be lost when a terminal is maximized or minimized from or to the panel. There is no fix for this issue.\n\n\n### Terminal panel background issue\n\nIf you are using DA UI and your terminal panel has weired background color,\ntry playing with the setting `panel_background_color` in `DA UI: Theme\nSettings`.\n\n<img src=\"https://user-images.githubusercontent.com/1690993/41728204-31a9a2a2-7544-11e8-9fb6-a37b59da852a.png\" width=\"50%\" />\n\n```json\n{\n    \"panel_background_color\": \"$background_color\"\n}\n```\n\n### Cmd.exe rendering issue in panel\n\nDue to a upstream bug (may winpty or cmd.exe?), there may be arbitrary empty lines inserted between prompts if the panel is too short. It seems that cmder and powershell are not affected by this bug.\n\n\n### Acknowledgments\n\nThis package won't be possible without [pyte](https://github.com/selectel/pyte), [pywinpty](https://github.com/spyder-ide/pywinpty) and [ptyprocess](https://github.com/pexpect/ptyprocess).\n"
 },
 {
  "repo": "scanapi/scanapi",
  "language": "Python",
  "readme_contents": "![](https://github.com/scanapi/design/raw/main/images/github-hero-dark.png)\n\n<p align=\"center\">\n  <a href=\"https://app.circleci.com/pipelines/github/scanapi/scanapi?branch=main\">\n    <img alt=\"CircleCI\" src=\"https://img.shields.io/circleci/build/github/scanapi/scanapi\">\n  </a>\n  <a href=\"https://codecov.io/gh/scanapi/scanapi\">\n    <img alt=\"Codecov\" src=\"https://img.shields.io/codecov/c/github/scanapi/scanapi\">\n  </a>\n  <a href=\"https://github.com/scanapi/scanapi/actions/workflows/lint.yml/badge.svg\">\n    <img alt=\"LintCheck\" src=\"https://github.com/scanapi/scanapi/workflows/Lint%20check/badge.svg?event=push\">\n  </a>\n  <a href=\"https://github.com/scanapi/scanapi/actions/workflows/run-examples.yml/badge.svg\">\n    <img alt=\"Examples\" src=\"https://github.com/scanapi/scanapi/workflows/ScanAPI%20Examples/badge.svg?event=push\">\n  </a>\n  <a href=\"https://badge.fury.io/py/scanapi\">\n    <img alt=\"PyPI version\" src=\"https://badge.fury.io/py/scanapi.svg\">\n  </a>\n\n  <a href=\"https://discord.scanapi.dev\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/discord/847208162993242162?color=7389D8&label=discord&logo=6A7EC2&logoColor=ffffff&style=flat-square\">\n  </a>\n</p>\n\nA library for **your API** that provides:\n\n- Automated Integration Testing\n- Automated Live Documentation\n\nGiven an API specification, written in YAML/JSON format, ScanAPI hits the specified\nendpoints, runs the test cases, and generates a detailed report of this execution - which can also\nbe used as the API documentation itself.\n\nWith almost no Python knowledge, the user can define endpoints to be hit, the expected behavior\nfor each response and will receive a full real-time diagnostic report of the API!\n\n## Contents\n\n- [Contents](#contents)\n- [Requirements](#requirements)\n- [How to install](#how-to-install)\n- [Basic Usage](#basic-usage)\n- [Documentation](#documentation)\n- [Examples](#examples)\n- [Contributing](#contributing)\n\n## Requirements\n\n- [pip][pip-installation]\n\n## How to install\n\n```bash\n$ pip install scanapi\n```\n\n## Basic Usage\n\nYou will need to write the API's specification and save it as a **YAML** or **JSON** file.\nFor example:\n\n```yaml\nendpoints:\n  - name: scanapi-demo # The API's name of your API\n    path: http://demo.scanapi.dev/api/v1 # The API's base url\n    requests:\n      - name: list_all_users # The name of the first request\n        path: users/ # The path of the first request\n        method: get # The HTTP method of the first request\n        tests:\n          - name: status_code_is_200 # The name of the first test for this request\n            assert: ${{ response.status_code == 200 }} # The assertion\n```\n\nAnd run the scanapi command\n\n```bash\n$ scanapi run <file_path>\n```\n\nThen, the lib will hit the specified endpoints and generate a `scanapi-report.html` file with the report results.\n\n<p align=\"center\">\n  <img\n    src=\"https://raw.githubusercontent.com/scanapi/scanapi/main/images/report-print-closed.png\"\n    width=\"700\",\n    alt=\"An overview screenshot of the report.\"\n  >\n  <img\n    src=\"https://raw.githubusercontent.com/scanapi/scanapi/main/images/report-print-opened.png\"\n    width=\"700\"\n    alt=\"A screenshot of the report showing the request details.\"\n  >\n</p>\n\n## Documentation\n\nThe full documentation is available at [scanapi.dev][website]\n\n## Examples\n\nYou can find complete examples at [scanapi/examples][scanapi-examples]!\n\nThis tutorial helps you to create integration tests for your REST API using ScanAPI\n\n[![Watch the video](https://raw.githubusercontent.com/scanapi/scanapi/main/images/youtube-scanapi-tutorial.png)](https://www.youtube.com/watch?v=JIo4sA8LHco&t=2s)\n\n## Contributing\n\nCollaboration is super welcome! We prepared the [Newcomers Guide][newcomers-guide] to help you in the first steps. Every little bit of help counts! Feel free to create new [GitHub issues][github-issues] and interact here.\n\nLet's build it together \ud83d\ude80\ud83d\ude80\n\n[github-issues]: https://github.com/scanapi/scanapi/issues\n[newcomers-guide]: https://github.com/scanapi/scanapi/wiki/Newcomers\n[pip-installation]: https://pip.pypa.io/en/stable/installing/\n[scanapi-examples]: https://github.com/scanapi/examples\n[website]: https://scanapi.dev\n"
 },
 {
  "repo": "nsoojin/coursera-ml-py",
  "language": "Python",
  "readme_contents": "# Coursera Machine Learning Assignments in Python\n[![author](https://img.shields.io/badge/author-nsoojin-red.svg)](https://www.linkedin.com/in/soojinro) [![python](https://img.shields.io/badge/python-3.6-blue.svg)]() [![license](https://img.shields.io/github/license/mashape/apistatus.svg)]() [![contribution](https://img.shields.io/badge/contribution-welcome-brightgreen.svg)]()\n\n![title_image](title_image.png)\n\n## About\nIf you've finished the amazing introductory Machine Learning on Coursera by Prof. Andrew Ng, you probably got familiar with Octave/Matlab programming. With this repo, you can re-implement them in Python, step-by-step, visually checking your work along the way, just as the course assignments.\n\n## How to start\n### Dependencies\nThis project was coded in Python 3.6\n* numpy\n* matplotlib\n* scipy\n* scikit-learn\n* scikit-image\n* nltk\n\n### Installation\nThe fastest and easiest way to install all these dependencies at once is to use [Anaconda](https://www.continuum.io/downloads).\n\n\n## Important Note\nThere are a couple of things to keep in mind before starting.\n* all column vectors from octave/matlab are flattened into a simple 1-dimensional ndarray. (e.g., y's and thetas are no longer m x 1 matrix, just a 1-d ndarray with m elements.)\nSo in Octave/Matlab, \n    ```matlab\n    >> size(theta)\n    >> (2, 1)\n    ```\n    Now, it is\n    ```python\n    >>> theta.shape\n    >>> (2, )\n    ```\n* numpy.matrix is never used, just plain ol' numpy.ndarray\n\n## Contents\n#### [Exercise 1](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex1)\n* Linear Regression\n* Linear Regression with multiple variables\n#### [Exercise 2](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex2)\n* Logistic Regression\n* Logistic Regression with Regularization\n#### [Exercise 3](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex3)\n* Multiclass Classification\n* Neural Networks Prediction fuction\n#### [Exercise 4](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex4)\n* Neural Networks Learning\n#### [Exercise 5](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex5)\n* Regularized Linear Regression\n* Bias vs. Variance\n#### [Exercise 6](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex6)\n* Support Vector Machines\n* Spam email Classifier\n#### [Exercise 7](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex7)\n* K-means Clustering\n* Principal Component Analysis\n#### [Exercise 8](https://github.com/nsoojin/coursera-ml-py/tree/master/machine-learning-ex8)\n* Anomaly Detection\n* Recommender Systems\n\n## Solutions\nYou can check out my implementation of the assignments [here](https://github.com/nsoojin/coursera-ml-py-sj). I tried to vectorize all the solutions.\n"
 },
 {
  "repo": "salu133445/musegan",
  "language": "Python",
  "readme_contents": "# MuseGAN\n\n[MuseGAN](https://salu133445.github.io/musegan/) is a project on music\ngeneration. In a nutshell, we aim to generate polyphonic music of multiple\ntracks (instruments). The proposed models are able to generate music either from\nscratch, or by accompanying a track given a priori by the user.\n\nWe train the model with training data collected from\n[Lakh Pianoroll Dataset](https://salu133445.github.io/lakh-pianoroll-dataset/)\nto generate pop song phrases consisting of bass, drums, guitar, piano and\nstrings tracks.\n\nSample results are available\n[here](https://salu133445.github.io/musegan/results).\n\n## Important Notes\n\n- The latest implementation is based on the network architectures presented in BinaryMuseGAN, where the temporal structure is handled by 3D convolutional layers. The advantage of this design is its smaller network size, while the disadvantage is its reduced controllability, e.g., capability of feeding different latent variables for different measures or tracks.\n- The original code we used for running the experiments in the paper can be found in the `v1` folder.\n- Looking for a PyTorch version? Check out [this repository](https://github.com/salu133445/ismir2019tutorial).\n\n## Prerequisites\n\n> __Below we assume the working directory is the repository root.__\n\n### Install dependencies\n\n- Using pipenv (recommended)\n\n  > Make sure `pipenv` is installed. (If not, simply run `pip install pipenv`.)\n\n  ```sh\n  # Install the dependencies\n  pipenv install\n  # Activate the virtual environment\n  pipenv shell\n  ```\n\n- Using pip\n\n  ```sh\n  # Install the dependencies\n  pip install -r requirements.txt\n  ```\n\n### Prepare training data\n\n> The training data is collected from\n[Lakh Pianoroll Dataset](https://salu133445.github.io/lakh-pianoroll-dataset/)\n(LPD), a new multitrack pianoroll dataset.\n\n```sh\n# Download the training data\n./scripts/download_data.sh\n# Store the training data to shared memory\n./scripts/process_data.sh\n```\n\nYou can also download the training data manually\n([train_x_lpd_5_phr.npz](https://docs.google.com/uc?export=download&id=14rrC5bSQkB9VYWrvt2IhsCjOKYrguk3S)).\n\n> As pianoroll matrices are generally sparse, we store only the indices of\nnonzero elements and the array shape into a npz file to save space, and later\nrestore the original array. To save some training data `data` into this format,\nsimply run\n`np.savez_compressed(\"data.npz\", shape=data.shape, nonzero=data.nonzero())`\n\n## Scripts\n\nWe provide several shell scripts for easy managing the experiments. (See\n[here](scripts/README.md) for a detailed documentation.)\n\n> __Below we assume the working directory is the repository root.__\n\n### Train a new model\n\n1. Run the following command to set up a new experiment with default settings.\n\n   ```sh\n   # Set up a new experiment\n   ./scripts/setup_exp.sh \"./exp/my_experiment/\" \"Some notes on my experiment\"\n   ```\n\n2. Modify the configuration and model parameter files for experimental settings.\n\n3. You can either train the model:\n\n     ```sh\n     # Train the model\n     ./scripts/run_train.sh \"./exp/my_experiment/\" \"0\"\n     ```\n\n   or run the experiment (training + inference + interpolation):\n\n     ```sh\n     # Run the experiment\n     ./scripts/run_exp.sh \"./exp/my_experiment/\" \"0\"\n     ```\n\n### Collect training data\n\nRun the following command to collect training data from MIDI files.\n\n  ```sh\n  # Collect training data\n  ./scripts/collect_data.sh \"./midi_dir/\" \"data/train.npy\"\n  ```\n\n### Use pretrained models\n\n1. Download pretrained models\n\n   ```sh\n   # Download the pretrained models\n   ./scripts/download_models.sh\n   ```\n\n   You can also download the pretrained models manually\n   ([pretrained_models.tar.gz](https://docs.google.com/uc?export=download&id=19RYAbj_utCDMpU7PurkjsH4e_Vy8H-Uy)).\n\n2. You can either perform inference from a trained model:\n\n   ```sh\n   # Run inference from a pretrained model\n   ./scripts/run_inference.sh \"./exp/default/\" \"0\"\n   ```\n\n   or perform interpolation from a trained model:\n\n   ```sh\n   # Run interpolation from a pretrained model\n   ./scripts/run_interpolation.sh \"./exp/default/\" \"0\"\n   ```\n\n## Outputs\n\nBy default, samples will be generated alongside the training. You can disable\nthis behavior by setting `save_samples_steps` to zero in the configuration file\n(`config.yaml`). The generated will be stored in the following three formats by\ndefault.\n\n- `.npy`: raw numpy arrays\n- `.png`: image files\n- `.npz`: multitrack pianoroll files that can be loaded by the\n  _[Pypianoroll](https://salu133445.github.io/pypianoroll/index.html)_\n  package\n\nYou can disable saving in a specific format by setting `save_array_samples`,\n`save_image_samples` and `save_pianoroll_samples` to `False`  in the\nconfiguration file.\n\nThe generated pianorolls are stored in .npz format to save space and processing\ntime. You can use the following code to write them into MIDI files.\n\n```python\nfrom pypianoroll import Multitrack\n\nm = Multitrack('./test.npz')\nm.write('./test.mid')\n```\n\n## Sample Results\n\nSome sample results can be found in `./exp/` directory. More samples can be\ndownloaded from the following links.\n\n- [`sample_results.tar.gz`](https://docs.google.com/uc?export=download&id=1BsNtc8_mpLK5l2F5jncIkHbTcJqtZu2w) (54.7 MB):\n  sample inference and interpolation results\n- [`training_samples.tar.gz`](https://docs.google.com/uc?export=download&id=1pZk0YCElcHHSBfhbV8j_zaRr1zhEQUzN) (18.7 MB):\n  sample generated results at different steps\n\nCiting\n------\n\nPlease cite the following paper if you use the code provided in this repository.\n\nHao-Wen Dong,\\* Wen-Yi Hsiao,\\* Li-Chia Yang and Yi-Hsuan Yang, \"MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic\nMusic Generation and Accompaniment,\" _Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)_, 2018. (\\*equal contribution)\n<br>\n[[homepage](https://salu133445.github.io/musegan)]\n[[arXiv](http://arxiv.org/abs/1709.06298)]\n[[paper](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-paper.pdf)]\n[[slides](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-slides.pdf)]\n[[code](https://github.com/salu133445/musegan)]\n\n## Papers\n\n__MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment__<br>\nHao-Wen Dong,\\* Wen-Yi Hsiao,\\* Li-Chia Yang and Yi-Hsuan Yang (\\*equal contribution)<br>\n_Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)_, 2018.<br>\n[[homepage](https://salu133445.github.io/musegan)]\n[[arXiv](http://arxiv.org/abs/1709.06298)]\n[[paper](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-paper.pdf)]\n[[slides](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-slides.pdf)]\n[[code](https://github.com/salu133445/musegan)]\n\n__Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation__<br>\nHao-Wen Dong and Yi-Hsuan Yang<br>\n_Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)_, 2018.<br>\n[[homepage](https://salu133445.github.io/bmusegan)]\n[[video](https://youtu.be/r9C2Q2oR9Ik)]\n[[paper](https://salu133445.github.io/bmusegan/pdf/bmusegan-ismir2018-paper.pdf)]\n[[slides](https://salu133445.github.io/bmusegan/pdf/bmusegan-ismir2018-slides.pdf)]\n[[slides (long)](https://salu133445.github.io/bmusegan/pdf/bmusegan-tmac2018-slides.pdf)]\n[[poster](https://salu133445.github.io/bmusegan/pdf/bmusegan-ismir2018-poster.pdf)]\n[[arXiv](https://arxiv.org/abs/1804.09399)]\n[[code](https://github.com/salu133445/bmusegan)]\n\n__MuseGAN: Demonstration of a Convolutional GAN Based Model for Generating Multi-track Piano-rolls__<br>\nHao-Wen Dong,\\* Wen-Yi Hsiao,\\* Li-Chia Yang and Yi-Hsuan Yang (\\*equal contribution)<br>\n_Late-Breaking Demos of the 18th International Society for Music Information Retrieval Conference (ISMIR)_, 2017.<br>\n[[paper](https://salu133445.github.io/musegan/pdf/musegan-ismir2017-lbd-paper.pdf)]\n[[poster](https://salu133445.github.io/musegan/pdf/musegan-ismir2017-lbd-poster.pdf)]\n"
 },
 {
  "repo": "brouberol/marcel",
  "language": "Python",
  "readme_contents": "# Marcel, the french Docker - Marcel, le docker fran\u00e7ais\n[![Build Status](https://travis-ci.org/brouberol/marcel.svg?branch=master)](https://travis-ci.org/brouberol/marcel) [![Coverage Status](https://coveralls.io/repos/github/brouberol/marcel/badge.svg)](https://coveralls.io/github/brouberol/marcel?branch=master)\n\n![logo](https://brouberol.github.io/marcel/images/logo/marcel-logo-yellow.png)\n\nMarcel is a french wrapper around the docker CLI, intended as a drop-in replacement of docker, for the future french sovereign operating system.\n\n## Examples\n\n* ``docker run`` \u2192 ``marcel chauffe``\n* ``docker images`` \u2192 ``marcel cederoms``\n* ``docker login`` \u2192 ``marcel vos-papiers``\n* ``docker logs`` \u2192 ``marcel b\u00fbches``\n* ``docker pause`` \u2192 ``marcel rtt``\n* ``docker suspend`` \u2192 ``marcel gr\u00e8ve``\n* ``docker tag`` \u2192 ``marcel graffiti``\n* ``docker rmi`` \u2192 ``marcel rsa``\n\n## Dockerfile\n\nObviously, the ``Dockerfile`` name is not sovereign enough for us. That's why instead of ``Dockerfile``s, marcel uses ``Recette\u00c0Marcel`` files.\nFor now, they use the exact same syntax as ``Dockerfile``, but we'll see about that.\n\nFor it to work, you just need to include a ``Recette\u00c0Marcel`` file in the current directory where you execute your ``marcel bricole`` command, are you're good to go.\n\n## Bash completion\n\nAdd this line to your ``~/.bashrc``:\n```bash\nsource <(marcel compl\u00e8te bash)\n```\n\n## Contributing.\n\nFirst of all, thanks for even considering contributing to the splendor of the French tech industry. You'll need to install the dev dependencies in your virtualenv:\n\n```bash\n$ pip install -r requirements/dev.txt\n```\n\nThen, create a branch from master and commit your feature (and tests please :). You can test that everything works correctly by running the ``tox`` command.\nWhen all tests are green, push your feature, and create a pull request. Thats it!\n\n## How is marcel related to the french OS ?\n\nThis project aims at providing an acceptable technology background on which the Great OS (not firewall) of France could be based on. Take a look at [the official document](http://www.assemblee-nationale.fr/14/amendements/3318/CION_LOIS/CL129.asp) to see how compliant and willingfull to help we are, already developing the technologies for the OS of tomorrow, just like RedStar OS is. Help us make the World better by destroying non-compliant operating systems, e.g. thoses who includes encryption without backdoors.\n\n## Thanks\nThe [original idea](https://github.com/docker/docker/issues/19396) came of [@ndeloof](https://github.com/ndeloof)'s mind.\nThe logo was provided by [jkneb](https://github.com/jkneb).\n"
 },
 {
  "repo": "douban/rexxar-android",
  "language": "Java",
  "readme_contents": "# Rexxar Android\n\n\n[![Test Status](https://travis-ci.org/douban/rexxar-android.svg?branch=master)](https://travis-ci.org/douban/rexxar-android)\n[![IDE](https://img.shields.io/badge/Android-Studio-blue.svg)]()\n[![Android](https://img.shields.io/badge/Android-4.0-green.svg)]()\n[![Language](https://img.shields.io/badge/language-Java-blue.svg)]()\n\n\n**Rexxar** \u662f\u4e00\u4e2a\u9488\u5bf9\u79fb\u52a8\u7aef\u7684\u6df7\u5408\u5f00\u53d1\u6846\u67b6\u3002\u73b0\u5728\u652f\u6301 Android \u548c iOS \u5e73\u53f0\u3002`rexxar-android` \u662f Rexxar \u5728 Android \u7cfb\u7edf\u4e0a\u7684\u5ba2\u6237\u7aef\u5b9e\u73b0\u3002\n\n\u901a\u8fc7 Rexxar\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u5305\u62ec javascript\uff0ccss\uff0chtml \u5728\u5185\u7684\u4f20\u7edf\u524d\u7aef\u6280\u672f\u5f00\u53d1\u79fb\u52a8\u5e94\u7528\u3002Rexxar \u7684\u5ba2\u6237\u7aef\u5b9e\u73b0 Rexxar Container \u5bf9\u4e8e Web \u7aef\u4f7f\u7528\u4f55\u79cd\u6280\u672f\u5e76\u65e0\u8981\u6c42\u3002\u6211\u4eec\u73b0\u5728\u7684 Rexxar \u7684\u524d\u7aef\u5b9e\u73b0 Rexxar Web\uff0c\u4ee5\u53ca Rexxar Container \u5728\u4e24\u4e2a\u5e73\u53f0\u7684\u5b9e\u73b0 rexxar-ios \u548c rexxar-android \u9879\u76ee\u4e2d\u6240\u5e26\u7684 Demo \u90fd\u4f7f\u7528\u4e86 [React](https://facebook.github.io/react/)\u3002\u4f46\u4f60\u5b8c\u5168\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u7684\u524d\u7aef\u6846\u67b6\u5728 Rexxar Container \u4e2d\u8fdb\u884c\u5f00\u53d1\u3002\n\nrexxar-android \u73b0\u5728\u652f\u6301 Android 4.0 \u53ca\u4ee5\u4e0a\u7248\u672c\u3002\n\n## Rexxar \u7b80\u4ecb\n\n\u5173\u4e8e Rexxar \u7684\u6574\u4f53\u4ecb\u7ecd\uff0c\u53ef\u4ee5\u770b\u770b\u8fd9\u7bc7\u535a\u5ba2\uff1a[\u8c46\u74e3\u7684\u6df7\u5408\u5f00\u53d1\u6846\u67b6 -- Rexxar](http://lincode.github.io/Rexxar-OpenSource)\u3002\n\nRexxar \u5305\u542b\u4e09\u4e2a\u5e93\uff1a\n\n- Rexxar Web \uff1a[https://github.com/douban/rexxar-web](https://github.com/douban/rexxar-web)\u3002\n\n- Rexxar Android\uff1a[https://github.com/douban/rexxar-android](https://github.com/douban/rexxar-android)\u3002\n\n- Rexxar iOS\uff1a[https://github.com/douban/rexxar-ios](https://github.com/douban/rexxar-ios)\u3002\n\n## \u4f7f\u7528\n\n\u4f60\u53ef\u4ee5\u67e5\u770b Demo \u4e2d\u7684\u4f8b\u5b50\u3002\u4e86\u89e3\u5982\u4f55\u4f7f\u7528 Rexxar\u3002Demo \u7ed9\u51fa\u4e86\u5b8c\u5584\u7684\u793a\u4f8b\u3002\n\nDemo \u4e2d\u4f7f\u7528 github \u7684 raw \u6587\u4ef6\u670d\u52a1\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u7684\u8def\u7531\u8868\u6587\u4ef6 routes.json\uff0cdemo.html \u4ee5\u53ca\u76f8\u5173 javascript \u8d44\u6e90\u7684\u8bbf\u95ee\u670d\u52a1\u3002\u5728\u4f60\u7684\u7ebf\u4e0a\u670d\u52a1\u4e2d\uff0c\u5f53\u7136\u4f1a\u9700\u8981\u4e00\u4e2a\u771f\u6b63\u7684\u751f\u4ea7\u73af\u5883\uff0c\u4ee5\u5e94\u4ed8\u66f4\u5927\u89c4\u6a21\u7684\u8def\u7531\u8868\u6587\u4ef6\uff0c\u4ee5\u53ca javascript\uff0ccss\uff0chtml \u8d44\u6e90\u6587\u4ef6\u7684\u8bbf\u95ee\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u670d\u52a1\u7aef\u6846\u67b6\u3002Rexxar \u5bf9\u670d\u52a1\u7aef\u6846\u67b6\u5e76\u65e0\u8981\u6c42\u3002\n\n### \u5b89\u88c5\n\n#### gradle\n\n```groovy\n   compile 'com.douban.rexxar:core:0.6.9'\n```\n\n\n### \u914d\u7f6e\n\n#### 1. \u521d\u59cb\u5316\n\n\u5728Application\u7684`onCreate`\u4e2d\u8c03\u7528\n\n```Java\n  Rexxar.initialize(Context context);\n```\n\n#### 2. \u8bbe\u7f6e\u8def\u7531\u8868\u6587\u4ef6 api\uff1a\n\n```Java\n  RouteManager.getInstance().setRouteApi(\"https://raw.githubusercontent.com/douban/rexxar-web/master/example/dist/routes.json\");\n```\n\nRexxar \u4f7f\u7528 uri \u6765\u6807\u8bc6\u9875\u9762\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6b63\u786e\u7684 uri \u5c31\u53ef\u4ee5\u6253\u5f00\u5bf9\u5e94\u7684\u9875\u9762\uff0c\u8def\u7531\u8868\u63d0\u4f9b\u4e86\u6bcf\u4e2a uri \u5bf9\u5e94\u7684 html \u8d44\u6e90\u7684\u4e0b\u8f7d\u5730\u5740\u3002\n\nDemo \u4e2d\u7684\u8def\u7531\u8868\u5982\u4e0b\uff1a\n\n```json\n\n{\n  \"items\": [\n    {\n      \"deploy_time\": \"Sun, 09 Oct 2016 05:54:22 GMT\",\n      \"remote_file\": \"https://raw.githubusercontent.com/douban/rexxar-web/master/example/dist/rexxar/demo-252452ae58.html\",\n      \"uri\": \"douban://douban.com/rexxar_demo[/]?.*\"\n    }\n  ],\n  \"partial_items\": [\n    {\n      \"deploy_time\": \"Sun, 09 Oct 2016 05:54:22 GMT\",\n      \"remote_file\": \"https://raw.githubusercontent.com/douban/rexxar-web/master/example/dist/rexxar/demo-252452ae58.html\",\n      \"uri\": \"douban://partial.douban.com/rexxar_demo/_.*\"\n    }\n  ],\n  \"deploy_time\": \"Sun, 09 Oct 2016 05:54:22 GMT\"\n}\n\n\n```\n\n#### 3. \u8bbe\u7f6e\u9700\u8981\u4ee3\u7406\u6216\u7f13\u5b58\u7684\u8bf7\u6c42host\n\n```Java\n  ResourceProxy.getInstance().addProxyHosts(List<>() hosts);\n```\n\nRexxar\u662f\u901a\u8fc7`WebViewClient`\u7684`shouldInterceptRequest`\u65b9\u6cd5\u6765\u62e6\u62e6\u622a\u8bf7\u6c42\uff0c\u8bf7\u6c42\u7ebf\u4e0a\u6570\u636e\u5e76\u8fd4\u56de\u7ed9'webview'\u3002\u4e3a\u4e86\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6d41\u7a0b\u7834\u574f\uff0c\u53ea\u6709\u660e\u786e\u9700\u8981\u62e6\u622a\u7684hosts\uff08\u652f\u6301\u6b63\u5219\uff09\u7684\u8bf7\u6c42\u624d\u4f1a\u88ab\u62e6\u622a\u4ee3\u7406\uff0c\u5e76\u6839\u636emime-type\u51b3\u5b9a\u54ea\u4e9b\u5185\u5bb9\u9700\u8981\u7f13\u5b58\u3002\n\n#### 4. \u9884\u7f6e\u8d44\u6e90\u6587\u4ef6\n\n\u4f7f\u7528 Rexxar \u4e00\u822c\u4f1a\u9884\u7f6e\u4e00\u4efd\u8def\u7531\u8868\uff0c\u4ee5\u53ca\u8d44\u6e90\u6587\u4ef6\u5728\u5e94\u7528\u5305\u4e2d\u3002\u8fd9\u6837\u5c31\u53ef\u4ee5\u51cf\u5c11\u7528\u6237\u7684\u4e0b\u8f7d\uff0c\u52a0\u5feb\u7b2c\u4e00\u6b21\u6253\u5f00\u9875\u9762\u7684\u901f\u5ea6\u3002\u5728\u6ca1\u6709\u7f51\u7edc\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u6570\u636e\u8bf7\u6c42\u7684\u8bdd\uff0c\u9875\u9762\u4e5f\u53ef\u8bbf\u95ee\u3002\u8fd9\u90fd\u6709\u5229\u4e8e\u7528\u6237\u4f53\u9a8c\u3002\n\u9884\u7f6e\u6587\u4ef6\u8def\u5f84\u662f`assets/rexxar`, \u6682\u4e0d\u652f\u6301\u4fee\u6539\u3002\n\n\n\n### \u4f7f\u7528 RexxarWebView\n\n\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 `RexxarWebView` \u4f5c\u4e3a\u4f60\u7684\u6df7\u5408\u5f00\u53d1\u5ba2\u6237\u7aef\u5bb9\u5668\u3002\u6216\u8005\u4f60\u4e5f\u53ef\u4ee5\u5728 `RexxarWebView` \u57fa\u7840\u4e0a\u5b9e\u73b0\u4f60\u81ea\u5df1\u7684\u5ba2\u6237\u7aef\u5bb9\u5668\u3002\n\n\u4e3a\u4e86\u521d\u59cb\u5316 RexxarWebView\uff0c\u4f60\u9700\u8981\u53ea\u4e00\u4e2a url\u3002\u5728\u8def\u7531\u8868\u6587\u4ef6 api \u63d0\u4f9b\u7684\u8def\u7531\u8868\u4e2d\u53ef\u4ee5\u627e\u5230\u8fd9\u4e2a url\u3002\u8fd9\u4e2a url \u6807\u8bc6\u4e86\u8be5\u9875\u9762\u6240\u9700\u4f7f\u7528\u7684\u8d44\u6e90\u6587\u4ef6\u7684\u4f4d\u7f6e\u3002Rexxar Container \u4f1a\u901a\u8fc7 url \u5728\u8def\u7531\u8868\u4e2d\u5bfb\u627e\u5bf9\u5e94\u7684 javascript\uff0ccss\uff0chtml \u8d44\u6e90\u6587\u4ef6\u3002\n\n```Java\n  // \u6839\u636euri\u6253\u5f00\u6307\u5b9a\u7684web\u9875\u9762\n  mWebView.loadUri(\"douban://douban.com/rexxar_demo\");\n```\n\n## \u5b9a\u5236\u4f60\u81ea\u5df1\u7684 Rexxar Container\n\n\u6211\u4eec\u66b4\u9732\u4e86\u4e09\u7c7b\u63a5\u53e3\u3002\u4f9b\u5f00\u53d1\u8005\u66f4\u65b9\u4fbf\u5730\u6269\u5c55\u5c5e\u4e8e\u81ea\u5df1\u7684\u7279\u5b9a\u529f\u80fd\u5b9e\u73b0\u3002\n\n### \u5b9a\u5236 RexxarWidget\n\nRexxar Container \u63d0\u4f9b\u4e86\u4e00\u4e9b\u539f\u751f UI \u7ec4\u4ef6\uff0c\u4f9b Rexxar Web \u4f7f\u7528\u3002RexxarWidget \u662f\u4e00\u4e2a Java \u534f\u8bae\uff08Protocol\uff09\u3002\u8be5\u534f\u8bae\u662f\u5bf9\u8fd9\u7c7b\u539f\u751f UI \u7ec4\u4ef6\u7684\u62bd\u8c61\u3002\u5982\u679c\uff0c\u4f60\u9700\u8981\u5b9e\u73b0\u67d0\u4e9b\u539f\u751f UI \u7ec4\u4ef6\uff0c\u4f8b\u5982\uff0c\u5f39\u51fa\u4e00\u4e2a Toast\uff0c\u6216\u8005\u6dfb\u52a0\u539f\u751f\u6548\u679c\u7684\u4e0b\u62c9\u5237\u65b0\uff0c\u4f60\u5c31\u53ef\u4ee5\u5b9e\u73b0\u4e00\u4e2a\u7b26\u5408 RexxarWidget \u534f\u8bae\u7684\u7c7b\uff0c\u5e76\u5b9e\u73b0\u4ee5\u4e0b\u65b9\u6cd5\uff1a`getPath:`, `handle:`\u3002\n\n\u5728 Demo \u4e2d\u53ef\u4ee5\u627e\u5230\u4e00\u4e2a\u4f8b\u5b50\uff1a`TitleWidget` \uff0c\u901a\u8fc7\u5b83\u53ef\u4ee5\u8bbe\u7f6e\u5bfc\u822a\u680f\u7684\u6807\u9898\u6587\u5b57\u3002\n\n```Java\n\n    public class TitleWidget implements RexxarWidget {\n\n    static final String KEY_TITLE = \"title\";\n\n    @Override\n    public String getPath() {\n        return \"/widget/nav_title\";\n    }\n\n    @Override\n    public boolean handle(WebView view, String url) {\n        if (TextUtils.isEmpty(url)) {\n            return false;\n        }\n        Uri uri = Uri.parse(url);\n        if (TextUtils.equals(uri.getPath(), getPath())) {\n            String title = uri.getQueryParameter(KEY_TITLE);\n            if (null != view && view.getContext() instanceof Activity) {\n                ((Activity)view.getContext()).setTitle(Uri.decode(title));\n            }\n            return true;\n        }\n        return false;\n    }\n}\n```\n\n### \u5b9a\u5236 RexxarContainerAPI\n\n\u6211\u4eec\u5e38\u5e38\u9700\u8981\u5728 Rexxar Container \u548c Rexxar Web \u4e4b\u95f4\u505a\u6570\u636e\u4ea4\u4e92\u3002\u6bd4\u5982 Rexxar Container \u53ef\u4ee5\u4e3a Rexxar Web \u63d0\u4f9b\u4e00\u4e9b\u8ba1\u7b97\u7ed3\u679c\u3002\u5982\u679c\u4f60\u9700\u8981\u63d0\u4f9b\u4e00\u4e9b\u7531\u539f\u751f\u4ee3\u7801\u8ba1\u7b97\u7684\u6570\u636e\u7ed9 Rexxar Web \u4f7f\u7528\uff0c\u4f60\u5c31\u53ef\u4ee5\u9009\u62e9\u5b9e\u73b0 RexxarContainerAPI \u534f\u8bae\uff08Protocol\uff09\uff0c\u5e76\u5b9e\u73b0\u4ee5\u4e0b\u4e09\u4e2a\u65b9\u6cd5\uff1a`getPath:`, `call:`\u3002\n\n\u5728 Demo \u4e2d\u53ef\u4ee5\u627e\u5230\u4e00\u4e2a\u4f8b\u5b50\uff1a`LocationAPI`\u3002\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c`LocationAPI` \u8fd4\u56de\u4e86\u8bbe\u5907\u6240\u5728\u57ce\u5e02\u4fe1\u606f\u3002\u5f53\u7136\uff0c\u8fd9\u4e2a ContainerAPI \u4ec5\u4ec5\u662f\u4e00\u4e2a\u793a\u4f8b\uff0c\u5b83\u63d0\u4f9b\u7684\u662f\u4e00\u4e2a\u5047\u6570\u636e\uff0c\u6570\u636e\u6c38\u8fdc\u4e0d\u4f1a\u53d8\u5316\u3002\u4f60\u5f53\u7136\u53ef\u4ee5\u9075\u5b88 `RexxarContainerAPI` \u534f\u8bae\uff0c\u5b9e\u73b0\u4e00\u4e2a\u7c7b\u4f3c\u7684\u4f46\u662f\u6570\u636e\u662f\u771f\u5b9e\u7684\u529f\u80fd\u3002\n\n```Java\n\n    static class LocationAPI implements RexxarContainerAPI {\n\n        @Override\n        public String getPath() {\n            return \"/loc\";\n        }\n\n        @Override\n        public Response call(Request request) {\n            Response.Builder responseBuilder = newResponseBuilder(request);\n            try {\n                JSONObject jsonObject = new JSONObject();\n                jsonObject.put(\"lat\", \"0.0\");\n                jsonObject.put(\"lng\", \"0.0\");\n                responseBuilder.body(ResponseBody.create(MediaType.parse(Constants.MIME_TYPE_JSON), jsonObject.toString()));\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n            return responseBuilder.build();\n        }\n    }\n```\n\n\n### \u5b9a\u5236 Rexxar Decorator\n\n\u5982\u679c\u4f60\u9700\u8981\u4fee\u6539\u8fd0\u884c\u5728 Rexxar Container \u4e2d\u7684 Rexxar Web \u6240\u53d1\u51fa\u7684\u8bf7\u6c42\u3002\u4f8b\u5982\uff0c\u5728 http \u5934\u4e2d\u6dfb\u52a0\u767b\u5f55\u4fe1\u606f\uff0c\u4f60\u53ef\u4ee5\u81ea\u5b9a\u4e49OkHttpClient\uff0c`Rexxar.setOkHttpClient(OkHttpClient okHttpClient)`\n\n\u5728 Demo \u4e2d\u53ef\u4ee5\u627e\u5230\u4e00\u4e2a\u4f8b\u5b50\uff1a`AuthInterceptor`\u3002\u8fd9\u4e2a\u4f8b\u5b50\u4e3a Rexxar Web \u53d1\u51fa\u7684\u8bf7\u6c42\u6dfb\u52a0\u4e86\u767b\u5f55\u4fe1\u606f\u3002\n\n```Java\n\n    public class AuthInterceptor implements Interceptor{\n\n        @Override\n        public Response intercept(Chain chain) throws IOException {\n            Request request = chain.request();\n\n            String url = request.url().toString();\n            if (TextUtils.isEmpty(url)) {\n                return null;\n            }\n\n            Request.Builder builder = request.newBuilder();\n            builder.header(\"Authorization\", \"123456789\");\n            return chain.proceed(builder.build());\n        }\n    }\n\n    // Rexxar\u521d\u59cb\u5316\u65f6\u8bbe\u7f6e\n    Rexxar.setOkHttpClient(new OkHttpClient().newBuilder()\n            .retryOnConnectionFailure(true)\n            .addNetworkInterceptor(new AuthInterceptor())\n            .build());\n```\n\n## \u9ad8\u7ea7\u4f7f\u7528\n\n### native\u8c03\u7528js\u65b9\u6cd5\n\n```\n\n    // \u65b9\u6cd5\u540d\n    RexxarWebView.callFunction(String functionName)\n    \n    // \u65b9\u6cd5\u540d\u548cjson\u6570\u636e\n    RexxarWebView.callFunction(String functionName, String jsonString)\n    \n```\n  \n## Partial RexxarWebView\n\n\u5982\u679c\uff0c\u4f60\u53d1\u73b0\u4e00\u4e2a\u9875\u9762\u65e0\u6cd5\u5168\u90e8\u4f7f\u7528 Rexxar \u5b9e\u73b0\u3002\u4f60\u53ef\u4ee5\u5728\u4e00\u4e2a\u539f\u751f\u9875\u9762\u5185\u5185\u5d4c\u4e00\u4e2a `RexxarWebView`\uff0c\u90e8\u5206\u529f\u80fd\u4f7f\u7528\u539f\u751f\u5b9e\u73b0\uff0c\u53e6\u4e00\u90e8\u5206\u529f\u80fd\u4f7f\u7528 Rexxar \u5b9e\u73b0\u3002\n\n\nDemo \u4e2d\u7684 PartialRexxarViewController \u7ed9\u51fa\u4e86\u4e00\u4e2a\u793a\u4f8b\u3002\n\n\n\n## License\n\nRexxar is released under the MIT license. See [LICENSE](LICENSE) for details.\n"
 },
 {
  "repo": "KaptainWutax/SeedCracker",
  "language": "Java",
  "readme_contents": "# SeedCracker [![Github All Releases](https://img.shields.io/github/downloads/KaptainWutax/SeedCracker/total.svg)]()\n\n## Installation\n\n ### Vanilla Launcher\n\n  Download and install the [fabric mod loader](https://fabricmc.net/use/).\n\n ### MultiMC\n\n  Add a new minecraft instance and press \"Install Fabric\" in the instance options.\n\n\n  Then download the lastest [release](https://github.com/KaptainWutax/SeedCracker/releases) of SeedCracker and put the `.jar` file    in your mods directory, either `%appdata%/.minecraft/mods/` folder for the vanilla launcher or your own MultiMC instance folder.\n\n## Usage\n\n  Run minecraft with the mod installed and run around in the world. Once the mod has collected enough data, it will start the cracking process automatically and output the seed in chat. For the process to start, the amount of data that needs to be collected varies depending on the type of feature. `/seed data bits` can be used to see how much progress has been done. \n  \n  ### Supported Structures\n    - Ocean Monument\n    - End City\n    - Buried Treasure\n    - Desert Pyramid\n    - Jungle Temple\n    - Swamp Hut\n    - Shipwreck\n  \n  ### Supported Decorators\n    - Dungeon\n    - End Gateway\n    - Desert Well\n    - Emerald Ore\n\n## Commands\n\n  The command prefix for this mod is /seed.\n  \n  ### Render Command  \n  -`/seed render outlines <ON/OFF/XRAY>`\n    \n  This command only affects the renderer feedback. The default value is 'XRAY' and highlights data through blocks. You can set    the render mod to 'ON' for more standard rendering. \n  \n  ### Finder Command\n  -`/seed finder type <FEATURE_TYPE> (ON/OFF)`\n  \n  -`/seed finder category (BIOMES/ORES/OTHERS/STRUCTURES) (ON/OFF)`\n  \n  This command is used to disable finders in case you are aware the data is wrong. For example, a map generated in 1.14 has different decorators and would require you to disable them while going through those chunks.\n\n  ### Data Command\n  - `/seed data clear`\n  \n  Clears all the collected data without requiring a relog. This is useful for multi-world servers.\n  \n  - `/seed data bits`\n  \n  Display how many bits of information have been collected. Even though this is an approximate, it serves as a good basis to guess when the brute-forcing should start.\n  \n  ### Cracker Command\n  - `/seed cracker <ON/OFF>`\n \n  Enables or disables the mod completely. Unlike the other commands, this one is persistent across reloads.\n  \n## Video Tutorial\n\nhttps://youtu.be/1ChmLi9og8Q\n\n## Upcoming Features\n\nA list of features I have on my mind... they won't necessarily be implemented in this order if at all.\n\n    - SHA2 brute-forcing, auxiliary to biomes search. /implemented\n    - Dungeon floor cracker, fast lattice reversal. /implemented\n    - Stronghold portal room cracker. (alternative to dungeon floor?)\n    - Faster brute-forcing by reorganizing located features list. /implemented\n    - End and nether biome finders. (nether would mostly be in preparation for 1.16) /implemented\n\n## Setting up the Workspace\n\n-Clone the repository.\n\n-Run `gradlew genSources <idea|eclipse>`.\n\n## Building the Mod\n\n-Update the version in `build.gradle` and `fabric.mod.json`.\n\n-Run `gradlew build`.\n \n## Contributors\n\n[KaptainWutax](https://github.com/KaptainWutax) - Author\n\n[neil](https://www.youtube.com/watch?v=aUuPSZVPH8E) - Video Tutorial\n\n[Nekzuris](https://github.com/Nekzuris) - README\n"
 },
 {
  "repo": "DantSu/ESCPOS-ThermalPrinter-Android",
  "language": "Java",
  "readme_contents": "[![Jitpack package repository - ESCPOS-ThermalPrinter-Android v3.2.0](https://jitpack.io/v/DantSu/ESCPOS-ThermalPrinter-Android.svg)](https://jitpack.io/#DantSu/ESCPOS-ThermalPrinter-Android/3.2.0)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n# Android library for ESC/POS Thermal Printer\n\nUseful library to help Android developers to print with (Bluetooth, TCP, USB) ESC/POS thermal printers.\n\n## \u2728 Supporting\n\n\u2b50 Star this repository to support this project. You will contribute to increase the visibility of this library \ud83d\ude42\n\n## Table of contents\n\n- [Android version](#android-version)\n- [Tested printers](#tested-printers)\n- [Test It !](#test-it-)\n- [Installation](#installation)\n- [Bluetooth](#bluetooth)\n  - [Bluetooth permission](#bluetooth-permission)\n  - [Bluetooth code example](#bluetooth-code-example)\n- [TCP](#tcp)\n  - [TCP permission](#tcp-permission)\n  - [TCP code example](#tcp-code-example)\n- [USB](#usb)\n  - [USB permission](#usb-permission)\n  - [USB code example](#usb-code-example)\n- [Charset encoding](#charset-encoding)\n- [Formatted text : syntax guide](#formatted-text--syntax-guide)\n- [Class list](#class-list)\n  - [BluetoothPrintersConnections](#user-content-class--comdantsuescposprinterconnectionbluetoothbluetoothprintersconnections)\n  - [UsbPrintersConnections](#user-content-class--comdantsuescposprinterconnectionusbusbprintersconnections)\n  - [EscPosPrinter](#user-content-class--comdantsuescposprinterescposprinter)\n  - [PrinterTextParserImg](#user-content-class--comdantsuescposprintertextparserprintertextparserimg)\n  - [EscPosCharsetEncoding](#user-content-class--comdantsuescposprinterescposcharsetencoding)\n- [Projects using this library](#projects-using-this-library)\n- [Contributing](#contributing)\n\n\n## Android version\n\nDeveloped for SDK version 16 (Android 4.1 Jelly Bean) and above.\n\n\n## Tested printers\n\n1. [HOIN HOP H58 Thermal Printer ESC/POS](https://www.gearbest.com/printers/pp_662658.html).\n2. [XPRINTER XP-P300](https://xprinter.vn/xprinter-xp-p300-may-in-hoa-don-di-dong-bluetooth/).\n3. [MUNBYN IMP001](https://www.munbyn.com/collections/portable-receipt-printer/products/58mm-bluetooth-thermal-printer-imp001).\n4. [JP-Q2 POS Terminal PDA](https://www.aliexpress.com/item/32971775060.html) (Embedded printer is configured as Bluetooth device)\n5. [MUNBYN ITPP047](https://www.munbyn.com/products/munbyn-itpp047-wifi-thermal-printer) (tested over USB)\n\n## Test it !\n\nTo test this library, it's pretty simple !\n\n- Create a directory and open a terminal inside\n- Run `git clone https://github.com/DantSu/ESCPOS-ThermalPrinter-Android.git .`\n- Open the directory with Android Studio\n- Test it !\n\n## Installation\n\n**Step 1.** Add the [JitPack](https://jitpack.io/#DantSu/ESCPOS-ThermalPrinter-Android/3.2.0) repository to your build file. Add it in your root `/build.gradle` at the end of repositories:\n\n```\nallprojects {\n    repositories {\n        ...\n        maven { url 'https://jitpack.io' }\n    }\n}\n```\n\n**Step 2.** Add the dependency in `/app/build.gradle` :\n\n```\ndependencies {\n    ...\n    implementation 'com.github.DantSu:ESCPOS-ThermalPrinter-Android:3.2.0'\n}\n```\n\n## Bluetooth\n\n### Bluetooth permission\n\nBe sure to have `<uses-permission android:name=\"android.permission.BLUETOOTH\" />`, `<uses-permission android:name=\"android.permission.BLUETOOTH_ADMIN\" />`, `<uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\" />`, `<uses-permission android:name=\"android.permission.BLUETOOTH_SCAN\" />` in your `AndroidMenifest.xml`.\n\nAlso, you have to check the bluetooth permission in your app like this :\n\n```java\nif (ContextCompat.checkSelfPermission(this, Manifest.permission.BLUETOOTH) != PackageManager.PERMISSION_GRANTED) {\n    ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.BLUETOOTH}, MainActivity.PERMISSION_BLUETOOTH);\n} else if (ContextCompat.checkSelfPermission(this, Manifest.permission.BLUETOOTH_ADMIN) != PackageManager.PERMISSION_GRANTED) {\n    ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.BLUETOOTH_ADMIN}, MainActivity.PERMISSION_BLUETOOTH_ADMIN);\n} else if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.S && ContextCompat.checkSelfPermission(this, Manifest.permission.BLUETOOTH_CONNECT) != PackageManager.PERMISSION_GRANTED) {\n    ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.BLUETOOTH_CONNECT}, MainActivity.PERMISSION_BLUETOOTH_CONNECT);\n} else if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.S && ContextCompat.checkSelfPermission(this, Manifest.permission.BLUETOOTH_SCAN) != PackageManager.PERMISSION_GRANTED) {\n    ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.BLUETOOTH_SCAN}, MainActivity.PERMISSION_BLUETOOTH_SCAN);\n} else {\n    // Your code HERE\n}\n```\n\n### Bluetooth code example\n\nThe code below is an example to write in your activity :\n\n```java\nEscPosPrinter printer = new EscPosPrinter(BluetoothPrintersConnections.selectFirstPaired(), 203, 48f, 32);\nprinter\n    .printFormattedText(\n        \"[C]<img>\" + PrinterTextParserImg.bitmapToHexadecimalString(printer, this.getApplicationContext().getResources().getDrawableForDensity(R.drawable.logo, DisplayMetrics.DENSITY_MEDIUM))+\"</img>\\n\" +\n        \"[L]\\n\" +\n        \"[C]<u><font size='big'>ORDER N\u00b0045</font></u>\\n\" +\n        \"[L]\\n\" +\n        \"[C]================================\\n\" +\n        \"[L]\\n\" +\n        \"[L]<b>BEAUTIFUL SHIRT</b>[R]9.99e\\n\" +\n        \"[L]  + Size : S\\n\" +\n        \"[L]\\n\" +\n        \"[L]<b>AWESOME HAT</b>[R]24.99e\\n\" +\n        \"[L]  + Size : 57/58\\n\" +\n        \"[L]\\n\" +\n        \"[C]--------------------------------\\n\" +\n        \"[R]TOTAL PRICE :[R]34.98e\\n\" +\n        \"[R]TAX :[R]4.23e\\n\" +\n        \"[L]\\n\" +\n        \"[C]================================\\n\" +\n        \"[L]\\n\" +\n        \"[L]<font size='tall'>Customer :</font>\\n\" +\n        \"[L]Raymond DUPONT\\n\" +\n        \"[L]5 rue des girafes\\n\" +\n        \"[L]31547 PERPETES\\n\" +\n        \"[L]Tel : +33801201456\\n\" +\n        \"[L]\\n\" +\n        \"[C]<barcode type='ean13' height='10'>831254784551</barcode>\\n\" +\n        \"[C]<qrcode size='20'>http://www.developpeur-web.dantsu.com/</qrcode>\"\n    );\n```\n\nBelow a picture of the receipt printed with the code above :\n\n![Example of a printed receipt](http://www.developpeur-web.dantsu.com/files/librairie/receipt-thermal-printer.png?1)\n\n## TCP\n\n### TCP permission\n\nBe sure to have `<uses-permission android:name=\"android.permission.INTERNET\"/>` in your `AndroidMenifest.xml`.\n\n### TCP code example\n\nThe code below is an example to write in your activity :\n\n```java\nnew Thread(new Runnable() {\n    public void run() {\n        try {\n            EscPosPrinter printer = new EscPosPrinter(new TcpConnection(\"192.168.1.3\", 9300, 15), 203, 48f, 32);\n            printer\n                .printFormattedText(\n                    \"[C]<img>\" + PrinterTextParserImg.bitmapToHexadecimalString(printer, getApplicationContext().getResources().getDrawableForDensity(R.drawable.logo, DisplayMetrics.DENSITY_MEDIUM)) + \"</img>\\n\" +\n                    \"[L]\\n\" +\n                    \"[C]<u><font size='big'>ORDER N\u00b0045</font></u>\\n\" +\n                    \"[L]\\n\" +\n                    \"[C]================================\\n\" +\n                    \"[L]\\n\" +\n                    \"[L]<b>BEAUTIFUL SHIRT</b>[R]9.99e\\n\" +\n                    \"[L]  + Size : S\\n\" +\n                    \"[L]\\n\" +\n                    \"[L]<b>AWESOME HAT</b>[R]24.99e\\n\" +\n                    \"[L]  + Size : 57/58\\n\" +\n                    \"[L]\\n\" +\n                    \"[C]--------------------------------\\n\" +\n                    \"[R]TOTAL PRICE :[R]34.98e\\n\" +\n                    \"[R]TAX :[R]4.23e\\n\" +\n                    \"[L]\\n\" +\n                    \"[C]================================\\n\" +\n                    \"[L]\\n\" +\n                    \"[L]<font size='tall'>Customer :</font>\\n\" +\n                    \"[L]Raymond DUPONT\\n\" +\n                    \"[L]5 rue des girafes\\n\" +\n                    \"[L]31547 PERPETES\\n\" +\n                    \"[L]Tel : +33801201456\\n\" +\n                    \"[L]\\n\" +\n                    \"[C]<barcode type='ean13' height='10'>831254784551</barcode>\\n\" +\n                    \"[C]<qrcode size='20'>http://www.developpeur-web.dantsu.com/</qrcode>\"\n                );\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}).start();\n```\n\n## USB\n\n### USB permission\n\nBe sure to have `<uses-feature android:name=\"android.hardware.usb.host\" />` in your `AndroidMenifest.xml`.\n\nYou have to check the USB permission in your app like this :\n\n```java\nprivate static final String ACTION_USB_PERMISSION = \"com.android.example.USB_PERMISSION\";\nprivate final BroadcastReceiver usbReceiver = new BroadcastReceiver() {\n    public void onReceive(Context context, Intent intent) {\n        String action = intent.getAction();\n        if (MainActivity.ACTION_USB_PERMISSION.equals(action)) {\n            synchronized (this) {\n                UsbManager usbManager = (UsbManager) getSystemService(Context.USB_SERVICE);\n                UsbDevice usbDevice = (UsbDevice) intent.getParcelableExtra(UsbManager.EXTRA_DEVICE);\n                if (intent.getBooleanExtra(UsbManager.EXTRA_PERMISSION_GRANTED, false)) {\n                    if (usbManager != null && usbDevice != null) {\n                        // YOUR PRINT CODE HERE\n                    }\n                }\n            }\n        }\n    }\n};\n\npublic void printUsb() {\n    UsbConnection usbConnection = UsbPrintersConnections.selectFirstConnected(this);\n    UsbManager usbManager = (UsbManager) this.getSystemService(Context.USB_SERVICE);\n    if (usbConnection != null && usbManager != null) {\n        PendingIntent permissionIntent = PendingIntent.getBroadcast(\n            this,\n            0,\n            new Intent(MainActivity.ACTION_USB_PERMISSION),\n            android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.S ? PendingIntent.FLAG_MUTABLE : 0\n        );\n        IntentFilter filter = new IntentFilter(MainActivity.ACTION_USB_PERMISSION);\n        registerReceiver(this.usbReceiver, filter);\n        usbManager.requestPermission(usbConnection.getDevice(), permissionIntent);\n    }\n}\n```\n\n### USB code example\n\nThe code below is an example to write in your activity :\n\n```java\nEscPosPrinter printer = new EscPosPrinter(new UsbConnection(usbManager, usbDevice), 203, 48f, 32);\nprinter\n    .printFormattedText(\n        \"[C]<img>\" + PrinterTextParserImg.bitmapToHexadecimalString(printer, this.getApplicationContext().getResources().getDrawableForDensity(R.drawable.logo, DisplayMetrics.DENSITY_MEDIUM))+\"</img>\\n\" +\n        \"[L]\\n\" +\n        \"[C]<u><font size='big'>ORDER N\u00b0045</font></u>\\n\" +\n        \"[L]\\n\" +\n        \"[C]================================\\n\" +\n        \"[L]\\n\" +\n        \"[L]<b>BEAUTIFUL SHIRT</b>[R]9.99e\\n\" +\n        \"[L]  + Size : S\\n\" +\n        \"[L]\\n\" +\n        \"[L]<b>AWESOME HAT</b>[R]24.99e\\n\" +\n        \"[L]  + Size : 57/58\\n\" +\n        \"[L]\\n\" +\n        \"[C]--------------------------------\\n\" +\n        \"[R]TOTAL PRICE :[R]34.98e\\n\" +\n        \"[R]TAX :[R]4.23e\\n\" +\n        \"[L]\\n\" +\n        \"[C]================================\\n\" +\n        \"[L]\\n\" +\n        \"[L]<font size='tall'>Customer :</font>\\n\" +\n        \"[L]Raymond DUPONT\\n\" +\n        \"[L]5 rue des girafes\\n\" +\n        \"[L]31547 PERPETES\\n\" +\n        \"[L]Tel : +33801201456\\n\" +\n        \"[L]\\n\" +\n        \"[C]<barcode type='ean13' height='10'>831254784551</barcode>\\n\" +\n        \"[C]<qrcode size='20'>http://www.developpeur-web.dantsu.com/</qrcode>\"\n    );\n```\n\n\n## Charset encoding\n\nTo change charset encoding of the printer, use `EscPosCharsetEncoding` class :\n\n```java\nEscPosPrinter printer = new EscPosPrinter(deviceConnection, 203, 48f, 32, new EscPosCharsetEncoding(\"windows-1252\", 16));\n```\n\n`escPosCharsetId` may change with printer model.\n[Follow this link to find `escPosCharsetId` that works with many printers](https://www.epson-biz.com/modules/ref_escpos/index.php?content_id=32)\n\n## Formatted text : syntax guide\n\n### New line\n\nUse `\\n` to create a new line of text.\n\n### Text alignment and column separation\n\nAdd an alignment tag on a same line of text implicitly create a new column.\n\nColumn alignment tags :\n\n- `[L]` : left side alignment\n- `[C]` : center alignment\n- `[R]` : right side alignment\n\nExample :\n\n- `[L]Some text` : One column aligned to left\n- `[C]Some text` : One column aligned to center\n- `[R]Some text` : One column aligned to right\n- `[L]Some text[L]Some other text` : Two columns aligned to left. `Some other text` starts in the center of the paper.\n- `[L]Some text[R]Some other text` : Two columns, first aligned to left, second aligned to right. `Some other text` is printed at the right of paper.\n- `[L]Some[R]text[R]here` : Three columns.\n- `[L][R]text[R]here` : Three columns. The first is empty but it takes a third of the available space.\n\n### Font\n\n#### Size\n\n`<font></font>` tag allows you to change the font size and color. Default size is `normal` / `black`.\n\n- `<font size='normal'>Some text</font>` : Normal size\n- `<font size='wide'>Some text</font>` : Double width of medium size\n- `<font size='tall'>Some text</font>` : Double height of medium size\n- `<font size='big'>Some text</font>` : Double width and height of medium size\n- `<font size='big-2'>Some text</font>` : 3 x width and height\n- `<font size='big-3'>Some text</font>` : 4 x width and height\n- `<font size='big-4'>Some text</font>` : 5 x width and height\n- `<font size='big-5'>Some text</font>` : 6 x width and height\n- `<font size='big-6'>Some text</font>` : 7 x width and height\n\n- `<font color='black'>Some text</font>` : black text - white background\n- `<font color='bg-black'>Some text</font>` : white text - black background\n- `<font color='red'>Some text</font>` : red text - white background (Not working on all printer)\n- `<font color='bg-red'>Some text</font>` : white text - red background (Not working on all printer)\n\n#### Bold\n\n`<b></b>` tag allows you to change the font weight.\n\n- `<b>Some text</b>`\n\n#### Underline\n\n`<u></u>` tag allows you to underline the text.\n\n- `<u>Some text</u>` text underlined\n- `<u type='double'>Some text</u>` text double-strike (Not working on all printer)\n\n### Image\n\n`<img></img>` tag allows you to print image. Inside the tag you need to write a hexadecimal string of an image.\n\nUse `PrinterTextParserImg.bitmapToHexadecimalString` to convert `Drawable`, `BitmapDrawable` or `Bitmap` to hexadecimal string.\n\n- `<img>`hexadecimal string of an image`</img>`\n\n**\u26a0 WARNING \u26a0** : This tag has several constraints :\n\n- A line that contains `<img></img>` can have only one alignment tag and it must be at the beginning of the line.\n- `<img>` must be directly preceded by nothing or an alignment tag (`[L][C][R]`).\n- `</img>` must be directly followed by a new line `\\n`.\n- You can't write text on a line that contains `<img></img>`.\n- Maximum height of printed image is 256px, If you want to print larger bitmap. Please refer to this solution: [#70](https://github.com/DantSu/ESCPOS-ThermalPrinter-Android/issues/70#issuecomment-714390014)\n\n### Barcode\n\n`<barcode></barcode>` tag allows you to print a barcode. Inside the tag you need to write the code number to print.\n\n- `<barcode>451278452159</barcode>` : **(12 numbers)**  \nPrints a EAN13 barcode (height: 10mm, width: ~70% printer width, text: displayed below).\n- `<barcode type='ean8'>4512784</barcode>` : **(7 numbers)**  \nPrints a EAN8 barcode (height: 10mm, width: ~70% printer width, text: displayed below).\n- `<barcode type='upca' height='20'>4512784521</barcode>` : **(11 numbers)**  \nPrints a UPC-A barcode (height: 20mm, width: ~70% printer width, text: displayed below).\n- `<barcode type='upce' height='25' width='50' text='none'>512789</barcode>` : **(6 numbers)**  \nPrints a UPC-E barcode (height: 25mm, width: ~50mm, text: hidden).\n- `<barcode type='128' width='40' text='above'>DantSu</barcode>` : **(string)**  \nPrints a barcode 128 (height: 10mm, width: ~40mm, text: displayed above).\n\n**\u26a0 WARNING \u26a0** : This tag has several constraints :\n\n- A line that contains `<barcode></barcode>` can have only one alignment tag and it must be at the beginning of the line.\n- `<barcode>` must be directly preceded by nothing or an alignment tag (`[L][C][R]`).\n- `</barcode>` must be directly followed by a new line `\\n`.\n- You can't write text on a line that contains `<barcode></barcode>`.\n\n### QR Code\n\n`<qrcode></qrcode>` tag allows you to print a QR code. Inside the tag you need to write the QR code data.\n\n- `<qrcode>http://www.developpeur-web.dantsu.com/</qrcode>` :\nPrints a QR code with a width and height of 20 millimeters.\n- `<qrcode size='25'>123456789</qrcode>` :\nPrints a QR code with a width and height of 25 millimeters.\n\n**\u26a0 WARNING \u26a0** : This tag has several constraints :\n\n- A line that contains `<qrcode></qrcode>` can have only one alignment tag and it must be at the beginning of the line.\n- `<qrcode>` must be directly preceded by nothing or an alignment tag (`[L][C][R]`).\n- `</qrcode>` must be directly followed by a new line `\\n`.\n- You can't write text on a line that contains `<qrcode></qrcode>`.\n\n## Class list\n\n### Class : `com.dantsu.escposprinter.connection.bluetooth.BluetoothPrintersConnections`\n\n#### **Static** Method : `selectFirstPaired()`\nEasy way to get the first bluetooth printer paired / connected.\n- **return** `BluetoothConnection`\n\n#### Method : `getList()`\nGet a list of bluetooth printers.\n- **return** `BluetoothConnection[]`\n\n\u26a0\ufe0f If the arrray returned by `getList()` does not contain you printer or if `selectFirstPaired()` does not return your printer. Read this issue : https://github.com/DantSu/ESCPOS-ThermalPrinter-Android/issues/80#issuecomment-729759832\n\n### Class : `com.dantsu.escposprinter.connection.tcp.TcpConnection`\n\n#### Constructor : `TcpConnection(String address, int port[, int timeout])`\n- **param** `String address` : Targeted ip address\n- **param** `int port` : Targeted tcp port\n- **param** `int timeout` *(optional)* : Connection timeout (default : 30)\n\n### Class : `com.dantsu.escposprinter.connection.usb.UsbPrintersConnections`\n\n#### **Static** Method : `selectFirstConnected()`\nEasy way to get the first USB printer connected.\n- **return** `UsbConnection`\n\n#### Method : `getList()`\nGet a list of USB printers.\n- **return** `UsbConnection[]`\n\n### Class : `com.dantsu.escposprinter.EscPosPrinter`\n\n#### Constructor : `EscPosPrinter(DeviceConnection printer, int printerDpi, float printingWidthMM, int nbrCharactersPerLine [, EscPosCharsetEncoding charsetEncoding])`\n- **param** `DeviceConnection printer` : Instance of a connected printer\n- **param** `int printerDpi` : DPI of the connected printer\n- **param** `float printerWidthMM` : Printing width in millimeters\n- **param** `int printerNbrCharactersPerLine` : The maximum number of medium sized characters that can be printed on a line.\n- **param** `EscPosCharsetEncoding charsetEncoding` *(optional)* : Set the charset encoding.\n\n#### Method : `disconnectPrinter()`\nClose the connection with the printer.\n- **return** `Printer` : Fluent interface\n\n#### Method : `getNbrCharactersPerLine()`\nGet the maximum number of characters that can be printed on a line.\n- **return** `int`\n\n#### Method : `getPrinterWidthMM()`\nGet the printing width in millimeters\n- **return** `float`\n\n#### Method : `getPrinterDpi()`\nGet the printer DPI\n- **return** `int`\n\n#### Method : `getPrinterWidthPx()`\nGet the printing width in dot\n- **return** `int`\n\n#### Method : `getPrinterCharSizeWidthPx()`\nGet the number of dot that a printed character contain\n- **return** `int`\n\n#### Method : `mmToPx(float mmSize)`\nConvert the mmSize variable from millimeters to dot.\n- **param** `float mmSize` : Distance in millimeters to be converted\n- **return** `int` : Dot size of mmSize.\n\n#### Method : `useEscAsteriskCommand(boolean enable)`\nActive \"ESC *\" command for image printing.\n- **param** `boolean enable` : true to use \"ESC *\", false to use \"GS v 0\"\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedText(String text)`\nPrint a formatted text and feed paper (20 millimeters). Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedTextAndCut(String text)`\nPrint a formatted text, feed paper (20 millimeters) and cut the paper. Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedText(String text, float mmFeedPaper)`\nPrint a formatted text and feed paper (`mmFeedPaper` millimeters). Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **param** `float mmFeedPaper` : Millimeter distance feed paper at the end.\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedTextAndCut(String text, float mmFeedPaper)`\nPrint a formatted text, feed paper (`mmFeedPaper` millimeters) and cut the paper. Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **param** `float mmFeedPaper` : Millimeter distance feed paper at the end.\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedTextAndOpenCashBox(String text, float mmFeedPaper)`\nPrint a formatted text, feed paper (`mmFeedPaper` millimeters), cut the paper and open the cash box. Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **param** `float mmFeedPaper` : Millimeter distance feed paper at the end.\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedText(String text, int dotsFeedPaper)`\nPrint a formatted text and feed paper (`dotsFeedPaper` dots). Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **param** `int dotsFeedPaper` : Distance feed paper at the end.\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedTextAndCut(String text, int dotsFeedPaper)`\nPrint a formatted text, feed paper (`dotsFeedPaper` dots) and cut the paper. Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **param** `int dotsFeedPaper` : Distance feed paper at the end.\n- **return** `Printer` : Fluent interface\n\n#### Method : `printFormattedTextAndOpenCashBox(String text, int dotsFeedPaper)`\nPrint a formatted text, feed paper (`dotsFeedPaper` dots), cut the paper and open the cash box. Read the [\"Formatted Text : Syntax guide\" section](#formatted-text--syntax-guide) for more information about text formatting options.\n- **param** `String text` : Formatted text to be printed.\n- **param** `int dotsFeedPaper` : Distance feed paper at the end.\n- **return** `Printer` : Fluent interface\n\n#### Method : `bitmapToBytes(Bitmap bitmap)`\nConvert Bitmap object to ESC/POS image.\n- **param** `Bitmap bitmap` : Instance of Bitmap\n- **return** `byte[]` : Bytes contain the image in ESC/POS command\n\n### Class : `com.dantsu.escposprinter.textparser.PrinterTextParserImg`\n\n#### **Static** Method : `bitmapToHexadecimalString(Printer printer, Drawable drawable)`\nConvert Drawable instance to a hexadecimal string of the image data.\n- **param** `Printer printer` : A Printer instance that will print the image.\n- **param** `Drawable drawable` : Drawable instance to be converted.\n- **return** `String` : A hexadecimal string of the image data. Empty string if Drawable cannot be cast to BitmapDrawable.\n\n#### **Static** Method : `bitmapToHexadecimalString(Printer printer, BitmapDrawable bitmapDrawable)`\nConvert BitmapDrawable instance to a hexadecimal string of the image data.\n- **param** `Printer printer` : A Printer instance that will print the image.\n- **param** `BitmapDrawable bitmapDrawable` : BitmapDrawable instance to be converted.\n- **return** `String` : A hexadecimal string of the image data.\n\n#### **Static** Method : `bitmapToHexadecimalString(Printer printer, Bitmap bitmap)`\nConvert Bitmap instance to a hexadecimal string of the image data.\n- **param** `Printer printer` : A Printer instance that will print the image.\n- **param** `Bitmap bitmap` : Bitmap instance to be converted.\n- **return** `String` : A hexadecimal string of the image data.\n\n#### **Static** Method : `bytesToHexadecimalString(byte[] bytes)`\nConvert byte array to a hexadecimal string of the image data.\n- **param** `byte[] bytes` : Bytes contain the image in ESC/POS command.\n- **return** `String` : A hexadecimal string of the image data.\n\n#### **Static** Method : `hexadecimalStringToBytes(String hexString)`\nConvert hexadecimal string of the image data to bytes ESC/POS command.\n- **param** `String hexString` : Hexadecimal string of the image data.\n- **return** `byte[]` : Bytes contain the image in ESC/POS command.\n\n### Class : `com.dantsu.escposprinter.EscPosCharsetEncoding`\n\n#### Constructor : `EscPosCharsetEncoding(String charsetName, int escPosCharsetId)`\n- **param** `charsetName` Name of charset encoding (Ex: ISO-8859-1)\n- **param** `escPosCharsetId` Id of charset encoding for your printer (Ex: 6)\n\n## Projects using this library\n\n- [AllInOneYT/react-native-thermal-printer : A React Native bridge](https://github.com/AllInOneYT/react-native-thermal-printer)\n- [paystory-de/thermal-printer-cordova-plugin : A Cordova / Ionic bridge](https://github.com/paystory-de/thermal-printer-cordova-plugin)\n- [asukiaaa/react-native-escpos-android : A React Native bridge](https://github.com/asukiaaa/react-native-escpos-android)\n- [android_bluetooth_printer : A Flutter bridge](https://pub.dev/packages/android_bluetooth_printer)\n\n## Contributing\n\nPlease fork this repository and contribute back using pull requests.\n\nAny contributions, large or small, major features, bug fixes, are welcomed and appreciated but will be thoroughly reviewed\n"
 },
 {
  "repo": "bit4woo/domain_hunter",
  "language": "Java",
  "readme_contents": "[![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badges/)[![MIT Licence](https://badges.frapsoft.com/os/mit/mit.svg?v=103)](https://opensource.org/licenses/mit-license.php)\n\n# #\u66f4\u5f3a\u5927\u7684\u7248\u672c\u8bf7\u770b [https://github.com/bit4woo/domain_hunter_pro](https://github.com/bit4woo/domain_hunter_pro)  \u8be5\u7248\u672c\u540e\u7eed\u53ef\u80fd\u4f1a\u8f83\u5c11\u66f4\u65b0\u4e86\u3002\n\n# A more powerful version, please see [https://github.com/bit4woo/domain_hunter_pro](https://github.com/bit4woo/domain_hunter_pro) , this one perhaps will not be updated.\n\n**author**\n\n[bit4woo](https://github.com/bit4woo)\n\n**domain_hunter**\n\nA Burp Suite extender that try to find *<u>**sub-domains,similar domains and related domains**</u>* of an organization automatically, not only domain.\nSome times similar domain and related domains give you surprise^_^. that's why I care about it.\n\n**video(\u89c6\u9891\u6559\u7a0b)**\n\nhttps://www.bilibili.com/video/BV1Jt4y1U7YG/\n\n**usage**\n\n1. download this burp extender from [here](https://github.com/bit4woo/domain_hunter/releases).\n2. add it to burp suite. you will see a new tab named \u201cDomain Hunter\u201d, if  no error encountered. \n3. visit your target website(or App) with burp proxy enabled, ensure burp recorded http or https traffic of your target.\n4. you can just switch to the \"domain hunter\" tab, input the domain that you want to search and click \"Search\" button.\n5. or you can  run \"Crawl\" firstly to try to find more sub-domains and similar domains. \n\n![usage](doc/usage.gif)\n\n**screenshot**\n\n![domain-hunter-v1.1](doc/domain-hunter-v1.2.png)\n\n**change log**\n\n2017-07-28: Add a function to crawl all known subdomains; fix some bug.\n\n2018-07-06: Add the ability to get related domains by get SANs object of certification.  \n\n2018-08-03: Use thread to speed up get related-domains.\n\n2018-09-18: Optimize some steps to reduce memory usage.\n\n2018-09-19: Update getSANs() method to void get domains of CDN provider.\n\n2018-09-20: Update logic of getting possible https URLs that may contain related-domains\n\n2018-09-21: Update logic of \"includeInScope\" and \"sendToSpider\" to reduces UI action time\n\n2018-09-29: Add Upload function to support  upload result to your site or system\n\n2018-10-30: Big Change: try to find sub-domains, similar domains , related domains of an organization(enterprise), not only a domain.\n\n2018-11-01: Add \"Add to domain hunter\"  menu  in site map tree.\n\n2019-07-06: Use multiple thread to improve search speed. Use regex to find more domain in every response.\n\n**xmind of domain collection**\n\n![xmind](doc/xmind.png)\n\n**Burp\u63d2\u4ef6\u5fae\u4fe1\u4ea4\u6d41\u7fa4**\uff1a\n\n![wechat_group](doc/wechat_group.jpg)\n"
 },
 {
  "repo": "GDGAhmedabad/Awesome-Learning-Resources",
  "language": "Java",
  "readme_contents": "# Learning Resources\n\nAn initiaitve by GDG Ahmedabad, for community and by community!\n\nWe at GDG Ahmedabad are getting different queries (like I want to learn android development, where to start?, I want to integrate App Engine, how can start with it?) from our community members, so here we want to prepare a repository with all the useful resources/materials for every technologies that there are in market.\n\nYou can call this repository as \"Technology Gold mine\" :)\n\n## Table of Contents\n\n* [Android](android)\n\t* [Blogs](android/blogs.md)\n\t* [Books](android/books.md)\n\t* [Courses](android/courses.md)\n\t* [Experts To Follow](android/experts_to_follow.md)\n\t* [Podcasts](android/podcasts.md)\n\t* [Tips And Tricks](android/tips_and_tricks.md)\n\t* [Useful Links](android/Useful_links.md)\n\t* [Video Tutorials](android/video_tutorials.md)\n* [AngularJS](AngularJS)\n\t* [Blogs](AngularJS/blogs.md)\n\t* [Books](AngularJS/books.md)\n\t* [Courses](AngularJS/courses.md)\n\t* [Experts To Follow](AngularJS/experts_to_follow.md)\n\t* [Useful Links](AngularJS/Useful_links.md)\n\t* [Video Tutorials](AngularJS/video_tutorials.md)\n* [BackboneJS](BackboneJS)\n\t* [Blogs](BackboneJS/blogs.md)\n\t* [Books](BackboneJS/books.md)\n\t* [Courses](BackboneJS/courses.md)\n\t* [Experts To Follow](BackboneJS/experts_to_follow.md)\n\t* [Useful Links](BackboneJS/Useful_links.md)\n\t* [Video Tutorials](BackboneJS/video_tutorials.md)\n* [BigData](BigData)\n\t* [Bigdatatutorial](BigData/BigDataTutorial.md)\n\t* [Books](BigData/books.md)\n* [C-C++](C-C++)\n\t* [Blogs](C-C++/blogs.md)\n\t* [Books](C-C++/books.md)\n\t* [Courses](C-C++/courses.md)\n\t* [Experts To Follow](C-C++/experts_to_follow.md)\n\t* [Tools](C-C++/Tools.md)\n\t* [Useful Links](C-C++/Useful_links.md)\n\t* [Video Tutorials](C-C++/video_tutorials.md)\n* [Data Structures and Algorithm](Data%20Structures%20and%20Algorithm)\n\t* [Books](Data%20Structures%20and%20Algorithm/books.md)\n\t* [Useful Links](Data%20Structures%20and%20Algorithm/Useful_links.md)\n\t* [Video Tutorials](Data%20Structures%20and%20Algorithm/video_tutorials.md)\n* [Docker](Docker)\n\t* [Blogs](Docker/blogs.md)\n\t* [Books](Docker/books.md)\n\t* [Courses](Docker/courses.md)\n\t* [Experts To Follow](Docker/experts_to_follow.md)\n\t* [Useful Links](Docker/Useful_links.md)\n\t* [Video Tutorials](Docker/video_tutorials.md)\n* [Firebase](Firebase)\n\t* [Useful Links](Firebase/Useful_links.md)\n* [Flutter](Flutter)\n\t* [Blogs](Flutter/blogs.md)\n\t* [Books](Flutter/books.md)\n\t* [Courses](Flutter/courses.md)\n\t* [Experts To Follow](Flutter/experts_to_follow.md)\n\t* [Useful Links](Flutter/Useful_links.md)\n\t* [Video Tutorials](Flutter/video_tutorials.md)\n* [Git](Git)\n\t* [Blogs](Git/blogs.md)\n\t* [Books](Git/books.md)\n\t* [Courses](Git/courses.md)\n\t* [Experts To Follow](Git/experts_to_follow.md)\n\t* [Useful Links](Git/Useful_links.md)\n\t* [Video Tutorials](Git/video_tutorials.md)\n* [Go](Go)\n\t* [Blogs](Go/blogs.md)\n\t* [Books](Go/books.md)\n\t* [Courses](Go/courses.md)\n\t* [Experts To Follow](Go/experts_to_follow.md)\n\t* [Useful Links](Go/Useful_links.md)\n\t* [Video Tutorials](Go/video_tutorials.md)\n* [Google Cloud Platform](GoogleCloudPlatform)\n\t* [Blogs](GoogleCloudPlatform/blogs.md)\n\t* [Books](GoogleCloudPlatform/books.md)\n\t* [Courses](GoogleCloudPlatform/courses.md)\n\t* [Experts To Follow](GoogleCloudPlatform/experts_to_follow.md)\n\t* [Useful Links](GoogleCloudPlatform/Useful_links.md)\n\t* [Video Tutorials](GoogleCloudPlatform/video_tutorials.md)\n* [Gradle](Gradle)\n\t* [Useful Links](Gradle/Useful_links.md)\n* [HTML5](HTML5)\n\t* [Blogs](HTML5/blogs.md)\n\t* [Books](HTML5/books.md)\n\t* [Courses](HTML5/courses.md)\n\t* [Experts To Follow](HTML5/experts_to_follow.md)\n\t* [Useful Links](HTML5/Useful_links.md)\n\t* [Video Tutorials](HTML5/video_tutorials.md)\n* [iOS](iOS)\n\t* [Blogs](iOS/blogs.md)\n\t* [Books](iOS/books.md)\n\t* [Courses](iOS/courses.md)\n\t* [Experts To Follow](iOS/experts_to_follow.md)\n\t* [Useful Links](iOS/Useful_links.md)\n\t* [Video Tutorials](iOS/video_tutorials.md)\n* [Java](Java)\n\t* [Core Java](Java/Core%20Java)\n\t* [Links Of Ide To Build Java Applications](Java/Links%20of%20IDE%20to%20build%20Java%20Applications)\n\t* [Video Tutorials](Java/video_tutorials.md)\n* [JSP and Servlets](JSP%20and%20Servlets)\n\t* [Video Tutorials](JSP%20and%20Servlets/video_tutorials.md)\n* [Kotlin](Kotlin)\n\t* [Blogs](Kotlin/blogs.md)\n\t* [Books](Kotlin/books.md)\n\t* [Courses](Kotlin/courses.md)\n\t* [Experts To Follow](Kotlin/experts_to_follow.md)\n\t* [Useful Links](Kotlin/Useful_links.md)\n\t* [Video Tutorials](Kotlin/video_tutorials.md)\n* [Laravel](Laravel)\n\t* [Blogs](Laravel/blogs.md)\n\t* [Books](Laravel/books.md)\n\t* [Courses](Laravel/courses.md)\n\t* [Experts To Follow](Laravel/experts_to_follow.md)\n\t* [Useful Links](Laravel/Useful_links.md)\n\t* [Video Tutorials](Laravel/video_tutorials.md)\n* [Linux](Linux)\n\t* [Blogs](Linux/blogs.md)\n\t* [Books](Linux/books.md)\n\t* [Courses](Linux/courses.md)\n\t* [Experts To Follow](Linux/experts_to_follow.md)\n\t* [Useful Links](Linux/Useful_links.md)\n\t* [Video Tutorials](Linux/video_tutorials.md)\n* [Machine Learning](Machine%20Learning)\n\t* [Blogs](Machine%20Learning/blogs.md)\n\t* [Books](Machine%20Learning/books.md)\n\t* [Courses](Machine%20Learning/courses.md)\n\t* [Experts To Follow](Machine%20Learning/experts_to_follow.md)\n\t* [Podcasts](Machine%20Learning/podcasts.md)\n\t* [Tensorflow Examples](Machine%20Learning/TensorflowExamples)\n* [Magento](Magento)\n\t* [Blogs](Magento/blogs.md)\n\t* [Books](Magento/books.md)\n\t* [Courses](Magento/courses.md)\n\t* [Experts To Follow](Magento/experts_to_follow.md)\n\t* [Useful Links](Magento/Useful_links.md)\n\t* [Video Tutorials](Magento/video_tutorials.md)\n* [Maven](Maven)\n\t* [Video Tutorials](Maven/video_tutorials.md)\n* [Meteor](Meteor)\n\t* [Blogs](Meteor/blogs.md)\n\t* [Books](Meteor/books.md)\n\t* [Courses](Meteor/courses.md)\n\t* [Experts To Follow](Meteor/experts_to_follow.md)\n\t* [Useful Links](Meteor/Useful_links.md)\n\t* [Video Tutorials](Meteor/video_tutorials.md)\n* [NodeJS](Node.Js)\n\t* [Blogs](Node.Js/blogs.md)\n\t* [Books](Node.Js/books.md)\n\t* [Courses](Node.Js/courses.md)\n\t* [Important Links](Node.Js/Important_links.md)\n\t* [Social Experts](Node.Js/Social_Experts.md)\n\t* [Tools](Node.Js/Tools.md)\n\t* [Videotutorials](Node.Js/VideoTutorials.md)\n* [PHP](PHP)\n\t* [Blogs](PHP/blogs.md)\n\t* [Books](PHP/books.md)\n\t* [Courses](PHP/courses.md)\n\t* [Experts To Follow](PHP/experts_to_follow.md)\n\t* [Useful Links](PHP/Useful_links.md)\n\t* [Video Tutorials](PHP/video_tutorials.md)\n* [Projects](Projects)\n\t* [Readme](Projects/README.md)\n* [Python](Python)\n\t* [Blogs](Python/blogs.md)\n\t* [Books](Python/books.md)\n\t* [Courses](Python/courses.md)\n\t* [Experts To Follow](Python/experts_to_follow.md)\n\t* [Podcast](Python/Podcast.md)\n\t* [Tools](Python/Tools.md)\n\t* [Useful Links](Python/Useful_links.md)\n\t* [Video Tutorials](Python/video_tutorials.md)\n* [R](R)\n\t* [Books](R/books.md)\n\t* [Useful Links](R/Useful_links.md)\n* [Rails](Rails)\n\t* [Books](Rails/books.md)\n\t* [Courses](Rails/courses.md)\n\t* [Experts To Follow](Rails/experts_to_follow.md)\n\t* [Tutorials](Rails/tutorials.md)\n\t* [Useful Links](Rails/useful_links.md)\n* [Ruby](Ruby)\n\t* [Blogs](Ruby/blogs.md)\n\t* [Books](Ruby/books.md)\n\t* [Podcast](Ruby/Podcast.md)\n\t* [Tutorials](Ruby/tutorials.md)\n\t* [Useful Links](Ruby/useful_links.md)\n* [Rust](Rust)\n  * [Blogs](Rust/blogs.md)\n\t* [Books](Rust/books.md)\n\t* [Useful Links](Rust/useful_links.md)\n* [RxJava](RxJava)\n\t* [Blogs](RxJava/blogs.md)\n\t* [Books](RxJava/books.md)\n\t* [Courses](RxJava/courses.md)\n\t* [Experts To Follow](RxJava/experts_to_follow.md)\n\t* [Useful Links](RxJava/Useful_links.md)\n\t* [Video Tutorials](RxJava/video_tutorials.md)\n* [UX-Design](UX-Design)\n\t* [Ux Blogs & Magazines](UX-Design/UX%20Blogs%20&%20Magazines.md)\n\t* [Ux Newsletters](UX-Design/UX%20Newsletters.md)\n* [Wordpress](Wordpress)\n\t* [Blogs](Wordpress/blogs.md)\n\t* [Books](Wordpress/books.md)\n\t* [Courses](Wordpress/courses.md)\n\t* [Experts To Follow](Wordpress/experts_to_follow.md)\n\t* [Useful Links](Wordpress/Useful_links.md)\n\t* [Video Tutorials](Wordpress/video_tutorials.md)\n\n### Contribution\nContribution from each one of you matters a lot, let's contribute and create a solid repository. We request you please read [Contribution guidelines](https://github.com/GDGAhmedabad/Learning-Resources/blob/master/CONTRIBUTING.md) before you contribute, to avoid further changes and commits!\n\n### Guidelines for contributing\n\n* **Books** Original publisher URL of any book, it can be by amazon, Oreilly, head first or any other publisher. Please don't share PDF/HTML/ePub file shared over any free e-book site or drive link or dropbox link.\n* **Courses** A course is a learning material which is not a book and where there is no interactive tool embedded in the site.\n* **Interactive Tutorials** An interactive website which lets the user type code or commands and evaluates the result.\n* **Tools** It's kind a software which are helpful in writing,debugging,and testing code.\n* **Video tutorials** are single topic at single time.\n* **Experts** are individual, who holds the reputation in respected area.\n* **Blogs** are something which covers personal experience, they are neither tutorial or website. EX:- My personal experience with Android APK optimization, performance improvements and code optimization.\n* **Useful links** are URL's which belongs to item official guide, documentation, discussion channel, forum, blogs etc.\n"
 },
 {
  "repo": "keets2012/Auth-service",
  "language": "Java",
  "readme_contents": "[![Build Status](https://travis-ci.org/keets2012/Auth-service.svg?branch=master)](https://travis-ci.org/keets2012/Auth-service)\n[![codebeat badge](https://codebeat.co/badges/4594f615-67af-46b0-9adf-b69b476dc250)](https://codebeat.co/projects/github-com-keets2012-auth-service-master)\n![](https://img.shields.io/badge/license-MIT-000000.svg)\n\n## change logs\n\n- 2018.9.1    \n\n\t\u7248\u672c\u5347\u5230`2.0-SNAPSHOT`\uff0c\u6b32\u4f7f\u7528`sb1.5.x`\u7248\u672c\uff0c\u8bf7\u5207\u6362\u5230TAG `1.0-RELEASE`\u3002by [CANGWU](https://github.com/CANGWU)\n\n  Spring Cloud Security \u5347\u7ea7\u5230`Finchley.RELEASE`\uff0cSpring Boot\u75311.5.X\u5347\u7ea7\u52302.0.X\u3002\n\n    ```xml\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-dependencies</artifactId>\n            <version>Finchley.RELEASE</version>\n       </dependency>\n    ```\n\n    ```xml\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.0.RELEASE</version>\n    </parent>\n    ```\n\n## quick start\n\u672c\u6b21\u5bf9\u9879\u76ee\u7ed3\u6784\u8fdb\u884c\u4e86\u66f4\u65b0\uff0ctoken\u7684\u5b58\u50a8\u673a\u5236\u57fa\u4e8eredis\uff0c\u5f53\u7136\u5b58\u50a8\u65b9\u5f0f\u53ef\u4ee5\u81ea\u7531\u5207\u6362\uff0cSpring Security\u63d0\u4f9b\u4e86SPI\u7684\u591a\u79cd\u5b9e\u73b0\u3002\n\n\u5ba2\u6237\u7aef\u7684\u4fe1\u606f\u8fd8\u662f\u57fa\u4e8ejdbc\u5b9e\u73b0\uff0c\u6240\u4ee5\u9700\u8981\u5bfc\u5165\u9879\u76ee\u4e2d\u63d0\u4f9b\u7684\u8868`oauth_client_details` \u3002\n\n\u63a8\u8350\u9996\u5148\u9605\u8bfb\u4e13\u680f\u6587\u7ae0\uff1a[\u8ba4\u8bc1\u9274\u6743\u4e0eAPI\u6743\u9650\u63a7\u5236\u5728\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0](http://blueskykong.com/categories/Security/)\n\n**\u5355\u72ec\u7684\u6574\u5408\u9879\u76ee\u5730\u5740\u4e3a\uff1a   \nGitHub\uff1ahttps://github.com/keets2012/microservice-integration   \n\u6216\u8005 \u7801\u4e91\uff1ahttps://gitee.com/keets/microservice-integration**\n\n### maintainer\n- keets2012\n- CANGWU\n\n### password\u6a21\u5f0f\n\u9879\u76ee\u514b\u9686\u4e4b\u540e\uff1a\n\n1. ~~\u5b89\u88c5\u4e00\u4e0b\uff0c`mvn clean install`~~\n2. \u4fee\u6539Auth\u9879\u76ee\u4e2d\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u5199\u4e86`XXXX`\u7684\u5730\u65b9\uff0c\u66ff\u6362\u6210\u81ea\u5df1\u7684\u5b9e\u9645\u5730\u5740\uff08redis\u548cmysql\uff09\n3. \u6570\u636e\u5e93\u5bfc\u5165\uff0csql\u811a\u672c\u5728\u9879\u76ee\u4e2d\u3002\u521b\u5efaauth\u6570\u636e\u5e93\uff0c\u8fd0\u884cauth.sql\n4. `mvn clean spring-boot:run`\n5. \u5176\u4ed6\u7ec6\u8282\u53c2\u8003\u535a\u5ba2\n6. **\u4f60\u7684star\u662f\u5bf9\u6211\u6700\u597d\u7684\u9f13\u52b1^_^**\n\n\u8fdb\u884c\u8bf7\u6c42\u83b7\u53d6Token\u6388\u6743\uff1a\n![head](http://image.blueskykong.com/login1-header1.png \"\u5934\u90e8\u4fe1\u606f\")\n![form](http://image.blueskykong.com/loginform3.png \"\u8868\u5355\u4fe1\u606f\")\n\n\n\u7b14\u8005\u81ea\u5df1\u8fd0\u884c\u4e86\u7ed3\u679c\u5982\u4e0b\uff1a\n\n```yaml\n{\n    \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1MDkwNzMzMjcsIlgtQU9ITy1Vc2VySWQiOiIxNGY1MmE0OS0yYTgxLTRhMmYtOGI5Mi01ZmU0NzUzZGRmZGEiLCJ1c2VyX25hbWUiOiIxODM2MjkxNjcyNiIsImp0aSI6IjM5NDEzN2I5LTNjZGItNGUyNy04NGRjLWM5YjEyYzk3ZTA4YyIsImNsaWVudF9pZCI6ImZyb250ZW5kIiwic2NvcGUiOlsiYWxsIl19.pGZhGNVECg0b4LB_pYXTTVKjNn8FA5biM04Bhcd-MEE\",\n    \"token_type\": \"bearer\",\n    \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiIxODM2MjkxNjcyNiIsInNjb3BlIjpbImFsbCJdLCJhdGkiOiIzOTQxMzdiOS0zY2RiLTRlMjctODRkYy1jOWIxMmM5N2UwOGMiLCJleHAiOjE1MTE2MjIxMjcsIlgtQU9ITy1Vc2VySWQiOiIxNGY1MmE0OS0yYTgxLTRhMmYtOGI5Mi01ZmU0NzUzZGRmZGEiLCJqdGkiOiJkYTBmOTMxMS1lZjc0LTRiMjQtODViZi04ZTNjNDVhNGEyNzkiLCJjbGllbnRfaWQiOiJmcm9udGVuZCJ9.2MRdqEogAwbesRfj2TKoWhMazItBlpjbQx7dlgfFpHE\",\n    \"expires_in\": 43199,\n    \"scope\": \"all\",\n    \"X-AOHO-UserId\": \"14f52a49-2a81-4a2f-8b92-5fe4753ddfda\",\n    \"jti\": \"394137b9-3cdb-4e27-84dc-c9b12c97e08c\",\n    \"X-AOHO-ClientId\": \"frontend\"\n}\n```\n\nps: \u767b\u5f55\u7684\u7528\u6237\u540d\u5bc6\u7801\u8981\u5728\u8868\u5355\u91cc\u9762\u5199\uff0c\u5185\u5bb9\u968f\u610f\uff0c\u56e0\u4e3a\u5728\u4ee3\u7801\u4e2d\u5df2\u7ecf\u53bb\u6389\u4e86\u5bf9user\u670d\u52a1\u7684\u6821\u9a8c\u3002\n\n### \u6388\u6743\u7801\u6a21\u5f0f\n\u672c\u6b21\u66f4\u65b0\u6dfb\u52a0\u4e86\u5bf9\u6388\u6743\u7801\u6a21\u5f0f\u7684\u4f7f\u7528\n\n\u6388\u6743\u7801\u6a21\u5f0f\u9700\u8981\u7528\u6237\u767b\u5f55\uff0c\u6240\u4ee5\u501f\u52a9\u6d4f\u89c8\u5668\n\n\u9996\u5148\u7ed9\u6570\u636e\u5e93\u8868\u4e2d\u7684`oauth_client_details`\u8868\u4e2d`client_id`\u4e3a`frontend`\u7684\u884c`authorized_grant_types`\u6dfb\u52a0`authorization_code`\uff0c`web_server_redirect_uri`\u8bbe\u7f6e\u4e3a`http://localhost:8080`\u3002\u8868\u793a\u8be5\u5ba2\u6237\u7aef\u5141\u8bb8\u6388\u6743\u7801\u6a21\u5f0f\u4ee5\u53ca\u6388\u6743\u7801\u56de\u8c03\u5730\u5740\u4e3ahttp://localhost:8080\n\n\u6d4f\u89c8\u5668\u8bbf\u95ee\u5730\u5740\n\n```yaml\nhttp://localhost:9000/oauth/authorize?response_type=code&client_id=frontend&\nscope=all&redirect_uri=http://localhost:8080\n```\n\n\n\u8fdb\u5165\u767b\u5f55\u6388\u6743\u9875\u9762\u5e76\u540c\u610f\u6388\u6743\uff0c\u4ece\u56de\u8c03\u5730\u5740\u4e2d\u83b7\u53d6\u6388\u6743\u7801\n\n```yaml\nhttp://localhost:8080/?code=xGjrTm\n```\n\n\u901a\u8fc7\u6388\u6743\u7801\u83b7\u53d6access_token\n\n```yaml\nmethod: post\nurl: http://localhost:9000/oauth/token?grant_type=authorization_code\nheader:\n{\n \u00a0Authorization: Basic ZnJvbnRlbmQ6ZnJvbnRlbmQ=,\n  Content-Type: application/x-www-form-urlencoded\n}\nbody:\n{\n  code: xGjrTm,\n \u00a0redirect_uri: http://localhost:8080\n}\n```\n\n### \u5199\u5728\u6700\u540e\n\n\u9879\u76ee\u6574\u5408\u5982\u679c\u9047\u5230\u95ee\u9898\uff0c\u53ef\u4ee5\u52a0\u5165qq\u7fa4(649932629)\u4ea4\u6d41\u3002\n\n![](http://image.blueskykong.com/qq-chat1.JPG)\n\nps: qq\u4ea4\u6d41\u7fa4\u5df2\u8bbe\u7f6e\u4ed8\u8d39\uff0c\u53ef\u4ee5\u8fc7\u6ee4\u5f88\u591a\u5e7f\u544a\uff0c\u8425\u9020\u9ad8\u8d28\u91cf\u7684\u6280\u672f\u4ea4\u6d41\u7fa4\u3002\n\n*\u6709\u95ee\u9898\u8054\u7cfb aoho002#gmail.com*\n\n### MIT License\n\nCopyright (c) 2018 aoho\n"
 },
 {
  "repo": "premnirmal/Magnet",
  "language": "Java",
  "readme_contents": "[![Build Status](https://circleci.com/gh/premnirmal/Magnet.svg?style=shield)](https://circleci.com/gh/premnirmal/Magnet)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.premnirmal.magnet/library/badge.svg)](http://search.maven.org/#artifactdetails|com.premnirmal.magnet|library|1.2.7|)\n[![Android Weekly](http://img.shields.io/badge/Android%20Weekly-%23112-2CB3E5.svg?style=flat)](http://androidweekly.net/issues/issue-112)\n[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Magnet-brightgreen.svg?style=flat)](https://android-arsenal.com/details/1/1139)\n\n# Magnet\n\nThis library enables you to create a window icon similar to Facebooks chat icon, and also similar to the [Link Bubble](https://play.google.com/store/apps/details?id=com.linkbubble.playstore&hl=en) app.\nSee the demo project for sample implementations.\n\n### Sample play store app:\n\n[![Google play link](google-play-badge.png)](https://play.google.com/store/apps/details?id=com.premnirmal.smoothie)\n\nThe library takes care of all the touching and dragging of the window icon, leaving you with callbacks so you can save your time to\nimplement your app.\n\n![](img/magnet.gif)\n\n## Usage (gradle)\n\n#### Via maven central\n\n``` xml\n<dependency>\n  <groupId>com.premnirmal.magnet</groupId>\n  <artifactId>library</artifactId>\n  <version>1.2.7</version>\n  <type>aar</type>\n</dependency>\n```\n\nAdd the following to your build.gradle\n\n``` groovy\ncompile 'com.premnirmal.magnet:library:1.2.7'\ncompile 'com.facebook.rebound:rebound:0.3.8'\ncompile 'com.tumblr:backboard:0.1.2'\n```\n\n---\n\n## How to create a Magnet\n\n### Use the required permission\n\n``` xml\n<uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"/>\n```\n\n#### Android api level 23+\n\nRequest the permission at runtime in your activity, before calling `Magnet#show()`:\n\n``` java\n  void checkDrawOverlayPermission() {\n    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {\n      if (!Settings.canDrawOverlays(this)) {\n        final Intent intent = new Intent(Settings.ACTION_MANAGE_OVERLAY_PERMISSION,\n            Uri.parse(\"package:\" + getPackageName()));\n        startActivityForResult(intent, REQUEST_CODE);\n      } else {\n        // continue here - permission was granted\n      }\n    } else {\n      // continue here - permission was granted\n    }\n  }\n\n  @Override protected void onActivityResult(int requestCode, int resultCode, Intent data) {\n    if (requestCode == REQUEST_CODE) {\n      if (Settings.canDrawOverlays(this)) {\n        // continue here - permission was granted\n      }\n    }\n}\n```\n\n\n### Use the magnet builder in your Activity or Service\n\n``` java\n\nfinal ImageView iconView = new ImageView(this);\niconView.setImageResource(R.drawable.ic_launcher);\nfinal Magnet magnet = Magnet.newBuilder(this)\n        // a view is required\n        .setIconView(iconView)\n        // all the parameters below are optional\n        .setIconCallback(this)\n        .setHideFactor(0.2f)\n        .setShouldShowRemoveView(true)\n        .setRemoveIconShouldBeResponsive(true)\n        .setRemoveIconResId(R.drawable.ic_close)\n        .setRemoveIconShadow(R.drawable.bottom_shadow)\n        .setShouldStickToWall(true)\n        .setRemoveIconShouldBeResponsive(true)\n        .setInitialPosition(300, 400)\n        .withSpringConfig(springConfig)\n        .build();\nmagnet.show();\n\n        ...\n\nmagnet.setPosition(200, 800); // to manually move the magnet\n\n        ...\n\nmagnet.goToWall(); // to stick the magnet to the wall (only works if shouldStickToXWall or shouldStickToYWall is enabled)\n\n        ...\n\nmagnet.destroy(); // to remove the magnet\n```\n\n### Subclassing\n\nInstantiate your subclass of `Magnet` using the `Builder`\n\n``` java\n\nfinal MyMagnet magnet = new Magnet.Builder<MyMagnet>(MyMagnet.class, context)\n                        .setIconView(iconView)\n                        .build();\n```\n\n\n### Use the callbacks per your needs\n\n``` java\n  @Override public void onFlingAway() {\n    Log.i(TAG, \"onFlingAway\");\n  }\n\n  @Override public void onMove(float x, float y) {\n    Log.i(TAG, \"onMove(\" + x + \",\" + y + \")\");\n  }\n\n  @Override public void onIconClick(View icon, float iconXPose, float iconYPose) {\n    Log.i(TAG, \"onIconClick(..)\");\n  }\n\n  @Override public void onIconDestroyed() {\n    Log.i(TAG, \"onIconDestroyed()\");\n  }\n```\n\n---\n\n## API Requirements\n\n- The minimum supported Android version is Android 4.0 Ice cream sandwich (API Level 14)\n- Requires the permission `android.permission.SYSTEM_ALERT_WINDOW`\n\n## Dependencies\n\n- [rebound](http://facebook.github.io/rebound)\n- [backboard](https://github.com/tumblr/Backboard)\n\n## Contributing\n\nPlease fork this repository and contribute back using [pull requests](https://github.com/premnirmal/Magnet/pulls).\n\n## License\n\n```\nWWWWWW||WWWWWW\n W W W||W W W\n      ||\n    ( OO )__________\n     /  |           \\\n    /o o|    MIT     \\\n    \\___/||_||__||_|| *\n         || ||  || ||\n        _||_|| _||_||\n       (__|__|(__|__|\n\nThe MIT License (MIT)\n\nCopyright (c) 2014 Prem Nirmal\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n\n---\n\n### Author\n[Prem Nirmal](http://premnirmal.me/)\n"
 },
 {
  "repo": "Cleveroad/FireworkyPullToRefresh",
  "language": "Java",
  "readme_contents": "#FireworkyPullToRefresh [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) <img src=\"https://www.cleveroad.com/public/comercial/label-android.svg\" height=\"20\"> <a href=\"https://www.cleveroad.com/?utm_source=github&utm_medium=label&utm_campaign=contacts\"><img src=\"https://www.cleveroad.com/public/comercial/label-cleveroad.svg\" height=\"20\"></a>\n![Header image](/images/header_.jpg)\n\n## Meet Fireworky Pull To Refresh for Android\n\nIt\u2019s time to add some creativity to good old pull to refresh view. You can now literally launch your feed content or search results like a firework! Entertain your app users while they are waiting for refreshment \u2014 it\u2019s easy with Fireworky Pull To Refresh library for Android apps.\n\n![Demo image](/images/demo_.gif)\n#####Check out the animation on the <strong><a target=\"_blank\" href=\"https://youtu.be/sJhBKyvF7i4?list=PLi-FH7__aeiydOwY_1q5I8P2EUSseqUCj\">Fireworky Pull To Refresh for Android on YouTube</a></strong> in HD quality.\n\nWhat will you receive by using the Fireworky Pull To Refresh component in your app? The answer is pretty obvious: original and inspiring design element to generate well-disposed users. So, here you go!\n \n\n[![Awesome](/images/logo-footer.png)](https://www.cleveroad.com/?utm_source=github&utm_medium=label&utm_campaign=contacts)\n\n## Setup and usage ##\n### Installation ###\nby Gradle:\n```groovy\n    compile 'com.cleveroad:fireworkypulltorefresh:1.0.3'\n```\nor just download zip and import module \"fireworky-pull-to-refresh\" to be able to modify the sources.\n### Supported Views ###\n\n* RecyclerView\n* ListView\n* ScrollView\n* NestedScrollView\n\n### How do I get set up? ###\nJust wrap your view:\n\n```XML\n\n<com.cleveroad.pulltorefresh.firework.FireworkyPullToRefreshLayout\n        android:id=\"@+id/pullToRefresh\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\">\n\n        <android.support.v7.widget.RecyclerView\n            android:id=\"@+id/recyclerView\"\n            android:layout_width=\"match_parent\"\n            android:layout_height=\"match_parent\"/>\n\n</com.cleveroad.ptr.FireworkyPullToRefreshLayout>\n```\n\n## Configuration ##\n### Via XML ###\n\n```XML\n\n<com.cleveroad.pulltorefresh.firework.FireworkyPullToRefreshLayout\n        xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n        ...\n        app:ptr_fireworkColors=\"@array/fireworkColors\"\n        app:ptr_background=\"@drawable/background\"\n        app:ptr_rocketAnimDuration=\"1000\">\n```\n|  attribute name | description |\n|---|---|\n| ptr_fireworkColors  | An array of colors which will be used for firework animation |\n| ptr_background  | Background drawable |\n| ptr_backgroundColor | Background color |\n| ptr_rocketAnimDuration  | Rocket flight duration |\n| ptr_fireworkStyle | Fireworks animation style - `classic` (by default) or `modern` |\n### Via Java code ###\n\n```Java\n//use .config() methods:\n\nmPullToRefresh.getConfig().setBackground(backgroundDrawable);\nmPullToRefresh.getConfig().setBackground(backgroundBitmap);\nmPullToRefresh.getConfig().setBackground(R.drawable.background);\nmPullToRefresh.getConfig().setBackgroundColor(Color.BLACK);\nmPullToRefresh.getConfig().setBackgroundColorFromResources(R.color.background);\n\nmPullToRefresh.getConfig().setFireworkColors(colorsIntArray);\nmPullToRefresh.getConfig().setFireworkColors(R.array.fireworkColors);\n\nmPullRefreshView.getConfig().setFireworkStyle(Configuration.FireworkStyle.MODERN);\n\nmPullToRefresh.getConfig().setRocketAnimDuration(1000L);\n\n```\n\n## Animation ##\n### Refreshing callback ###\nJust implement `PullToRefreshView.OnRefreshListener`:\n\n```Java\n\nmPullToRefresh.setOnRefreshListener(new PullToRefreshView.OnRefreshListener() {\n    @Override\n    public void onRefresh() {\n        //refresh your data here        \n    }\n});\n```\n###To start or stop animation:###\n\n```Java\nmPullRefreshView.setRefreshing(isRefreshing);\n```\n\n### Using custom views ###\nFor using custom views just implement `FireworkyPullToRefreshLayout.OnChildScrollUpCallback`:\n```Java\n\nmPullToRefresh.setOnChildScrollUpCallback(new FireworkyPullToRefreshLayout.OnChildScrollUpCallback() {\n    @Override\n    public boolean canChildScrollUp(@NonNull FireworkyPullToRefreshLayout parent, @Nullable View child) {\n        //put your implementation here\n    }\n});\n```\n## Support\nIf you have any questions regarding the use of this tutorial, please contact us for support\nat info@cleveroad.com (email subject: \u00abFireworkyPullToRefresh for Android. Support request.\u00bb)\n<br>or\n<br>Use our contacts:\n<br><a href=\"https://www.cleveroad.com/?utm_source=github&utm_medium=link&utm_campaign=contacts\">Cleveroad.com</a>\n<br><a href=\"https://www.facebook.com/cleveroadinc\">Facebook account</a>\n<br><a href=\"https://twitter.com/CleveroadInc\">Twitter account</a>\n<br><a href=\"https://plus.google.com/+CleveroadInc/\">Google+ account</a>\n\n## License\n\n\n        The MIT License (MIT)\n\n        Copyright (c) 2015-2016 Cleveroad\n\n        Permission is hereby granted, free of charge, to any person obtaining a copy\n        of this software and associated documentation files (the \"Software\"), to deal\n        in the Software without restriction, including without limitation the rights\n        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n        copies of the Software, and to permit persons to whom the Software is\n        furnished to do so, subject to the following conditions:\n\n        The above copyright notice and this permission notice shall be included in all\n        copies or substantial portions of the Software.\n\n        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n        SOFTWARE.\n"
 },
 {
  "repo": "devunwired/custom-touch-examples",
  "language": "Java",
  "readme_contents": "### Mastering Android Touch Examples ###\n\nThis repository contains a sample application demonstrating many of the techniques for doing custom touch event handling discussed in \"Mastering the Android Touch System\".\n\nExamples include:\n\n- Overriding onTouchEvent()\n- Overriding onInterceptTouchEvent()\n- GestureDetector\n- ScaleGestureDetector\n- TouchDelegate\n\nThe sample code is licensed under the MIT Open Source License"
 },
 {
  "repo": "MatthewYork/Colours",
  "language": "Java",
  "readme_contents": "![ScreenShot](https://raw.github.com/MatthewYork/Resources/master/Colours-Android/ColoursBanner.png)\n\nColours is a port of the Colours Library for iOS made by my good friend [**Ben Gordon**](https://github.com/bennyguitar). You can find that project [**here**](https://github.com/bennyguitar/Colours). \n\n## Installation\n\n#### Maven Central\n Colours is available in Maven Central under the groupId <code>com.github.matthewyork</code> and the artifactId <code>ColoursLibrary</code>\n\n Incorporate Colours via Gradle with:\n\n `compile 'com.github.matthewyork:ColoursLibrary:1.0.+@aar'`\n \n#### Manual Installation\n- **Import** the Colours library into your workspace, found in the ColoursLibrary Folder.\n- Right-click on your android **project folder** and click on the project properties.\n- Click on the \"Android\" tab and then \"Add\" under the **Library** section.\n- Select the **ColoursLibrary** project to link it with your project\n\n## Color Palette\n\n<table><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/0.png\" width=\"50\" height=\"50\" alt=\"infoBlueColor\" /></td><td><b>infoBlueColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/1.png\" width=\"50\" height=\"50\" alt=\"successColor\" /></td><td><b>successColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/2.png\" width=\"50\" height=\"50\" alt=\"warningColor\" /></td><td><b>warningColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/3.png\" width=\"50\" height=\"50\" alt=\"dangerColor\" /></td><td><b>dangerColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/4.png\" width=\"50\" height=\"50\" alt=\"antiqueWhiteColor\" /></td><td><b>antiqueWhiteColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/5.png\" width=\"50\" height=\"50\" alt=\"oldLaceColor\" /></td><td><b>oldLaceColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/6.png\" width=\"50\" height=\"50\" alt=\"ivoryColor\" /></td><td><b>ivoryColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/7.png\" width=\"50\" height=\"50\" alt=\"seashellColor\" /></td><td><b>seashellColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/8.png\" width=\"50\" height=\"50\" alt=\"ghostWhiteColor\" /></td><td><b>ghostWhiteColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/9.png\" width=\"50\" height=\"50\" alt=\"snowColor\" /></td><td><b>snowColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/10.png\" width=\"50\" height=\"50\" alt=\"linenColor\" /></td><td><b>linenColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/11.png\" width=\"50\" height=\"50\" alt=\"black25PercentColor\" /></td><td><b>black25PercentColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/12.png\" width=\"50\" height=\"50\" alt=\"black50PercentColor\" /></td><td><b>black50PercentColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/13.png\" width=\"50\" height=\"50\" alt=\"black75PercentColor\" /></td><td><b>black75PercentColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/14.png\" width=\"50\" height=\"50\" alt=\"warmGrayColor\" /></td><td><b>warmGrayColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/15.png\" width=\"50\" height=\"50\" alt=\"coolGrayColor\" /></td><td><b>coolGrayColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/16.png\" width=\"50\" height=\"50\" alt=\"charcoalColor\" /></td><td><b>charcoalColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/17.png\" width=\"50\" height=\"50\" alt=\"tealColor\" /></td><td><b>tealColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/18.png\" width=\"50\" height=\"50\" alt=\"steelBlueColor\" /></td><td><b>steelBlueColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/19.png\" width=\"50\" height=\"50\" alt=\"robinEggColor\" /></td><td><b>robinEggColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/20.png\" width=\"50\" height=\"50\" alt=\"pastelBlueColor\" /></td><td><b>pastelBlueColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/21.png\" width=\"50\" height=\"50\" alt=\"turquoiseColor\" /></td><td><b>turquoiseColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/22.png\" width=\"50\" height=\"50\" alt=\"skyBlueColor\" /></td><td><b>skyBlueColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/23.png\" width=\"50\" height=\"50\" alt=\"indigoColor\" /></td><td><b>indigoColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/24.png\" width=\"50\" height=\"50\" alt=\"denimColor\" /></td><td><b>denimColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/25.png\" width=\"50\" height=\"50\" alt=\"blueberryColor\" /></td><td><b>blueberryColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/26.png\" width=\"50\" height=\"50\" alt=\"cornflowerColor\" /></td><td><b>cornflowerColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/27.png\" width=\"50\" height=\"50\" alt=\"babyBlueColor\" /></td><td><b>babyBlueColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/28.png\" width=\"50\" height=\"50\" alt=\"midnightBlueColor\" /></td><td><b>midnightBlueColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/29.png\" width=\"50\" height=\"50\" alt=\"fadedBlueColor\" /></td><td><b>fadedBlueColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/30.png\" width=\"50\" height=\"50\" alt=\"icebergColor\" /></td><td><b>icebergColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/31.png\" width=\"50\" height=\"50\" alt=\"waveColor\" /></td><td><b>waveColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/32.png\" width=\"50\" height=\"50\" alt=\"emeraldColor\" /></td><td><b>emeraldColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/33.png\" width=\"50\" height=\"50\" alt=\"grassColor\" /></td><td><b>grassColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/34.png\" width=\"50\" height=\"50\" alt=\"pastelGreenColor\" /></td><td><b>pastelGreenColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/35.png\" width=\"50\" height=\"50\" alt=\"seafoamColor\" /></td><td><b>seafoamColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/36.png\" width=\"50\" height=\"50\" alt=\"paleGreenColor\" /></td><td><b>paleGreenColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/37.png\" width=\"50\" height=\"50\" alt=\"cactusGreenColor\" /></td><td><b>cactusGreenColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/38.png\" width=\"50\" height=\"50\" alt=\"chartreuseColor\" /></td><td><b>chartreuseColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/39.png\" width=\"50\" height=\"50\" alt=\"hollyGreenColor\" /></td><td><b>hollyGreenColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/40.png\" width=\"50\" height=\"50\" alt=\"oliveColor\" /></td><td><b>oliveColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/41.png\" width=\"50\" height=\"50\" alt=\"oliveDrabColor\" /></td><td><b>oliveDrabColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/42.png\" width=\"50\" height=\"50\" alt=\"moneyGreenColor\" /></td><td><b>moneyGreenColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/43.png\" width=\"50\" height=\"50\" alt=\"honeydewColor\" /></td><td><b>honeydewColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/44.png\" width=\"50\" height=\"50\" alt=\"limeColor\" /></td><td><b>limeColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/45.png\" width=\"50\" height=\"50\" alt=\"cardTableColor\" /></td><td><b>cardTableColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/46.png\" width=\"50\" height=\"50\" alt=\"salmonColor\" /></td><td><b>salmonColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/47.png\" width=\"50\" height=\"50\" alt=\"brickRedColor\" /></td><td><b>brickRedColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/48.png\" width=\"50\" height=\"50\" alt=\"easterPinkColor\" /></td><td><b>easterPinkColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/49.png\" width=\"50\" height=\"50\" alt=\"grapefruitColor\" /></td><td><b>grapefruitColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/50.png\" width=\"50\" height=\"50\" alt=\"pinkColor\" /></td><td><b>pinkColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/51.png\" width=\"50\" height=\"50\" alt=\"indianRedColor\" /></td><td><b>indianRedColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/52.png\" width=\"50\" height=\"50\" alt=\"strawberryColor\" /></td><td><b>strawberryColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/53.png\" width=\"50\" height=\"50\" alt=\"coralColor\" /></td><td><b>coralColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/54.png\" width=\"50\" height=\"50\" alt=\"maroonColor\" /></td><td><b>maroonColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/55.png\" width=\"50\" height=\"50\" alt=\"watermelonColor\" /></td><td><b>watermelonColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/56.png\" width=\"50\" height=\"50\" alt=\"tomatoColor\" /></td><td><b>tomatoColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/57.png\" width=\"50\" height=\"50\" alt=\"pinkLipstickColor\" /></td><td><b>pinkLipstickColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/58.png\" width=\"50\" height=\"50\" alt=\"paleRoseColor\" /></td><td><b>paleRoseColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/59.png\" width=\"50\" height=\"50\" alt=\"crimsonColor\" /></td><td><b>crimsonColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/60.png\" width=\"50\" height=\"50\" alt=\"eggplantColor\" /></td><td><b>eggplantColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/61.png\" width=\"50\" height=\"50\" alt=\"pastelPurpleColor\" /></td><td><b>pastelPurpleColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/62.png\" width=\"50\" height=\"50\" alt=\"palePurpleColor\" /></td><td><b>palePurpleColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/63.png\" width=\"50\" height=\"50\" alt=\"coolPurpleColor\" /></td><td><b>coolPurpleColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/64.png\" width=\"50\" height=\"50\" alt=\"violetColor\" /></td><td><b>violetColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/65.png\" width=\"50\" height=\"50\" alt=\"plumColor\" /></td><td><b>plumColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/66.png\" width=\"50\" height=\"50\" alt=\"lavenderColor\" /></td><td><b>lavenderColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/67.png\" width=\"50\" height=\"50\" alt=\"raspberryColor\" /></td><td><b>raspberryColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/68.png\" width=\"50\" height=\"50\" alt=\"fuschiaColor\" /></td><td><b>fuschiaColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/69.png\" width=\"50\" height=\"50\" alt=\"grapeColor\" /></td><td><b>grapeColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/70.png\" width=\"50\" height=\"50\" alt=\"periwinkleColor\" /></td><td><b>periwinkleColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/71.png\" width=\"50\" height=\"50\" alt=\"orchidColor\" /></td><td><b>orchidColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/72.png\" width=\"50\" height=\"50\" alt=\"goldenrodColor\" /></td><td><b>goldenrodColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/73.png\" width=\"50\" height=\"50\" alt=\"yellowGreenColor\" /></td><td><b>yellowGreenColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/74.png\" width=\"50\" height=\"50\" alt=\"bananaColor\" /></td><td><b>bananaColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/75.png\" width=\"50\" height=\"50\" alt=\"mustardColor\" /></td><td><b>mustardColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/76.png\" width=\"50\" height=\"50\" alt=\"buttermilkColor\" /></td><td><b>buttermilkColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/77.png\" width=\"50\" height=\"50\" alt=\"goldColor\" /></td><td><b>goldColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/78.png\" width=\"50\" height=\"50\" alt=\"creamColor\" /></td><td><b>creamColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/79.png\" width=\"50\" height=\"50\" alt=\"lightCreamColor\" /></td><td><b>lightCreamColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/80.png\" width=\"50\" height=\"50\" alt=\"wheatColor\" /></td><td><b>wheatColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/81.png\" width=\"50\" height=\"50\" alt=\"beigeColor\" /></td><td><b>beigeColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/82.png\" width=\"50\" height=\"50\" alt=\"peachColor\" /></td><td><b>peachColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/83.png\" width=\"50\" height=\"50\" alt=\"burntOrangeColor\" /></td><td><b>burntOrangeColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/84.png\" width=\"50\" height=\"50\" alt=\"pastelOrangeColor\" /></td><td><b>pastelOrangeColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/85.png\" width=\"50\" height=\"50\" alt=\"cantaloupeColor\" /></td><td><b>cantaloupeColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/86.png\" width=\"50\" height=\"50\" alt=\"carrotColor\" /></td><td><b>carrotColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/87.png\" width=\"50\" height=\"50\" alt=\"mandarinColor\" /></td><td><b>mandarinColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/88.png\" width=\"50\" height=\"50\" alt=\"chiliPowderColor\" /></td><td><b>chiliPowderColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/89.png\" width=\"50\" height=\"50\" alt=\"burntSiennaColor\" /></td><td><b>burntSiennaColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/90.png\" width=\"50\" height=\"50\" alt=\"chocolateColor\" /></td><td><b>chocolateColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/91.png\" width=\"50\" height=\"50\" alt=\"coffeeColor\" /></td><td><b>coffeeColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/92.png\" width=\"50\" height=\"50\" alt=\"cinnamonColor\" /></td><td><b>cinnamonColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/93.png\" width=\"50\" height=\"50\" alt=\"almondColor\" /></td><td><b>almondColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/94.png\" width=\"50\" height=\"50\" alt=\"eggshellColor\" /></td><td><b>eggshellColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/95.png\" width=\"50\" height=\"50\" alt=\"sandColor\" /></td><td><b>sandColor</b></td></tr><tr><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/96.png\" width=\"50\" height=\"50\" alt=\"mudColor\" /></td><td><b>mudColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/97.png\" width=\"50\" height=\"50\" alt=\"siennaColor\" /></td><td><b>siennaColor</b></td><td><img src=\"https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/Colors/98.png\" width=\"50\" height=\"50\" alt=\"dustColor\" /></td><td><b>dustColor</b></td></tr></table>\n\n**Note** all of the colors in ActionBarCompat included [Holo Colors](https://code.google.com/p/actionbarcompat/source/browse/res/values/holo__colors.xml?r=a03d3fd20e4b7b04740d7541dbcd756a7fa760ca).\n\n\n## Using Predefined Colors\nColours works exactly like the predefined Android colors. In fact, the Colour class is a subclass of android.graphics.Color, so you can actually use the <code>Colour</code> class where you normally use <code>Color</code> to gain access to the cool new methods of the Colour Library without losing any methods in the <code>Color</code>  class. \n\n### XML\n\nTo use your **HUGE** new palette of colors in XML, reference a color just as you would a color in a local Color.xml resource file:\n\n```xml\n  <View\n    .\n    .\n    .\n    android:background=\"@color/seafoamColor\" />\n```\n\nHuzzah! Colours automagically integrates all of its colors to your project, just as if you had defined them yourself. (You can tell all your friends that you made them. We won't tell!)\n\n### Code\n\nLet's say, however, that you would like to set the color of something in code. Colours has you covered. Every single color available in XML is also avalable as a static method, much like the android system colors. To retrieve a predefined color's int representation, simply call it's corresponding method:\n\n```java\nint seashellColor = Colour.seashellColor();\n```\n\n## Color Spaces\n\nAndroid comes pre-baked with RGB and HSV color space methods. However, this may not be enough. This library adds CMYK, which is normally used for printing, and CIE_LAB, a color space meant for modeling an equal space between each color. You can access these methods like so:\n\n```java\nfloat[] cmyk = Colour.colorToCMYK(inputColor);\nint color = Colour.CMYKToColor(cmyk);\nfloat[] cie_lab = Colour.colorToCIE_LAB(inputColor);\nint color = Colour.CIE_LABToColor(cie_lab);\n```\n\n## Color Helper Methods\n\nBeyond giving you a list of a ton of colors with no effort, this category also gives you some methods that allow different color manipulations and translations. Here's how you use these:\n\n**Generating white or black that contrasts with a Color**\n\nA lot of times you may want to put text on top of a view that is a certain color, and you want to be sure that it will look good on top of it. With this method you will return either white or black, depending on the how well each of them contrast on top of it. Here's how you use this:\n\n```java\nint contrastingColor = Colour.blackOrWhiteContrastingColor(inputColor)\n```\n\n**Generating a complementary color**\n\nThis method will create a color int that is the exact opposite color from another color int on the color wheel. The same saturation and brightness are preserved, only the hue is changed.\n\n```java\nint complementaryColor = Colour.complementaryColor(inputColor);\n```\n\n## Distance between 2 Colors\n\nDetecting a difference in two colors is not as trivial as it sounds. One's first instinct is to go for a difference in RGB values, leaving you with a sum of the differences of each point. It looks great! Until you actually start comparing colors. Why do these two reds have a different distance than these two blues *in real life* vs computationally? Human visual perception is next in the line of things between a color and your brain. Some colors are just perceived to have larger variants inside of their respective areas than others, so we need a way to model this human variable to colors. Enter CIELAB. This color formulation is supposed to be this model. So now we need to standardize a unit of distance between any two colors that works independent of how humans visually perceive that distance. Enter CIE76,94,2000. These are methods that use user-tested data and other mathematically and statistically significant correlations to output this info. You can read the wiki articles below to get a better understanding historically of how we moved to newer and better color distance formulas, and what their respective pros/cons are.\n\n**Finding Distance**\n\n```java\ndouble distance = Colour.distanceBetweenColorsWithFormula(colorA, colorB, ColorDistanceFormulaCIE94);\nboolean isNoticablySimilar = distance < threshold;\n```\n\n**Resources**\n\n* [Color Difference](http://en.wikipedia.org/wiki/Color_difference)\n* [Just Noticeable Difference](http://en.wikipedia.org/wiki/Just_noticeable_difference)\n* [CIELAB Specification](http://en.wikipedia.org/wiki/CIELAB)\n\n## Generating Color Schemes ##\n\nYou can create a 5-color scheme based off of a color using the following method. It takes in a color int and one of the ColorSchemeTypes defined in Colours. It returns an int[] of 4 new colors to create a pretty nice color scheme that complements the root color you passed in.\n\n```java\nint[] complementaryColors = Colour.colorSchemeOfType(inputColor, ColorScheme.ColorSchemeComplementary);\n```\n\n**ColorSchemeTypes**\n\n* ColorSchemeAnalagous\n* ColorSchemeMonochromatic\n* ColorSchemeTriad\n* ColorSchemeComplementary\n\nHere are the different examples starting with a color scheme based off of <code>Colour.seafoamColor()</code>.\n\n**ColorSchemeAnalagous**\n\n![Analagous](https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/analagous.png)\n\n**ColorSchemeMonochromatic**\n\n![Monochromatic](https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/monochromatic.png)\n\n**ColorSchemeTriad**\n\n![Triad](https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/triad.png)\n\n**ColorSchemeComplementary**\n\n![Complementary](https://raw.github.com/bennyguitar/Colours-for-iOS/master/Screenshots/complementary.png)\n\n## Credits\n\n* [**Matthew York**](https://github.com/MatthewYork) - Author of Colours for Android \n* [**Ben Gordon**](https://github.com/bennyguitar) - Author of original iOS version of Colours\n* [**Aaron Fleshner**](https://github.com/adfleshner) - Teaching me Android and being awesome like that. (Also adding holo to the mix)\n\nI would also like to thank **God** through whom all things live and move and have their being. [Acts 17:28](http://www.biblegateway.com/passage/?search=Acts+17%3A16-34&version=NIV)\n\n\n## License\nThe MIT License (MIT)\n\nCopyright (c) 2014 Matthew York\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
 },
 {
  "repo": "sirupsen/posix-mqueue",
  "language": "C",
  "readme_contents": "# posix-mqueue [![Build Status](https://travis-ci.org/Sirupsen/posix-mqueue.png?branch=master)](https://travis-ci.org/Sirupsen/posix-mqueue)\n\nMinimal wrapper around the [POSIX message queue](pmq). The POSIX message queue\noffers:\n\n* Persistence. Push messages while nothing is listening.\n* Simplicity. Nothing to set up. Built into Linux.\n* IPC. Blazingly fast communication between processes on the same machine.\n* Blocking and non-blocking. Listeners block until a message arrives on the\n  queue. No polling. Sending messages doesn't block.\n\n\nNote that this requires no third party message broker. The messages are handled\nby the kernel of your computer. Not all kernels have support for POSIX message\nqueues, a notably example is Darwin (OS X). Darwin implements the older System V\nIPC API. See my [SysV MQ wrapper](https://github.com/Sirupsen/sysvmq).\n\n## Usage\n\nIn your Gemfile:\n\n`gem 'posix-mqueue'`\n\n### Important notes\n\n1. This will not work on OS X, but on Linux and probably BSD (not tested).\n2. `send` and `receive` block. `timedsend` and `timedreceive` do not.\n3. The default message size is `4096` bytes.\n4. Linux's default queue size is `10` bytes.\n\nRead on for details.\n\n### Example\n\n```ruby\nrequire 'posix/mqueue'\n\n# On Linux the queue name must be prefixed with a slash. Note it is not a file\n# created at `/whatever`. It's just the name of the queue.\n# Set maximum default Linux options. See next section to push those limits.\n# Default options are msgsize: 4096 and maxmsg: 10\nm = POSIX::Mqueue.new(\"/whatever\", msgsize: 8192, maxmsg: 10)\nm.send \"hello\"\nm.receive\n# => \"hello\"\n\nfork { POSIX::Mqueue.new(\"/whatever\").send(\"world\") }\n\n# Blocks until the forked process pushes to the queue\nm.receive\n# => \"world\"\n\n# Queue is now full by default Linux settings, see below on how to increase it.\n10.times { m.send rand(100).to_s }\n\n# #size returns the size of the queue\nm.size\n# => 10\n\n# #send will block until something is popped off the now full queue.\n# timesend takes timeout arguments (first one is seconds, second is\n# nanoseconds). Pass 0 for for both to not block, this is default.\n\nassert_raises POSIX::Mqueue::QueueFull do\n  m.timedsend \"I will fail\"\nend\n\n# Empty the queue again\n10.times { m.receive }\n\n# Like timedsend, timedreceive takes timeout arguments and will raise\n# POSIX::Mqueue::Queueempty when it would otherwise block.\nassert_raises POSIX::Mqueue::QueueEmpty do\n  m.timedreceive\nend\n\n# Deletes the queue and any messages remaining.\n# None in this case. If not unlinked, the queue will persist till reboot.\nm.unlink\n\n```\n\n### mqueue\n\nMost important information from the manpages, with a little added information\nabout the behavior of `posix-mqueue`.\n\n### /proc interfaces\n\nLinux has some default limits you can easily change.\n\n1. `/proc/sys/fs/mqueue/msg_max`. Contains the maximum number of messages in a\n   single queue. Defaults to 10. You should increase that number. `#send` will\n   eventually block if the queue is full. `#timedsend` will throw `QueueFull`.\n2. `/proc/sys/fs/mqueue/msgsize_max`. Maximum size of a single message. Defaults\n   to 8192 bytes. `posix-mqueue` defaults to 4096 bytes. Overwrite this by\n   passing `{msgsize: 8192}` as the second argument when initializing.\n3. `/proc/sys/fs/mqueue/queues_max`. Maximum number of queues on the system.\n   Defaults to 256.\n\n## Virtual filesystem\n\nThe message queue is created as a virtual file system. That means you can mount\nit:\n\n```bash\n# sudo mkdir /dev/queue\n# sudo mount -t mqueue none /dev/queue\n```\n\nAdd a queue and a few tasks, count the characters (19):\n\n```ruby\n$ irb\n> require 'posix/mqueue'\n=> true\n> m = POSIX::Mqueue.new(\"/queue\")\n=> #<POSIX::Mqueue:0xb8c9fe88>\n> m.send \"narwhal\"\n=> true\n> m.send \"walrus\"\n=> true\n> m.send \"ponies\"\n=> true\n```\n\nInspect the mounted filesystem:\n\n```bash\n$ ls /dev/queue/\nimportant  mails  queue\n$ cat /dev/queue/queue\nQSIZE:19         NOTIFY:0     SIGNO:0     NOTIFY_PID:0\n```\n\nHere `QSIZE` is the bytes of data in the queue. The other flags are about\nnotifications which `posix-mqueue` does not support currently, read about them\nin [mq_overview(7)][pmq].\n\n[pmq]: http://man7.org/linux/man-pages/man7/mq_overview.7.html\n"
 },
 {
  "repo": "clbx/Cosmic",
  "language": "C",
  "readme_contents": "# Cosmic\n<p align=\"center\">\n    <a target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"./doc/img/logo.png\" alt=\"Cosmic Logo\"></a>\n<br></p>\n\n\n----\n\n\n<p align=\"center\">\n<a href=\"https://travis-ci.org/clbx/Cosmic\"><img src=\"https://travis-ci.org/clbx/Cosmic.svg?branch=master\"/></a>\n<a href=\"https://www.codefactor.io/repository/github/clbx/cosmic\"><img src=\"https://www.codefactor.io/repository/github/clbx/cosmic/badge\" alt=\"CodeFactor\" /></a>\n<a href=\"https://gitter.im/CosmicProcessor/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge\"><img src=\"https://badges.gitter.im/CosmicProcessor/community.svg\"/></a>\n<img alt=\"GitHub repo size\" src=\"https://img.shields.io/github/repo-size/clbx/Cosmic\">\n</br>\n<a href=\"https://gitpod.io/#https://github.com/clbx/Cosmic\"><img src=\"https://gitpod.io/button/open-in-gitpod.svg\"></a>\n</p>\n\nCosmic is a fully simulated computer architecture that provides a full environment to use the Cosmic Processor, specialized devices, ROM, RAM, graphics, and more. The cosmic processor can also be used on a Raspberry Pi to interface with real-world devices. The cosmic system is all entirely accessible in a rich GUI interface that allows for full customization of the system environment and code execution.*\n\n<p align=\"center\">\n    <a target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"800\" src=\"./doc/img/cosmicGUI.png\" alt=\"Cosmic GUI\"></a>\n<br></p>\n\nCosmic is designed to act similar to a real, physical chip but take advantage of it being software-based and leaving some of the nuances of the physical world behind.*\n\nWith an instruction set that makes sense (and is fun), and simple memory-mapped I/O, Cosmic is an excellent \"chip\" to write for, using the cosmic assembler.*\n\n<sub>* in deveopment</sub>\n\n#### What is Cosmic?\n* A great educational tool to teach how processors work and assembly language\n* A very interesting project\n* A Proof of Concept\n* Kinda cool\n* Fun to play with\n\n\n-----\n\nCosmic is the Senior Project for Clay Buxton ([@clbx](https://github.com/clbx)) and Kevin Carman ([@carmank](https://github.com/carmank)) at Elizabethtown College. All academically required reports and documentation is in the ``/doc`` folder.\n\nSpecial thanks to [@Gwarks](https://github.com/Gwarks), who did a lot of base work on the graphics.\n\n----\n\n## Documentation\n* [Cosmic Processor Specification](https://github.com/clbx/Cosmic/blob/master/doc/Cosmic%20Processor%20Specifications.md). This has information about how the processor works, the instruction set, and other general information\n* [Cosmic Assembler Specification](https://github.com/clbx/Cosmic/blob/master/doc/Cosmic%20Assembler%20Specifications.md). This has information about how to write assembly programs for Cosmic.\n* [Cosmic System Specifications](https://github.com/clbx/Cosmic/blob/master/doc/Cosmic%20System%20Specifications.md) This has information about other parts of the Cosmic system work (Video, Audio, etc.)\n\n## Installation Instructions\n**Be sure to clone recursively, Cosmic uses git submodules**\n\n``git clone --recursive https://github.com/clbx/Cosmic``\n\nIf you have already cloned:\n```\ngit submodule init\ngit submodule update\n```\n\n\n### Linux\nInstall ``SDL2`` using your distributions package manager\n```\nmake\n```\n\n\n### macOS\nInstall SDL2 using brew. Get brew [here](brew.sh) if you don't already have it. \n```\nbrew install sdl2\nmake\n```\n\n### Windows \nA pre-compiled binary can be downloaded from the release page. You will still need to install MinGW and SDL2\nBe sure to add both MinGW and SDL2.dll to your path\n\n**Compilation Instructions:**\n\nGet [Chocolatey](https://chocolatey.org/install) if you don't have it already, it helps for installing tools\n\nInstall MinGW ``choco install mingw``\n\nInstall make ``choco install make``\n\n\nDownload SDL2 Development Library\n\nUntar SDL to the MinGW install directory\n\n\n```\nmake\n```\n\nMake sure any missing .dll's are in your path\n\n### Assembler\n\nThe assembler is written in Python, so if you plan on assembling anything for Cosmic, Python 3 is required\n\n----\n\n``/lib`` contains some included software:\n*  [ImGui](https://github.com/ocornut/imgui) is a GUI library used for the interface, along with a slightly modified memory editor addon. This is under the MIT License and the license file is included accordingly\n\n* [ImTui](https://github.com/ggerganov/imtui) A library that takes ImGui and puts it into a terminal.\n\n* [gl3w](https://github.com/skaslev/gl3w) A OpenGL core loader. This is under the unlicense. \n\n* [catch2](https://github.com/catchorg/Catch2) is used for testing. This is under the BSL Software License\n\n\nSDL2 is also required to run Cosmic.\n\n\nCosmic is under the MIT License, feel free to use any part of it, but if you do please include the license file and a link to this repository.\n"
 },
 {
  "repo": "OpenGene/repaq",
  "language": "C",
  "readme_contents": "[![install with conda](\nhttps://anaconda.org/bioconda/repaq/badges/version.svg)](https://anaconda.org/bioconda/repaq)\n# repaq\nA tool to compress FASTQ files with ultra-high compression ratio and high speed. `repaq` supports compressing the FASTQ to `.rfq` or `.rfq.xz` formats. Compressing to `.rfq` is ultra fast, while compressing to `.rfq.xz` provides very high compression ratio. \n\nFor NovaSeq data, as an example:  \n* the `.rfq` file can be much smaller than `.fq.gz`, and the compressing time is usually less than 1/5 of gzip compression.\n* The `.rfq.xz` file can be as small as 5% of the original FASTQ file, or smaller than 30% of the `.fq.gz` file.\n\nFor paired-end FASTQ files, `repaq` compresses them into one single file to provide higher compression ratio.\n\nThis tool also supports non-Illumina format FASTQ (i.e. the BGI-SEQ format), but the compression ratio is not as good Illumina format FASTQ.\n\n***WARNING: be careful about using repaq for production before v1.0 is released, since its spec v1.0 has not been frozen.***\n\n# take a look at the compression ratio\nHere we demonstrate the compression ratio of two paired-end NovaSeq data. You can download these files and test locally.\n* `nova.R1.fq`: 1704 MB, the original read1 file, http://opengene.org/repaq/testdata/nova.R1.fq\n* `nova.R2.fq`: 1704 MB, the original read2 file, http://opengene.org/repaq/testdata/nova.R2.fq\n* `nova.R1.fq.gz`: 308 MB (CR 18.08%), the gzipped read1, http://opengene.org/repaq/testdata/nova.R1.fq.gz\n* `nova.R2.fq.gz`: 325 MB (CR 19.07%), the gzipped read2, http://opengene.org/repaq/testdata/nova.R2.fq.gz\n* `nova.rfq`: 333 MB (CR 9.77%), the repacked file of read1+read2, http://opengene.org/repaq/testdata/nova.rfq\n* `nova.rfq.xz`: 134 MB (CR 3.93%), the xz compressed `nova.rfq`, http://opengene.org/repaq/testdata/nova.rfq.xz\n\nSee? The size of final `nova.rfq.xz` is only 3.39% of the original FASTQ files! You can decompress it and check the md5 to see whether they are identical! \n\nTypically with one single CPU core, it takes less than 1 minute to convert `nova.R1.fq + nova.R2.fq` to `nova.rfq`, and takes less than 5 minutes to compress the `nova.rfq` to `nova.rfq.xz` by xz.\n\n# get repaq\n## install with Bioconda\n[![install with conda](\nhttps://anaconda.org/bioconda/repaq/badges/version.svg)](https://anaconda.org/bioconda/repaq)\n```shell\nconda install -c bioconda repaq\n```\n## download binary \nThis binary is only for Linux systems: http://opengene.org/repaq/repaq\n```shell\n# this binary was compiled on CentOS, and tested on CentOS/Ubuntu\nwget http://opengene.org/repaq/repaq\nchmod a+x ./repaq\n```\n## or compile from source\n```shell\n# get source (you can also use browser to download from master or releases)\ngit clone https://github.com/OpenGene/repaq.git\n\n# build\ncd repaq\nmake\n\n# Install\nsudo make install\n```\n\n# usage\nFor single-end mode:\n```shell\n# compress to .rfq.xz\nrepaq -c -i in.fq -o out.rfq.xz\n\n# decompress from .rfq.xz\nrepaq -d -i in.rfq.xz -o out.fq\n```\n\nFor paired-end mode:\n```shell\n# compress to .rfq.xz\nrepaq -c -i in.R1.fq -I in.R2.fq -o out.rfq.xz\n\n# decompress from .rfq.xz\nrepaq -d -i in.rfq.xz -o out.R1.fq -O out.R2.fq\n```\n\nTips:\n* `-i` and `-I` always denote the first and second input files, while `-o` and `-O` always denote the first and second output files.\n* the FASTQ input/output files can be gzipped if their names are ended with `.gz`.\n* for paired-end data. the .rfq file created in paired-end mode is usually much smaller than the sum of the .rfq files created in single-end mode for R1 and R2 respectively. To obtain high compression rate, please always use PE mode for PE data.\n* if you want higher speed and are not concern with compression ratio, replace `xxx.rfq.xz` with `xxx.rfq`, then repaq will compress or decompress `.rfq` format.\n\n# system requirements\n* Memory: 16G RAM\n* CPU: 4 cores\n\n# verify the compressed file\nrepaq offers a `compare` mode to check the consistency of the original FASTQ file(s) and the compressed .rfq or .rfq.xz file. \n* set `--compare` to enable the `compare` mode\n* specify the .rfq or .rfq.xz file by `-r` option\n* specify the FASTQ files by `-i` and `-I` options.\n\nExamples:\n```shell\n# for single-end data\nrepaq --compare -i original.R1.fq  -r compressed.rfq.xz\n\n# for paired-end data\nrepaq --compare -i original.R1.fq.gz -I original.R2.fq.gz  -r compressed.rfq.xz\n```\nWithout any expection, you will get an output of a JSON like:\n```json\n{\n\t\"result\":\"passed\",\n\t\"msg\":\"\",\n\t\"fastq_reads\":50000,\n\t\"rfq_reads\":50000,\n\t\"fastq_bases\":7419082,\n\t\"rfq_bases\":7419082\n}\n```\nThe `result` will be \"failed\" if the compressed file is not consistent with the original FASTQ files.\n\n# STDIN and STDOUT\nrepaq can read the input from STDIN, and write the output to STDOUT.\n* specify `--stdin` if you want to read the STDIN for compression or decompression.\n* specify `--stdout` if you want to output to the STDOUT for compression or decompression\n* in decompression mode, if `--stdout` is specified, the output will be interleaved PE stream.\n* if the STDIN is an interleaved paired-end stream, specify `--interleaved_in` to indicate that.\n* be noted that STDIN cannot be read when the input is a .xz file, and STDOUT cannot be written when the output is a .xz file\n\nHere gives you an example of compressing the interleaved PE output from fastp by directly using pipes:\n```shell\nfastp -i R1.fq -I R2.fq --stdout | repaq -c --interleaved_in --stdin -o out.rfq.xz\n```\n\n# FASTQ Format compatibility  \nrepaq was initially designed for compressing Illumina data, but it also works with data from other platforms, like BGI-Seq. To work with repaq, the FASTQ format should meet following condidtions:\n* only has bases A/T/C/G/N.\n* each FASTQ record has, and only has four lines (name, sequence, strand, quality).\n* the name and strand line cannot be longer than 255 bytes.\n* the number of different quality characters cannot be more than 127.\n\n`repaq` works best for Illumina data directly output by `bcl2fastq`.\n\n# all options\n```shell\noptions:\n  -i, --in1                    input file name (string [=])\n  -o, --out1                   output file name (string [=])\n  -I, --in2                    read2 input file name when encoding paired-end FASTQ files (string [=])\n  -O, --out2                   read2 output file name when decoding to paired-end FASTQ files (string [=])\n  -c, --compress               compress input to output\n  -d, --decompress             decompress input to output\n  -k, --chunk                  the chunk size (kilo bases) for encoding, default 1000=1000kb. (int [=1000])\n      --stdin                  input from STDIN. If the STDIN is interleaved paired-end FASTQ, please also add --interleaved_in.\n      --stdout                 write to STDOUT. When decompressing PE data, this option will result in interleaved FASTQ output for paired-end input. Disabled by defaut.\n      --interleaved_in         indicate that <in1> is an interleaved paired-end FASTQ which contains both read1 and read2. Disabled by defaut.\n  \n# following options are used to check the consistency of the compressed data\n  -p, --compare                compare the files read by read to check the compression consistency. <rfq_to_compare> should be specified in this mode.\n  -r, --rfq_to_compare         the RFQ file to be compared with the input. This option is only used in compare mode. (string [=])\n  -j, --json_compare_result    the file to store the comparison result. This is optional since the result is also printed on STDOUT. (string [=])\n\n# options for .xz output\n  -t, --thread                 thread number for xz compression. Higher thread num means higher speed and lower compression ratio (1~16), default 1. (int [=1])\n  -z, --compression            compression level. Higher level means higher compression ratio, and more RAM usage (1~9), default 4. (int [=4])\n\n  -?, --help                   print this message\n```\n\n# external dependency\n`repaq` makes a system call in order to run the xz compression tool available on GNU/Linux systems. If xz isn't installed, `repaq` will fail with the message: \n\n```\nfailed to call xz, please confirm that xz is installed in your system\n```\n"
 },
 {
  "repo": "mattt/MsgPackSerialization",
  "language": "C",
  "readme_contents": "# MsgPackSerialization\n\n> This project is no longer maintained.\n> See [@Flight-School/MessagePack](https://github.com/Flight-School/MessagePack)\n> for a Swift Codable implementation of the MessagePack format.\n\n`MsgPackSerialization` encodes and decodes between Objective-C objects and\n[MsgPack](http://msgpack.org) data,\nfollowing the API conventions of Foundation's `NSJSONSerialization` class.\n\n## Usage\n\n```objective-c\nid obj = @{\n           @\"foo\": @(42.0),\n           @\"bar\": @\"lorem ipsum\",\n           @\"baz\": @[@1, @2, @3, @4]\n          };\n\nNSError *error = nil;\n\nCFAbsoluteTime t_0 = CFAbsoluteTimeGetCurrent();\nNSData *data = [MsgPackSerialization dataWithMsgPackObject:obj options:0 error:&error];\nNSLog(@\"Packed: %@ (Elapsed: %g)\", data, CFAbsoluteTimeGetCurrent() - t_0);\n\nCFAbsoluteTime t_1 = CFAbsoluteTimeGetCurrent();\nNSLog(@\"Unpacked: %@ (Elapsed: %g)\", [MsgPackSerialization MsgPackObjectWithData:data options:0 error:&error], CFAbsoluteTimeGetCurrent() - t_1);\n```\n\n---\n\n## Contact\n\n[Mattt](https://twitter.com/mattt)\n\n## License\n\nMsgPackSerialization is available under the MIT license.\nSee the LICENSE file for more info.\n"
 },
 {
  "repo": "cadwallion/spinel",
  "language": "C",
  "readme_contents": "# Spinel - Ruby-infused Game Engine\n\nSpinel is a new free and open source game engine under development that utilizes [mruby](https://github.com/mruby/mruby/)\nas its integral scripting layer. Under the hood is C/C++, however wherever possible the engine uses Ruby.\n\nIncluded in the repo is the source of Spinel, as well as a demo application for testing the engine.\n\nI want to stress that this is in **EARLY DEVELOPMENT**. What you see is the result of initial research\ntaken place during an Agora Games Hackathon in 24 hours.  Most of what you see will likely be rewritten,\nbut given the requirement of having a working demo in 24 hours, some temporary structures had to be put\nin place.\n\n## Installation\n\nSpinel requires compiling mruby, and comes with mruby as a git submodule linked to working builds with Spinel.\n\n```\ngit clone git://github.com/cadwallion/spinel.git\ncd spinel\ngit submodule init && git submodule update\n```\n\nFor compilation, spinel comes with an Xcode project.  For non-Xcode users, the project is compiled via make:\n\n```\nmake [game|engine|mruby|all|distclean|help]\n```\n\nTo build the game, either use the 'game' target in Xcode or run `make game`. If problems occur while compiling,\nplease run `make distclean` before running `make game`. Use `make help` for more information. Build output is\nset to ./build/.\n\n## Engine\n\nThe Engine is a static library containing core Engine configurations and the hooks for custom game development.\nThe primary components right now are Spinel::Engine and EngineSetup. Creating a game using Spinel requires\nlinking `libspinel.a` and including the `engine.h` header. See the code in `src/` for more information on how\nto use Spinel in your own game.\n\n## Game Demo\n\nYou'll find a demonstration of initializing an mruby VM within C++, loading source files into the VM, creation\nof Ruby objects from C and inserting them into the VM, as well as Ruby-defined function calls from C. This\nshows the capability of interoperability and the ability to maintain C/C++-level game constructs with Ruby-level\nabstractions, as well as core class loading from both sides of the VM barrier. Note that script loading uses two\nsetup values in game/main.cpp: `script_diectory` and `core_directory`.  These are relative paths to the build path,\nand the files will be copied to the build directory.\n\n## Next Steps\n\nNow that the hackathon is over, I'll begin ripping out the temporary constructs and being adding in the real\nengine structure.  Some features on the top of the list:\n\n* Game Loop Standardization\n* Better structure of Scripting vs Core class loading\n* Platform-specific Window Initialization Hooks\n* Scene Manager Creation\n\n## Contribution\n\nOnce I am further along in development I'll create a ROADMAP to help reflect where the project is going to make it\neasier for contributions in line with where my head is taking this.  I am releasing the code as an effort to show\nthe progress to the public and use it as a talking piece as I find challenges with mruby/Ruby design along the\nway.\n\nThat said, pull-requests with feature branches are welcome.\n\n## Author\n\nCreated by [Andrew Nordman](https://github.com/cadwallion).  Announcement of project at RubyConf 2012, initial\ndevelopment as part of [Agora Games](https://www.agoragames.com/) Hackathon X.\n"
 },
 {
  "repo": "erkyrath/glulxe",
  "language": "C",
  "readme_contents": "# Glulxe: the Glulx VM interpreter\n\n- Version 0.6.0\n- Designed by Andrew Plotkin <erkyrath@eblong.com>\n- [Glulx home page][glulx]\n\n[glulx]: http://eblong.com/zarf/glulx/index.html\n[glk]: http://eblong.com/zarf/glk/index.html\n\n## Compiling\n\nSince this is a Glk program, it must be built with a Glk library. See\nthe [Glk home page][glk].\n\nThe Unix Makefile that comes with this package is designed to link any\nof the Unix libraries (CheapGlk, GlkTerm, RemGlk, etc.) You'll have to go\ninto the Makefile and set three variables to find the library. There are\ninstructions at the top of the Makefile. Then just type\n\n    make glulxe\n\nThat should suffice. When the program is built, type\n\n    ./glulxe filename.ulx\n\nwhere \"filename.ulx\" is a Glulx game file to execute.\n\nTo build this program with a Mac or Windows interface, or any other \ninterface, you'll need the appropriate Glk library.\n\nThis program supports floating-point operations, which are implemented\nusing the standard C99 math functions. The Makefile uses \"-lm\" to link\nthese in. If your platform does not support these functions, you can\ncomment out the \"#define FLOAT_SUPPORT\" line in glulxe.h.\n\nIf you define the VM_DEBUGGER symbol (uncomment the \"#define VM_DEBUGGER\"\nline in glulxe.h), you must include the libxml2 library. See the\nXMLLIB definition in the Makefile.\n\n## Autosave\n\nThis interpreter supports autosave if the Glk library does. Currently\nonly two do: RemGlk and IosGlk. (The latter is no longer supported on\nmodern iOS, so RemGlk is your only real option.)\n\nThe --autosave option tells the interpreter to write out autosave\nfiles at the end of each turn. The --autorestore tells it to load\nthose files at startup time, thus starting the game where it was last\nautosaved. Note that --autosave will overwrite the autosave files if\npresent, but you should not use --autorestore unless the files exist.\n\nThere are two autosave files, by default kept in the current directory\nand named \"autosave.json\" and \"autosave.glksave\". You can change the\ndirectory with --autodir and the base filename with --autoname.\n\nIn some contexts it is useful for every game to have a unique autosave\nlocation. You can do this by giving an --autoname value with a hash\nmark, e.g.:\n\n    ./glulxe --autosave --autoname auto-# filename.ulx\n\nThe # character will be replaced with a (long) hex string that\nuniquely identifies the game file. (Pretty uniquely, at least. It's\nnot a cryptographically strong hash.)\n\nAutosave covers two slightly different scenarios:\n\n### Hedging against the possibility of process termination\n\nThis was how the iOS interpreters work (worked). The app would start\nand run normally, but it *could* be killed at any time (when in the\nbackground). Therefore, we autosave every turn. At startup time, if\nautosave files exist, we restore them and continue play.\n\nTo operate in this mode in a Unix environment:\n\n    ./glulxe --autosave --autoskiparrange filename.ulx\n\nThe --autosave argument causes an autosave every turn. The\n--autoskiparrange argument skips this on Arrange (window resize)\nevents. (We may get several Arrange events in a row, and they don't\nrepresent progress that a player would care about losing.)\n\nWhen relaunching, if autosave files exist, do:\n\n    ./glulxe --autosave --autoskiparrange --autorestore -autometrics filename.ulx\n\nThe --autorestore arguments loads the autosave. The -autometrics\nargument (a RemGlk argument, hence the single dash) tells RemGlk to\nskip the step of waiting for an Init event with metrics. (This is not\nneeded because the game will already be in progress. But you can send\na normal Arrange event if you think your window size might be\ndifferent from the autosave state.)\n\n### Single-turn operation\n\nThis mode allows you to play a game without keeping a long-term process\nactive. On every player input, the interpreter will launch, autorestore,\nprocess the input, autosave, and exit.\n\nTo operate in this mode in a Unix environment:\n\n    ./glulxe --autosave -singleturn filename.ulx\n\nThe -singleturn argument (a RemGlk argument) tells the interpreter to\nexit as soon as an output stanza is generated. When you pass in the\ninitial Init event, the interpreter will process the start-of-game\nactivity, display the initial window state, and exit.\n\nWhen relaunching, if autosave files exist, do:\n\n    ./glulxe --autosave --autorestore -singleturn -autometrics filename.ulx\n\nYou should only do this when the UI has a player input ready to process.\nLaunch the game and pass in the input. The interpreter will process it,\ndisplay the update, and then (without delay) exit.\n\n## Version\n\n0.6.0 (Jun 25, 2022):\n\n- Added @hasundo and @discardundo opcodes. (Glulx spec 3.1.3.)\n- Added the double-precision opcodes. (Glulx spec 3.1.3.)\n- Added autosave support to the Unix startup code. (Previously the\n  autosave support only existed in the iOS startup code, which was\n  ObjC.) Autosave now works with the RemGlk library.\n- Added an --undo argument to set the number of undo states.\n- Fixed a bug where accelerated functions could write error messages\n  to Glk regardless of the current I/O system.\n- Added array bounds checking on stack access.\n- Added a guard against too-deep recursion when creating the string\n  cache.\n\n0.5.4 (Jan 23, 2017):\n\n- Added an internal debugger. Compile with \"#define VM_DEBUGGER\" \n  (and a debug-supporting Glk library) to use it.\n- Expanded the TOLERATE_SUPERGLUS_BUG behavior to tolerate more\n  Superglus game files.\n\n0.5.3 (Oct 25, 2016):\n\n- Turn on SERIALIZE_CACHE_RAM in the default build. This speeds up\n  save and save-undo operations.\n- Other tweaks to speed up launch, restart, restore, etc.\n- Fixed a bug where accelerated functions were not being autosaved.\n  (Only relevant for iOS, currently.)\n- When profiling, restore/restore-undo operations will now fail\n  (and the game will continue) instead of causing a fatal error.\n- Added a build option to tolerate the Superglus bug where (very old)\n  game files would try to write to memory address zero. (Not on by\n  default.)\n- Switched from my old ad-hoc license to the MIT license.\n\n0.5.2 (Mar 27, 2014):\n\n- Added acceleration functions 8 through 13, which work correctly when\n  NUM_ATTR_BYTES is changed.\n\n0.5.1 (Mar 10, 2013):\n\n- Fixed a bug in glkop.c that prevented get_buffer_stream() from\n  working right.\n- Updated profile-analyze.py to understand the upcoming, updated I6\n  debug file format. (See http://inform7.com/mantis/view.php?id=1073)\n  The old format is still supported.\n\n0.5.0 (Oct 18, 2012):\n\n- Turned on memory-range checking in the default build. (Should have\n  done this years ago.)\n- Fixed a bug where @setmemsize could crash an open char memory stream\n  or char line input request.\n- Updated glkop.c to handle arrays of Glk objects. (This is needed to\n  support glk_schannel_play_multi().)\n- Added hooks for the library to execute at startup and select time.\n- Added a hook which allows the library to get and pass on a game ID\n  string.\n- Clean up all memory allocation when the VM exits.\n\n0.4.7 (Oct 10, 2011):\n\n- Abstracted powf() to an osdepend wrapper. (Needed for Windows.)\n- Fixed a @ceil bug, for some C math libraries.\n- Improved the profiling system in several ways.\n- Fixed a bug in glkop.c dispatching, to do with optional array\n  arguments.\n\n0.4.6 (Aug 17, 2010):\n\n- Added floating-point math feature.\n- Updated winstart.c. (Thanks David Kinder.)\n- Fixed @random even more, on Windows.\n- @verify works right on game files with extended memory.\n- @getiosys works right when the two store operands are different\n  variable types. (E.g., one local and one global.)\n\n0.4.5 (Nov 23, 2009):\n\n- VERIFY_MEMORY_ACCESS now detects writes to the ROM section of memory.\n- Fixed off-by-eight bug in @astorebit and @aloadbit with negative bit\n  numbers.\n- Fixed an obscure bug with division and modulo of $80000000. (Thanks \n  Evin Robertson.)\n- Fixed an extremely obscure problem with changing I/O mode in the middle\n  of printing a number.\n- Glk array/string operations are now checked for memory overflows\n  (though not for ROM writing). This generates a warning at present;\n  in the future, it will be a fatal error.\n- Better fix for the @random bug.\n\n0.4.4 (Mar 11, 2009):\n\n- Added profiling code, which is turned off by default. To compile it \n  in, define VM_PROFILING in Makefile or in glulxe.h.\n- Added function-accleration feature.\n- Fixed bug where @random 0 was returning only positive numbers.\n\n0.4.3 (Jan 23, 2008):\n\n- Verify the presence of Unicode calls in the Glk library at runtime.\n  (Thanks Simon Baldwin.)\n- Added a compile-time option to check for invalid memory accesses.\n  (This is slower, but safer. Define VERIFY_MEMORY_ACCESS in Makefile\n  or in glulxe.h. Thanks Evin Robertson.)\n- Fixed a memory leak of undo states. (Thanks Matthew Wightman.)\n- Fixed a linked-list handling error for Glk unicode arrays. (Thanks\n  David Kinder.)\n\n0.4.2 (Feb 15, 2007):\n\n- Fixed a bug that preventing compiling with old (pre-Unicode) Glk\n  libraries.\n\n0.4.1 (Feb 11, 2007):\n\n- Added array copy and heap allocation functionality. (Glulx spec \n  3.1.0.)\n\n0.4.0 (Aug 13, 2006):\n\n- Added Unicode functionality. (Glulx spec 3.0.0.)\n\n0.3.5 (Aug 24, 2000):\n\n- Fixed El-Stupido bug in the modulo opcode.\n\n0.3.4 (Jul 11, 2000):\n\n- Finally supports string arguments to Glk calls.\n\n0.3.3 (Mar 29, 2000):\n\n- Added setiosys, getiosys opcodes.\n- Fixed bug in binarysearch.\n\n0.3.2 (Feb 21, 2000):\n\n- Added search, jumpabs, callf, and gestalt opcodes.\n\n0.3.1 (Aug 23, 1999):\n\n- Startup code now handles Blorb files correctly.\n\n0.3.0 (Aug 17, 1999):\n\n- Added support for compressed strings.\n\n0.2.2 (Jun 15, 1999):\n\n- Another pre-release version.\n\n0.2.0 (May 30, 1999):\n\n- A pre-release version.\n\n## Permissions\n\nThe source code in this package is copyright 1999-2016 by Andrew Plotkin.\nIt is distributed under the MIT license; see the \"[LICENSE][]\" file.\n\n[LICENSE]: ./LICENSE\n"
 },
 {
  "repo": "julman99/eatmemory",
  "language": "C",
  "readme_contents": "eatmemory\n=========\n\n# 1. Introduction\nSimple utility to allocate memory on a computer\n\n# 2. What can I use this for?\n- Test swap\n- Test behaviors on a machine when there is little memory available\n\n# 3. Installation\n\n## Compile from sources\n\n```\ncd /tmp\ngit clone https://github.com/julman99/eatmemory.git\ncd eatmemory\nsudo make install\n```\n\n## MacOS Homebrew\n```\nbrew tap julman99/toolbox\nbrew install eatmemory\n```\n\n## Using Docker\n\nSee section 5\n\n# 4. Running\n\n```\neatmemory <size>\n```\n\nSize is in number of bytes, megabytes or gigabytes.\n\n## Examples\n\n```\neatmemory 1024\neatmemory 10M\neatmemory 4G\n```\n\n# 5. Docker image\n\n## Running a container to eat 128MB:\n\n**eatmemory** is [available](https://hub.docker.com/r/julman99/eatmemory) in Dockerhub, so you can just run it without going\nthrough the build process\n\n```\n$ docker run -d --rm --name hungry_container julman99/eatmemory 128M\n```\n\nCheck the memory consumption of the container:\n\n```\n$ docker stats --no-stream=true hungry_container\nCONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O             PIDS\nhungry_container    0.00%               133.9 MiB / 3.651 GiB   3.58%               2.01 kB / 1.08 kB   1.217 MB / 3.265 MB   4\n```\n\n## Building the container\n\nYou need at least Docker 17.05 to use the [multi-stage](https://docs.docker.com/engine/userguide/eng-image/multistage-build/) build feature\n\n```\n$ docker build . -t eatmemory\n```\n# 6. Support this project\n\nBitcoin Address: `14LFRrMX3HmyAH9zQsnzYoVKDH6bVWiBu3`\n"
 },
 {
  "repo": "liyuming1978/NativeLibCompression",
  "language": "C",
  "readme_contents": "\ufeff<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">NativeLibCompression<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">====================<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">There are lots of NDK apps on Google software market. To\r\nreduce package size, some ISV will only release Separate APK. A native library\r\ncompression sdk is given to solve the apk size problem. It is easy to integrate\r\nand will get max 50% size decreasing. Beside sdk, a Java tool for package is\r\nprovided to convert normal apk to compressed apk.<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">&nbsp;</span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><b><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">HOW TO USE IT:</span></b><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">1.Include DecRawso into your project (if you use ant,\r\nplease copy&nbsp;<b>DecRawso_Jar&nbsp;</b>to your project , and add the\r\nDecrawso.jar, do not use the jar in the sdk bin folder)<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">2.Call DecRawso.NewInstance&nbsp;<b>before any native\r\nlibrary loading!!!</b><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">3.Replace all system.loadlibrary(***) to\r\nsystem.load(DecRawso . GetInstance ().GetPath(***))<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; now, it is\r\nrecommend to change to system.load, but system.loadlibrary also work.<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">--- build your apk as usual, and run your apk as usual\r\nwhen in your development, the apk is not compressed.<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">&nbsp;</span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><b><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">HOW TO COMPRESS THE APK: -- Use compress tool :\r\nApkLibCompress/bin/ ComPressApk.jar</span></b><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">1.You can use it as:&nbsp;<b>&nbsp;ComPressApk.jar -a\r\nC:/my/test.apk -k c:/key storepass keypass alias [your keyname] -x86\r\nhttp://www.test.com</b><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">2.if \u201c-k\u201d is missing, eclipse default test key will be\r\nused to sign this apk.&nbsp;<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">3.[you keyname] is optional, if not have it. the\r\ndefalt&nbsp;<b>CERT&nbsp;</b>will be used<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">4.If -x86 with link is used, then x86 library will be\r\nstored on http://www.test.com/cloudrawso_x86, &nbsp; you must store the lib on\r\nthe network bu manuanlly.<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span lang=\"EN-US\" style=\"font-family: Arial, sans-serif;\">5.you can put&nbsp;<b>arm lib on x86 folder&nbsp;</b>to\r\navoid library miss on x86 devices, use -<b>nox86check&nbsp;</b>to forbidden the\r\ncheck (x86 directly cal arm lib is&nbsp;<b>unsafed</b>)<o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">6.you can copy all of \"<b>DecRawso_Jar</b>\"\r\ninto your project if you use \"ant\" to package your project</span><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">7.<strong>new flag</strong>:&nbsp;</span><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">&nbsp; -o outputfilename&nbsp;&nbsp;&nbsp;&nbsp; define\r\nthe finaly output file name</span><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">&nbsp;&nbsp;&nbsp; -slience &nbsp; &nbsp; &nbsp; &nbsp;\r\n&nbsp; &nbsp; no popup window, that is suitable for ant package</span><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">&nbsp;&nbsp;&nbsp; -nosign &nbsp; &nbsp; &nbsp; &nbsp;\r\n&nbsp; &nbsp; do not sign the apk, that is suitable for ant package , due to\r\nthe ant will sign apk</span><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"text-indent: 14.25pt; line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">-nox86check&nbsp;&nbsp; &nbsp; &nbsp;do not check x86 library missing and mix use of arm\r\nissue (x86 directly call arm library is forbidden default)</span><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"text-indent: 14.25pt; line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">-noarm &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; do\r\nnot compress arm lib. just put x86 lib on the cloud (with \u2013x86)</span><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><o:p></o:p></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\"><strong>8.how to know the result (when you use ant)</strong></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"line-height: 170%;\"><span style=\"line-height: 100%; font-family: Arial, sans-serif;\">&nbsp; &nbsp; now will create 3 files in the\r\nApkLibCompress.jar folder</span><span style=\"line-height: 100%; font-family: Arial, sans-serif;\"><o:p></o:p></span></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"line-height: 170%;\"><span style=\"line-height: 100%; font-family: Arial, sans-serif;\">&nbsp;&nbsp;&nbsp; :Done.flag &nbsp;&nbsp;&nbsp;&nbsp; you can check whether the file is exist , if\r\nexist , then&nbsp;<b>compression is ok</b></span><span style=\"line-height: 100%; font-family: Arial, sans-serif;\"><o:p></o:p></span></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"line-height: 170%;\"><span style=\"line-height: 100%; font-family: Arial, sans-serif;\">&nbsp;&nbsp;&nbsp; :error.log &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if generation fail, the log will has the\r\nreason</span><span style=\"line-height: 100%; font-family: Arial, sans-serif;\"><o:p></o:p></span></span></p>\r\n\r\n<p class=\"MsoNormal\" align=\"left\" style=\"line-height: 17.85pt;\"><span style=\"font-family: Arial, sans-serif; line-height: 170%;\">&nbsp;&nbsp;&nbsp; :porting.log&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; it will show the x86/arm mix using or x86\r\nlib missing issue</span></p>\r\n\r\nhttp://blog.csdn.net/onlySound/article/details/58620502\r\n"
 },
 {
  "repo": "NevermindZZT/cpost",
  "language": "C",
  "readme_contents": "# cpost\n\nc\u8bed\u8a00\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u5207\u6362\u548c\u89e3\u8026\u7684\u5de5\u5177\n\n- [cpost](#cpost)\n  - [\u7b80\u4ecb](#\u7b80\u4ecb)\n  - [\u4f7f\u7528](#\u4f7f\u7528)\n    - [cpost\u4f7f\u7528](#cpost\u4f7f\u7528)\n  - [cevent\u4f7f\u7528](#cevent\u4f7f\u7528)\n  - [Api](#api)\n    - [post](#post)\n    - [post delay](#post-delay)\n    - [event post](#event-post)\n    - [event export](#event-export)\n\n## \u7b80\u4ecb\n\n[cpost](https://github.com/NevermindZZT/cpost)\u662f\u4e00\u4e2ac\u8bed\u8a00\u7f16\u5199\u7684\uff0c\u7528\u4e8ec\u8bed\u8a00\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u5207\u6362\u548c\u89e3\u8026\u7684\u5de5\u5177\n\n`cpost`\u5305\u542b`cpost`\u548c`cevent`\u4e24\u4e2a\u5de5\u5177\uff0c`cpost`\u7528\u4e8ec\u8bed\u8a00\u7684\u4e0a\u4e0b\u6587\u5207\u6362\uff0c`cevent`\u7528\u4e8e\u7a0b\u5e8f\u6a21\u5757\u4e4b\u95f4\u7684\u89e3\u8026\n\n`cpost`\u501f\u9274\u4e86Android\u7684`Handler`\u673a\u5236\uff0c\u5728c\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528`cpost`\u63a5\u53e3\uff0c\u5c06\u51fd\u6570\u629b\u51fa\u5230\u53e6\u5916\u7684\u7ebf\u7a0b(\u4e0a\u4e0b\u6587)\u4e2d\u8fd0\u884c\uff0c\u5bf9\u4e8e\u67d0\u4e9b\u573a\u666f\uff0c\u5c24\u5176\u662f\u5d4c\u5165\u5f0f\u7f16\u7a0b\u65e0\u64cd\u4f5c\u7cfb\u7edf\u73af\u5883\u4e0b\u7684\u4e2d\u65ad\u5ef6\u8fdf\u5904\u7406\n\n`cevent`\u501f\u9274\u4e86Android\u7684\u5e7f\u64ad\u673a\u5236\uff0c\u5728c\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u5f53\u7a0b\u5e8f\u8fd0\u884c\u81f3\u76f8\u5e94\u7684\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7`cevent`\u63a5\u53e3\u629b\u51fa\u4e00\u4e2a\u4e8b\u4ef6\uff0c\u5176\u4ed6\u6a21\u5757\u53ef\u4ee5\u901a\u8fc7\u6ce8\u518c\u7684\u65b9\u5f0f\uff0c\u76d1\u542c\u8fd9\u4e2a\u4e8b\u4ef6\uff0c\u5f53\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u8c03\u7528\u6ce8\u518c\u7684\u51fd\u6570\uff0c\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5b9e\u73b0\u6a21\u5757\u95f4\u7684\u89e3\u8026\n\n## \u4f7f\u7528\n\n`cpost`\u548c`cevent`\u53ef\u4ee5\u540c\u65f6\u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u72ec\u7acb\u4f7f\u7528\uff0c\u7279\u522b\u9488\u5bf9\u4e8e\u5d4c\u5165\u5f0f\u65e0\u64cd\u4f5c\u7cfb\u7edf\u7684\u73af\u5883\uff0c\u4f7f\u7528`cpost`\u548c`cevent`\uff0c\u53ef\u4ee5\u7b80\u5316\u7f16\u7a0b\uff0c\u66f4\u5bb9\u6613\u5b9e\u73b0\u6a21\u5757\u5316\u7684\u7f16\u7a0b\n\n### cpost\u4f7f\u7528\n\n\u4ee5\u4f7f\u7528\u5728\u5d4c\u5165\u5f0f\u65e0\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u4e3a\u4f8b\uff0c\u4e3b\u8981\u7528\u4f5c\u4e2d\u65ad\u5ef6\u8fdf\u5904\u7406\u7684\u60c5\u51b5\n\n1. \u914d\u7f6e\u7cfb\u7edftick\n\n    \u914d\u7f6e`cpost.h`\u4e2d\u7684\u5b8f`CPOST_GET_TICK()`\uff0c\u914d\u7f6e\u6210\u83b7\u53d6\u7cfb\u7edftick\n\n2. \u914d\u7f6e\u5904\u7406\u8fdb\u7a0b\n\n    \u5728main loop\u8c03\u7528`cpostProcess`\u51fd\u6570\uff0c\u901a\u8fc7`cpost`\u6267\u884c\u7684\u51fd\u6570\u90fd\u4f1a\u5728`cpostProcess`\u4e2d\u6267\u884c\n\n## cevent\u4f7f\u7528\n\n`cevent`\u4f7f\u7528\u6ce8\u518c\u7684\u65b9\u5f0f\u76d1\u542c\u4e8b\u4ef6\uff0c\u4f1a\u4f9d\u8d56\u4e8e\u7f16\u8bd1\u73af\u5883\uff0c\u76ee\u524d\u652f\u6301keil\uff0ciar\uff0c\u548cgcc\uff0c\u5bf9\u4e8egcc\uff0c\u9700\u8981\u4fee\u6539\u94fe\u63a5\u6587\u4ef6(.ld)\uff0c\u5728\u53ea\u8bfb\u6570\u636e\u533a\u6dfb\u52a0\uff1a\n\n```ld\n_cevent_start = .;\nKEEP (*(cEvent))\n_cevent_end = .;\n```\n\n1. \u521d\u59cb\u5316cevent\n\n    \u7cfb\u7edf\u521d\u59cb\u5316\u65f6\uff0c\u8c03\u7528`ceventInit`\n\n    ```c\n    ceventInit();\n    ```\n\n2. \u6ce8\u518ccevent\u4e8b\u4ef6\u76d1\u542c\n\n    \u5728c\u6587\u4ef6\u4e2d\uff0c\u8c03\u7528`CEVENT_EXPORT`\u5bfc\u51fa\u4e8b\u4ef6\u76d1\u542c\n\n    ```c\n    CEVENT_EXPORT(0, handler, (void *)param);\n    ```\n\n3. \u53d1\u9001cevent\u4e8b\u4ef6\n\n    \u5728\u4e8b\u4ef6\u53d1\u751f\u7684\u5730\u65b9\uff0c\u8c03\u7528`ceventPost`\u629b\u51fa\u4e8b\u4ef6\n\n    ```c\n    ceventPost(0);\n    ```\n\n## Api\n\n`cpost`\u63d0\u4f9b\u4e86\u7b80\u5355\u7684\u63a5\u53e3\u7528\u4e8e\u5ef6\u8fdf\u5904\u7406\u51fd\u6570\uff0c\u5176\u63a5\u53e3\u8bbe\u8ba1\u548cAndroid\u7684`Handler`\u8bbe\u8ba1\u7c7b\u4f3c\uff0c`cevnet`\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u9001\u63a5\u53e3\uff0c\u7528\u4e8e\u629b\u51fa\u4e8b\u4ef6\n\n### post\n\npost\u63a5\u53e3\u4f1a\u5c06\u51fd\u6570\u629b\u51fa\uff0c\u7136\u540e\u5728`cpostProcess`\u4e2d\u7acb\u523b\u6267\u884c\n\n```c\nsigned char cpost(void *handler);\n```\n\n- \u53c2\u6570\n\n  - `handler` \u88ab\u629b\u51fa\u7684\u51fd\u6570\n\n- \u8fd4\u56de\u503c\n\n  - `signed char` 0 post\u6210\u529f -1 post\u5931\u8d25\n\n```c\nsigned char cpostEx(void *handler, void *param);\n```\n\n- \u53c2\u6570\n\n  - `handler` \u88ab\u629b\u51fa\u7684\u51fd\u6570\n  - `param` \u4f20\u9012\u7ed9\u88ab\u629b\u51fa\u51fd\u6570\u7684\u53c2\u6570\n\n- \u8fd4\u56de\u503c\n\n  - `signed char` 0 post\u6210\u529f -1 post\u5931\u8d25\n\n### post delay\n\npost delay\u63a5\u53e3\u4f1a\u5c06\u51fd\u6570\u629b\u51fa\uff0c\u7136\u540e\u5728\u6302\u8d77\u5728`cpostProcess`\u4e2d\uff0c\u5f53\u5ef6\u65f6\u65f6\u95f4\u8fbe\u5230\u540e\u6267\u884c\n\n```c\nsigned char cpostDelay(void *handler, size_t delay);\n```\n\n- \u53c2\u6570\n\n  - `handler` \u88ab\u629b\u51fa\u7684\u51fd\u6570\n  - `delay` \u5ef6\u65f6\u65f6\u95f4(tick)\n\n- \u8fd4\u56de\u503c\n\n  - `signed char` 0 post\u6210\u529f -1 post\u5931\u8d25\n\n```c\nsigned char cpostDelayEx(void *handler, void *param, size_t delay);\n```\n\n- \u53c2\u6570\n\n  - `handler` \u88ab\u629b\u51fa\u7684\u51fd\u6570\n  - `param` \u4f20\u9012\u7ed9\u88ab\u629b\u51fa\u51fd\u6570\u7684\u53c2\u6570\n  - `delay` \u5ef6\u65f6\u65f6\u95f4(tick)\n\n- \u8fd4\u56de\u503c\n\n  - `signed char` 0 post\u6210\u529f -1 post\u5931\u8d25\n\n### event post\n\nevent post\u63a5\u53e3\u4f1a\u629b\u51fa\u4e8b\u4ef6\uff0c\u4e8b\u4ef6\u5b9a\u4e49\u4e3a\u4e00\u4e2a`unsigned short`\u6574\u5f62\uff0c\u5efa\u8bae\u901a\u8fc7\u5b8f\u5b9a\u4e49\u6240\u6709\u4e8b\u4ef6\n\n```c\nvoid ceventPost(unsigned short event);\n```\n\n- \u53c2\u6570\n\n  - `event` \u4e8b\u4ef6\n\n### event export\n\nevent post\u63a5\u53e3\u4f1a\u9759\u6001\u6ce8\u518c\u4e00\u4e2a\u4e8b\u4ef6\u76d1\u542c\uff0c\u6ce8\u610f\uff0c\u6b64\u63a5\u53e3\u5b9e\u9645\u4e0a\u4e3a\u5b9a\u4e49\u4e00\u4e2a\u5168\u5c40\u7684\u5e38\u91cf\uff0c\u6240\u4ee5\u9700\u8981\u5199\u5728\u51fd\u6570\u4f53\u5916\n\n```c\nCEVENT_EXPORT(_event, _func, ...)\n```\n\n- \u53c2\u6570\n\n  - `_event` \u88ab\u76d1\u542c\u7684\u4e8b\u4ef6\n  - `_func` \u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u6267\u884c\u7684\u51fd\u6570\n  - `...` \u9700\u8981\u4f20\u9012\u7ed9`_func`\u51fd\u6570\u7684\u53c2\u6570\n"
 },
 {
  "repo": "micro-bitcoin/uBitcoin",
  "language": "C",
  "readme_contents": "# Micro-Bitcoin\n\nC++ Bitcoin library for 32-bit microcontrollers. The library supports [Arduino IDE](https://www.arduino.cc/), [ARM mbed](https://www.mbed.com/en/) and bare metal.<br>\nIt provides a collection of convenient classes for Bitcoin: private and public keys, HD wallets, generation of the recovery phrases, PSBT transaction formats, scripts \u2014 everything required for a hardware wallet or other bitcoin-powered device.\n\nThe library should work on any decent 32-bit microcontroller, like esp32, riscV, stm32 series and others. It *doesn't work* on 8-bit microcontrollers like a classic Arduino as these microcontrollers are not powerful enough to run complicated crypto algorithms.\n\nWe use elliptic curve implementation from [trezor-crypto](https://github.com/trezor/trezor-firmware/tree/master/crypto). API is inspired by [Jimmy Song's](https://github.com/jimmysong/) Porgramming Blockchain class and the [book](https://github.com/jimmysong/programmingbitcoin).\n\n## Documentation\n\nCheck out our [tutorial](https://micro-bitcoin.github.io/#/tutorial/README) where we write a minimal hardware wallet, or browse the [API docs](https://micro-bitcoin.github.io/#/api/README). We also have a collection of [recepies](https://micro-bitcoin.github.io/#/recepies/README) for some common use-cases.\n\nTelegram group: https://t.me/arduinoBitcoin\n\n## Alternative libraries\n\n[DIY Bitcoin Hardware website](https://diybitcoinhardware.com/) has a nice collection of bitcoin-related projects, resources and libraries for makers.\n\nA few bitcoin libraries:\n\n- [secp256k1](https://github.com/bitcoin-core/secp256k1) \u2014 elliptic curve library  from Bitcoin Core and a [version](https://github.com/diybitcoinhardware/secp256k1-embedded) working with Arduino IDE & Mbed out of the box.\n- [libwally](https://github.com/ElementsProject/libwally-core/) - bitcoin library from Blockstream and a [version](https://github.com/diybitcoinhardware/libwally-embedded) working with Arduino IDE.\n- [f469-disco](https://github.com/diybitcoinhardware/f469-disco) - micropython bitcoin bundle for STM Discovery board and other platforms.\n\n## Installation\n\nThe library is [available](https://www.arduino.cc/reference/en/libraries/ubitcoin/) in the Arduino Library manager, or you can download and install it manually.\n\n[Download](https://github.com/micro-bitcoin/uBitcoin/archive/master.zip) the zip file from our [repository](https://github.com/micro-bitcoin/uBitcoin/) and select in Arduino IDE `Sketch` \u2192 `Include library` \u2192 `Add .ZIP library...`.\n\nOr clone it into your `Documents/Arduino/libraries` folder:\n\n```sh\ngit clone https://github.com/micro-bitcoin/uBitcoin.git\n```\n\nWhen installed you will also see a few examples in `File` \u2192 `Examples` \u2192 `Bitcoin` menu.\n\n## Basic usage example\n\nFirst, don't forget to include necessary headers:\n\n```cpp\n// we use these two in our sketch:\n#include \"Bitcoin.h\"\n#include \"PSBT.h\"       // if using PSBT functionality\n// other headers of the library\n#include \"Conversion.h\" // to get access to functions like toHex() or fromBase64()\n#include \"Hash.h\"       // if using hashes in your code\n```\n\nNow we can write a simple example that does the following:\n\n1. Creates a master private key from a recovery phrase and empty password\n2. Derives account and prints master public key for a watch-only wallet (`zpub` in this case)\n3. Derives and print first segwit address\n4. Parses, signs and prints signed PSBT transaction\n\n```cpp\n// derive master private key\nHDPrivateKey hd(\"add good charge eagle walk culture book inherit fan nature seek repair\", \"\");\n// derive native segwit account (bip-84) for tesnet\nHDPrivateKey account = hd.derive(\"m/84'/1'/0'/\");\n// print xpub: vpub5YkPqVJTA7gjK...AH2rXvcAe3p781G\nSerial.println(account.xpub());\n// or change the account type to UNKNOWN_TYPE to get tpub\nHDPublicKey xpub = account.xpub();\nxpub.type = UNKNOWN_TYPE;\n// this time prints tpubDCnYy4Ty...dL4fLKsBFjFQwz\nSerial.println(xpub);\n// set back correct type to get segwit addresses by default\nxpub.type = P2WPKH;\nSerial.println(hd.fingerprint());\n\n// print first address: tb1q6c8m3whsag5zadgl32nmhuf9q0qmtklws25n6g\nSerial.println(xpub.derive(\"m/0/0\").address());\n\nPSBT tx;\n// parse unsigned transaction\ntx.parseBase64(\"cHNidP8BAHECAAAAAUQS8FqBzYocPDpeQmXBRBH7NwZHVJF39dYJDCXxq\"\n\"zf6AAAAAAD+////AqCGAQAAAAAAFgAUuP0WcSBmiAZYi91nX90hg/cZJ1U8AgMAAAAAABYAF\"\n\"C1RhUR+m/nFyQkPSlP0xmZVxlOqAAAAAAABAR/gkwQAAAAAABYAFNYPuLrw6igutR+Kp7vxJ\"\n\"QPBtdvuIgYDzkBZaAkSIz0P0BexiPYfzInxu9mMeuaOQa1fGEUXcWIYoyAeuFQAAIABAACAA\"\n\"AAAgAAAAAAAAAAAAAAiAgMxjOiFQofq7l9q42nsLA3Ta4zKpEs5eCnAvMnQaVeqsBijIB64V\"\n\"AAAgAEAAIAAAACAAQAAAAAAAAAA\");\n// sign with the root key\ntx.sign(hd);\n// print signed transaction\nSerial.println(tx.toBase64());\n```\n\nReady for more? Check out the [tutorial](https://micro-bitcoin.github.io/#/tutorial/README) and start writing your very own hardware wallet!\n"
 },
 {
  "repo": "marcinbor85/cAT",
  "language": "C",
  "readme_contents": "[![Build Status](https://travis-ci.org/marcinbor85/cat.svg?branch=master)](https://travis-ci.org/marcinbor85/cat)\n# libcat (cAT)\nPlain C library for parsing AT commands for use in host devices.\n\n## Features\n* blazing fast, non-blocking, robust implementation\n* 100% static implementation (without any dynamic memory allocation)\n* very small footprint (both RAM and ROM)\n* support for READ, WRITE, TEST and RUN type commands\n* commands shortcuts (auto select best command candidate)\n* single request - multiple responses\n* unsolicited read/test command support\n* hold state for delayed responses for time-consuming tasks\n* high-level memory variables mapping arguments parsing\n* variables accessors (read and write, read only, write only)\n* automatic arguments types validating\n* automatic format test responses for commands with variables\n* CRLF and LF compatible\n* case-insensitive\n* dedicated for embedded systems\n* object-oriented architecture\n* separated interface for low-level layer\n* fully asynchronous input/output operations\n* multiplatform and portable\n* asynchronous api with event callbacks\n* print registered commands list feature\n* only two source files\n* wide unit tests\n\n## Build\n\nBuild and install:\n\n```sh\ncmake .\nmake\nmake test\nsudo make install\n```\n\n## Example basic demo posibilities\n\n```console\nAT+PRINT=?                                              # TEST command\n+PRINT=<X:UINT8[RW]>,<Y:UINT8[RW]>,<MESSAGE:STRING[RW]> # Automatic response\nPrinting something special at (X,Y).                    # Automatic response\nOK                                                      # Automatic acknowledge\n\nAT+PRINT?                                               # READ command\n+PRINT=0,0,\"\"                                           # Automatic response\nOK                                                      # Automatic acknowledge\n\nAT+PRINT=xyz,-2                                         # WRITE command\nERROR                                                   # Automatic acknowledge\n\nAT+PRINT=1,2,\"test\"                                     # WRITE command\nOK                                                      # Automatic acknowledge\n\nAT+PRINT                                                # RUN command\nsome printing at (1,2) with text \"test\"                 # Manual response\nOK                                                      # Automatic acknowledge\n```\n\n## Example unsolicited demo posibilities\n\n```console\nAT+START=?                                              # TEST command\n+START=<MODE:UINT32[WO]>                                # Automatic response\nStart scanning after write (0 - wifi, 1 - bluetooth).   # Automatic response\nOK                                                      # Automatic acknowledge\n\nAT+START=0                                              # WRITE command\n+SCAN=-10,\"wifi1\"                                       # Unsolicited read response\n+SCAN=-50,\"wifi2\"                                       # Unsolicited read response\n+SCAN=-20,\"wifi3\"                                       # Unsolicited read response\nOK                                                      # Unsolicited acknowledge\n\nAT+START=1                                              # WRITE command\n+SCAN=-20,\"bluetooth1\"                                  # Unsolicited read response\nOK                                                      # Unsolicited acknowledge\n\nAT+SCAN=?                                               # TEST command\n+SCAN=<RSSI:INT32[RO]>,<SSID:STRING[RO]>                # Automatic response\nScan result record.                                     # Automatic response\nOK                                                      # Automatic acknowledge\n```\n\n## Usage\n\nDefine High-Level variables:\n\n```c\n\nstatic uint8_t x;\nstatic uint8_t y;\nstatic char msg[32];\n\nstatic struct cat_variable go_vars[] = {\n        {\n                .type = CAT_VAR_UINT_DEC, /* unsigned int variable */\n                .data = &x,\n                .data_size = sizeof(x),\n                .write = x_write,\n                .name = \"X\",\n                .access = CAT_VAR_ACCESS_READ_WRITE,\n        },\n        {\n                .type = CAT_VAR_UINT_DEC, /* unsigned int variable */\n                .data = &y,\n                .data_size = sizeof(y),\n                .write = y_write,\n                .access = CAT_VAR_ACCESS_READ_WRITE,\n        },\n        {\n                .type = CAT_VAR_BUF_STRING, /* string variable */\n                .data = msg,\n                .data_size = sizeof(msg),\n                .write = msg_write,\n                .access = CAT_VAR_ACCESS_READ_WRITE,\n        }\n};\n```\n\nDefine AT commands descriptor:\n\n```c\nstatic struct cat_command cmds[] = {\n        {\n                .name = \"TEST\",\n                .read = test_read, /* read handler for ATTEST? command */\n                .write = test_write, /* write handler for ATTEST={val} command */\n                .run = test_run /* run handler for ATTEST command */\n        },\n        {\n                .name = \"+NUM\",\n                .write = num_write, /* write handler for AT+NUM={val} command */\n                .read = num_read /* read handler for AT+NUM? command */\n        },\n        {\n                .name = \"+GO\",\n                .write = go_write, /* write handler for AT+GO={x},{y},{msg} command */\n                .var = go_vars, /* attach variables to command */\n                .var_num = sizeof(go_vars) / sizeof(go_vars[0]),\n                .need_all_vars = true\n        },\n        {\n                .name = \"RESTART\",\n                .run = restart_run /* run handler for ATRESTART command */\n        }\n};\n```\n\nDefine AT command parser descriptor:\n\n```c\n\nstatic char working_buf[128]; /* working buffer, must be declared manually */\n\nstatic struct cat_command_group cmd_group = {\n        .cmd = cmds,\n        .cmd_num = sizeof(cmds) / sizeof(cmds[0]),\n};\n\nstatic struct cat_command_group *cmd_desc[] = {\n        &cmd_group\n};\n\nstatic struct cat_descriptor desc = {\n        .cmd_group = cmd_desc,\n        .cmd_group_num = sizeof(cmd_desc) / sizeof(cmd_desc[0]),\n\n        .buf = working_buf,\n        .buf_size = sizeof(working_buf),\n};\n```\n\nDefine IO low-level layer interface:\n\n```c\nstatic int write_char(char ch)\n{\n        putc(ch, stdout);\n        return 1;\n}\n\nstatic int read_char(char *ch)\n{\n        *ch = getch();\n        return 1;\n}\n\nstatic struct cat_io_interface iface = {\n        .read = read_char,\n        .write = write_char\n};\n```\n\nInitialize AT command parser and run:\n\n```c\nstruct cat_object at; /* at command parser object */\n\ncat_init(&at, &desc, &iface, NULL); /* initialize at command parser object */\n\nwhile (1) {\n        cat_service(&at) /* periodically call at command parser service */\n\n        ... /* other stuff, running in main loop */\n}\n\n```\n"
 },
 {
  "repo": "AndrewFromMelbourne/raspidmx",
  "language": "C",
  "readme_contents": "# Dispmanx\n\nThere are a number of APIs available for the Raspberry Pi that can make use\nof the computers GPU. These include OpenMAX, Open GL ES(1 and 2) and OpenVG.\nThe raspberrypi/firmware repository has short examples for these and other\nAPIs. They can be found in /opt/vc/src/hello_pi/ on the Raspbian 'wheezy'\nimage. Among these examples is a program called hello_dispmanx. It is a\nvery small example of the Dispmanx windowing system. Apart from this:-\n\nhttps://github.com/raspberrypi/firmware/tree/master/opt/vc/src/hello_pi/hello_dispmanx\n\nexample, there is very little documentation available for this API. There\nare snippets of information on the Raspberry Pi forum, but I have not found\na single place with detailed information on DispmanX.  Hopefully these\nprograms can be used as a starting point for anyone wanting to make use of\nDispmanX.\n\nThe programs demonstrate layers with the following types: 4BPP (4 bit\nindexed), 8BPP (8 bit indexed), RGB565 (16 bit), RGB888 (24 bit), RGBA16\n(16 bit with transparency) and RGBA32 (32 bit with transparency)\n\n## test_pattern\n\nThis test pattern should be familiar to anyone who has used the Raspberry\nPi. It is the same four colour square displayed when the Raspberry Pi boots.\n\n## rgb_triangle\n\nDisplays a triangle in a layer with red, green and blue gradients starting\nat each corner respectively. Blends to grey in the center. Demonstrates\nchanging size of source and destination rectangles.\n\n## life\n\nConway's game of life. Demonstrates double buffering.\n\n## worms\n\nThe program raspiworms uses a single 16 or 32 bit RGBA layer to display a\nnumber of coloured worms on the screen of the Raspberry Pi.\n\n## pngview\n\nLoad a png image file and display it as a Dispmanx layer.\n\n## spriteview\n\nLoads a sprite (png) image file and displays it as an animation.\n\n## game\n\nDemonstrates a seamless background image that can be scolled in any\ndirection. As well as animated sprites.\n\n## mandelbrot\n\nThe famous (in the 1990s) Mandelbrot set.\n\n## radar_sweep\n\nAn animation of a 'radar sweep' using 16 bit (rgb) palette animation.\n\n## radar_sweep_alpha\n\nAn animation of a 'radar sweep' using 32 bit (rgba) palette animation.\n\n## offscreen\n\nAn example of using an offscreen display to resize an image.\n\n## common\n\nCode that may be common to some of the demonstration programs is in this\nfolder.\n\n## building\n\nIf you type make in the top level directory, the make file will build all\nthe different programs in this repository. Each program has its own make\nfile, so you can build them individually if you wish.\n\nYou will need to install libpng before you build the program. On Raspbian\n\nsudo apt-get install libpng12-dev\n\n"
 },
 {
  "repo": "picosonic/bbc-fdc",
  "language": "C",
  "readme_contents": "# bbc-fdc\nFloppy disk interface for Raspberry Pi\n\nThis project is to allow the direct connection of floppy disk drives with 34-pin [Shugart bus](https://en.wikipedia.org/wiki/Shugart_bus) ribbon cables to the [Raspberry Pi](https://www.raspberrypi.org/) for the purpose of reading floppy disks.\n\nIt controls the drive using GPIO and samples the read data pin using SPI to obtain a forensic level capture of the raw magnetic flux transitions on the floppy disk.\n\nInitially this was created to read my BBC Micro 5.25 inch disks formatted in [Acorn DFS](http://beebwiki.mdfs.net/Acorn_DFS_disc_format), but I've also been able to read and extract data from Acorn ADFS, MS-DOS, Commodore 64 and Apple II 5.25 inch disks.\n\nThe Cumana dual 5.25 inch disk drive (built November 1985) I'm using is capable of reading multiple different logical formats.\n\nI've read both 40 and 80 track 5.25 inch disks, both using the 40/80 track selector switch on the back of the drive and by double stepping. These tracks are spaced at 48 and 96 tpi respectively.\n\nSingle and double sided disks can be read by switching between upper and lower heads during the capture process. Flippy disks can also be detected and imaged without the need for flipping the disk in the drive.\n\nThe BBC Micro Acorn DFS format used 40 or 80 tracks, 10 sectors per track (numbered 0 to 9), 256 bytes per sector, with FM encoding (single density). This gives a maximum data capacity for a double sided 80 track DFS disk of 409,600 bytes. However 2 sectors on each side of the disk are reserved for the catalogue.\n\nMore recently I've also been able to read 3.5 inch disks using a TEAC FD-235HG PC drive. Due to the drive being internally set to DS1 I've had to connect it to the 34 pin ribbon cable prior to the swap and by moving the drive select jumper on my board to DS1.\n\nI've so far been able to read and extract data from several 3.5 inch disks including Archimedes Acorn ADFS S/M/L/D/E/F, MS-DOS, Amiga and Atari ST.\n\nFor testing purposes I've run some disk images via a Gotek running the excellent [FlashFloppy](https://github.com/keirf/flashfloppy) by Keir Fraser into the Pi via BBC-FDC and got good results on several image formats. This also means I should be able to support cataloguing disk contents for more formats which I don't have physical media for.\n\n![Top of board](/circuit/top.jpg?raw=true \"Top of board\")\n"
 },
 {
  "repo": "Phenomite/AMP-Research",
  "language": "C",
  "readme_contents": "# AMP-Research\n\n## Research on exotic UDP/TCP amplification vectors, payloads and mitigations\n\n**The subfolders in this repository will contain the following:**\n\n* Overview README.md\n  * Name, Ports, Amplification factors, Update Info\n  * Request <> Response Example with test IP (netcat yay!)\n  * Potential official documentation\n  * Potential mitigation strategies\n* The raw payload (e.g. for use in zmap) OR potential scanning script (C).\n* Raw socket flood script (C) for analysis to build flowspec or ACL mitigations.\n\n## Who referenced this repository (Kudos!)\n\n* Honeypot research shows variety of DDoS amplification methods (SRLabs 2021-07-30) - <https://www.srlabs.de/bites/honeypot-research-shows-variety-of-ddos-amplification-methods>\n* DHCPDiscover Reflection/Amplification DDoS Attack Mitigation Recommendations | NETSCOUT (2021-07-07) - <https://www.netscout.com/blog/asert/dhcpdiscover-reflectionamplification-ddos-attack-mitigation>\n* Powerhouse VPN Servers being abused (2021-02-22) - <https://www.zdnet.com/article/powerhouse-vpn-products-can-be-abused-for-large-scale-ddos-attacks/> \n* DVR reflection abused against Azure R6 and Ark Evolved servers (2021-02-04) - <https://azure.microsoft.com/en-au/blog/azure-ddos-protection-2020-year-in-review/>\n* MS-RDPEUDP Scanning has begun by The Shadowserver Foundation (2021-01-25) - <https://www.shadowserver.org/news/scanning-for-accessible-ms-rdpeudp-services/>\n\n## What is \"amplification\" in respect to Denial of Service? Give me an Example!\n\nAmplification is where well-formed or malformed socket or application data requests elicit a response larger than the input data. This can then be abused to \"amplify\" a request, usually by means of Distributed Reflected Denial of Service (DRDoS) attacks. This distinction is usually lumped under the one banner of \"DDoS\"; however the former indicates that the traffic does not directly come from bots or single servers but is reflected off of usually benign services, thus typically rendering blacklists and simple firewall solutions useless.\n\nBest way to show what this means is using the network protocol MSSQL over TCP/IP UDP port 1434 as an example.\n\n### Example UDP response size from **1 byte** to a MSSQL (Microsoft SQL Server) listener\n\n> `echo -ne '\\x02' | nc -u -q 2 190.xx.xx.xx 1434|xxd -p|wc -c`\n\t<pre>629 bytes</pre>\n\nThat's an amplification factor of over **23** times.\n\n### Example hex response from a discovery probe to an ARD (Apple Remote Desktop) listener\n\n> `echo -ne '\\x00\\x14\\x00\\x01\\x03' |nc -u 89.xx.xx.xx 3283|hexdump`\n    <pre>\n\t0000000 0100 ea03 3100 0000 0000 0000 0000 0000\n\t0000010 0000 0000 0000 0000 0000 0000 0000 0000\n\t0000020 0000 0000 0000 0000 0100 0000 0000 0000\n\t0000030 0000 0000 0000 0000 0000 0000 0000 0000\n\t_\n\t0000050 0000 1200 0000 0000 0000 0000 0000 0000\n\t0000060 0000 0000 0000 0000 0000 0000 0000 0000\n\t0000070 0000 0000 0000 0000 0000 0000 0000 640a\n\t0000080 7461 6861 6565 6472 0034 0000 0000 0000\n\t0000090 0000 0000 0000 0000 0000 0000 0000 0000\n\t_\n\t00000c0 0000 0001 0000 0000 0000 0000 0000 0000\n\t00000d0 0000 0000 0000 9803 0000 0100 18f0 ed98\n\t00000e0 9288 0000 0000 0a00 6400 6100 7400 6100\n\t00000f0 6800 6500 6500 7200 6400 3400 0000 0000\n\t0000100 0000 0000 0000 0000 0000 0000 0000 0000\n\t</pre>\n\n## Compiling the C code in this repo?\n\nGeneral C scripts:\n\n```bash\ngcc -pthread -O2 -o binary file.c\n```\n\nTCP scripts (requires 32bit compilation to avoid invalid checksum function return values):\n\n```bash\ngcc -m32 -pthread -O2 -o binary file.c\n```\n\n## Vulnerable reflectors\n\nThis repo is here to help everyone mitigate amplification vectors that have yet to be abused or are being actively abused with little related or consolidated information.\n\nReflector lists are scanned and provided on a case by case basis or as necessary for remediation [on pastebin here](https://pastebin.com/u/Phenomite/1/6WuyRz1m).\n\n* Examples of cases include:\n  * Infected hosts that need to be quickly added to a blacklist to get the attention of network owners.\n  * Protocols that are devastating (e.g. MemcacheD) and require publicized lists to blackhole or to bulk contact network owners.\n  * Because shodan already has you.\n"
 },
 {
  "repo": "MDeiml/tree-sitter-markdown",
  "language": "C",
  "readme_contents": "# tree-sitter-markdown\nA markdown parser for tree-sitter\n\nFor now this implements the [CommonMark Spec](https://spec.commonmark.org/). Maybe it will be extended to support [Github flavored markdown](https://github.github.com/gfm/)\n\n## Structure\n\nThe parser is spit into two grammars. One for the [block structure](https://spec.commonmark.org/0.30/#blocks-and-inlines) which can be found in `/tree-sitter-markdown` and one for the [inline structure](https://spec.commonmark.org/0.30/#inlines) which is in `/tree-sitter-markdown-inline`.\nBecause of this the entire document has to be scanned twice in order to be fully parsed.\nThis is motivated by the [parsing strategy section](https://spec.commonmark.org/0.30/#appendix-a-parsing-strategy) of the CommonMark Spec which suggests doing exactly this: Parsing the document twice, first determining the block structure and then parsing any inline content.\n\nIt also helps managing complexity, which was a problem with earlier versions of this parser, by allowing block and inline structure to be considered seperately. This was not the case as tree-sitters dynamic precedence can create hard to predict effects.\n\n## Usage\n\nTo use the two grammars, first parse the document with the block grammar. Then perform a second parse with the inline grammar using `ts_parser_set_included_ranges` to specify which parts are inline content. These parts are marked as `inline` nodes. Children of those inline nodes should be excluded from these ranges. For an example implementation see `lib.rs` in the `bindings` folder.\n"
 },
 {
  "repo": "zoogie/DSP1",
  "language": "C",
  "readme_contents": ""
 },
 {
  "repo": "wojciech-graj/TermGL",
  "language": "C",
  "readme_contents": "# TermGL\n\n[![Language grade: C/C++](https://img.shields.io/lgtm/grade/cpp/g/wojciech-graj/TermGL.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/wojciech-graj/TermGL/context:cpp)\n\nA terminal-based graphics library for both 2D and 3D graphics.\\\nWorks in all terminals supporting ANSI escape codes.\\\nSupport for Windows and UNIX.\\\nC99 compliant, with no external dependencies.\\\nRealtime input reading from terminal for user-interaction.\\\n16 Background colors, 16 foreground colors, bold and underline.\n\n## Table of Contents\n\n[Gallery](https://github.com/wojciech-graj/TermGL/blob/master/README.md#Gallery)\\\n[Build](https://github.com/wojciech-graj/TermGL/blob/master/README.md#Build)\\\n[Documentation](https://github.com/wojciech-graj/TermGL/blob/master/README.md#Documentation)\n\n## Gallery\n\n![LOGO](demodir/logo.gif)\n\n![CANYON](demodir/canyon.gif)\n\n![TEAPOT](demodir/teapot.gif)\n\n## Build\n\n### C\n\nTo enable 3D functionality, use the ```-DTERMGL3D``` compiler flag\\\nTo enable utility functions, use the ```-DTERMGLUTIL``` compiler flag\\\nWhen running makefile, additional cflags can be specified be passing a command line argument ```CFLAGS=-DTERMGL3D\\ -DTERMGLUTIL```\n\n#### Method 1: Regular source file\n\n1. Add the following flags to your compiler ```-I/path/to/TermGL/lib -lm```\n2. Add termgl.c as a source file to be compiled\n\n#### Method 2: Shared library\n\n1. Run the makefile ```make shared``` to create ```lib/libtermgl.so```\n2. Copy to library directory ```sudo cp lib/libtermgl.so /usr/local/lib/libtermgl.so```\n3. Add the following flags to your compiler ```-I/path/to/TermGL/lib -ltermgl -lm```\n\n#### Method 3: Static library\n\n1. Run the makefile ```make shared``` to create ```lib/libtermgl.so```\n2. Add the following flags to your compiler ```-I/path/to/TermGL/lib -L/path/to/TermGL/lib -ltermgl -lm```\n\n### C++\n\nThe above Methods 2 and 3 for C can be used to use TermGL in C++\n\n### Windows\n\nCompilation for Windows can be done by chaning the compiler using the following command line argument when running the makefile ```COMPILER=```.\nIf compiling using mingw on linux, use ```COMPILER=i686-w64-mingw32-gcc-win32```.\n\n### Demo\n\nTo compile a demo program, run the makefile ```make demo```.\n\n## Documentation\n\nCertain settings can be changed at the top of [src/termgl.c](src/termgl.c) prior to compilation, e.g. memory allocation functions, clear screen command, compiler-specific commands.\\\nThe header file [lib/termgl.h](lib/termgl.h) contains brief documentation for all functions and structs.\\\nCompiler-specific functionality is used, therefore it is recommened to always compile using GCC. If other compilers are used for other sections of code, see Build: Methods 2 & 3.\\\nThe TermGLUtil extension contains functions for reading keyboard input, but requires either Windows of UNIX headers.\n\n### Demo\n\nA demo can be found here: [demodir/termgl_demo.c](demodir/termgl_demo.c).\\\nAvailable demos and TermGL features used:\n1. Utah Teapot\\\nRenders a rotating 3D Utah Teapot.\n\t- Backface culling\n\t- Z buffering\n\t- Double-width characters\n\t- 3D camera\n\t- 3D transformations\n\t- 3D rendering\n\t- 3D Shaders\n2. Star Polygon\\\nRenders a star polygon in steps using random colors.\n\t- Colors\n\t- Line rendering\n3. Color Palette\\\nRenders a palette of various text colors and styles.\n\t- Colors & Modifiers\n4. Mandelbrot\\\nRenders an infinitely zooming-in Mandelbrot set.\n\t- Point rendering\n5. Realtime Keyboard\\\nDisplays keyboard input in realtime.\n\t- Text rendering\n\t- Realtime keyboard input\n"
 },
 {
  "repo": "DavidLeeds/hashmap",
  "language": "C",
  "readme_contents": "# hashmap\n[![ci](https://github.com/DavidLeeds/hashmap/workflows/CI/badge.svg)](https://github.com/DavidLeeds/hashmap/actions/workflows/ci.yml)\n\nTemplated type-safe hashmap implementation in C using open addressing and linear probing for collision resolution.\n\n## Summary\nThis project came into existence because there are a notable lack of flexible and easy to use data structures available in C. C data structures with efficient, type-safe interfaces are virtually non-existent.  Sure, higher level languages have built-in libraries and templated classes, but plenty of embedded projects or higher level libraries are implemented in C.  It was undesirable to add a bulky library like Glib as a dependency to my projects, or grapple with a restrictive license agreement.  Searching for \"C hashmap\" yielded results with questionable algorithms and code quality, projects with difficult or inflexible interfaces, or projects with less desirable licenses.  I decided it was time to create my own.\n\n\n## Goals\n* **To scale gracefully to the full capacity of the numeric primitives in use.**  E.g. on a 32-bit machine, you should be able to load a billion+ entries without hitting any bugs relating to integer overflows.  Lookups on a hashtable with a billion entries should be performed in close to constant time, no different than lookups in a hashtable with 20 entries.  Automatic rehashing occurs and maintains a load factor of 0.75 or less.\n* **To provide a clean and easy-to-use interface.**  C data structures often struggle to strike a balance between flexibility and ease of use.  To this end, I wrapped a generic C backend implementation with light-weight pre-processor macros to create a templated type-safe interface. All required type information is encoded in the hashmap declaration using the`HASHMAP()` macro. Unlike with header-only macro libraries, there is no code duplication or performance disadvantage over a traditional library with a non-type-safe `void *` interface.\n* **To enable easy iteration and safe entry removal during iteration.**  Applications often need these features, and the data structure should not hold them back.  Easy to use `hashmap_foreach()` macros and a more flexible iterator interface are provided.  This hashmap also uses an open addressing scheme, which has superior iteration performance to a similar hashmap implemented using separate chaining (buckets with linked lists).  This is because fewer instructions are needed per iteration, and array traversal has superior cache performance than linked list traversal.\n* **To use a very unrestrictive software license.**  Using no license was an option, but I wanted to allow the code to be tracked, simply for my own edification.  I chose the MIT license because it is the most common open source license in use, and it grants full rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell the code.  Basically, take this code and do what you want with it.  Just be nice and leave the license comment and my name at top of the file.  Feel free to add your name if you are modifying and redistributing.\n\n## API Examples\n\n### Declaring a type-specific hashmap\n\nUse the `HASHMAP(key_type, value_type)` macro to declare a hashmap state struct specific to your needs. Keys and values are always passed in by pointer. Keys are const.\n\n```C\n/* Map with string key (const char *) and integer value (int *) */\nHASHMAP(char, int) map1;\n\n/* Map with uint64 key (const uint64_t *) and struct value (struct my_value *) */\nHASHMAP(uint64_t, struct my_value) map2;\n```\n\nThe structure defined by the `HASHMAP()` macro may be used directly, or named using `typedef`. For example:\n```C\ntypedef HASHMAP(char, struct my_value) value_map_t;\n```\n\n### Initialization and cleanup\n\nMaps must be initialized with a key hash function and a key comparator. \n```C\n/* Initialize the map structure */\nhashmap_init(&map, my_key_hash, my_key_compare);\n\n/* Use the map... */\n\n/* Free resources associated with the map */\nhashmap_cleanup(&map);\n```\n\nThis library provides some hash functions, so you may not have to write your own:\n* [hashmap_hash_string()](https://github.com/DavidLeeds/hashmap/blob/137d60b3818c22c79d2be5560150eb2eff981a68/include/hashmap_base.h#L54) - Case sensitive string hash\n* [hashmap_hash_string_i()](https://github.com/DavidLeeds/hashmap/blob/137d60b3818c22c79d2be5560150eb2eff981a68/include/hashmap_base.h#L55) - Case insensitive string hash\n* [hashmap_hash_default()](https://github.com/DavidLeeds/hashmap/blob/137d60b3818c22c79d2be5560150eb2eff981a68/include/hashmap_base.h#L53) - Hash function for arbitrary bytes that can be used by a user-defined hash function\n\nI recommend using these, unless you have very specific needs.\n\n```C\n/* Initialize a map with case-sensitive string keys */\nhashmap_init(&map, hashmap_hash_string, strcmp);\n```\n\nNote that memory associated with map keys and values is not managed by the map, so you may need to free this before calling `hashmap_cleanup()`. Keys are often stored in the same structure as the value, but it is possible to have the map manage key memory allocation internally, by calling `hashmap_set_key_alloc_funcs()`.\n\n\n### Value insertion and access\n\n```C\n/* Insert a my_value (fails and returns -EEXIST if the key already exists) */\nint result = hashmap_put(&map, \"KeyABC\", val);\n\n/* Access the value with a given key */\nstruct my_value *val = hashmap_get(&map, \"KeyABC\");\n```\n\n### Value removal\n\n```C\n/* Erase the entry with the given key */\nstruct my_value *val = hashmap_remove(&map, \"KeyABC\");\n\n/* Erase all entries */\nhashmap_clear(&map);\n\n/* Erase all entries and reset the hash table to its initial size */\nhashmap_reset(&map);\n```\n\n### Iteration\n\nIteration may be accomplished using the \"convenience\" `foreach` macros, or by using the iterator interface directly. Generally, the `foreach` macros are the most intuitive and convenient.\n\n```C\nconst char *key;\nstruct my_value *val;\n\n/* Iterate over all map entries and access both keys and values */\nhashmap_foreach(key, val, &map) {\n    /* Access each entry */\n}\n\n/* Iterate over all map entries and access just keys */\nhashmap_foreach_key(key, &map) {\n    /* Access each entry */\n}\n\n/* Iterate over all map entries and access just values */\nhashmap_foreach_data(val, &map) {\n    /* Access each entry */\n}\n```\n\nThe above iteration macros are only safe for read-only access. To safely remove the current element during iteration, use the macros with a `_safe` suffix. These require an additional pointer parameter. For example:\n```C\nconst char *key;\nstruct my_value *val;\nvoid *temp;\n\n/* Okay */\nhashmap_foreach_key_safe(key, &map, temp) {\n    hashmap_remove(&map, key);\n}\n```\n\nIteration using the iterator interface.\n```C\nHASHMAP_ITER(map) it;\n\nfor (it = hashmap_iter(&map); hashmap_iter_valid(&it); hashmap_iter_next(&it) {\n\t/*\n\t * Access entry using:\n\t *   hashmap_iter_get_key()\n\t *   hashmap_iter_get_data()\n\t *   hashmap_iter_set_data()\n\t */\n}\n```\n\n### Additional examples\nAre located in the `examples` directory in the source tree.\n\n## How to Build and Install\nThis project uses CMake to orchestrate the build and installallation process. To build and install on your host system, follow these easy steps:\n1. `git clone https://github.com/DavidLeeds/hashmap.git` - download the source\n2. `mkdir build-hashmap && cd build-hashmap` - create a build directory outside the source tree\n3. `cmake ../hashmap` - run CMake to setup the build\n4. `make` - compile the code\n5. `make test` - run the unit tests (if enabled)\n6. `sudo make install` - _OPTIONAL_ install the library on this system\n\n##### CMake Options\n\n* `HASHMAP_BUILD_TESTS` - Set to `ON` to generate unit tests.\n* `HASHMAP_BUILD_EXAMPLES` - Set to `ON` to build example code. \n\n## Contibutions and Questions\nI welcome all questions and contributions. Feel free to e-mail me, or put up a pull request. The core algorithm is stable, but I'm happy to consider CMake improvements, compiler compatibility fixes, or API additions.\n\n"
 },
 {
  "repo": "ClockVapor/rpi_gpio",
  "language": "C",
  "readme_contents": "# rpi_gpio v0.5.0\n\nRuby conversion of [RPi.GPIO Python module](https://pypi.python.org/pypi/RPi.GPIO)\n\n## Features\n\nManipulate your Raspberry Pi's GPIO pins from Ruby!\n\n- Boolean input/output\n- Software-driven PWM (written in C for speed)\n- Event-driven input (blocking and non-blocking)\n\nUp-to-date with RPi.GPIO Python module version 0.7.0, so it works on all Raspberry Pi models!\n\n## Sample Usage\n\nI aimed to make the gem's usage exactly the same as its Python counterpart -- only with a few semantic differences to utilize Ruby's readability. If anything is confusing, you can always check [here](http://sourceforge.net/p/raspberry-gpio-python/wiki/Examples/) for the original Python module's documentation.\n\n#### Download the gem\n\nThe easiest way to download the gem is to use [Bundler](http://bundler.io/) with a Gemfile. In your Gemfile, include the line \n```ruby\ngem 'rpi_gpio'\n```\nThen you can run `bundle install` to automatically download and compile the gem for your system. To include the gem in a Ruby file, use the line `require 'rpi_gpio'`.\n\n#### Pin numbering\n\nBefore you can do anything with the GPIO pins, you need to specify how you want to number them.\n```ruby\nRPi::GPIO.set_numbering :board\n# or\nRPi::GPIO.set_numbering :bcm\n````\n`:board` numbering refers to the physical pin numbers on the Pi, whereas `:bcm` numbering refers to the Broadcom SOC channel numbering. Note that `:bcm` numbering differs between Pi models, while `:board` numbering does not.\n\n#### Input\n\nTo receive input from a GPIO pin, you must first initialize it as an input pin:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :input\n# or\nRPi::GPIO.setup [PIN1_NUM, PIN2_NUM, ...], :as => :input\n```\nThe pin number will differ based on your selected numbering system and which pin you want to use.\n\nYou can use the additional hash argument `:pull` to apply a pull-up or pull-down resistor to the input pin like so:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :input, :pull => :down\n# or\nRPi::GPIO.setup PIN_NUM, :as => :input, :pull => :up\n# or (not necessary; :off is the default value)\nRPi::GPIO.setup PIN_NUM, :as => :input, :pull => :off\n```\n\nNow you can use the calls\n```ruby\nRPi::GPIO.high? PIN_NUM\nRPi::GPIO.low? PIN_NUM\n```\nto receive either `true` or `false`.\n\nIf you prefer to use a callback when a pin edge is detected, you can use the `watch` method:\n```ruby\nRPi::GPIO.watch PIN_NUM, :on => :rising do |pin, value| # :on supports :rising, :falling, and :both\n  ...\nend\n```\n\n`watch` also supports the optional `bounce_time` parameter found in the Python module to prevent duplicate events from firing:\n```ruby\nRPi::GPIO.watch PIN_NUM, :on => :falling, :bounce_time => 200 do |pin, value|\n  ...\nend\n```\n\nTo stop watching a pin, use `stop_watching`:\n```ruby\nRPi::GPIO.stop_watching PIN_NUM\n```\n\nIf you want to block execution until a pin edge is detected, there's `wait_for_edge`:\n```ruby\nputs 'Waiting to start...'\nRPi::GPIO.wait_for_edge PIN_NUM, :rising # :rising, :falling, and :both are also supported here\nputs 'Here we go!'\n```\n\n`wait_for_edge` accepts optional `bounce_time` and `timeout` arguments too:\n```ruby\nputs 'Waiting to start...'\nvalue = RPi::GPIO.wait_for_edge PIN_NUM, :falling, :bounce_time => 200, :timeout => 5000\nif value.nil? # nil is returned if the timeout is reached\n  print 'You took too long. '\nend\nputs 'Here we go!'\n```\n\n#### Output\n\nTo send output to a GPIO pin, you must first initialize it as an output pin:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :output\n# or\nRPi::GPIO.setup [PIN1_NUM, PIN2_NUM, ...], :as => :output\n```\nNow you can use the calls\n```ruby\nRPi::GPIO.set_high PIN_NUM\nRPi::GPIO.set_low PIN_NUM\n```\nto set the pin either high or low.\n\nYou can use the additional hash argument `:initialize` to set the pin's initial state like so:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :output, :initialize => :high\n# or\nRPi::GPIO.setup PIN_NUM, :as => :output, :initialize => :low\n```\n\n#### PWM (pulse-width modulation)\n\nPulse-width modulation is a useful tool for controlling things like LED brightness or motor speed. To utilize PWM, first create a PWM object for an [output pin](#output).\n```ruby\npwm = RPi::GPIO::PWM.new(PIN_NUM, PWM_FREQ)\n```\nThe `PWM_FREQ` is a value in hertz that specifies the amount of pulse cycles per second.\n\nNow you can call the following method to start PWM:\n```ruby\npwm.start DUTY_CYCLE\n```\n`DUTY_CYCLE` is a value from `0.0` to `100.0` indicating the percent of the time that the signal will be high.\n\nOnce running, you can get/set the PWM duty cycle with\n```ruby\npwm.duty_cycle # get\npwm.duty_cycle = NEW_DUTY_CYCLE # set\n```\nget/set the PWM frequency with\n```ruby\npwm.frequency # get\npwm.frequency = NEW_FREQUENCY # set\n```\nand get the PWM GPIO number with\n```ruby\npwm.gpio\n```\nNote that this number corresponds to `:bcm` numbering of the GPIO pins, so it will be different than pin number you used if you created the PWM with `:board` numbering.\n\nTo stop PWM, use\n```ruby\npwm.stop\n```\n\nTo check if a PWM object is currently running, use\n```ruby\npwm.running?\n```\n\n#### Cleaning up\n\nAfter your program is finished using the GPIO pins, it's a good idea to release them so other programs can use them later. Simply call\n```ruby\nRPi::GPIO.clean_up PIN_NUM\n```\nto release a specific pin, or\n```ruby\nRPi::GPIO.clean_up\n```\nto release all allocated pins.\n\nAlternatively, you can call\n```ruby\nRPi::GPIO.reset\n```\nto clean up all pins and to also reset the selected numbering mode.\n\n## Credits\n\nOriginal Python code by Ben Croston modified for Ruby by Nick Lowery\n\nCopyright (c) 2014-2020 [Nick Lowery](https://github.com/ClockVapor)\n\nView LICENSE for full license.\n\n"
 },
 {
  "repo": "ViGEm/FireShock",
  "language": "C",
  "readme_contents": "<img src=\"assets/FireShock.png\" align=\"right\" />\n\n# FireShock\n\nWindows USB Driver for Sony DualShock Controllers\n\n[![Build status](https://ci.appveyor.com/api/projects/status/qtn7klq26ho8atg6/branch/master?svg=true)](https://ci.appveyor.com/project/nefarius/fireshock/branch/master) [![GitHub All Releases](https://img.shields.io/github/downloads/ViGEm/FireShock/total)](https://somsubhra.github.io/github-release-stats/?username=ViGEm&repository=FireShock)\n\n---\n\n\u26a0\ufe0f **This project is no longer maintained. It has been superseded by [DsHidMini](https://github.com/ViGEm/DsHidMini).** \u26a0\ufe0f\n\n---\n\n## Summary\n\n`FireShock` consists of a custom USB user-mode driver and a [user-mode dispatch service](https://github.com/ViGEm/Shibari) handling wired communication with Sony DualShock **3** Controllers. It allows 3rd party developers to handle controller inputs and outputs via a simple plug-in system.\n\n## How it works\n\nOnce installed the `fireshock.dll` user-mode driver will be loaded on any compatible DualShock 3 Controller connected to the system via USB. It replaces the default `HIDUSB.SYS` driver with `WinUSB.sys`.\n\nIf a DualShock 3 gets connected to the USB hub, the filter will send a \"magic\" start packet to the _control endpoint_ so the controller will continuously start sending HID input reports via the _interrupt in endpoint_ on interface 0. If an _interrupt in_ transfer arrives, the contents of the transfer buffer (the HID report) get streamed to any user-mode application calling [`ReadFile(...)`](https://docs.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-readfile) on the device. If a packet war written to the device via [`WriteFile(...)`](https://docs.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-writefile), the request gets converted into an output report and redirected to the _control endpoint_.\n\n## How to use\n\n**Important:** this is *not* an HID/XInput compatible driver, you **need** [the Shibari companion application](https://github.com/ViGEm/Shibari#documentation) and follow its setup instructions to get the controller recognized by games!\n\n## Supported systems\n\nThe driver is built for and tested with Windows 8.1 up to Windows 10 (x86 and amd64).\n\n## Download\n\n### Latest stable builds (signed)\n\n- [GitHub](../../releases/latest)\n- [Mirror](https://downloads.vigem.org/projects/FireShock/stable/)\n\n## Sources\n\n- [Eleccelerator Wiki](http://eleccelerator.com/wiki/index.php?title=DualShock_3)\n- [felis/USB_Host_Shield_2.0 - PS3 Information](https://github.com/felis/USB_Host_Shield_2.0/wiki/PS3-Information)\n- [PS3 and Wiimote Game Controllers on the Arduino Host Shield: Part 2](https://web.archive.org/web/20160326093555/https://www.circuitsathome.com/mcu/ps3-and-wiimote-game-controllers-on-the-arduino-host-shield-part-2)\n- [ribbotson/USB-Host](https://github.com/ribbotson/USB-Host/tree/master/ps3/PS3USB)\n- [Windows-driver-samples/hid/firefly/driver](https://github.com/Microsoft/Windows-driver-samples/tree/master/hid/firefly/driver)\n- [Windows-driver-samples/general/toaster/toastDrv/kmdf/filter/sideband](https://github.com/Microsoft/Windows-driver-samples/tree/master/general/toaster/toastDrv/kmdf/filter/sideband)\n- [wdfusb.h header](https://docs.microsoft.com/en-us/windows-hardware/drivers/ddi/wdfusb/index)\n- [USB Descriptor and Request Parser](http://eleccelerator.com/usbdescreqparser/)\n- [PS4 Developer wiki - DS4-USB](http://www.psdevwiki.com/ps4/DS4-USB)\n- [HID: sony: Update device ids](https://patchwork.kernel.org/patch/9367441/)\n"
 },
 {
  "repo": "mbrossar/denoise-imu-gyro",
  "language": "Python",
  "readme_contents": "# Denoising IMU Gyroscope with Deep Learning for Open-Loop Attitude Estimation\n\n## Overview [[IEEE paper](https://ieeexplore.ieee.org/document/9119813), [preprint paper](https://hal.archives-ouvertes.fr/hal-02488923v4/document)]\n\nThis repo contains a learning method for denoising gyroscopes of Inertial Measurement Units (IMUs) using\nground truth data. In terms of attitude dead-reckoning estimation, the obtained algorithm is able to beat top-ranked\nvisual-inertial odometry systems [3-5] in terms of attitude estimation\nalthough it only uses signals from a low-cost IMU. The obtained\nperformances are achieved thanks to a well chosen model, and a\nproper loss function for orientation increments. Our approach builds upon a neural network based\non dilated convolutions, without requiring any recurrent neural\nnetwork.\n\n## Code\nOur implementation is based on Python 3 and [Pytorch](https://pytorch.org/). We\ntest the code under Ubuntu 16.04, Python 3.5, and Pytorch 1.5. The codebase is licensed under the MIT License.\n\n### Installation & Prerequies\n1.  Install the correct version of [Pytorch](http://pytorch.org)\n```\npip install --pre torch  -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n```\n\n2.  Clone this repo and create empty directories\n```\ngit clone https://github.com/mbrossar/denoise-imu-gyro.git\nmkdir denoise-imu-gyro/data\nmkdir denoise-imu-gyro/results\n```\n\n3.  Install the following required Python packages, e.g. with the pip command\n```\npip install -r denoise-imu-gyro/requirements.txt\n```\n\n### Testing\n\n1. Download reformated pickle format of the _EuRoC_ [1] and _TUM-VI_ [2] datasets at this [url](https://cloud.mines-paristech.fr/index.php/s/d2lHqzIk1PxzWmb/download), extract and copy then in the `data` folder.\n```\nwget \"https://cloud.mines-paristech.fr/index.php/s/d2lHqzIk1PxzWmb/download\"\nunzip download -d denoise-imu-gyro/data\nrm download\n```\nThese file can alternatively be generated after downloading the _EuRoC_ and\n_TUM-VI_ datasets. They will be generated when lanching the main file after\nproviding data paths.\n\n2. Download optimized parameters at this [url](https://cloud.mines-paristech.fr/index.php/s/OLnj74YXtOLA7Hv/download), extract and copy in the `results` folder.\n```\nwget \"https://cloud.mines-paristech.fr/index.php/s/OLnj74YXtOLA7Hv/download\"\nunzip download -d denoise-imu-gyro/results\nrm download\n```\n3. Test on the dataset on your choice !\n```\ncd denoise-imu-gyro\npython3 main_EUROC.py\n# or alternatively\n# python3 main_TUMVI.py\n```\n\nYou can then compare results with the evaluation [toolbox](https://github.com/rpng/open_vins/) of [3].\n\n### Training\nYou can train the method by\nuncomment the two lines after # train in the main files. Edit then the\nconfiguration to obtain results with another sets of parameters. It roughly\ntakes 5 minutes per dataset with a decent GPU.\n\n## Schematic Illustration of the Proposed Method\n\n<p align=\"center\">\n<img src=\"figures/methode.jpg\" alt=\"Schematic illustration of the proposed\nmethod\" class=\"center\" width=\"600\"/>\n</p>\n\nThe convolutional neural network\ncomputes gyro corrections (based on past IMU measurements) that filters\nundesirable errors in the raw IMU signals. We\nthen perform open-loop time integration on the noise-free measurements\nfor regressing low frequency errors between ground truth and estimated\norientation increments.\n\n## Results\n\n<p align=\"center\">\n<img src=\"figures/rpy.jpg\" alt=\"Orientation estimates\"  width=\"800\"/>\n</p>\n\nOrientation estimates on the test sequence _MH 04 difficult_ of [1] (left), and\n_room 4_ of [2] (right). Our method removes errors of the  IMU.\n\n<p align=\"center\">\n<img src=\"figures/boxplot.jpg\" alt=\"Relative Orientation Error\" width=\"500\"/>\n</p>\n\nRelative Orientation Error (ROE) in terms of 3D orientation and\nyaw errors on the test sequences. Our method competes with VIO methods albeit based only on IMU signals.\n\n## Paper\nThe paper M. Brossard, S. Bonnabel and A. Barrau, \"Denoising IMU Gyroscopes With Deep Learning for Open-Loop Attitude Estimation,\" in _IEEE Robotics and Automation Letters_, vol. 5, no. 3, pp. 4796-4803, July 2020, doi: 10.1109/LRA.2020.3003256., relative to this repo, is\navailable at this [url](https://ieeexplore.ieee.org/document/9119813) and a preprint at this [url](https://hal.archives-ouvertes.fr/hal-02488923/document).\n\n\n## Citation\n\nIf you use this code in your research, please cite:\n\n```\n@article{brossard2020denoising,\n  author={M. {Brossard} and S. {Bonnabel} and A. {Barrau}},\n  journal={IEEE Robotics and Automation Letters}, \n  title={Denoising IMU Gyroscopes With Deep Learning for Open-Loop Attitude Estimation}, \n  year={2020},\n  volume={5},\n  number={3},\n  pages={4796-4803},\n  }\n\n```\n\n## Authors\n\nThis code was written by the [Centre of Robotique](http://caor-mines-paristech.fr/en/home/) at the\nMINESParisTech, Paris, France.\n\n[Martin\nBrossard](mailto:martin.brossard@mines-paristech.fr)^, [Axel\nBarrau](mailto:axel.barrau@safrangroup.com)^ and [Silv\u00e8re\nBonnabel](mailto:silvere.bonnabel@mines-paristech.fr)^.\n\n^[MINES ParisTech](http://www.mines-paristech.eu/), PSL Research University,\nCentre for Robotics, 60 Boulevard Saint-Michel, 75006 Paris, France.\n\n## Biblio\n\n[1] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari,\nM. W. Achtelik, and R. Siegwart, ``_The EuRoC Micro Aerial Vehicle\nDatasets_\", The International Journal of Robotics Research, vol. 35,\nno. 10, pp. 1157\u20131163, 2016.\n\n[2] D. Schubert, T. Goll, N. Demmel, V. Usenko, J. Stuckler, and\nD. Cremers, ``_The TUM VI Benchmark for Evaluating Visual-Inertial\nOdometry_\", in International Conference on Intelligent Robots and\nSystems (IROS). IEEE, pp. 1680\u20131687, 2018.\n\n[3] P. Geneva, K. Eckenhoff, W. Lee, Y. Yang, and G. Huang, ``_OpenVINS:\nA Research Platform for Visual-Inertial Estimation_\", IROS Workshop\non Visual-Inertial Navigation: Challenges and Applications, 2019.\n\n[4] T. Qin, P. Li, and S. Shen, ``_VINS-Mono: A Robust and Versatile\nMonocular Visual-Inertial State Estimator_\", IEEE Transactions on\nRobotics, vol. 34, no. 4, pp. 1004\u20131020, 2018.\n\n[5] M. Bloesch, M. Burri, S. Omari, M. Hutter, and R. Siegwart, ``_Iterated\nExtended Kalman Filter Based Visual-Inertial Odometry Using Direct Photometric\nFeedback_\", The International Journal of Robotics Research,vol. 36, no. 10, pp.\n1053\u00f11072, 2017.\n"
 },
 {
  "repo": "willyg302/clip.py",
  "language": "Python",
  "readme_contents": "![clip.py](https://raw.github.com/willyg302/clip.py/master/clip-logo.png \"It looks like you're trying to make a CLI.\")\n\n-----\n\n[![Travis](https://img.shields.io/travis/willyg302/clip.py.svg?style=flat-square)](https://travis-ci.org/willyg302/clip.py)\n[![docs](https://readthedocs.org/projects/clippy/badge/?style=flat-square)](http://clippy.readthedocs.org/)\n[![license](http://img.shields.io/badge/license-MIT-red.svg?style=flat-square)](https://raw.githubusercontent.com/willyg302/clip.py/master/LICENSE)\n\nEmbeddable, composable **c**ommand **l**ine **i**nterface **p**arsing\n\n## Installing\n\nclip is just a `pip install clip.py` away.\n\n## Basic Example\n\nThis example is just to whet your appetite. For a more in-depth guide to using clip, please see the [docs](http://clippy.readthedocs.org/).\n\n```python\nimport clip\n\napp = clip.App()\n\n@app.main(description='A very unhelpful shopping list CLI program')\ndef shopping():\n\tpass\n\n@shopping.subcommand(description='Add an item to the list')\n@clip.arg('item', required=True)\n@clip.opt('-q', '--quantity', default=1, help='How many of the item to get')\ndef add(item, quantity):\n\tclip.echo('Added \"{} - {}\" to the list'.format(item, quantity))\n\n@shopping.subcommand(description='See all items on the list')\n@clip.flag('--sorted', help='View items in alphabetical order')\ndef view(sorted):\n\tclip.echo('This is your {}sorted list'.format('' if sorted else 'un'))\n\nif __name__ == '__main__':\n\ttry:\n\t\tapp.run()\n\texcept clip.ClipExit:\n\t\tpass\n```\n\nIf you save the above code in a file called `shopping.py`, you can then do the following:\n\n```\n$ python shopping.py -h\nshopping: A very unhelpful shopping list CLI program\n\nUsage: shopping {{options}} {{subcommand}}\n\nOptions:\n  -h, --help  Show this help message and exit\n\nSubcommands:\n  add   Add an item to the list\n  view  See all items on the list\n$ python shopping.py add -h\nshopping add: Add an item to the list\n\nUsage: add {{arguments}} {{options}}\n\nArguments:\n  item [text]  \n\nOptions:\n  -h, --help            Show this help message and exit\n  -q, --quantity [int]  How many of the item to get (default: 1)\n$ python shopping.py add\nError: Missing parameter \"item\".\n$ python shopping.py add cookies -q 10\nAdded \"cookies - 10\" to the list\n$ python shopping.py view\nThis is your unsorted list\n$ python shopping.py view --sorted\nThis is your sorted list\n```\n\n## Testing\n\nCall tests with `python setup.py test`.\n\n## Credits\n\n- **[Aaargh](https://github.com/wbolster/aaargh)**: Some parsing logic\n- **[Click](http://click.pocoo.org/3/)**: Decorator systems, parameter features\n- **[docopt](http://docopt.org/)**: Help text formatting\n"
 },
 {
  "repo": "lzx1413/LabelImgTool",
  "language": "Python",
  "readme_contents": "# LabelImg\n\n[![Build Status](https://travis-ci.org/lzx1413/labelImgPlus.svg?branch=master)](https://travis-ci.org/lzx1413/labelImgPlus)\n\nLabelImg is a graphical image annotation tool.\n\nIt is written in Python and uses Qt for its graphical interface.\n\nThe annotation file will be saved as an XML file. The annotation format is PASCAL VOC format, and the format is the same as [ImageNet](http://www.image-net.org/)\n\ntask mode change\n\n![](screenshot/setting_panel.jpg)\n\nDET mode\n\n![](screenshot/bbox_label.jpg)\n\nSEG mode\n\n![](screenshot/parse_label.jpg)\n\nCLS mode\n\n![](screenshot/cls_task.jpg)\n\nBrush SEG mode(in development: brush branch)\n\n![](screenshot/brush_task.jpg)\n\n## Release software for windows\n[baiduyun](https://pan.baidu.com/s/1iREYsJiQCzPPZGf6dFvauw)\n[googledriver](https://drive.google.com/open?id=118bUKQGlfwLgRTpptNzgBijJneInvw7T)\n\n## Build source and use it\n\nRequires at least [Python 2.6](http://www.python.org/getit/) and has been tested with [PyQt4.8](http://www.riverbankcomputing.co.uk/software/pyqt/intro).\n\nIn order to build the resource and assets, you need to install pyqt4-dev-tools:\n\n* Ubuntu\n\n`sudo apt-get install pyqt4-dev-tools`\n* Mac\n   install pyqt4 with [instructions](https://robonobodojo.wordpress.com/2017/02/08/installing-pyqt4-on-mac-osx/)\n\n`sudo apt-get install python-opencv`\n\n`pip install lxml`\n\n`pip install qdarkstyle`\n\n`./labelImg.py`\n\n* Windows\n\nNeed to download and setup [Python 2.6](https://www.python.org/downloads/windows/) or later and [PyQt4](https://www.riverbankcomputing.com/software/pyqt/download),lxml,qdarkstyle.\nOpen cmd and go to $labelImg, \n\n`$ pyrcc4 -o resources.py resources.qrc`\n\n`$ python labelImg.py`\n\n## Default file framework\n\n|---Images\n\n\u200b         |---images_1\n\n\u200b         |---images_2\n\n|---Annotation\n\n\u200b          |---images_1\n\n\u200b          |---images_2        \n\nthe file containing annotations will be created by default.\n\n## Usage\nAfter cloning the code, you should run `$ make all` to generate the resource file.\n\nYou can then start annotating by running `$ ./labelImg.py`. For usage\ninstructions you can see [Here](https://youtu.be/p0nR2YsCY_U)\n\nAt the moment annotations are saved as an XML file. The format is PASCAL VOC format, and the format is the same as [ImageNet](http://www.image-net.org/)\n\nYou can also see [ImageNet Utils](https://github.com/tzutalin/ImageNet_Utils) to download image, create a label text for machine learning, etc\n\n### Label and  parsing\n\nsupport rectangle label and parsing labels\n\n### Create pre-defined classes\n\nYou can edit the [data/predefined_classes.txt](https://github.com/tzutalin/labelImg/blob/master/data/predefined_classes.txt) to load pre-defined classes\n\nYou also can create labels with two levels in [data/predefined_sub_classes.txt](https://github.com/lzx1413/labelImg/blob/master/data/predefined_sub_classes.txt) \n\nAnd the labels will be ranked by the frequency you use it.\n\n### General steps from scratch\n\n* Build and launch: `$ make all; python labelImg.py`\n\n* Click 'Change default saved annotation folder' in Menu/File\n\n* Click 'Open Dir'\n\n* Click 'Create RectBox'\n\nThe annotation will be saved to the folder you specifiy\n\n### Hotkeys\n\n* Ctrl + r : Change the defult target dir which saving annotation files\n\n* Ctrl + n : Create a bounding box\n\n* Ctrl + s : Save\n\n* Right : Next image\n\n* Left : Previous image\n\n### Online image data mode\n\nthe server have to make the images in a folder that clint can get from http/https with **get** function\n\n* settings\n\nopen File -->RemoteDBSettings(ctrl+m) like that\n\n![](screenshot/remote_settings.JPG)\n\nthe remote image list is a file contenting the name of the images (a line is a image) .\n\nthe image will be cached in a folder created in the software file named database/pics/XXXX and this will take a lot of memory if there are a lot of images,and this will be modified in the future.\n\nopen File   -->ChangedDefaultSavedAnnotationDir(ctrl+r) to set the folder to save the results\n\n2. if your settings are right,you will find the **Get Images** button becomes enabled and click it ,then you can annotate the images as before\n\n### Change list\n* 18-08-19 py3 pyqt5 supported in the branch py3\n\n* 17-08-14  add class label function\n\n### Todo list\n* support pyqt5 and python 3\n* add more functions while adding parsing labels\n* refine the setting functions\n\n### How to contribute\nSend a pull request\n\n### License\n[License](LICENSE.md)\n"
 },
 {
  "repo": "Youngestdev/fastapi-mongo",
  "language": "Python",
  "readme_contents": "# FastAPI and MongoDB Boilerplate\n\nA simple starter for building RESTful APIs with FastAPI and MongoDB. \n\n## Features\n\n+ Python FastAPI backend.\n+ MongoDB database.\n+ Authentication\n+ Deployment\n\n## Using the applicaiton\n\nTo use the application, follow the outlined steps:\n\n1. Clone this repository and create a virtual environment in it:\n\n```console\n$ python3 -m venv venv\n```\n\n2. Install the modules listed in the `requirements.txt` file:\n\n```console\n(venv)$ pip3 install -r requirements.txt\n```\n3. You also need to start your mongodb instance either locally or on Docker as well as create a `.env.dev` file. See the `.env.sample` for configurations.\n\n4. Start the application:\n\n```console\npython main.py\n```\n\n\nThe starter listens on port 8000 on address [0.0.0.0](0.0.0.0:8080). \n\n![FastAPI-MongoDB starter](https://user-images.githubusercontent.com/31009679/165318867-4a0504d5-1fd0-4adc-8df9-db2ff3c0c3b9.png)\n\n## Deployment\n\nThis application can be deployed on any PaaS such as [Heroku](https://heroku.com) or [Okteto](https://okteto) and any other cloud service provider.\n\n## Contributing ?\n\n\nFork the repo, make changes and send a PR. We'll review it together!\n\n## License\n\nThis project is licensed under the terms of MIT license.\n"
 },
 {
  "repo": "laffra/auger",
  "language": "Python",
  "readme_contents": "# Auger\nAuger is a project to automatically generate unit tests for Python code.\n\nSee\n[these slides](http://goo.gl/PuZsgX)\nor\n[this blog](http://chrislaffra.blogspot.com/2016/12/auger-automatic-unit-test-generation.html)\nentry for more information.\n\n# Installation\n\nInstall auger with:\n\n    pip install auger-python\n\n# Running Auger\n    \nTo generate a unit test for any class or module, for Python 2 or 3, do this:\n\n```python\nimport auger\n\nwith auger.magic([ <any list of modules or classes> ]):\n    <any code that exercises your application>\n```\n\n# A Simple Example\n\nHere is a simple example that does not rely on Auger at all:\n\n```python\nclass Foo:                # Declare a class with a method\n    def bar(self, x):\n        return 2 * x      # Duplicate x and return it\n\ndef main():\n    foo = Foo()           # Create an instance of Foo\n    print(foo.bar(32))    # Call the bar method and print the result\n\nmain()\n```\n\nInside the `main` function we call the `bar` method and it will print 64.\n\n# Running Auger on our Simple Example\n\nTo generate a unit test for this class, we run the code again, but this time in the context of Auger:\n\n```python\nimport auger\n\nwith auger.magic([Foo]):\n    main()\n```\n\nThis will print out the following:\n\n    64\n    Auger: generated test: tests/test_Foo.py\n\nThe test that is generated looks like this, with some imports and test for main removed:\n\n```python\nimport unittest\n\nclass FooTest(unittest.TestCase):\n    def test_bar(self):\n        foo_instance = Foo()\n        self.assertEquals(\n            foo_instance.bar(x=32),\n            64\n        )\n\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\n# Running Auger in verbose mode\n\nRather than emit tests in the file system, Auger can also print out the test to the console,\nby using the `verbose` parameter:\n\n```python\nimport auger\n\nwith auger.magic([Foo], verbose=True):\n    main()\n```\n\nIn that case, Auger will not generate any tests, but just print them out.\n\n# A larger example\n\nConsider the following example, `pet.py`, included in the `sample` folder, that lets us create a `Pet` with a name and a species:\n\n```python\nfrom animal import Animal\n\nclass Pet(Animal):\n  def __init__(self, name, species):\n    Animal.__init__(self, species)\n    self.name = name\n\n  def getName(self):\n    return self.name\n\n  def __str__(self):\n    return \"%s is a %s\" % (self.getName(), self.getSpecies())\n\ndef createPet(name, species):\n  return Pet(name, species)\n```\n\nA `Pet` is really a special kind of `Animal`, with a name, which is defined in `animal.py`:\n\n```python\nclass Animal(object):\n  def __init__(self, species):\n    self.species = species\n\n  def getSpecies(self):\n    return self.species\n```\n\nWith those two definitions, we can create a `Pet` instance and print out some details:\n\n```python\nimport animal\nimport pet\n\ndef main():\n  p = pet.createPet(\"Polly\", \"Parrot\")\n  print(p, p.getName(), p.getSpecies())\n  \nmain()\n```\n\nThis produces:\n\n    Polly is a Parrot Polly Parrot\n\n# Calling Auger on our larger example\n\nWith Auger, we can record all calls to all functions and methods defined in `pet.py`,\nwhile also remembering the details for all calls going out from `pet.py` to other modules,\nso they can be mocked out.\n\nInstead of saying:\n\n```python\nif __name__ == \"__main__\":\n  main()\n```\n\nWe would say:\n\n```python\nimport auger\n\nif __name__ == \"__main__\":\n  with auger.magic([pet]):   # this is the new line and invokes Auger\n    main()\n```\n\nThis produces the following automatically generated unit test for `pet.py`:\n\n```python\nfrom mock import patch\nfrom sample.animal import Animal\nimport sample.pet\nfrom sample.pet import Pet\nimport unittest\n\n\nclass PetTest(unittest.TestCase):\n    @patch.object(Animal, 'get_species')\n    @patch.object(Animal, 'get_age')\n    def test___str__(self, mock_get_age, mock_get_species):\n        mock_get_age.return_value = 12\n        mock_get_species.return_value = 'Dog'\n        pet_instance = Pet('Clifford', 'Dog', 12)\n        self.assertEquals(pet_instance.__str__(), 'Clifford is a dog aged 12')\n\n    def test_create_pet(self):\n        self.assertIsInstance(sample.pet.create_pet(age=12,species='Dog',name='Clifford'), Pet)\n\n    def test_get_name(self):\n        pet_instance = Pet('Clifford', 'Dog', 12)\n        self.assertEquals(pet_instance.get_name(), 'Clifford')\n\n    def test_lower(self):\n        self.assertEquals(Pet.lower(s='Dog'), 'dog')\n\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\nNote that auger detects object creation, method invocation, and static methods. It automatically\ngenerate mocks for `Animal`. The mock for `get_species` returns 'Dog' and `get_age` returns 12. \nNamely, those were the values Auger recorded when we ran our sample code the last time.\n\n# Benefits of Auger\n\nBy automatically generating unit tests, we dramatically cut down the cost of software\ndevelopment. The tests themselves are intended to help developers get going on their unit testing\nand lower the learning curve for how to write tests.\n\n# Known limitations of Auger\n\nAuger does not do try to substitue parameters with synthetic values such as `-1`, `None`, or `[]`. \nAuger also does not act well when code uses exceptions. Auger also does not like methods that have a decorator.\n\nAuger only records a given execution run and saves the run as a test. Auger does not know if the code actually\nworks as intended. If the code contains a bug, Auger will simply record the buggy behavior. There is no free\nlunch here. It is up to the developer to verify the code actually works.\n"
 },
 {
  "repo": "MaxHalford/chime",
  "language": "Python",
  "readme_contents": "<div align=\"center\">\n  <h1>chime</h1>\n  <q><i>Python sound notifications made easy.</i></q>\n</div>\n<br>\n\n<div align=\"center\">\n  <!-- Tests -->\n  <a href=\"https://github.com/MaxHalford/chime/actions?query=workflow%3Atests\">\n    <img src=\"https://github.com/MaxHalford/chime/workflows/tests/badge.svg?style=flat-square\" alt=\"tests\">\n  </a>\n  <!-- Soundboard -->\n  <a href=\"https://chime-soundboard.herokuapp.com/\">\n    <img src=\"https://github.com/MaxHalford/chime/workflows/soundboard/badge.svg?style=flat-square\" alt=\"soundboard\">\n  </a>\n  <!-- PyPI -->\n  <a href=\"https://pypi.org/project/chime\">\n    <img src=\"https://img.shields.io/pypi/v/chime.svg?label=release&color=blue&style=flat-square\" alt=\"pypi\">\n  </a>\n  <!-- PePy -->\n  <a href=\"https://pepy.tech/project/chime\">\n    <img src=\"https://img.shields.io/badge/dynamic/json?style=flat-square&maxAge=86400&label=downloads&query=%24.total_downloads&url=https%3A%2F%2Fapi.pepy.tech%2Fapi%2Fprojects%2Fchime\" alt=\"pepy\">\n  </a>\n  <!-- License -->\n  <a href=\"https://opensource.org/licenses/MIT\">\n    <img src=\"https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square\" alt=\"license\">\n  </a>\n</div>\n<br>\n\n## Table of contents\n\n- [Table of contents](#table-of-contents)\n- [Motivation](#motivation)\n- [Installation](#installation)\n- [Basic usage](#basic-usage)\n- [Theming](#theming)\n- [IPython/Jupyter magic](#ipythonjupyter-magic)\n- [Exception notifications](#exception-notifications)\n- [Command-line usage](#command-line-usage)\n- [Platform support](#platform-support)\n- [I can't hear anything \ud83d\ude49](#i-cant-hear-anything-)\n- [Setting a default theme](#setting-a-default-theme)\n- [Adding a new theme](#adding-a-new-theme)\n- [Things to do](#things-to-do)\n- [Acknowledgements](#acknowledgements)\n- [License](#license)\n\n## Motivation\n\nI made this because I wanted a simple auditory cue system to tell me when a long-running number crunching script had finished. I didn't want to have to fiddle with the command-line, and also wanted a cross-platform solution. Thus was born `chime`!\n\n## Installation\n\n```sh\npip install chime\n```\n\nThis library has **no dependencies**. The IPython/Jupyter functionality is only imported if you've installed the `ipython` library. It should work for any Python version above or equal to 3.6.\n\n## Basic usage\n\n`chime` puts four functions at your disposal:\n\n```py\n>>> import chime\n\n>>> chime.success()\n>>> chime.warning()\n>>> chime.error()\n>>> chime.info()\n\n```\n\nCalling any of the above functions will play a sound. Note that the sounds are played in asynchronous processes, and are thus non-blocking. Each function should take around 2ms to execute, regardless of the sound length. You're free to use each sound notification in any way you see fit. I'm not your mama.\n\n## Theming\n\nThe sounds that are played depend on which theme is being used.\n\n```py\n>>> chime.theme()  # return the current theme\n'chime'\n\n```\n\nSeveral themes are available:\n\n```py\n>>> chime.themes()\n['big-sur', 'chime', 'mario', 'material', 'sonic', 'zelda']\n\n```\n\nThe theme can be changed by passing a theme name to the `theme` function:\n\n```py\n>>> chime.theme('zelda')\n\n```\n\nA couple of things to note:\n\n- You can listen to the sounds interactively via [this soundboard](https://chime-soundboard.herokuapp.com/), which is made with [Streamlit](https://www.streamlit.io/).\n- A random theme will be picked each time you play a sound if you set the theme to `'random'`.\n\n## IPython/Jupyter magic\n\nLoad the extension as so:\n\n```py\n%load_ext chime\n```\n\nYou can wrap a line:\n\n```py\n%chime print(\"I'm a line\")\n```\n\nYou can also wrap an entire cell:\n\n```py\n%%chime\n\nprint(\"I'm a cell\")\n```\n\nThe magic command will call `chime.success` when the line/cell finishes successfully. Otherwise, `chime.error` is called whenever an exception is raised.\n\n## Exception notifications\n\nIf you run `chime.notify_exceptions`, then `chime.error` will be called whenever an exception is raised.\n\n```py\nchime.notify_exceptions()\n\nraise ValueError(\"I'm going to make some noise\")\n```\n\n## Command-line usage\n\nYou can run `chime` from the command-line:\n\n```sh\n$ chime\n```\n\nBy default, this will play the success sound. You can also choose which sound to play, like so:\n\n```sh\n$ chime info\n```\n\nYou can also choose which theme to use:\n\n```sh\n$ chime info --theme zelda\n```\n\nIf you're using bash, then you can use `chime` to notify you when a program finishes:\n\n```sh\n$ echo \"Hello world!\"; chime\n```\n\nThis will play the sound regardless of the fact that the first command succeeded or not. If you're running on Windows, then you can run the following equivalent:\n\n```sh\n> echo \"Hello world!\" & chime\n```\n\n## Platform support\n\nUnder the hood, `chime` runs a command in the shell to play a `.wav` file. The command-line program that is used depends on the [platform](https://www.wikiwand.com/en/Computing_platform) that you're using. Platform information is available in the [`sys.platform` variable](https://docs.python.org/3/library/sys.html#sys.platform) as well as the [`platform` module](https://docs.python.org/3/library/platform.html) from the standard library. Currently, the supported platforms are:\n\n- Darwin\n- Linux\n- Windows\n\nA `UserWarning` is raised if you run a `chime` sound on an unsupported platform. Feel free to get in touch or issue a pull request if you want to add support for a specific platform. Likewise, don't hesitate if you're encountering trouble with one of the above platforms. I won't bite.\n\n## I can't hear anything \ud83d\ude49\n\nDid you check if you turned your sound on? Just kidding. \ud83d\ude1c\n\nThis library is designed to be non-invasive. By default, sounds are played asynchronously in unchecked processes. Therefore, if something goes wrong, the process dies silently. If you can't hear anything and you think that the issue is coming from `chime`, then set the `sync` parameter when you play a sound:\n\n```py\n>>> chime.info(sync=True)\n\n```\n\nThis will play the sound synchronously and issue a warning if something goes wrong, which should allow you to debug the issue. You can also raise an exception instead of sending a warning by setting the `raise_error` parameter:\n\n```py\n>>> chime.info(sync=True, raise_error=True)\n\n```\n\nNote that setting `raise_error` won't do anything if `sync` is set to `False`.\n\n## Setting a default theme\n\nTo change the default theme a configuration file may be created in `~/.config/chime/chime.conf` on Unix or `%APPDATA%\\chime\\chime.ini` on Windows.\n\nFor example, to change the default theme to `'zelda'` the configuration file would contain:\n\n```ini\n[chime]\ntheme = zelda\n\n```\n\n## Adding a new theme\n\nI have toyed with the idea of allowing users to add their own theme(s), but at the moment I rather keep things minimal. However, I'm happy to integrate new themes into the library. You can propose a new theme by [opening a pull request](https://github.com/MaxHalford/chime/issues/new) that adds the necessary .wav files to the [`themes` directory](https://github.com/MaxHalford/chime/tree/main/themes). A theme is made up of four files: `success.wav`, `warning.wav`, `error.wav`, and `info.wav`. That's all you need to do: the theme will picked up be automatically once the necessary files are provided.\n\nBe creative! \ud83d\udc69\u200d\ud83c\udfa8\n\n## Things to do\n\n- Some mechanism to automatically call `chime.warning` when a warning occurs.\n- Make it work with a remote machine. For instance a Jupyter Notebook hosted on a remote machine.\n- More themes!\n\n## Acknowledgements\n\n- Special thanks to [Michael Vlah](https://github.com/vlahm) for being a gentleman by giving up the \"chime\" name on PyPI.\n- Thanks to u/Pajke on reddit for helping me debug Windows support.\n- Thanks to [David Chen](https://github.com/dchen327) for adding Linux support by suggesting the use of [aplay](https://linux.die.net/man/1/aplay).\n- Thanks to [Vincent Warmerdam](https://twitter.com/fishnets88) for suggesting a command-line interface.\n- Calmcode made a [video introduction to chime](https://calmcode.io/chime/introduction.html) \u2764\ufe0f\n- Thanks to [Paulo S. Costa](https://github.com/paw-lu) for contributing in many different ways.\n- Thanks to [d34d_m8](https://github.com/d34dm8) for adding OpenBSD support.\n\n## License\n\nAs you would probably expect, this is [MIT licensed](LICENSE).\n"
 },
 {
  "repo": "dkeras-project/dkeras",
  "language": "Python",
  "readme_contents": "<p align=\"center\">\n  <img src=\"https://github.com/gndctrl2mjrtm/dkeras/blob/master/assets/dkeras_logo.png?raw=true\" alt=\"dKeras logo\"/>\n</p>\n\n# dKeras: Distributed Keras Engine\n### ***Make Keras faster with only one line of code.***\n\ndKeras is a distributed Keras engine that is built on top of \n[Ray](https://github.com/ray-project/ray). By wrapping dKeras around your\noriginal Keras model, it allows you to use many distributed deep learning\ntechniques to automatically improve your system's performance.\n\n\nWith an easy-to-use API and a backend framework that can be deployed from\nthe laptop to the data center, dKeras simpilifies what used to be a complex\nand time-consuming process into only a few adjustments.\n\n#### Why Use dKeras?\n\nDistributed deep learning can be essential for production systems where you \nneed fast inference but don't want expensive hardware accelerators or when\nresearchers need to train large models made up of distributable parts.\n\nThis becomes a challenge for developers because they'll need expertise in not\nonly deep learning but also distributed systems. A production team might also\nneed a machine learning optimization engineer to use neural network \noptimizers in terms of precision changes, layer fusing, or other techniques. \n\nDistributed inference is a simple way to get better inference FPS. The graph \nbelow shows how non-optimized, out-of-box models from default frameworks can \nbe quickly sped up through data parallelism:\n\n<p align=\"center\">\n  <img src=\"https://github.com/gndctrl2mjrtm/dkeras/blob/master/assets/inference_comparison.png?raw=true\" alt=\"dKeras graph\"/>\n</p>\n\n\n#### Current Capabilities:\n- Data Parallelism Inference\n\n#### Future Capabilities:\n- Model Parallelism Inference\n- Distributed Training\n- Easy Multi-model production-ready building\n- Data stream input distributed inference\n- PlaidML Support\n- Autoscaling\n- Automatic optimal hardware configuration \n- PBS/Torque support\n\n## Installation\nThe first official release of dKeras will be available soon. For \nnow, install from source.\n```bash\npip install git+https://github.com/dkeras-project/dkeras\n```\n\n### Requirements\n\n- Python 3.6 or higher\n- ray\n- psutil\n- Linux (or OSX, dKeras works on laptops too!)\n- numpy\n\n\n### Coming Soon: [PlaidML](https://github.com/plaidml/plaidml) Support\ndKeras will soon work alongside [PlaidML](https://github.com/plaidml/plaidml), \na \"portable tensor compiler for enabling deep learning on laptops, embedded devices, \nor other devices where the available computing hardware is not well \nsupported or the available software stack contains unpalatable \nlicense restrictions.\" \n\n## Distributed Inference\n\n### Example\n\n#### Original\n```python\nmodel = ResNet50()\nmodel.predict(data)\n```\n#### dKeras Version\n```python\nfrom dkeras import dKeras\n\nmodel = dKeras(ResNet50)\nmodel.predict(data)\n```\n\n#### Full Example\n```python\nfrom tensorflow.keras.applications import ResNet50\nfrom dkeras import dKeras\nimport numpy as np\nimport ray\n\nray.init()\n\ndata = np.random.uniform(-1, 1, (100, 224, 224, 3))\n\nmodel = dKeras(ResNet50, init_ray=False, wait_for_workers=True, n_workers=4)\npreds = model.predict(data)\n```\n\n#### Multiple Model Example\n```python\nimport numpy as np\nfrom tensorflow.keras.applications import ResNet50, MobileNet\n\nfrom dkeras import dKeras\nimport ray\n\nray.init()\n\nmodel1 = dKeras(ResNet50, weights='imagenet', wait_for_workers=True, n_workers=3)\nmodel2 = dKeras(MobileNet, weights='imagenet', wait_for_workers=True, n_workers=3)\n\ntest_data = np.random.uniform(-1, 1, (100, 224, 224, 3))\n\nmodel1.predict(test_data)\nmodel2.predict(test_data)\n\nmodel1.close()\nmodel2.close()\n```"
 },
 {
  "repo": "PacktPublishing/Generative-Adversarial-Networks-Projects",
  "language": "Python",
  "readme_contents": "# Generative-Adversarial-Networks-Projects\nGenerative Adversarial Networks Projects, published by Packt\n# Generative-Adversarial-Networks-Projects\n\n<a href=\"https://www.packtpub.com/big-data-and-business-intelligence/generative-adversarial-networks-projects?utm_source=github&utm_medium=repository&utm_campaign=9781789136678\"><img src=\"https://d255esdrn735hr.cloudfront.net/sites/default/files/imagecache/ppv4_main_book_cover/9781789136678_cover.png\" alt=\"Book Name\" height=\"256px\" align=\"right\"></a>\n\nThis is the code repository for [Generative-Adversarial-Networks-Projects](https://www.packtpub.com/big-data-and-business-intelligence/generative-adversarial-networks-projects?utm_source=github&utm_medium=repository&utm_campaign=9781789136678), published by Packt.\n\n**Build next-generation generative models using TensorFlow and Keras**\n\n## What is this book about?\nGenerative Adversarial Networks (GANs) have the potential to build next-generation models, as they can mimic any distribution of data. Major research and development work is being undertaken in this field since it is one of the rapidly growing areas of machine learning. This book will test unsupervised techniques for training neural networks as you build seven end-to-end projects in the GAN domain.\n\nThis book covers the following exciting features:\n* Train a network on the 3D ShapeNet dataset to generate realistic shapes\n* Generate anime characters using the Keras implementation of DCGAN\n* Implement an SRGAN network to generate high-resolution images\n* Train Age-cGAN on Wiki-Cropped images to improve face verification\n* Use conditional GANs for image-to-image translation\n\nIf you feel this book is for you, get your [copy](https://www.amazon.com/dp/10DigitISBN) today!\n\n<a href=\"https://www.packtpub.com/?utm_source=github&utm_medium=banner&utm_campaign=GitHubBanner\"><img src=\"https://raw.githubusercontent.com/PacktPublishing/GitHub/master/GitHub.png\" \nalt=\"https://www.packtpub.com/\" border=\"5\" /></a>\n\n\n## Instructions and Navigations\nAll of the code is organized into folders. For example, Chapter02.\n\nThe code will look like the following:\n```\nimport scipy.io as io\nvoxels = io.loadmat(\"path to .mat file\")[ 'instance' ]\n```\n\n**Following is what you need for this book:**\nIf you\u2019re a data scientist, machine learning developer, deep learning practitioner, or AI enthusiast looking for a project guide to test your knowledge and expertise in building real-world GANs models, this book is for you.\n\nWith the following software and hardware list you can run all code files present in the book (Chapter 1-09).\n\n### Software and Hardware List\n\n| Chapter  | Software required                   | OS required                        |\n| -------- | ------------------------------------| -----------------------------------|\n| 1        | Python 3.5                          | Windows, Mac OS X, and Linux (Any) |\n| 2        | AWS                                 | Windows, Mac OS X, and Linux (Any) |\n| 3        | GPU                                 | Windows, Mac OS X, and Linux (Any) |\n\n\n\n### Related products <Other books you may enjoy>\n* Generative Adversarial Networks Cookbook [[Packt]](https://www.packtpub.com/big-data-and-business-intelligence/generative-adversarial-networks-cookbook?utm_source=github&utm_medium=repository&utm_campaign=9781789139907) [[Amazon]](https://www.amazon.com/dp/1789139902)\n\n* Python Deep Learning - Second Edition [[Packt]](https://www.packtpub.com/big-data-and-business-intelligence/python-deep-learning-second-edition?utm_source=github&utm_medium=repository&utm_campaign=9781789348460) [[Amazon]](https://www.amazon.com/dp/1789348463)\n\n## Get to Know the Author(s)\n**Kailash Ahirwar**\nKailash Ahirwar is a machine learning and deep learning enthusiast. He has worked in many areas of Artificial Intelligence (AI), ranging from natural language processing and computer vision to generative modeling using GANs. He is a co-founder and CTO of Mate Labs. He uses GANs to build different models, such as turning paintings into photos and controlling deep image synthesis with texture patches. He is super optimistic about AGI and believes that AI is going to be the workhorse of human evolution.\n\n\n\n### Suggestions and Feedback\n[Click here](https://docs.google.com/forms/d/e/1FAIpQLSdy7dATC6QmEL81FIUuymZ0Wy9vH1jHkvpY57OiMeKGqib_Ow/viewform) if you have any feedback or suggestions.\n"
 },
 {
  "repo": "Bearle/django_mail_admin",
  "language": "Python",
  "readme_contents": "=============================\nDjango Mail Admin\n=============================\n\n.. image:: https://badge.fury.io/py/django_mail_admin.svg\n    :target: https://badge.fury.io/py/django_mail_admin\n\n.. image:: https://travis-ci.org/Bearle/django_mail_admin.svg?branch=master\n    :target: https://travis-ci.org/Bearle/django_mail_admin\n\n.. image:: https://codecov.io/gh/delneg/django_mail_admin/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/delneg/django_mail_admin\n\nThe one and only django app to receive & send mail with templates and multiple configurations.\n\n\nScreenshots\n-----------\n\n.. image:: https://github.com/Bearle/django_mail_admin/blob/master/screenshots/1.jpg?raw=true\n.. image:: https://github.com/Bearle/django_mail_admin/blob/master/screenshots/2.jpg?raw=true\n\nFeatures\n--------\n\n* Everything django-mailbox has\n* Everything django-post-office has\n* Everything django-db-email-backend has\n* Database configurations - activate an outbox to send from, activate a mailbox to receive from\n* Templates\n* Translatable\n* Mailings - using send_many() or 'cc' and 'bcc' or even recipients - all of those accept comma-separated lists of emails\n\nDependencies\n============\n\n* `django >= 1.9 <http://djangoproject.com/>`_\n* `django-jsonfield <https://github.com/bradjasper/django-jsonfield>`_\n\nDocumentation\n-------------\n\nThe full documentation is at https://django_mail_admin.readthedocs.io.\n\nQuickstart\n----------\n\n**Q**: What versions of Django/Python are supported?\n**A**: Take a look at https://travis-ci.org/delneg/django_mail_admin\n\nInstall django mail admin::\n\n    pip install django_mail_admin\n\nAdd it to your `INSTALLED_APPS`:\n\n.. code-block:: python\n\n    INSTALLED_APPS = (\n        ...\n        'django_mail_admin',\n        ...\n    )\n\n* Run ``migrate``::\n\n    python manage.py migrate django_mail_admin\n\n* Set ``django_mail_admin.backends.CustomEmailBackend`` as your ``EMAIL_BACKEND`` in django's ``settings.py``::\n\n    EMAIL_BACKEND = 'django_mail_admin.backends.CustomEmailBackend'\n\n\n* Set cron/Celery/RQ job to send/receive email, e.g. ::\n\n    * * * * * (cd $PROJECT; python manage.py send_queued_mail --processes=1 >> $PROJECT/cron_mail.log 2>&1)\n    * * * * * (cd $PROJECT; python manage.py get_new_mail >> $PROJECT/cron_mail_receive.log 2>&1)\n    0 1 * * * (cd $PROJECT; python manage.py cleanup_mail --days=30 >> $PROJECT/cron_mail_cleanup.log 2>&1)\n\n.. note::\n\n   Once you have entered a mailbox to receive emails, you can easily verify that you\n   have properly configured your mailbox by either:\n\n   * From the Django Admin, using the 'Get New Mail' action from the action\n     dropdown on the Mailbox changelist\n   * *Or* from a shell opened to your project's directory, using the\n     ``get_new_mail`` management command by running::\n\n       python manage.py get_new_mail\n\n   If you have also configured the Outbox, you can verify that it is working, e.g. ::\n\n        from django_mail_admin import mail, models\n\n        mail.send(\n            'from@example.com',\n            'recipient@example.com', # List of email addresses also accepted\n            subject='My email',\n            message='Hi there!',\n            priority=models.PRIORITY.now,\n            html_message='Hi <strong>there</strong>!',\n        )\n\nCustom Email Backends\n---------------------\n\nBy default, ``django_mail_admin`` uses custom Email Backends that looks up for Outbox models in database. If you want to\nuse a different backend, you can do so by configuring ``BACKENDS``, though you will not be able to use Outboxes and will have to set EMAIL_HOST etc. in django's ``settings.py``.\n\nFor example if you want to use `django-ses <https://github.com/hmarr/django-ses>`_::\n\n    DJANGO_MAIL_ADMIN = {\n        'BACKENDS': {\n            'default': 'django_mail_admin.backends.CustomEmailBackend',\n            'smtp': 'django.core.mail.backends.smtp.EmailBackend',\n            'ses': 'django_ses.SESBackend',\n        }\n    }\n\nYou can then choose what backend you want to use when sending mail:\n\n.. code-block:: python\n\n    # If you omit `backend_alias` argument, `default` will be used\n    mail.send(\n        'from@example.com',\n        ['recipient@example.com'],\n        subject='Hello',\n    )\n\n    # If you want to send using `ses` backend\n    mail.send(\n        'from@example.com',\n        ['recipient@example.com'],\n        subject='Hello',\n        backend='ses',\n    )\n\nCapture outgoing emails into Outbox\n-----------------------------------\n\nIf you want to store outgoing emails in the Outbox before they are submitted\nto the backend, set ``django_mail_admin.backends.OutboxEmailBackend`` as your\n``EMAIL_BACKEND`` in django's ``settings.py``::\n\n    EMAIL_BACKEND='django_mail_admin.backends.OutboxEmailBackend'\n\nEmails submitted using ``django.core.mail.send_mail`` will be stored in\nthe Outbox with the default backend selected for use when sending.\n\nThe emails will remain in the Outbox until ``send_queued_mail`` is run.\n\nThis can be used on development and test environments to capture emails\nso they are not sent automatically, and can be reviewed in Django Admin\nto ensure the contents are correct.\n\nOptional requirements\n---------------------\n\n1. `django_admin_row_actions` for some useful actions in the admin interface\n2. `requests` & `social-auth-app-django` for Gmail\n\n\nFAQ\n---\n\n**Q**: Why did you write this?\n\n**A**: In order to get both email sending & receiving you'll have to install post_office AND django_mailbox.\nEven if you do, you'll have to work on admin interface for it to look prettier, somehow link replies properly etc.\nSo I've decided merging those two and clearing the mess in between them as well as adding some other useful features.\n\n**Q**: Why did you remove support for Python 2?\n\n**A**: Because f*ck python2. Really, it's been 9 (NINE!) years since it came out. Go ahead and check out https://github.com/brettcannon/caniusepython3\n\n**Q**: Why is it named django_mail_admin, what does it have to do with admin ?\n\n**A**: Well, the first version of this package (which was living just in a really large admin.py) was used for easy mail management using standard Django admin interface.\n\n**Q**: What languages are available?\n\n**A**: Currently there's Russian and English languages available. Feel free to add yours:\n\n::\n\n    source <YOURVIRTUALENV>/bin/activate\n    python manage.py makemessages -l YOUR_LOCALE -i venv\n    python manage.py compilemessages -l YOUR_LOCALE\n\n\n**Q**: Why did you delete support for multi-lingual templates?\n\n**A**: Well, we have django-model-translations for that. You can easily fork this app and override EmailTemplate model (models/templates.py) accordingly.\nI think there's no need for such an overhead in a mail-related app.\n\n**Q**: I don't want my outgoing emails to be queued for sending after saving them in the admin interface, what do i do?\n\n**A**: Just override OutgoingEmailAdmin's save_model method.\n\n**Q**: Can i get in touch with you? I want a new feature to be implemented/bug fixed!\n\n**A**: Feel free to reach me out using issues and pull requests, I'll review them all and answer when I can.\n\n\n\nRunning Tests\n-------------\n\nDoes the code actually work?\n\n::\n\n    source <YOURVIRTUALENV>/bin/activate\n    (myenv) $ pip install tox\n    (myenv) $ tox\n\nCredits\n-------\n\nTools used in rendering this package:\n\n*  Cookiecutter_\n*  `cookiecutter-djangopackage`_\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`cookiecutter-djangopackage`: https://github.com/pydanny/cookiecutter-djangopackage\n"
 },
 {
  "repo": "nicolashahn/diffimg",
  "language": "Python",
  "readme_contents": "# diffimg\nGet the % difference in images using PIL's histogram + generate a diff image. Images\nshould have the same color channels (for example, RGB vs RGBA). If the image dimensions\ndiffer, the 2nd image will be resized to match the first before calculating the diff.\n\n[![PyPI version](https://badge.fury.io/py/diffimg.svg)](https://badge.fury.io/py/diffimg)\n\n\n### Installation\n\nNow available from PyPi: `pip install diffimg`\n\n### Usage\n\n```\n>>> from diffimg import diff\n>>> diff('mario-circle-cs.png', 'mario-circle-node.png')\n0.007319618135968298\n```\nThe [very simple](/diffimg/diff.py#L12) `diff` function returns a raw ratio instead of a\n% by default.\n\n```\ndiff(im1_file, \n     im2_file, \n     delete_diff_file=False, \n     diff_img_file=DIFF_IMG_FILE\n     ignore_alpha=False)\n```\n`im1_file, im2_file`: filenames of images to diff.\n\n`delete_diff_file`: a file showing the differing areas of the two images is generated in\norder to measure the diff ratio with the same dimensions as the first image. Setting\nthis to `True` removes it after calculating the ratio.\n\n`diff_img_file`: filename for the diff image file. Defaults to `diff_img.png`\n(regardless of inputed file's types).\n\n`ignore_alpha`: ignore the alpha channel for the ratio and if applicable, sets the alpha\nof the diff image to fully opaque.\n\n### As command line tool\n\n`python -m diffimg image1 image2 [-r/--ratio] [-d/--delete] [-f/--filename DIFF_IMG_FILE]`\n\n`--ratio` outputs a number between 0 and 1 instead of the default `Images differ by X%`.\n\n`--delete` removes the diff file after retrieving ratio/percentage.\n\n`--filename` specifies a filename to save the diff image under. Must use a valid extension.\n\n`--ignore-alpha` ignore the alpha channel.\n\n### Tests\n\n```\n$ ./test.py\n......\n----------------------------------------------------------------------\nRan 6 tests in 0.320s\n\nOK\n```\n\n### Formula \n\nThe difference is defined by the average % difference between each of the channels\n(R,G,B,A?) at each pair of pixels A<sub>xy</sub>, B<sub>xy</sub> at the same coordinates\nin each of the two images (why they need to be the same size), averaged over all pairs\nof pixels. \n\nFor example, compare two 1x1 images _A_ and _B_ (a trivial example, >1 pixels would have\nanother step to find the average of all pixels):\n\n_A_<sub>1,1</sub> = RGB(255,0,0) _(pure red)_\n\n_B_<sub>1,1</sub> = RGB(100,0,0) _(dark red)_\n\n((255-100)/255 + (0/0)/255 + (0/0)/255))/3 = (155/255)/3 = 0.202614379\n\nSo these two 1x1 images differ by __20.2614379%__ according to this formula.\n\n## Sample image 1\n![Alt text](/images/mario-circle-cs.png \"Image 1\")\n\n## Sample image 2\n![Alt text](/images/mario-circle-node.png \"Image 2\")\n\n## Resulting diff image\n![Alt text](/images/diff_img.png \"Difference Image\")\n\n## Difference percentage output\n`Images differ by 0.731961813597%`\n"
 },
 {
  "repo": "ekinlyw/android-badge",
  "language": "Java",
  "readme_contents": "Android-badge\n===============\nBadgeUtil provides static utility methods to set \"badge count\" on Launcher (by Samsung, LG).\n\nScreenshots\n===============\nCaptured from Samsung Galaxy S3\n\n![Captured from Samsung Galaxy S3](https://user-images.githubusercontent.com/1778805/72220549-06e08c80-3595-11ea-9e2e-6bd10a6b54d0.png)\n\nCaptured from Samsung Galaxy S4\n\n![Captured from Samsung Galaxy S4](https://user-images.githubusercontent.com/1778805/72220550-08aa5000-3595-11ea-8b86-798464dddbf3.png)\n\nCaptured from LG G3\n\n![Captured from LG G3](https://user-images.githubusercontent.com/1778805/72220551-09db7d00-3595-11ea-9d9f-ed576dcdf5db.png)\n\n\nAPI Usage\n===============\n\tBadgeUtil.setBadgeCount(Context, count);\n\n    BadgeUtil.resetBadgeCount(Context);\nLimitation\n===============\nCurrently, it's working from Android 4.0.\nBut some devices, which are released from the manufacturers, are not working.\n\nYou should reset badge count at first launch because the old  count could be saved even after uninstall application.\n\nLicense\n===============\nMIT License\n"
 },
 {
  "repo": "pgr0ss/bazel-deps",
  "language": "Java",
  "readme_contents": "# bazel-deps\n\nGenerate [bazel](http://bazel.io/) dependencies for maven artifacts\n\n## Deprecated\n\nI no longer use `bazel-deps` on my projects, nor do I actively maintain it. Now, I use the official [rules_jvm_external](https://github.com/bazelbuild/rules_jvm_external).\n\n## Usage\n\n```bash\nmvn package\njava -jar target/bazel-deps-2.0-SNAPSHOT.jar <maven artifact>...\n```\n\n## Example\n\n```bash\n% java -jar target/bazel-deps-2.0-SNAPSHOT.jar com.fasterxml.jackson.core:jackson-databind:2.5.0 junit:junit:jar:4.12\n\n\n--------- Add these lines to your WORKSPACE file ---------\n\nmaven_jar(name = \"org_hamcrest_hamcrest_core\", artifact = \"org.hamcrest:hamcrest-core:jar:1.3\")\nmaven_jar(name = \"com_fasterxml_jackson_core_jackson_annotations\", artifact = \"com.fasterxml.jackson.core:jackson-annotations:jar:2.5.0\")\nmaven_jar(name = \"com_fasterxml_jackson_core_jackson_core\", artifact = \"com.fasterxml.jackson.core:jackson-core:jar:2.5.0\")\nmaven_jar(name = \"com_fasterxml_jackson_core_jackson_databind\", artifact = \"com.fasterxml.jackson.core:jackson-databind:jar:2.5.0\")\nmaven_jar(name = \"junit_junit\", artifact = \"junit:junit:jar:4.12\")\n\n\n--------- Add these lines to your BUILD file ---------\n\njava_library(\n  name=\"jackson-databind\",\n  visibility = [\"//visibility:public\"],\n  exports = [\n    \"@com_fasterxml_jackson_core_jackson_annotations//jar\",\n    \"@com_fasterxml_jackson_core_jackson_core//jar\",\n    \"@com_fasterxml_jackson_core_jackson_databind//jar\",\n  ],\n)\n\njava_library(\n  name=\"junit\",\n  visibility = [\"//visibility:public\"],\n  exports = [\n    \"@junit_junit//jar\",\n    \"@org_hamcrest_hamcrest_core//jar\",\n  ],\n)\n```\n\nYou can also exclude a set of transitive dependencies:\n\n```bash\n\n% java -jar target/bazel-deps-1.0-SNAPSHOT.jar -x io.dropwizard:dropwizard-core:0.8.1 io.dropwizard:dropwizard-client:0.8.1\n\n\n--------- Add these lines to your WORKSPACE file ---------\n\nmaven_jar(name = \"commons_codec_commons_codec\", artifact = \"commons-codec:commons-codec:jar:1.6\")\nmaven_jar(name = \"io_dropwizard_dropwizard_client\", artifact = \"io.dropwizard:dropwizard-client:jar:0.8.1\")\nmaven_jar(name = \"io_dropwizard_dropwizard_core\", artifact = \"io.dropwizard:dropwizard-core:jar:0.8.1\")\nmaven_jar(name = \"org_apache_httpcomponents_httpclient\", artifact = \"org.apache.httpcomponents:httpclient:jar:4.3.5\")\nmaven_jar(name = \"org_apache_httpcomponents_httpcore\", artifact = \"org.apache.httpcomponents:httpcore:jar:4.3.2\")\nmaven_jar(name = \"org_glassfish_jersey_connectors_jersey_apache_connector\", artifact = \"org.glassfish.jersey.connectors:jersey-apache-connector:jar:2.17\")\nmaven_jar(name = \"io_dropwizard_metrics_metrics_httpclient\", artifact = \"io.dropwizard.metrics:metrics-httpclient:jar:3.1.1\")\n\n\n--------- Add these lines to your BUILD file ---------\n\njava_library(\n  name=\"dropwizard-client\",\n  visibility = [\"//visibility:public\"],\n  exports = [\n    \"@commons_codec_commons_codec//jar\",\n    \"@io_dropwizard_dropwizard_client//jar\",\n    \"@io_dropwizard_dropwizard_core//jar\",\n    \"@io_dropwizard_metrics_metrics_httpclient//jar\",\n    \"@org_apache_httpcomponents_httpclient//jar\",\n    \"@org_apache_httpcomponents_httpcore//jar\",\n    \"@org_glassfish_jersey_connectors_jersey_apache_connector//jar\",\n  ],\n)\n```\n\n## Code\n\nThis code was inspired by the [aether examples](https://github.com/eclipse/aether-demo/blob/322fa556494335faaf3ad3b7dbe8f89aaaf6222d/aether-demo-snippets/src/main/java/org/eclipse/aether/examples/GetDependencyTree.java) for walking maven dependencies.\n"
 },
 {
  "repo": "wumke/react-native-local-notifications",
  "language": "Java",
  "readme_contents": "# react-native-local-notifications\nManageable local notifications for React Native on iOS and Android. Create, update and delete local notifications by their unique id. The push notification title is the app name. When you open the app all displayed local notifications will be removed and the badge counter will be reset on iOS. \n\nNOTICE:\n- for React Native < 0.47 use react-native-local-notifications <1.x.x\n- for React Native > 0.47 use react-native-local-notifications >=1.x.x\n- for Android API lvl >=19 use react-native-local-notifications >=2.x.x\n\nNOTIFICATIONS WILL NOT BE SHOWN WHEN YOUR APP IS IN THE FOREGROUND. (options to handle this situation might be added in future releases)\n\nFor latest Android versions, please check notification settings and battery management settings if notifications are not shown at the specified time. This libary aims to deliver exact notifications, which are not delayed by the system.\nCreating a never ending back- or foregroundservice (API lvl >=26) can help stopping the device to kill your apps and not show your notifications on exact times. \nAlso see https://dontkillmyapp.com for more info and tips to solve this problem for devices with custom battery saving packages.\n\n## Setup\n\nFast and easy:\n```bash\nnpm install react-native-local-notifications --save\nreact-native link react-native-local-notifications\n```\nKeep in mind that the link step only links the project in the native projects, more steps have to be done for both platforms, which are described in the Android and iOS section below... please verify manually that the link command successfully linked the libray to your project!\n\nOr manual: add the latest version as dependeny to your package.json.\n\n```javascript\n{\n  \"name\": \"YourProject\",\n  ...\n  },\n  \"dependencies\": {\n    ...\n    \"react-native-local-notifications\": \"2.0.0\",\n    ...\n  }\n```\n\n#### iOS\n* {auto-link} Add RNLocalNotifications.xcoderproj into your project in the Libraries folder.\n* {auto-link}Add the .a file on the General tab of your target under Linked Frameworks And Libraries\n* {auto-link}Add the .a file on the Build Phases tab of your target under Link Binary With Libraries\n* In the AppDelegate.m file of your xcode project add: (this will clear all notifications when you open the app)\n    ```\n    - (void)applicationDidBecomeActive:(UIApplication *)application\n    {\n      [[UIApplication sharedApplication] setApplicationIconBadgeNumber:0]; //Allways reset number of notifications shown at the icon\n      for (UILocalNotification * notification in [[UIApplication sharedApplication] scheduledLocalNotifications]) { //Also remove all shown notifications\n        if ([notification.fireDate compare:[NSDate date]] == NSOrderedAscending) {\n          [[UIApplication sharedApplication] cancelLocalNotification:notification];\n        }\n      }\n    }\n    ```\n* In the AppDelegate.m file of your xcode project, in the didFinishLaunchingWithOptions function, add: (ask the user to allow notifications)\n    ```\n    if ([UIApplication instancesRespondToSelector:@selector(registerUserNotificationSettings:)]) {\n        [[UIApplication sharedApplication] registerUserNotificationSettings:[UIUserNotificationSettings settingsForTypes:UIUserNotificationTypeAlert|UIUserNotificationTypeSound categories:nil]];\n      }\n  ```\n* Add Alarm.caf and Silence.caf to the Resources folder of your xcode project. (can be found in react-native-local-notifications/ios/RNLocalNotifications)\n\n#### Android\n* In the AndroidManifest.xml file of your android studio project add:\n    ```\n    <receiver android:process=\":remote\" android:name=\"com.github.wumke.RNLocalNotifications.AlarmReceiver\" android:exported=\"true\"></receiver>\n    ```\n* In the MainActivity.java file of your android studio project add: (this will clear all notifications when you open the app)\n  ```\n  import android.content.Context;\n  import android.app.NotificationManager;\n  ...\n  @Override\n      public void onResume() {\n          super.onResume();\n          NotificationManager nMgr = (NotificationManager) getSystemService(Context.NOTIFICATION_SERVICE);\n          nMgr.cancelAll();\n      }\n  ```\n* {auto-link}In the settings.gradle\n  ```\n    include ':react-native-local-notifications', ':app'\n    project(':react-native-local-notifications').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-local-notifications/android')\n  ```\n* {auto-link}In the build.gradle\n  ```\n    compile project(':react-native-local-notifications')\n  ```\n* {auto-link}In MainApplication.java\n  ```\n    import com.github.wumke.RNLocalNotifications.RNLocalNotificationsPackage;\n    ...\n    @Override\n    protected List<ReactPackage> getPackages() {\n      return Arrays.<ReactPackage>asList(\n        ...\n        new RNLocalNotificationsPackage(),\n        ...\n      );\n    }\n    ...\n  ```\n * Aside from the big icon, for which the ic_launcher icon in mipmap folder is used by default, you also need a small transparent with white foreground icon which will be displayed in the status bar.\n See Android developer specifications for correct sizes, by default 'notification_small' from the drawable folder is used... \n  \n## Usage\n\n####Examples:\n```javascript\nimport RNLocalNotifications from 'react-native-local-notifications';\n...\n//RNLocalNotifications.setAndroidIcons(largeIconName, largeIconType, smallIconName, smallIconType);\nRNLocalNotifications.setAndroidIcons(\"ic_launcher\", \"mipmap\", \"notification_small\", \"drawable\"); //this are the default values, this function is optional\n\n//RNLocalNotifications.createNotification(id, text, datetime, sound[, hiddendata]);\nRNLocalNotifications.createNotification(1, 'Some text', '2017-01-02 12:30', 'default');\n\n//RNLocalNotifications.updateNotification(id, text, datetime, sound[, hiddendata]);\nRNLocalNotifications.updateNotification(1, 'Some modifications to text', '2017-01-02 12:35', 'silence');\n\n//RNLocalNotifications.deleteNotification(id);\nRNLocalNotifications.deleteNotification(1);\n...\n```\n#### Parameter explanation:\n* id (Integer): Unique value to be able to edit or cancel scheduled notifications.\n* text (String): The message text.\n* datetime (String): The date + time to show the notification, as a string in the format 'yyyy-mm-dd hh:mm'.\n* sound (String): Which sound is played: '' or 'silence' for vibration only, 'default' for system alarm sound, custom sound namefor self added ringtones.\n* hiddendata (String): Invisible data that can be used to perform custom actions when the mobile app is opened by clicking on the local notification.\n\n#### Add custom sounds:\n\nConvert your ringtone to .caf and .mp3 file formats.\n\n__iOS__: Add yoursound.caf to the Resources folder of your xcode project.  \n__Android__: Add yoursound.mp3 to the 'raw' folder\n\nUse 'yoursound' as string for the sound parameter.\n\n#### Hidden/extra data:\n\nWhen you need to include custom, non-visible, data (for example object id's) to your notifications provide the optional 'hiddendata' parameter to createNotification/updateNotification.\n\nThe value will be available as hiddendata (Android) or userData.hiddendata (iOS) when you click the notification.\n\nNote that 'hiddendata' must be a string, so if you want to include json objects you need to encode/decode the data yourself.\n\n## Versioning\n\nThis project uses semantic versioning: MAJOR.MINOR.PATCH.\nThis means that releases within the same MAJOR version are always backwards compatible. For more info see [semver.org](http://semver.org/).\n\n## Licence\n\nMIT (see LICENCE file)\n\n## Release notes\n\nSee https://www.npmjs.com/package/react-native-local-notifications?activeTab=versions\n\n#### 2.0.0\n\nBreaking changes\n- none\n\nNew features / Updates\n- Changed licence to MIT\n- Custom sounds\n- Hidden data\n- Set Android notification icons type/name\n- compileSdkVersion, buildToolsVersion and targetSdkVersion (equal to compileSdkVersion)\n\nFixes\n- Readme delete notification example\n- Updated android part to schedule exact notifications based on api lvl for api lvl 19, 21 and 23 (tested up to 26)\n- Added com.android.support:support-v4:+ dependency\n\nTodo\n- iOS UILocalNotification deprecation\n"
 },
 {
  "repo": "shkcodes/Lyrically",
  "language": "Java",
  "readme_contents": "# Lyrically\n\n<img src=\"https://github.com/shkcodes/Lyrically/blob/master/screenshots/screenshot1.png\" alt=\"alt text\" height=\"640\"> \n<img src=\"https://github.com/shkcodes/Lyrically/blob/master/screenshots/screenshot2.png\" alt=\"alt text\" height=\"640\">\n<img src=\"https://github.com/shkcodes/Lyrically/blob/master/screenshots/screenshot3.png\" alt=\"alt text\" height=\"640\">\n<img src=\"https://github.com/shkcodes/Lyrically/blob/master/screenshots/screenshot4.png\" alt=\"alt text\" height=\"640\">\n\n\n\n\nAn everywhere lyrics app which can be used by simply swiping a part of your screen. See the lyrics without interrupting what you're doing.\n\n\n## Supported Apps \n* Google Play Music\n* Spotify\n* Phonograph\n* Shuttle\n* Poweramp\n* BlackPlayer\n* Rocket Player\n\n\n\n\nI have plans to make it work with many other music players in the future.\n\n\n\n## [How to use](https://www.youtube.com/watch?v=g0XidfCGZHU)\n\n## Downloads\n* [Github](https://github.com/shkcodes/Lyrically/releases)\n* [F-Droid](https://f-droid.org/repository/browse/?fdid=com.shkmishra.lyrically)\n\n## [Icons](http://www.flaticon.com/)\n"
 },
 {
  "repo": "sanaehirotaka/logbook-kai",
  "language": "Java",
  "readme_contents": "\u822a\u6d77\u65e5\u8a8c (logbook-kai)\n--\n[![GitHub release (latest by date)](https://img.shields.io/github/v/release/sanaehirotaka/logbook-kai)](https://github.com/sanaehirotaka/logbook-kai/releases/latest)\n[![GitHub](https://img.shields.io/github/license/sanaehirotaka/logbook-kai)](LICENSE)\n[![GitHub All Releases](https://img.shields.io/github/downloads/sanaehirotaka/logbook-kai/total)](https://github.com/sanaehirotaka/logbook-kai/releases)\n[![GitHub Release Date](https://img.shields.io/github/release-date/sanaehirotaka/logbook-kai)](https://github.com/sanaehirotaka/logbook-kai/releases)\n\n## ****\u91cd\u8981\u306a\u304a\u77e5\u3089\u305b****\nv20.9.2 \u4ee5\u964d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u30aa\u30ea\u30b8\u30ca\u30eb\u306e [sanaehirotaka \u3055\u3093\u306e\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/sanaehirotaka/logbook-kai/)\u3067\u306f\u306a\u304f\n[Sdk0815 \u306e fork](https://github.com/Sdk0815/logbook-kai/)\u306b\u3066\u958b\u767a\u3092\u884c\u3044\u307e\u3059\u3002\n\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u3082[\u3053\u3061\u3089](https://github.com/Sdk0815/logbook-kai/releases)\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u4eca\u5f8c\u306f[Issue\uff08\u554f\u984c\u5831\u544a\u30fb\u8981\u671b\uff09](https://github.com/Sdk0815/logbook-kai/issues)\u3084[Pull Request\uff08\u5909\u66f4\u8981\u6c42\uff09](https://github.com/Sdk0815/logbook-kai/pulls)\u306a\u3069\u3082\u305d\u3061\u3089\u306b\u30aa\u30fc\u30d7\u30f3\u3057\u3066\u3044\u305f\u3060\u304d\u307e\u3059\u3088\u3046\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n\n### \u6982\u8981\n\n**\u822a\u6d77\u65e5\u8a8c (logbook-kai)** \u306f\u3001\u300c\u8266\u968a\u3053\u308c\u304f\u3057\u3087\u3093 \uff5e\u8266\u3053\u308c\uff5e\u300d\u3092\u3088\u308a\u904a\u3073\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306e\u5916\u90e8\u30c4\u30fc\u30eb\u3067\u3059\u3002\n\n\u753b\u9762\u304c\u30b3\u30f3\u30d1\u30af\u30c8\u306a\u306e\u304c\u7279\u5fb4\u3067\u3059\u3002\n\n![\u30e1\u30a4\u30f3\u753b\u9762](images/overview.png)\n\n### \u822a\u6d77\u65e5\u8a8c \u306b\u3064\u3044\u3066\n\n\u822a\u6d77\u65e5\u8a8c \u3067\u306f[Jetty](http://www.eclipse.org/jetty/) \u3067\u901a\u4fe1\u5185\u5bb9\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3057\u3066\u5185\u5bb9\u3092\u89e3\u6790\uff0f\u8868\u793a\u3057\u307e\u3059\u3002\n\u30d7\u30ed\u30ad\u30b7\u8a2d\u5b9a\u3092\u884c\u3046\u3053\u3068\u3067\u5225\u306e\u30c4\u30fc\u30eb\u3068\u9023\u643a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\n\n**\u300c\u8266\u968a\u3053\u308c\u304f\u3057\u3087\u3093 \uff5e\u8266\u3053\u308c\uff5e\u300d\u30b5\u30fc\u30d0\u30fc\u306b\u5bfe\u3059\u308b\u901a\u4fe1\u5185\u5bb9\u306e\u6539\u5909\u3001\u8ffd\u52a0\u306e\u901a\u4fe1\u7b49\u306f\u4e00\u5207\u884c\u3063\u3066\u3044\u307e\u305b\u3093\u3002**\n\nMIT \u30e9\u30a4\u30bb\u30f3\u30b9\u306e\u4e0b\u3067\u516c\u958b\u3059\u308b\u3001\u81ea\u7531\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3067\u3059\u3002\n\n### \u4e3b\u306a\u6a5f\u80fd\n\n* \u9060\u5f81\u30fb\u5165\u6e20\u306e\u901a\u77e5\u6a5f\u80fd : 1\u5206\u524d\u306b\u306a\u308b\u3068\u81ea\u52d5\u7684\u306b\u901a\u77e5\u3057\u307e\u3059\u3002\n* \u6d77\u6226\u30fb\u30c9\u30ed\u30c3\u30d7\u5831\u544a\u66f8 : \u6226\u95d8\u306e\u72b6\u6cc1\u3001\u30c9\u30ed\u30c3\u30d7\u8266\u5a18\u306a\u3069\u306e\u60c5\u5831\u306e\u53ce\u96c6\u3092\u884c\u3048\u307e\u3059\u3002\n* \u6240\u6709\u88c5\u5099\u4e00\u89a7 : \u8ab0\u304c\u3069\u306e\u88c5\u5099\u3092\u6301\u3063\u3066\u3044\u308b\u304b\u3092\u7c21\u5358\u306b\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\u3002\n* \u6240\u6709\u8266\u5a18\u4e00\u89a7 : \u8266\u5a18\u306e\u5404\u7a2e\u30d1\u30e9\u30e1\u30fc\u30bf(\u30b3\u30f3\u30c7\u30a3\u30b7\u30e7\u30f3\u3001\u5236\u7a7a\u5024\u3001\u706b\u529b\u5024\u7b49)\u306e\u95b2\u89a7\u3092\u884c\u3046\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\u3002\n* \u304a\u98a8\u5442\u306b\u5165\u308a\u305f\u3044\u8266\u5a18 : \u4fee\u7406\u304c\u5fc5\u8981\u306a\u8266\u5a18\u306e\u6642\u9593\u3068\u5fc5\u8981\u8cc7\u6750\u3092\u4e00\u89a7\u3067\u898b\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\u3002\n\n\n### \u52d5\u4f5c\u74b0\u5883\n![Java](https://img.shields.io/badge/-Java-007396.svg?logo=java)\n![Windows](https://img.shields.io/badge/-Windows-0078D6.svg?logo=windows)\n![Debian](https://img.shields.io/badge/-Debian-A81D33.svg?logo=debian)\n![Redhat](https://img.shields.io/badge/-Redhat-EE0000.svg?logo=red-hat)\n![macOS](https://img.shields.io/badge/-macOS-333333.svg?logo=apple)\n\nJava 8u40\u4ee5\u964d\u306eJava8\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305fWindows,Linux\u307e\u305f\u306fmacOS\u304c\u5fc5\u8981\u3067\u3059\u3002\n\n**\u6b21\u306eJavaVM\u3067\u52d5\u4f5c\u78ba\u8a8d\u3055\u308c\u3066\u3044\u307e\u3059\u3002**\n- **[Liberica JDK version 8](https://bell-sw.com/pages/java-8u232/)**\n   - \u65b0\u898f\u306b\u5c0e\u5165\u3059\u308b\u5834\u5408\u3001\u3053\u3061\u3089\u3092\u63a8\u5968\u3057\u307e\u3059\u3002\n- [Oracle JDK 8](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\n   - \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u306bOTN\u30a2\u30ab\u30a6\u30f3\u30c8\u304c\u5fc5\u8981\u3067\u3059\u3002\n\n\u6b21\u306eJavaVM\u3067\u306f\u3054\u5229\u7528\u3044\u305f\u3060\u3051\u307e\u305b\u3093\u3002\n- \u30d0\u30fc\u30b8\u30e7\u30f3\u304c8\u3067\u306f\u306a\u3044JavaVM\n- Amazon Corretto : \u4e00\u90e8\u6a5f\u80fd\u304c\u52d5\u4f5c\u3057\u307e\u305b\u3093(\u96c6\u8a08\u6a5f\u80fd\u30fb\u81ea\u52d5\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u304c\u5229\u7528\u4e0d\u53ef)\u3002\n- AdoptOpenJDK : \u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u305f\u3081\u8d77\u52d5\u3057\u307e\u305b\u3093\u3002\n\n### [\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9](https://github.com/sanaehirotaka/logbook-kai/releases)\n\n**\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044**\n\n**\u521d\u671f\u306e\u72b6\u614b\u3067\u306f\u8266\u5a18\u306e\u753b\u50cf\u304c\u8868\u793a\u51fa\u6765\u307e\u305b\u3093\u3002\u5fc5\u305a**[FAQ](faq.md)**\u3092\u304a\u8aad\u307f\u304f\u3060\u3055\u3044\u3002**\n\n### [\u30d6\u30e9\u30a6\u30b6\u306e\u8a2d\u5b9a(\u5fc5\u9808)](how-to-preference.md)\n\n### [FAQ](faq.md)\n\n#### \u30d7\u30e9\u30b0\u30a4\u30f3\n* [Pushbullet Plugin](https://github.com/rsky/logbook-kai-plugins)\n  * \u9060\u5f81\u30fb\u5165\u6e20\u306e\u901a\u77e5\u3092iPhone/Android\u7aef\u672b\u3078\u30d7\u30c3\u30b7\u30e5\u901a\u77e5\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\n### \u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\n\n* \u30e1\u30a4\u30f3\u753b\u9762\n\n![\u30e1\u30a4\u30f3\u753b\u9762](images/overview.png)\n\n* \u6240\u6709\u88c5\u5099\u4e00\u89a7\n\n![\u6240\u6709\u88c5\u5099\u4e00\u89a7\u305d\u306e\u3044\u3061](images/items1.png)\n![\u6240\u6709\u88c5\u5099\u4e00\u89a7\u305d\u306e\u306b](images/items2.png)\n\n* \u6226\u95d8\u30ed\u30b0\n\n![\u6226\u95d8\u30ed\u30b0\u305d\u306e\u3044\u3061](images/battlelog1.png)\n![\u6226\u95d8\u30ed\u30b0\u305d\u306e\u306b](images/battlelog2.png)\n\n### \u958b\u767a\u8005\u5411\u3051\n\n#### [\u30d3\u30eb\u30c9\u65b9\u6cd5](how-to-build.md)\n\n#### [\u30d7\u30e9\u30b0\u30a4\u30f3\u958b\u767a](how-to-develop.md)\n\n### \u30e9\u30a4\u30bb\u30f3\u30b9\n\n* [The MIT License (MIT)](LICENSE)\n\nMIT \u30e9\u30a4\u30bb\u30f3\u30b9\u306e\u4e0b\u3067\u516c\u958b\u3059\u308b\u3001\u81ea\u7531\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3067\u3059\u3002\n\n### \u4f7f\u7528\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u30e9\u30a4\u30bb\u30f3\u30b9\n\n\u4ee5\u4e0b\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n#### [JSON Processing(JSR 353)](https://jsonp.java.net/)\n\n* COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL - Version 1.1)\n* GNU General Public License (GPL - Version 2, June 1991) with the Classpath Exception\n* **\u30e9\u30a4\u30bb\u30f3\u30b9\u5168\u6587 :** [https://jsonp.java.net/license.html](https://jsonp.java.net/license.html)\n\n#### [Jetty](http://www.eclipse.org/jetty/)\n\n* Apache License 2.0\n* Eclipse Public License 1.0\n* **\u30e9\u30a4\u30bb\u30f3\u30b9\u5168\u6587 :** [http://www.eclipse.org/jetty/licenses.php](http://www.eclipse.org/jetty/licenses.php)\n\n#### [commons-logging](https://commons.apache.org/proper/commons-logging/)\n\n* Apache License 2.0\n* **\u30e9\u30a4\u30bb\u30f3\u30b9\u5168\u6587 :** [http://www.apache.org/licenses/](http://www.apache.org/licenses/)\n\n#### [Apache Log4j 2](http://logging.apache.org/log4j/2.x/)\n\n* Apache License 2.0\n* **\u30e9\u30a4\u30bb\u30f3\u30b9\u5168\u6587 :** [http://logging.apache.org/log4j/2.x/license.html](http://logging.apache.org/log4j/2.x/license.html)\n\n#### [ControlsFX](http://fxexperience.com/controlsfx/)\n\n* The BSD 3-Clause License\n* **\u30e9\u30a4\u30bb\u30f3\u30b9\u5168\u6587 :** [https://bitbucket.org/controlsfx/controlsfx/src/default/license.txt?fileviewer=file-view-default](https://bitbucket.org/controlsfx/controlsfx/src/default/license.txt?fileviewer=file-view-default)\n"
 },
 {
  "repo": "Glitchbone/CordovaYoutubeVideoPlayer",
  "language": "Java",
  "readme_contents": "# Cordova YoutubeVideoPlayer Plugin\n\n**Play Youtube Videos in a native Video Player on Android &amp; iOS.**\n\niOS plugin uses **XCDYouTubeKit** by C\u00e9dric Luthi:  \nhttps://github.com/0xced/XCDYouTubeKit\n\nAndroid version uses **OpenYoutubeActivity** by Keyes Labs:  \nhttps://code.google.com/p/android-youtube-player\n\n## Installation\n\n```sh\ncordova plugin add https://github.com/Glitchbone/CordovaYoutubeVideoPlayer.git\n```\n\n## Usage\n\n```javascript\nYoutubeVideoPlayer.openVideo('YOUTUBE_VIDEO_ID');\n```\n\n## Author\n\n**Adrien Glitchbone**\n\n+ [https://twitter.com/glitchbone](https://twitter.com/glitchbone)\n+ [http://github.com/Glitchbone](http://github.com/Glitchbone)\n\n## License\n\nCordovaYoutubeVideoPlayer is available under the MIT license. See the [LICENSE](LICENSE) file for more information.  \nXCDYouTubeKit is available under the MIT license.  \nOpenYoutubeActivity is available under the Apache License 2.0.  \n"
 },
 {
  "repo": "marcojakob/javafx-ui-sandbox",
  "language": "Java",
  "readme_contents": "# JavaFX UI Sandbox #\n\n---\n**Note:** If you are using JavaFX 8 you may want to check out the (official) [official JavaFX Dialogs](http://code.makery.ch/blog/javafx-dialogs-official/)!\nThis is the reason why this project will not be actively developed further and will only work for JavaFX 2.\n\n---\n\n\nOracle has opened up the [JavaFX UI Sandbox repository](http://hg.openjdk.java.net/openjfx/sandbox-8/controls/rt) on OpenJFX as announced on the [fxExperience blog](http://fxexperience.com/2012/10/announcing-the-javafx-ui-controls-sandbox/). \nControls in the OpenJFX repository might eventually be included in a next JavaFX release.\n\nThis GitHub project provides **simple `jar` files** for some of those sandbox controls.\n\n## JavaFX Dialogs ##\nJavaFX Dialogs are simple dialogs in the style of [JOptionPane](http://docs.oracle.com/javase/tutorial/uiswing/components/dialog.html) from Swing.\n\nDetails about [JavaFX Dialogs](https://github.com/marcojakob/javafx-ui-sandbox/tree/master/javafx-dialogs)\n\n---\nMarco Jakob (http://edu.makery.ch)\n"
 },
 {
  "repo": "sharish/SendEmailAnimation",
  "language": "Java",
  "readme_contents": "# Send Email Animation\n\nThis is a small attempt to attract users when they are using in app feature to send out an email. The usage of this repo serves the purpose like contacting the support desk, inviting another user over email or any other form of email based features.\n\n##### Why to use?\n\n- An user having used multiple applications is bored to see the traditional loading indicator, would always prefer to see something new and providing such user experience helps to retain users. \n\n- There are cases where users may not like the idea of taking them to email client app(passing an SEND_TO action intent) since the users do not like to promote an app feature from his direct email account. \n\n- More often users do not return to the origin application that took him to email client app as they become busy reading the other emails - leads to fall in user engagement.\n\n##### How to Use?\n\nVery simple - Have a button in your activity and click listener to call the following call:\n\n```java\nstartActivity(new Intent(YourActivity.this,EnvelopeActivity.class));\n```\n\nThe above will take the user to email client app for sending out an email. (I suggest not to use this because of last two reasons mentioned in 'Why to use' section.)\n\nFor handling the emails within the app:\n\n```java\nstartActivityForResult(new Intent(YourActivity.this,EnvelopeActivity.class), SOME_INTENT_REQUEST_CODE);\n```\n\nHave overriding onActivityResult in your calling activity to get the email subject and message part that you can use in your own app. (Refer MainActivity.java to check how to do that)\n\n\nNote: Email subject and message field validations are not done, it is left to your choice of handling and you are free to edit the source.\n\n##### Realtime Email:\n\nTo send a real time email along with this animation over mailgun.com server, refer these steps : [SendEmail.md](https://gist.github.com/cooltechworks/701487ac01841433ffd5)\n\n\n##### How it looks?\n\n![sendemailanimation](https://cloud.githubusercontent.com/assets/13122232/10564092/9f6f8be0-75c3-11e5-94bd-801aef62c529.gif)\n\n\n##### LICENSE:\n\nYou are free to use, copy, edit and the repo is provided under [MIT LICENSE](https://github.com/cooltechworks/ContactSupportAnimation/blob/master/LICENSE)\n\n"
 },
 {
  "repo": "thothbot/parallax",
  "language": "Java",
  "readme_contents": "![Parallax 3D library](https://github.com/thothbot/parallax/wiki/images/logo.png)\n\n[![Build Status](https://travis-ci.org/thothbot/parallax.svg?branch=master)](https://travis-ci.org/thothbot/parallax)\n\n![Demo](http://thothbot.github.com/parallax/static/examples_banner.jpg)\n\n### Cross-platform Java 3D SDK\n\n[Wiki](https://github.com/thothbot/parallax/wiki)\n| [Bugs](https://github.com/thothbot/parallax/issues)\n\n**Current status of** `Parallax 2.0` is *beta*\n\nParallax 2.0 is completely new Java 3D SDK. We wanted to make it as simple as possible.\nIf you know how to use [three.js](http://github.com/mrdoob/three.js) you can start quick, because Parallax implements almost all threejs API. But also you have a set of tools and libraries to write cross-platform 3D applications.\n\nParallax SDK allows you to write 3D application for the following platforms:\n* Browsers (GWT) - *ready*\n* Android - *in the pipeline, should be ready in Parallax 2.0*\n* Desktop Window, Linus, OS X - *future*\n\n#### Quick start\n\nWe prepared Parallax SDK application template.\nGo to the [parallax-application-template](https://github.com/thothbot/parallax-application-template) project and just follow the manual to make your first Parallax application.\n\nParallax 2.0 SDK will be in the [maven central repository](http://search.maven.org), but snapshots are located in the [sonatype repo](https://oss.sonatype.org/content/repositories/snapshots/):\n\n```xml\n<!-- pom.xml example -->\n<repositories>\n    <repository>\n        <id>oss-sonatype</id>\n        <name>oss-sonatype</name>\n        <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n        <snapshots>\n            <enabled>true</enabled>\n        </snapshots>\n    </repository>\n</repositories>\n```\n\nParallax SDK contains the following main packages:\n* **parallax** - Parallax SDK core. Should be included to all applications\n* **parallax-gwt** - Parallax for <a href=\"https://developers.google.com/web-toolkit/\">Google Web Toolkit</a> platform, to run your 3D applications in any modern browsers.\n* **parallax-android** - Parallax for Android platform.\n* **parallax-controllers** - Cross-platform *Extension* to use mouse, trackball etc\n* **parallax-loaders** - Cross-platform *Extension* to load models, fonts etc\n* **parallax-renderer-plugins** - Cross-platform *Extension* for post-processing rendering etc\n* **parallax-renderer-css-gwt** - GWT platform *Extension* for rendering using CSS.\n* **parallax-renderer-raytracing-gwt** - GWT platform *Extension* for experiments in raytracing rendering\n\nTo launch Parallax 2.0 SDK GWT tests run:\n\n```\ngradle :tests:parallax-tests-gwt:superDev\n```\n\n[Change log](https://github.com/thothbot/parallax/releases)\n| [1.x Download](http://github.com/thothbot/parallax/wiki/Download)\n| [1.x API Reference](http://thothbot.github.com/parallax/docs/index.html)\n| [1.x Demo](http://thothbot.github.com/parallax/demo/index.html)\n"
 },
 {
  "repo": "flipboxstudio/mvvm-starter",
  "language": "Java",
  "readme_contents": "# mvvm-starter\n\n[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-MVVM%20Starter-blue.svg?style=flat)]()\n\nA starter project for Android MVVM Project with DataBinding Library and RxJava 2\n\n### Archived\nThis repository will no longer be maintained. Further development will happen in [Modular Starter](https://github.com/medigoid/ModularStarter) using the latest and more scalable approach\n\n### How To Use \nYou can create new project using our project generator here [MVVM Starter Generator](http://mvvm.flipbox.co.id) - by using our generator you can save the hassle of renaming every package reference & changing folder structure manually. Just input your App & Package name and extract the downloaded file for your future development\n\n--\n\nLibraries used :\n\n* Retrofit `com.squareup.retrofit2:retrofit:2.3.0`\n* Glide `com.github.bumptech.glide:glide:4.0.0-RC1`\n* EasyPermission `pub.devrel:easypermissions:0.4.3`\n* Hawk `com.orhanobut:hawk:2.0.1`\n* Android Libraries ( `appcompat`, `design support`, `data binding`, etc )\n* Sosoito Loading Layout `com.github.flipboxstudio:sosoito:v1.0.3`\n* Data Binding Validator  `com.github.Ilhasoft:data-binding-validator:1.1.0`\n* Rx Java 2 + Rx Android `io.reactivex.rxjava2:rxandroid:2.0.1` & `io.reactivex.rxjava2:rxjava:2.1.2`\n\n--\n\nSetup included :\n\n* Data Binding\n\t* Using Android Data Binding Library\n\t* ViewModel is inside `viewmodels` package\n* Splash Screen\n\t* Implement splash screen the right way [https://www.bignerdranch.com/blog/splash-screens-the-right-way/]\n* Authentication Flow\n\t* Using Retrofit & ResponseInterceptor for request & response handling\n\t* Dummy API using `https://jsonplaceholder.typicode.com`\n\t* `AuthActivity` as Fragments Container\n\t* `LoginFragment` as Login UI with data binding validation\n\t* `ForgotPasswordFragment` as Forgot Password UI\n\t* `RetrofitErrorAdapter` to handle retrofit throwable error\n* RecyclerView Sample\n\t* Sample recyclerview implementation using viewholder & databinding\n\t* Using `User` as dummy object representation\n\t* See `RecyclerViewActivity` for details\n* ViewPager Sample\n\t* Sample viewpager implementation using tablayout\n\t* See `ViewPagerActivity` for details\n* Android M permission handler using `EasyPermissions`\n\t* Handle permission for Android M and above\n\t* Sample implementation for CAMERA access permission\n* Reusable Style\n\t* All colors are available inside `colors.xml`\n\t* Styles are available inside `styles.xml`\n\t* Custom Fonts are using `CustomTextView` on `utils` package and custom attribute on `attrs.xml`\n\t* Vector Drawable assets\n\t* Roboto fonts included\n* Rx Java 2 Samples\n    * Get data from API using `Maybe`\n    * Find existing data from cache & get from API if not exists\n    * Based on RxJava pattern described [here](https://medium.com/@andrew.kelly/rxjava-the-first-3-patterns-4c112a85b689)\n* Utilities classes\n\t* Camera Utils : Get image from camera / gallery\n\t* Calendar Utils : Parse & display Calendar object into various format\n* Product Flavors\n    * Flavor dimensions : using flavor dimensions enable app to combine multiple flavors ( currently using 1 dimension, env / environment )\n    * Product Flavors : `dev` for development variables and `prod` for production environment variables\n\t\n--\n![screenshots](https://puu.sh/v7Um1/e36c48b42f.png \"Screenshots\")\n--\n\n### ToDo\n\n- [ ] Documentation & Wiki\n- [ ] Analytics setup\n- [ ] RxJava 2 RetryWhen sample\n- [x] Retrofit Error Handler & sample\n- [x] JWT Library & Utils\n- [x] Data validation\n- [x] Basic Location detection\n- [x] Simple Custom toolbar & menu\n- [x] Android M permissions sample\n- [x] Calendar utils\n- [x] Camera utils\n- [x] RecyclerView sample\n- [x] ViewPager sample\n- [x] Product Flavors\n- [ ] Any suggestion?\n"
 },
 {
  "repo": "freakified/TimeStylePebble",
  "language": "C",
  "readme_contents": "# TimeStyle\nA stylish, modern watchface for the Pebble and Pebble Time watches.\n\n<img src=\"project_banner.png\" width=\"400\" height=\"300\">\n\nInspired by the visual language of the Timeline found on the Pebble Time, TimeStyle is designed as the \u201cpresent\u201d to complement the Timeline\u2019s \u201cpast\u201d and \u201cfuture\u201d.\n\n* Readable: With more than 80% of the display area devoted to the time and 6 font options, TimeStyle is designed for readability in all conditions. Unlike most other Pebble faces, time text is displayed using antialiasing, achieved using palette swapping.\n* Colorful: includes over 20 preset color schemes, and also supports custom colors using any color the Pebble Time can display&mdash;also supports saving, loading, and sharing custom presets!\n* Configurable: TimeStyle features a wide variety of different complications, including step counts, sleep times, weather forecasts, the week number, seconds, the time in another time zone, the battery level, and more.\n* Keeps you informed: TimeStyle automatically displays notifications when the battery is low or when your phone disconnects.\n* Works in 30 different languages, more than any other Pebble face: English, French, German, Spanish, Italian, Dutch, Turkish, Czech, Slovak, Portuguese, Greek, Swedish, Polish, Romanian, Vietnamese, Catalan, Norwegian, Russian, Estonian, Basque, Finnish, Danish, Lithuanian, Slovenian, Hungarian, Croatian, Serbian, Irish, Latviann, and Ukrainian.\n\n## Want to try it?\nDownload on the Pebble store at the link below:\nhttps://apps.rebble.io/en_US/application/55a5c024f4510f794c000071?section=watchfaces\n\n## Contributing\nWant to contribute to TimeStyle? Have a look at [the various feature requests that are still outstanding](https://github.com/freakified/TimeStylePebble/issues?q=is%3Aopen+is%3Aissue) -- just comment on one if you're interested in working on it!\n"
 },
 {
  "repo": "Themaister/GLFFT",
  "language": "C",
  "readme_contents": "# GLFFT\n\nGLFFT is a C++11/OpenGL library for doing the Fast Fourier Transform (FFT) on a GPU in one or two dimensions.\nThe target APIs are OpenGL 4.3 core profile and OpenGL ES 3.1.\nGLFFT is implemented entirely with compute shaders.\n\nThe FFT has several uses in graphics. The two main ones are Tessendorf's FFT water simulation technique\nas well as applying massive convolution filters to images more efficiently.\n\n## Features\n\nGLFFT is a well featured single-precision and half-precision (FP16) FFT library designed for\ngraphics use cases.\n\n - Power-of-two transforms\n - 1D/2D complex-to-complex transform\n - 1D/2D real-to-complex transform\n - 1D/2D complex-to-real transform\n - 1D/2D dual complex-to-complex transforms which pair two complex numbers into a vec4 (useful for working for RGBA data).\n - 1D/2D convolution support\n - Normalized and unnormalized FFT\n - Support for choosing if input and output to GLFFT is treated as packed FP16 or FP32.\n - Support for using Shader Storage Buffer Objects or Textures as inputs and outputs to GLFFT.\n - Applicable for many different GPUs with many performance oriented knobs to tweak.\n - Support for FFT \"wisdom\", a method where GLFFT will find optimal parameters for a particular GPU.\n - A serialization interface for storing GLFFT wisdom for later use.\n - A standalone CLI for verification and benchmarking.\n\n### Platform support\n\nDesktop Linux and Windows have been tested and are supported. Supports both OpenGL 4.3 core and OpenGL ES 3.1.\n\nThe GLSL code has been tested and verified on:\n\n - ARM Mali T-760 (Linux, Android 5.x)\n - nVidia GTX 760 (Linux)\n - Intel HD 4400 (Windows 7 x64)\n\nCompilers tested:\n\n - GCC 4.8\n - GCC 4.9\n - GCC 5.2\n - Clang 3.6\n\n## Performance\n\nGLFFT is a performance oriented FFT library, and aims to reach optimal efficiency on both desktop and mobile GPUs.\n\n - Supports different vector sizes to match both scalar and vector-based GPU hardware.\n - Supports techniques to reduce bank conflicts which greatly improves performance of GPUs with banked shared memory such as nVidia and AMD.\n - Supports almost any workgroup size decomposition to better match ideal memory access patterns on various hardware.\n - Supports mediump precision to improve performance in FP16 FFTs on mobile GPUs, e.g. ARM Mali.\n - Supports packed FP16 input and FP16 output to greatly reduce bandwidth when FP16 is accurate enough for the particular application.\n\n## Integrating GLFFT into a code base\n\nGLFFT is an OpenGL oriented implementation, but all API specifics of GLFFT have been abstracted away to\nmake it suitable for integration into engines which might have abstracted interfaces to the underlying graphics APIs.\nIt is possible to implement GLFFT without OpenGL, as long as GLSL is supported as a shading language,\nwhich is assumed to be feasible once SPIR-V becomes mainstream.\nThe abstracted interface is designed to match the spirit of next-generation APIs like Vulkan and D3D12.\n\nAs using GLFFT in a GL context is by far the most common use case, a default, ready-to-go OpenGL interface\nis supplied in the API. Due to the nature of OpenGL, it is not always feasible to build GLFFT as a standalone library, as applications\nmight have their own ways of getting to OpenGL symbols and headers which is not easy for GLFFT to work with.\nInstead, the GLFFT implementation will include a user-supplied header, `glfft_api_headers.hpp` which is responsible for including\nthe appropriate OpenGL or OpenGL ES 3.1 headers (or special headers like GLEW) the calling application uses,\nas well as defining various constants, such as the GLSL language strings to use.\nThis header can also override various functions such as logging functions and time functions.\nFor a reference, see `test/glfw/glfft_api_headers.hpp` as to how the internal test/bench suite does it.\n\n### Pre-baking GLSL shader source\n\nTo compile GLSL shader sources into header files, run:\n\n    make -C glsl\n\nThis only needs to be run when the internal GLSL shaders are modified.\n\n### Source files\n\nTo build source files, a `$(wildcard *.cpp)` in top level directory of GLFFT is sufficient to find the necessary files.\nWhen compiling, C++11 must be enabled, and `glfft_api_headers.hpp` must be found in an include path.\n\n## Snippets\n\n### Do a 1024x256 Complex-To-Complex FFT.\n\n```c++\n#include \"glfft.hpp\"\n#include \"glfft_gl_interface.hpp\"\n#include <memory>\n\nusing namespace GLFFT;\nusing namespace std;\n\nFFTOptions options; // Default initializes to something conservative in terms of performance.\noptions.type.fp16 = true; // Use mediump float (if GLES) in shaders.\noptions.type.input_fp16 = false; // Use FP32 input.\noptions.type.output_fp16 = true; // Use FP16 output.\noptions.type.normalize = true; // Normalized FFT.\n\nGLContext context;\n\nFFT fft(&context, 1024, 256, ComplexToComplex, Inverse, SSBO, SSBO, make_shared<ProgramCache>(), options);\n\nGLuint output_ssbo, input_ssbo;\n// Create GL_SHADER_STORAGE_BUFFERs and put some data in them.\n\n// Adapt raw GL types to types which GLContext uses internally.\nGLBuffer adaptor_output(output_ssbo);\nGLBuffer adaptor_input(input_ssbo);\n\n// Do the FFT\nCommandBuffer *cmd = context.request_command_buffer();\nfft.process(cmd, &adaptor_output, &adaptor_input);\ncontext.submit_command_buffer(cmd);\n\nglMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT);\n```\n\n### Do a 1024x256 Complex-To-Complex FFT more optimally using wisdom.\n\n```c++\n#include \"glfft.hpp\"\n#include \"glfft_wisdom.hpp\"\n#include \"glfft_gl_interface.hpp\"\n#include <memory>\n\nusing namespace GLFFT;\nusing namespace std;\n\nFFTOptions options; // Default initializes to something conservative in terms of performance.\noptions.type.fp16 = true; // Use mediump float (if GLES) in shaders.\noptions.type.input_fp16 = false; // Use FP32 input.\noptions.type.output_fp16 = true; // Use FP16 output.\noptions.type.normalize = true; // Normalized FFT.\n\nGLContext context;\n\nFFTWisdom wisdom;\n// Use some static wisdom to make the learning step faster.\n// Avoids searching for options which are known to be bogus for a particular vendor.\nwisdom.set_static_wisdom(FFTWisdom::get_static_wisdom_from_renderer(reinterpret_cast<const char*>(glGetString(GL_RENDERER))));\n// Learn how to do 1024x256 much faster!\nwisdom.learn_optimal_options_exhaustive(&context, 1024, 256, ComplexToComplex, SSBO, SSBO, options.type);\n\nGLContext context;\n\nFFT fft(&context, 1024, 256, ComplexToComplex, Inverse, SSBO, SSBO, make_shared<ProgramCache>(), options);\n\nGLuint output_ssbo, input_ssbo;\n// Create GL_SHADER_STORAGE_BUFFERs and put some data in them.\n\n// Adapt raw GL types to types which GLContext uses internally.\nGLBuffer adaptor_output(output_ssbo);\nGLBuffer adaptor_input(input_ssbo);\n\n// Do the FFT\nCommandBuffer *cmd = context.request_command_buffer();\nfft.process(cmd, &adaptor_output, &adaptor_input);\ncontext.submit_command_buffer(cmd);\n\nglMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT);\n```\n\n### Serializing wisdom to a string\n\n```c++\nGLContext context;\n\nFFTWisdom wisdom;\n// Use some static wisdom to make the learning step faster.\n// Avoids searching for options which are known to be bogus for a particular vendor.\nwisdom.set_static_wisdom(FFTWisdom::get_static_wisdom_from_renderer(reinterpret_cast<const char*>(glGetString(GL_RENDERER))));\n// Learn how to do 1024x256 much faster!\nwisdom.learn_optimal_options_exhaustive(&context, 1024, 256, ComplexToComplex, SSBO, SSBO, options.type);\n\n// Serialize to string.\nstring wisdom_json = wisdom.archive();\n\n// Unserialize wisdom.\nwisdom.extract(wisdom_json.c_str());\n```\n\n### Documentation\n\nProper documentation is still TODO. However, `test/glfft_test.cpp` and `test/glfft_cli.cpp` should give a good idea for how to use the API.\n\n## Verification and Benchmarking\n\nFor verification and benchmarking, GLFFT has a standalone executable which uses the [GLFW3](http://glfw.org) library to create a context.\n\n### Building\n\nBuilding requires GLFW3 to be installed, visible from pkg-config,\nalthough for Windows 64-bit a precompiled library with headers is included for convenience.\n\n    make\n    ./glfft_cli help\n\n#### Cross compilation for e.g. Windows from Linux\n\n    make PLATFORM=win TOOLCHAIN_PREFIX=x86_64-w64-mingw32-\n    wine64 ./glfft_cli help\n\n#### Dependencies\n\nOther than GLFW, muFFT and rapidjson are needed, which are included as git submodules.\n\n    git submodule init\n    git submodule update\n    # Or git clone --recursive when checking out GLFFT.\n\n### Tests\n\nTo verify GLFFT correctness, GLFFT has a large amount of tests which exhaustively tests the GLFFT API with almost any thinkable\ninput and output parameters.\n\n    ./glfft_cli test --test-all # Exhaustively tests everything (over 3000 tests currently), so will take some time.\n\nVerification is based on SNR compared to [muFFT](https://github.com/Themaister/muFFT) as a reference and maximum allowed delta from reference value.\nThe default precision requirements are fairly stringent, so particular GPUs might not support high enough precision.\nPrecision requirements can be overridden in such scenarios on the command line (see help).\n\n### Benchmarking\n\nTo evaluate GLFFT performance, GLFFT can be benchmarked with various parameters.\nThe benchmarking interface will run a full wisdom search first to find optimal parameters before running the actual benchmark.\n\n    ./glfft_cli bench --width 1024 --height 1024 --type ComplexToComplex --input-texture # Benchmark a 1024x1024 C2C FFT with texture as input and SSBO as output. See ./glfft_cli bench help for more.\n\n## FFT method\n\nGLFFT implements radix-4, radix-8, radix-16 (radix-4 two times in single pass) and radix-64 (radix-8 two times in single pass) FFT kernels.\nGLFFT will automatically find the optimal subdivision of a larger FFT problem based on either wisdom knowledge or estimations.\nRadix-16 and Radix-64 kernels are implemented by using shared memory to perform multiple passes without going to global memory between\nthe two passes.\n\nIn order to support a vast number of options, GLFFT will compile shaders on-demand during initialization\nand store them in a user-provided cache which can be shared between GLFFT instantiations.\n\nGLFFT is out-of-place using the Stockham auto-sort algorithm to avoid explicit reorder passes which is common for in-place algorithms.\n\nGLFFT's implementation is overall very similar to [muFFT](https://github.com/Themaister/muFFT),\nand more information about the algorithms can be found [here](https://github.com/Themaister/muFFT/blob/master/doxygen/fft.md).\n\n## License\n\nThe GLFFT library is licensed under the permissive MIT license, see COPYING and preambles in source files for more detail.\nThe verification and benchmarking library links against GLFW, whose license is [here](https://github.com/glfw/glfw/blob/master/COPYING.txt).\n\n"
 },
 {
  "repo": "videah/LovePotion",
  "language": "C",
  "readme_contents": "**NOTE: This project has since changed hands and as such this repo is now archived.**\n\n[You'll want to visit the LoveBrew repo instead!](https://github.com/lovebrew/LovePotion)\n\n<p align=\"center\">\n\t<img src=\"http://i.imgur.com/uJQNDys.png\"/>\n</p>\n\n<p align=\"center\">\n\t<img src=\"https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\"/>\n\t<img src=\"https://img.shields.io/github/stars/VideahGams/LovePotion.svg?style=flat-square\"/>\n\t<img src=\"https://img.shields.io/github/forks/VideahGams/LovePotion.svg?style=flat-square\"/>\n\t<img src=\"https://img.shields.io/github/issues/VideahGams/LovePotion.svg?style=flat-square\"/>\n\t<img src=\"https://img.shields.io/travis/VideahGams/LovePotion.svg?style=flat-square\"/>\n</p>\n\n**L\u00f6vePotion** is an unofficial work in progress implementation of the [L\u00d6VE](https://love2d.org/) API for 3DS Homebrew.\n\n<p align=\"center\">\n\t<img src=\"https://i.imgur.com/wsIfDuF.png\"/>\n*</p>\n\nGo [here](https://github.com/VideahGams/LovePotion/releases) for semi-stable releases. Join `#lovepotion` on OFTC for help/discussion/chat etc.\n\n# FAQ\n\n#### Can I run my L\u00d6VE game on this?\n\nRunning your game without any changes is very unlikely, so it's recommended to currently use L\u00f6vePotion to make games from the ground up.\n\n\n#### Can I help?\n\nYes, see the [CONTRIBUTING.md](https://www.github.com/VideahGams/LovePotion/tree/master/CONTRIBUTING.md) for more details.\n\n#### How do I build this?\n\nFollow [this](https://github.com/VideahGams/LovePotion/wiki/Building-L%C3%96VEPotion) guide, building should work on\nLinux, Windows and macOS if setup properly.\n\n#### How do I load my game?\nGames are loaded either from a `game` folder placed in the same directory as a built 3dsx, or\nloaded directly from RomFS if L\u00f6vePotion is built with a `game` folder in its directory.\n\n#### How do I run this?\n\nThere are a ton of ways to run L\u00f6vePotion.\n[Start here!](http://smealum.github.io/3ds/)\n\n**L\u00f6vePotion** is developed using [Citra](http://citra-emu.org/) and [MenuHax](http://smealum.github.io/3ds/).\n\nThere are also other ways, but I know next to nothing of these methods.\n\nTo run your game in Citra, place your game folder in these locations:\n\n| Platform | Path                            |\n|----------|---------------------------------|\n| Linux    | `~/.local/share/citra-emu/sdmc` |\n| Windows  | `[CITRA-FOLDER]/users/sdmc`     |\n\n#### Neat! I'm gonna send my game to Nintendo and get cartridge of my game and I'l-\n\nStop right there kiddo, this is for Homebrew only.\n\nThis is not for releasing a real 3DS title. I dunno, contact Nintendo if that's what you want.\n\n#### Can I use this to run Mari0?\nNo.\n\n# Showcase\n * [Picroxx!](https://gbatemp.net/threads/picroxx-the-ultimate-picross-clone.412055) by Substance12\n * [Space Shooter Game](http://novaember.com/s/8f9453/FIrGGQ.mp4) by Darkwater\n * [Ludum Dare 33 Entry](http://ludumdare.com/compo/ludum-dare-33/?action=preview&uid=31436) by me\n * [NumberFucker3DS](https://github.com/VideahGams/NumberFucker3DS) by unek & me\n * [SpaceFruit](https://gbatemp.net/threads/release-space-fruit.399088/) by TurtleP\n * [Loophole](https://gbatemp.net/threads/release-loophole-3ds-port.399585/) by Aaron Butterworth & CKlidify\n * [Hax0r](https://gbatemp.net/threads/preview-hax0r.401707) by TurtleP\n * [Idiot](https://gbatemp.net/threads/preview-idiot-a-puzzle-platformer.408774) by TurtleP\n * [ravimid](https://hoksy.itch.io/ravimid) by hoksy \"raisin bran\" jp\n * [Jetski 3DS](https://github.com/miltoncandelero/JETSKI-3DS/releases/tag/Potato-Version) by miltoncandelero\n \nSend me your projects so I can showcase them here!\n\n# Dependencies\nThe following dependencies are required to be installed properly in your dev environment for LovePotion to build.\n * [3ds_portlibs](https://github.com/cpp3ds/3ds_portlibs)\n * [citro3d](https://github.com/fincs/citro3d)\n * [sf2dlib](https://github.com/xerpi/sf2dlib)\n * [sftdlib](https://github.com/xerpi/sftdlib)\n * [sfillib](https://github.com/xerpi/sfillib)\n\n# Credits\n\n * Smealum and everyone who worked on [ctrulib](https://github.com/smealum/ctrulib) and ~~[Ninjhax](http://smealum.net/ninjhax)~~ all the Hax!\n * xerpi for [sf2dlib](https://github.com/xerpi/sf2dlib) and [sftdlib](https://github.com/xerpi/sftdlib) and [sfillib](https://githubcom/xerpi/sfillib)\n * Everyone who worked on [lua-compat-5.2](https://github.com/keplerproject/lua-compat-5.2)\n * Everyone who has worked on [Citra](http://citra-emu.org/)\n * Everyone who worked on [DevKitARM](http://devkitpro.org/)\n * rxi for [lovedos](https://github.com/rxi/lovedos) which has been good learning material (I also used his luaobj util)\n * Firew0lf, Reuh, Negi and everyone else who worked on [ctruLua](https://github.com/Firew0lf/ctruLua)\n * TurtleP for help/advice and being my best customer\n * Mik Embley for his contributions\n * All of the L\u00d6VE community, for being awesome\n * Anyone that I forgot\n \n\n# License\n\nThis code is licensed under the MIT Open Source License.\n\nCopyright (c) 2015-2016 Ruairidh Carmichael - ruairidhcarmichael@live.co.uk\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
 },
 {
  "repo": "Yu2erer/LuaJIT-5.3.6",
  "language": "C",
  "readme_contents": "# LuaJIT 5.3.6\n<p align=\"center\">\n<a href=\"https://github.com/Yu2erer/Lua-NOGC/actions?query=workflow%3ALua-5.3.6\"><img src=\"https://github.com/Yu2erer/Lua-NOGC/workflows/Lua5.3.6/badge.svg\"></a>\n</p>\n\nLuaJIT 5.3.6 \u539f\u540d `Lua-NOGC`\uff0c\u662f\u57fa\u4e8e `Lua 5.3.6` \u5b9e\u73b0\u7684\u4e00\u4e2a\u5783\u573e\u56de\u6536\u4f18\u5316\u7684\u6269\u5c55\u7248\u672c\u3002\n\n\u540e\u6765\u53c8\u57fa\u4e8e `Lua 5.3.6` \u5b9e\u73b0\u4e86 `Just-In-Time Compiler(JIT)`\uff0c\u6539\u540d\u4e3a `LuaJIT 5.3.6`\u3002\n\n## \u4f7f\u7528\u65b9\u6cd5\nLuaJIT 5.3.6 \u4e00\u5171\u63d0\u4f9b\u4e86\u4e09\u4e2a Lua \u51fd\u6570\uff0c\u5176\u4e2d\u5173\u4e8e\u5783\u573e\u56de\u6536\u4f18\u5316\u7684\u6709 `bggc` \u548c `nogc`\u3002\n\n#### BGGC \u793a\u4f8b\n\n> bggc([opt)\n\nbggc \u53ea\u6709\u4e00\u4e2a\u53c2\u6570 opt\uff0c\u901a\u8fc7\u53c2\u6570 opt \u63d0\u4f9b\u4e00\u7ec4\u4e0d\u540c\u7684\u529f\u80fd\u3002\n* \"open\": \u8868\u793a\u5f00\u542f\u540e\u53f0\u7ebf\u7a0b\u5783\u573e\u56de\u6536\uff0c\u9ed8\u8ba4\u5173\u95ed\n* \"close\": \u8868\u793a\u5173\u95ed\u540e\u53f0\u7ebf\u7a0b\u5783\u573e\u56de\u6536\n* \"isrunning\": \u8fd4\u56de\u503c\u4e3a0\u62161\uff0c\u8868\u793a\u540e\u53f0\u7ebf\u7a0b\u5783\u573e\u56de\u6536\u662f\u5426\u5f00\u542f\n\n#### NOGC \u793a\u4f8b\n\n> nogc([opt, [, arg])\n\n\u8fd9\u4e2a\u51fd\u6570\u662f NOGC \u7684\u901a\u7528\u63a5\u53e3\uff0c\u901a\u8fc7\u53c2\u6570 opt \u63d0\u4f9b\u4e86\u4e00\u7ec4\u4e0d\u540c\u7684\u529f\u80fd\uff0c\u7b2c\u4e8c\u53c2\u6570\u59cb\u7ec8\u53ea\u63a5\u53d7 table \u6216 nil\uff08\u5f53\u4e0d\u9700\u8981\u65f6\uff09\uff1a\n* \"open\": \u8868\u793a\u5bf9\u8fd9\u4e2a table \u8fdb\u884c\u9012\u5f52\u6807\u8bb0\uff0c\u4fdd\u8bc1\u5176\u53ca\u5176\u6210\u5458\u4e0d\u88ab\u626b\u63cf\u4e0e\u6e05\u7406\uff0c\u9700\u8981\u7b2c\u4e8c\u53c2\u6570(table)\n* \"close\": \u76f8\u5f53\u4e8e\u5173\u95ed nogc \u7684\u529f\u80fd\uff0c\u4f7f\u5176\u6062\u590d Lua \u5783\u573e\u56de\u6536\u7684\u63a5\u7ba1\uff0c\u9700\u8981\u7b2c\u4e8c\u53c2\u6570(table)\n* \"len\": \u8fd4\u56de\u5f53\u524d\u4e0d\u88ab Lua \u5783\u573e\u56de\u6536\u7ba1\u7406\u7684\u5bf9\u8c61\u4e2a\u6570\uff0c\u4e0d\u9700\u8981\u7b2c\u4e8c\u53c2\u6570\n* \"count\": \u8fd4\u56de\u5f53\u524d\u4e0d\u88ab Lua \u5783\u573e\u56de\u6536\u7ba1\u7406\u7684\u5bf9\u8c61\u7684\u603b\u5185\u5b58\u5927\u5c0f\uff0c\u5176\u5355\u4f4d\u4e3aK\uff0c\u4e0d\u9700\u8981\u7b2c\u4e8c\u53c2\u6570\n\n```lua\nconfigTable = {a = \"test\", b = true, c = 100} -- \u521b\u5efa\u4e00\u5f20\u914d\u7f6e\u8868\nnogc(\"open\", configTable) -- \u6807\u8bb0\u8be5 table \u4e0d\u88ab Lua \u5783\u573e\u56de\u6536\u7ba1\u7406\nprint(nogc(\"len\")) -- \u8fd4\u56de\u4e0d\u88ab Lua \u5783\u573e\u56de\u6536\u7ba1\u7406\u7684\u5bf9\u8c61\u4e2a\u6570\nprint(nogc(\"count\")) -- \u8fd4\u56de\u5f53\u524d\u4e0d\u88ab Lua \u5783\u573e\u56de\u6536\u7ba1\u7406\u7684\u5bf9\u8c61\u7684\u603b\u5185\u5b58\u5927\u5c0f\uff0c\u5176\u5355\u4f4d\u4e3aK\nnogc(\"close\", configTable) -- \u6062\u590d\u8be5 table \u7684\u6807\u8bb0\uff0c\u4f7f\u5176\u80fd\u591f\u88ab Lua \u5783\u573e\u56de\u6536\n```\n\n#### jit \u793a\u4f8b\n\n> jit([opt, [, arg])\n\n\u8be5\u51fd\u6570\u4e3a JIT \u7684\u901a\u7528\u63a5\u53e3\uff0c\u901a\u8fc7\u53c2\u6570 opt \u63d0\u4f9b\u4e86\u4e00\u7ec4\u4e0d\u540c\u7684\u529f\u80fd\uff0c\u7b2c\u4e8c\u53c2\u6570\u53ea\u63a5\u53d7 `Lua function` \u6216\u8005 `int`\u3002\n\n* \"open\": \u8868\u793a\u5f00\u542f\u81ea\u52a8 JIT \u7f16\u8bd1\uff0c\u9ed8\u8ba4\u5f00\u542f\n* \"close\": \u8868\u793a\u5173\u95ed\u81ea\u52a8 JIT \u7f16\u8bd1\n* \"isrunning\": \u8fd4\u56de\u81ea\u52a8 JIT \u7f16\u8bd1\u662f\u5426\u5f00\u542f\n* \"count\": \u8fd4\u56de\u5df2\u7ecf JIT \u7f16\u8bd1\u7684\u51fd\u6570\u4e2a\u6570\n* \"compile\": \u7b2c\u4e8c\u53c2\u6570\u4e3a Lua function, \u5c06\u5176\u7f16\u8bd1\n* \"iscompiled\": \u7b2c\u4e8c\u53c2\u6570\u4e3a Lua function, \u8fd4\u56de\u5176\u662f\u5426\u5df2\u7f16\u8bd1\n* \"setlimitsize\": \u7b2c\u4e8c\u53c2\u6570\u4e3a int, \u8868\u793a\u89e6\u53d1\u81ea\u52a8\u7f16\u8bd1\u7684\u51fd\u6570\u5927\u5c0f\u7684\u6700\u5c0f\u503c\n* \"setlimitcount\": \u7b2c\u4e8c\u53c2\u6570\u4e3a int, \u8868\u793a\u89e6\u53d1\u81ea\u52a8\u7f16\u8bd1\u51fd\u6570\u6267\u884c\u6b21\u6570\u7684\u6700\u5c0f\u503c\n\n\u53ea\u6709\u5f53 `\u51fd\u6570\u7684\u5927\u5c0f > limitsize` \u4e14 `\u51fd\u6570\u7684\u6267\u884c\u6b21\u6570 > limitcount` \u624d\u4f1a\u89e6\u53d1\u7f16\u8bd1\u3002\n\n## \u6ce8\u610f\u4e8b\u9879\n`BGGC` \u6ca1\u6709\u9700\u8981\u6ce8\u610f\u7684\uff0c\u8fd9\u610f\u5473\u7740\u4f60\u53ef\u4ee5\u53ea\u7528 `bggc` \u4ee5\u4e00\u4e2a\u6700\u5c0f\u7684\u6210\u672c\u83b7\u5f97\u4e00\u4e2a\u4e0d\u9519\u7684\u5783\u573e\u56de\u6536\u6548\u7387\u7684\u63d0\u5347\u3002\n\n`NOGC` \u5219\u9700\u8981\u6ce8\u610f\u4ee5\u4e0b\u51e0\u70b9\uff1a\ntable \u4e2d\u4ec5\u652f\u6301\u4ee5\u4e0b\u7c7b\u578b\n* table\uff08\u652f\u6301\u5d4c\u5957\uff09\n* metatable\n* Lua function\n* string\n* number\n* boolean\n\n\u4e0d\u652f\u6301\u4ee5\u4e0b\u7c7b\u578b\uff08\u4f1a\u8fdb\u884c\u68c0\u67e5\uff09\n* userdata\n* thread\n* C function\n\n\u5176\u4e2d table \u90fd\u4e0d\u652f\u6301 weak table\uff08\u4f1a\u8fdb\u884c\u68c0\u67e5\uff09\n\n\u540c\u65f6\u88ab NOGC \u63a5\u7ba1\u7684 table \u90fd\u4e0d\u652f\u6301\u589e\u52a0 \u5fc5\u987b\u662f\u53ea\u8bfb\uff01\uff08\u56e0\u4e3a\u65b0\u7684\u5bf9\u8c61\u63d2\u5165\u5230\u4e0d\u88ab\u626b\u63cf\u5230\u7684 table \u4e2d\uff0c\u4f1a\u5bfc\u81f4\u8be5\u5bf9\u8c61\u4e0d\u4f1a\u88ab\u6807\u8bb0\uff0c\u8fdb\u800c\u88ab\u5783\u573e\u56de\u6536\uff0c\u5bfc\u81f4\u6bb5\u9519\u8bef\uff09\n\n\u5982\u679c\u9700\u8981\u5f80 table \u4e2d\u589e\u52a0\u5bf9\u8c61\uff0c\u8bf7\u901a\u8fc7\u4ee5\u4e0b\u987a\u5e8f\u8fdb\u884c\u64cd\u4f5c\u3002\n\n```lua\nnogc(\"close\", table) -- \u5148\u5173\u95ed NOGC\ntable[#table] = ? -- \u63d2\u5165\u65b0\u7684\u503c\nnogc(\"open\", table) -- \u91cd\u65b0\u6253\u5f00 NOGC\n```\n\n`JIT` \u9700\u8981\u6ce8\u610f\u4ee5\u4e0b\u51e0\u70b9:\n* JIT \u51fd\u6570\u6267\u884c\u662f\u4ee5\u6a21\u62dfC\u51fd\u6570\u7684\u5f62\u5f0f\u6267\u884c\uff0c\u56e0\u6b64\u9650\u5236\u548cC\u51fd\u6570\u4e00\u81f4\uff0c\u8c03\u7528\u6b21\u6570\u4e0d\u80fd\u8d85\u8fc7 `LUAI_MAXCCALLS(200)\u6b21`\u3002\n* `OP_TAILCALL` \u9ed8\u8ba4\u5b9e\u73b0\u4e3a `OP_CALL`\uff0c\u56e0\u6b64\u8c03\u7528\u6b21\u6570\u4e5f\u4e0d\u80fd\u8d85\u8fc7 `LUAI_MAXCCALLS(200)`\u3002\n* \u5f53 Lua\u51fd\u6570 JIT\u7f16\u8bd1 \u540e\uff0c`debug` \u5e93\u7684\u76f8\u5173\u7684 hook\uff0c\u90fd\u4f1a\u5931\u6548\uff0c\u56e0\u4e3a Lua\u652f\u6301\u6309\u6307\u4ee4 hook\uff0c\u5bf9\u6027\u80fd\u6709\u4e00\u5b9a\u7684\u5f71\u54cd\u3002\n\n\n## \u5982\u4f55\u63a5\u5165\u9879\u76ee\uff1f\n\u8ddf\u5b98\u65b9\u7684 `Lua5.3` \u76f8\u540c\u7684\u4f7f\u7528\u65b9\u5f0f\u3002\n\n`BGGC` \u4e0d\u652f\u6301 `windows` \u4e3b\u8981\u662f `windows` \u4e0d\u652f\u6301 `pthread`\uff0c\u4f46\u662f\u4f9d\u7136\u53ef\u4ee5\u7f16\u8bd1\uff0c\u53ea\u662f\u7528\u4e0d\u4e86\u8fd9\u4e00\u4e2a\u529f\u80fd\u3002\n\n```sh\ngit clone https://github.com/Yu2erer/Lua-NOGC.git\nmake linux test # \u5176\u4e2d linux \u4e3a\u76f8\u5e94\u5e73\u53f0\uff0c\u4e0d\u652f\u6301 c89 \u7f16\u8bd1\u3002\n```\n\n## FAQ\n1. \u53ea\u6709 Lua 5.3.6 \u5417\uff1f\n> \u76ee\u524d\u4ec5\u63d0\u4f9b\u4e86 Lua 5.3.6\uff0c\u5982\u679c\u9700\u8981\u5176\u4ed6 Lua \u7248\u672c\uff0c\u53ef\u4ee5\u53c2\u8003 [patch](https://github.com/Yu2erer/Lua-NOGC/blob/master/Lua-5.3.6.patch)\u3002\n2. \u4f60\u4fee\u6539\u4e86 Lua \u4e2d\u7684\u4ec0\u4e48\u5185\u5bb9\uff1f\n> \u4e5f\u8bf7\u53c2\u8003 [patch](https://github.com/Yu2erer/Lua-NOGC/blob/master/Lua-5.3.6.patch)\u3002\n3. table \u652f\u6301\u5d4c\u5957\u5417\uff1f metatable\u5462\uff1fmetamethod\u5462\uff1f\n> \u7edf\u7edf\u652f\u6301\uff0c\u552f\u4e00\u4e0d\u652f\u6301\u7684\u5c31\u662f weak table\u3002\n4. \u4e3a\u4ec0\u4e48 NOGC \u540e\uff0c\u83b7\u53d6 nogc(\"len\") \u6216 nogc(\"count\") \u4e3a 0\uff1f\n> \u56e0\u4e3a nogc(\"open\", tb) \u53ea\u662f\u5bf9 table \u6253\u4e0a\u6807\u8bb0\uff0c\u771f\u6b63\u628a\u5b83\u4eceLua\u5783\u573e\u56de\u6536\u7684\u94fe\u8868\u4e2d\u53d6\u4e0b\u6765\u7684\u65f6\u95f4\u53d6\u51b3\u4e8e \u5783\u573e\u56de\u6536\u89e6\u53d1\u7684\u65f6\u673a\uff0c\u5982\u679c\u5e0c\u671b\u76f4\u63a5\u770b\u5230\u7ed3\u679c\uff0c\u53ef\u4ee5\u5728 open \u4e4b\u540e\u76f4\u63a5\u8c03\u7528 collectgarbage() \u8fdb\u884c\u4e00\u6b21\u5168\u91cf\u56de\u6536\uff0c\u89e6\u53d1\u5176\u4e0b\u94fe\u3002\n5. \u4e3a\u4ec0\u4e48\u6bcf\u6b21 nogc(\"len\") \u6216 nogc(\"count\") \u7684\u8fd4\u56de\u503c\u90fd\u4e0d\u540c\uff1f\n> \u53c2\u8003\u95ee\u98984\uff0c\u56e0\u4e3aLua\u7684\u5783\u573e\u56de\u6536\u662f\u6e10\u8fdb\u7684\uff0c\u5728\u90a3\u4e2a\u77ac\u95f4\u53ea\u4e0b\u94fe\u4e86\u90a3\u4e48\u591a\u4e2a\u5bf9\u8c61\u3002\n6. \u4e3a\u4ec0\u4e48 NOGC \u6807\u8bb0 table \u540e\u4f1a\u53d1\u751f\u6bb5\u9519\u8bef\uff1f\n> \u8bf7\u786e\u8ba4\u4f60\u7684 table \u662f\u4e0d\u662f\u63d2\u5165\u4e86\u65b0\u7684\u5bf9\u8c61\uff0c\u8be5 table \u5fc5\u987b\u662f\u53ea\u8bfb\u7684\uff01\n7. \u5982\u679c\u6211\u8981\u5f80 NOGC \u6807\u8bb0\u8fc7\u7684 table \u63d2\u5165\u65b0\u5bf9\u8c61\u600e\u4e48\u529e\uff1f\n> \u53c2\u8003 \u6ce8\u610f\u4e8b\u9879 \u4e2d\u7684\u4ee3\u7801\u793a\u4f8b\u3002\n8. \u4f60\u8bf4\u7684\u8fd9\u4e48\u597d\uff0c\u5783\u573e\u56de\u6536\u63d0\u901f\u591a\u5c11\u5462\uff1f\n> \u7ecf\u8fc7\u6d4b\u8bd5\uff0c`bggc` \u5f00\u542f\u4e86\u540e\u53f0\u7ebf\u7a0b\u4f18\u5316\u540e\uff0c\u5927\u7ea6\u80fd\u63d0\u5347 `50%`\u3002`nogc` \u5219\u53d6\u51b3\u4e8e\u4f60\u4e0d\u9700\u8981\u53c2\u4e0e\u5783\u573e\u56de\u6536\u7684\u5bf9\u8c61\u6570\u91cf\u3002\n9. JIT \u540e\uff0c\u80fd\u63d0\u901f\u591a\u5c11\u5462\uff1f\n> \u7ecf\u8fc7\u6d4b\u8bd5, `JIT` \u5f00\u542f\u540e, \u6700\u5927\u80fd\u6709 7\u500d \u7684\u63d0\u5347\u3002\n\n## Thanks\nLuaJIT 5.3.6 backend [mir](https://github.com/vnmakarov/mir)\n\n## LICENSE\nLuaJIT 5.3.6 is released under the MIT license. See LICENSE for details."
 },
 {
  "repo": "maelswarm/nymph",
  "language": "C",
  "readme_contents": "<p align=\"center\"><img src=\"https://cdn.pixabay.com/photo/2013/07/12/12/13/fairy-145352_960_720.png\" width=\"auto\" height=\"200\" /></p>\n\n<h1 align=center>Nymph</h1>\n\n<p align=\"center\">Let's see what we can achieve by reworking C syntax.</p>\n\n## Overview\n\nNymph is a simple C like programming language.\n\nNymph acts as a preprocessor, converting Nymph files (extension \\*.n) into C files.\n\nThis project is very much in development... It is not production ready.\n\n## What's New\n\nThis project just recieved a complete overhaul.\n\n## Goals\n\n### Completed\n\n* Class-Based OOP\n\n* Subtyping\n\n### In Progress\n\n* TBD\n\n### Pending\n\n* Destructors?\n\n* Type Inference?\n\n* Reflection?\n\n* Default function arguments?\n\n* Lambdas?\n\n## Example\n\nmammal.n\n```\n#include <stdio.h>\n#include <stdlib.h>\n\nclass Mammal {\n\n    + int population = 0;             // Class Variable (+)\n    - int height = 0, weight = 100;   // Object Variable (-)\n\n    + Mammal *init(int height, int weight) {  // Class Method (+) Constructor\n        this->height = height;\n        this->weight = weight;\n        Mammal->population++;\n        return this;\n    }\n\n    - void print() {                          // Object Method (-)\n        printf(\"print instance properties...\\n\");\n    }\n}\n```\n\nhuman.n\n```\n#include \"mammal.n\"\n#include <stdio.h>\n#include <stdlib.h>\n\nclass Human : Mammal {\n\n    - char *name = NULL; // Object Variable (-)\n\n    + Human *init(char *name, int height, int weight) { // Class Method (+) Constructor\n        this = super->init(height, weight);\n        this->name = name;\n        return this;\n    }\n\n    - void died() {                                     // Object Method (-) Constructor\n        free(this->name);\n        free(this);\n        Mammal->population--;\n    }\n}\n\nint main(void) {\n\n    char *name = malloc(5);\n    memset(name, 0, sizeof(name));\n    strcpy(name, \"Fred\");\n    Human *person1 = Human->init(name, 76, 146); // Class Method Constructor Call\n    person1->print();                            // Object Method Call\n    person1->died();                             // Object Method Call\n\n    return 0;\n}\n```\n\n```\nnymph -r human.n\n```"
 },
 {
  "repo": "groupgets/GetThermal",
  "language": "C",
  "readme_contents": "# GetThermal\n\n[![Build Status](https://travis-ci.org/groupgets/GetThermal.svg?branch=master)](https://travis-ci.org/groupgets/GetThermal)\n\nA cross-platform thermal camera viewer application.\n\n![GetThermal screenshot](https://groupgets-files.s3.amazonaws.com/lepton/getthermal_app.png)\n\n# Supported Platforms\n\nBinaries are available for desktop Linux (x64) and Mac OS platforms; see the Releases tab on GitHub. The Wiki tab has \ninstructions for building on Raspbian. Success has also been reported on NVIDIA Linux for Tegra. Ultimately, any\nplatform that runs QT 5.7+ and libusb should function with some effort.\n\n# Supported Cameras\n\nGetThermal supports all FLIR Lepton variants used with the USB\n[PureThermal 1](https://groupgets.com/manufacturers/getlab/products/purethermal-1-flir-lepton-smart-i-o-module) or\n[PureThermal 2](https://groupgets.com/manufacturers/getlab/products/purethermal-2-flir-lepton-smart-i-o-module) Smart I/O Modules,\nincluding the Radiometric Lepton 2.5 and 3.5\n\nIt also supports basic thermal data acquisition from [FLIR Boson](https://groupgets.com/manufacturers/flir/products/boson) 320 and 640.\n\n# Building\n\nPrerequisites: You will need `libusb-1.0` and [CMake](http://www.cmake.org/) installed. You will\nalso need [QT 5.7](https://www.qt.io/download-open-source/) or newer. On OS X, these packages should\nbe available via Homebrew.\n\n`GetThermal` uses a modified version of `libuvc` for camera image download and control. Even if\nyou have `libuvc` on your system already, you will have to build the fork.\n\n## Get the source code\n\n    git clone https://github.com/groupgets/GetThermal\n    cd GetThermal\n\n## Get libuvc and build\n\nTo build the `libuvc` fork, you can run these shell commands:\n\n    git submodule init\n    git submodule update\n    pushd libuvc\n    mkdir build\n    cd build\n    cmake ..\n    make\n    popd\n\n## Build GetThermal\n\nGetThermal is a QT qmake project. So all you should have to do now is to fire up QTCreator, open the project file,\nand hit run. \n\n### OSx Qt-Creator build prerequisites\nIf Qt Creator fails to run the application due to errors with linking the proper ImageIO.framework or libjpeg version, go to Projects -> Run -> Run Environment and edit the DYLD_FRAMEWORK_PATH and DYLD_LIBRARY_PATH by adding\n\n        /System/Library/Frameworks/ImageIO.framework/Resources:\n\nto the beginning of the line.\n\n### Via Command Line\nYou can also build from the command line:\n\n    qmake # or depending on your installation, maybe ~/Qt/5.7/clang_64/bin/qmake \n    make\n\n# Releases\n\nThis is a work in progress. See the Releases tab in github for OS X and Linux pre-release builds.\n"
 },
 {
  "repo": "phillbush/pmenu",
  "language": "C",
  "readme_contents": "# \u03c0menu\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/63266536/159123820-7f1a5c97-87bd-4be2-a4a5-870cb695a3ba.png\", title=\"demo\"/>\n</p>\n\n**WARNING:**\nThe options `-r`, `-m` and `-p` are deprecated and were replaced by `-x`\nand `-X`.  The option `-P` (used to provide path) is now replaced by the\nenvironment variable `$ICONPATH`, an environment variable which supports\nseveral paths delimited by colons. Read the manual for more information.\nUpdate your scripts after updating \u03c0menu.\n\n\u03c0menu is a pie menu utility for X.\n\u03c0menu receives a menu specification in stdin, shows a menu for the user\nto select one of the options, and outputs the option selected to stdout.\n\n\u03c0menu comes with the following features:\n\n* \u03c0menu reads something in and prints something out, the UNIX way.\n* Submenus (some pie-menu slices can spawn another menu).\n* Icons (pie-menu slices can contain icon image).\n* X resources support (you don't need to recompile \u03c0menu for configuring it).\n\n## Files\n\nThe files are:\n* `./README.md`:  This file.\n* `./Makefile`:   The makefile.\n* `./config.h`:   The hardcoded default configuration for \u03c0menu.\n* `./pmenu.1`:    The manual file (man page) for \u03c0menu.\n* `./pmenu.c`:    The source code of \u03c0menu.\n* `./pmenu.sh`:   A sample script illustrating how to use \u03c0menu.\n\n\n## Installation\n\nFirst, edit `./Makefile` or set the proper environment variables to\nmatch your local setup.\n\nIn order to build \u03c0menu you need the Imlib2, Xlib and Xft header files\n(and some X extensions: XShape and Xinerama).\nThe default configuration for \u03c0menu is specified in the file `config.h`,\nyou can edit it, but most configuration can be changed at runtime via\nX resources.  Enter the following command to build \u03c0menu.  This command\ncreates the binary file ./pmenu.\n\n\tmake\n\nBy default, \u03c0menu is installed into the `/usr/local` prefix.  Enter the\nfollowing command to install \u03c0menu (if necessary as root).  This command\ninstalls the binary file `./pmenu` into the `${PREFIX}/bin/` directory,\nand the manual file `./pmenu.1` into `${MANPREFIX}/man1/` directory.\n\n\tmake install\n\n\n## Running \u03c0menu\n\n\u03c0menu receives as input a menu specification where each line is a menu\nentry.  Each line can be indented with tabs to represent nested menus.\nEach line is made out of a label and a command separated by any number\nof tabs.\n\nSee the script ./pmenu.sh for an example of how to use \u03c0menu to draw a\nsimple pie menu.\n\nRead the manual for more information on running \u03c0menu.\n"
 },
 {
  "repo": "h2non/semver.c",
  "language": "C",
  "readme_contents": "# semver.c [![Build Status](https://travis-ci.org/h2non/semver.c.png)](https://travis-ci.org/h2non/semver.c) [![GitHub release](https://img.shields.io/github/tag/h2non/semver.c.svg)](https://github.com/h2non/semver.c/releases)\n\n[Semantic version](http://semver.org) v2.0 parser and render written in [ANSI C](https://en.wikipedia.org/wiki/ANSI_C) with zero dependencies.\n\n## Features\n\n- [x] Standard compliant (otherwise, open an issue)\n- [x] Version metadata parsing\n- [x] Version prerelease parsing\n- [x] Version comparison helpers\n- [x] Supports comparison operators\n- [x] Version render\n- [x] Version bump\n- [x] Version sanitizer\n- [x] 100% test coverage\n- [x] No regexp (ANSI C doesn't support it)\n- [x] Numeric conversion for sorting/filtering\n\n## Versions\n\n- [v0](https://github.com/h2non/semver.c/tree/89e66f36544e0250def32640b84b7e15c8585da4) - Legacy version. Beta. Not maintained anymore.\n- [v1](https://github.com/h2non/semver.c) - Current stable version.\n\n## Usage\n\nBasic comparison:\n```c\n#include <stdio.h>\n#include <semver.h>\n\nchar current[] = \"1.5.10\";\nchar compare[] = \"2.3.0\";\n\nint\nmain(int argc, char *argv[]) {\n    semver_t current_version = {};\n    semver_t compare_version = {};\n\n    if (semver_parse(current, &current_version)\n      || semver_parse(compare, &compare_version)) {\n      fprintf(stderr,\"Invalid semver string\\n\");\n      return -1;\n    }\n\n    int resolution = semver_compare(compare_version, current_version);\n\n    if (resolution == 0) {\n      printf(\"Versions %s is equal to: %s\\n\", compare, current);\n    }\n    else if (resolution == -1) {\n      printf(\"Version %s is lower than: %s\\n\", compare, current);\n    }\n    else {\n      printf(\"Version %s is higher than: %s\\n\", compare, current);\n    }\n\n    // Free allocated memory when we're done\n    semver_free(&current_version);\n    semver_free(&compare_version);\n    return 0;\n}\n```\n\nSatisfies version:\n\n```c\n#include <stdio.h>\n#include <semver.h>\n\nsemver_t current = {};\nsemver_t compare = {};\n\nint\nmain(int argc, char *argv[]) {\n    semver_parse(\"1.3.10\", &current);\n    semver_parse(\"1.5.2\", &compare);\n\n    // Use caret operator for the comparison\n    char operator[] = \"^\";\n\n    if (semver_satisfies(current, compare, operator)) {\n      printf(\"Version %s can be satisfied by %s\", \"1.3.10\", \"1.5.2\");\n    }\n\n    // Free allocated memory when we're done\n    semver_free(&current);\n    semver_free(&compare);\n    return 0;\n}\n```\n\n## Installation\n\nClone this repository:\n\n```bash\n$ git clone https://github.com/h2non/semver.c\n```\n\nOr install with [clib](https://github.com/clibs/clib):\n\n```bash\n$ clib install h2non/semver.c\n```\n\n## API\n\n#### struct semver_t { int\u00a0major, int minor, int patch, char * prerelease, char * metadata }\n\nsemver base struct.\n\n#### semver_parse(const char *str, semver_t *ver) => int\n\nParses a string as semver expression.\n\n**Returns**:\n\n- `-1` - In case of invalid semver or parsing error.\n- `0` - All was fine!\n\n#### semver_compare(semver_t a, semver_t b) => int\n\nCompare versions `a` with `b`.\n\nReturns:\n- `-1` in case of lower version.\n- `0` in case of equal versions.\n- `1` in case of higher version.\n\n#### semver_satisfies(semver_t a, semver_t b, char *operator) => int\n\nChecks if both versions can be satisfied\nbased on the given comparison operator.\n\n**Allowed operators**:\n\n- `=`  - Equality\n- `>=` - Higher or equal to\n- `<=` - Lower or equal to\n- `<`  - Lower than\n- `>`  - Higher than\n- `^`  - Caret operator comparison ([more info](https://docs.npmjs.com/misc/semver#caret-ranges-1-2-3-0-2-5-0-0-4))\n- `~`  - Tilde operator comparison ([more info](https://docs.npmjs.com/misc/semver#tilde-ranges-1-2-3-1-2-1))\n\n**Returns**:\n\n- `1` - Can be satisfied\n- `0` - Cannot be satisfied\n\n#### semver_satisfies_caret(semver_t a, semver_t b) => int\n\nChecks if version `x` can be satisfied by `y`\nperforming a comparison with caret operator.\n\nSee: https://docs.npmjs.com/misc/semver#caret-ranges-1-2-3-0-2-5-0-0-4\n\n**Returns**:\n\n- `1` - Can be satisfied\n- `0` - Cannot be satisfied\n\n#### semver_satisfies_patch(semver_t a, semver_t b) => int\n\nChecks if version `x` can be satisfied by `y`\nperforming a comparison with tilde operator.\n\nSee: https://docs.npmjs.com/misc/semver#tilde-ranges-1-2-3-1-2-1\n\n**Returns**:\n\n- `1` - Can be satisfied\n- `0` - Cannot be satisfied\n\n\n#### semver_eq(semver_t a, semver_t b) => int\n\nEquality comparison.\n\n#### semver_ne(semver_t a, semver_t b) => int\n\nNon equal comparison.\n\n#### semver_gt(semver_t a, semver_t b) => int\n\nGreater than comparison.\n\n#### semver_lt(semver_t a, semver_t b) => int\n\nLower than comparison.\n\n#### semver_gte(semver_t a, semver_t b) => int\n\nGreater than or equal comparison.\n\n#### semver_lte(semver_t a, semver_t b) => int\n\nLower than or equal comparison.\n\n#### semver_render(semver_t *v, char *dest) => void\n\nRender as string.\n\n#### semver_numeric(semver_t *v) => int\n\nRender as numeric value. Useful for ordering and filtering.\n\n#### semver_bump(semver_t *a) => void\n\nBump major version.\n\n#### semver_bump_minor(semver_t *a) => void\n\nBump minor version.\n\n#### semver_bump_patch(semver_t *a) => void\n\nBump patch version.\n\n#### semver_free(semver_t *a) => void\n\nHelper to free allocated memory from heap.\n\n#### semver_is_valid(char *str) => int\n\nChecks if the given string is a valid semver expression.\n\n#### semver_clean(char *str) => int\n\nRemoves invalid semver characters in a given string.\n\n## License\n\nMIT - Tomas Aparicio\n"
 },
 {
  "repo": "vrischmann/zig-sqlite",
  "language": "C",
  "readme_contents": "# zig-sqlite\n\nThis package is a thin wrapper around [sqlite](https://sqlite.org/index.html)'s C API.\n\n# Status\n\nWhile the core functionality works right now, the API is still subject to changes.\n\nIf you use this library, expect to have to make changes when you update the code.\n\n# Zig release support\n\n`zig-sqlite` only supports Zig master (as can be found [here](https://ziglang.org/download/)). The plan is to support releases once Zig 1.0 is released but this can still change.\n\nThe Zig self-hosted/stage2 compiler is now [the default](https://github.com/ziglang/zig/pull/12368) however currently it can't be used with `zig-sqlite` due to bugs.\n\nEventually `zig-sqlite` will only support stage2 but until a point I feel comfortable doing that, the `master` branch will stay compatible with stage1 and all work for stage2 will happen in the `stage2` branch.\n\n# Table of contents\n\n* [Status](#status)\n* [Requirements](#requirements)\n* [Features](#features)\n* [Installation](#installation)\n   * [zigmod](#zigmod)\n   * [Git submodule](#git-submodule)\n   * [Using the system sqlite library](#using-the-system-sqlite-library)\n   * [Using the bundled sqlite source code file](#using-the-bundled-sqlite-source-code-file)\n* [Usage](#usage)\n   * [Initialization](#initialization)\n   * [Preparing a statement](#preparing-a-statement)\n      * [Common use](#common-use)\n      * [Diagnostics](#diagnostics)\n   * [Executing a statement](#executing-a-statement)\n   * [Reuse a statement](#reuse-a-statement)\n   * [Reading data](#reading-data)\n      * [Type parameter](#type-parameter)\n      * [Non allocating](#non-allocating)\n      * [Allocating](#allocating)\n   * [Iterating](#iterating)\n      * [Non allocating](#non-allocating-1)\n      * [Allocating](#allocating-1)\n   * [Bind parameters and resultset rows](#bind-parameters-and-resultset-rows)\n   * [Custom type binding and reading](#custom-type-binding-and-reading)\n   * [Note about complex allocations](#note-about-complex-allocations)\n* [Comptime checks](#comptime-checks)\n   * [Check the number of bind parameters.](#check-the-number-of-bind-parameters)\n   * [Assign types to bind markers and check them.](#assign-types-to-bind-markers-and-check-them)\n* [User defined SQL functions](#user-defined-sql-functions)\n   * [Scalar functions](#scalar-functions)\n   * [Aggregate functions](#aggregate-functions)\n\n# Requirements\n\n[Zig master](https://ziglang.org/download/) is the only required dependency.\n\nFor sqlite, you have options depending on your target:\n* On Windows the only supported way at the moment to build `zig-sqlite` is with the bundled sqlite source code file.\n* On Linux we have two options:\n  * use the system and development package for sqlite (`libsqlite3-dev` for Debian and derivatives, `sqlite3-devel` for Fedora)\n  * use the bundled sqlite source code file.\n\n# Features\n\n* Preparing, executing statements\n* comptime checked bind parameters\n* user defined SQL functions\n\n# Installation\n\nThere are two primary ways to include `zig-sqlite` in your project:\n* using the [zigmod](https://github.com/nektro/zigmod) package manager\n* using a git submodule\n\n## zigmod\n\nAdd this to your `zig.mod` file:\n```\ndependencies:\n  - src: git https://github.com/vrischmann/zig-sqlite branch-master\n```\n\nNote that if you're building an executable and not a library you should use `dev_dependencies` instead.\n\nNext run `zigmod fetch`; it should create a `deps.zig` file.\n\nNow in your `build.zig` you can access the package like this:\n```zig\nconst deps = @import(\"deps.zig\");\n...\ndeps.addAllTo(exe);\n```\n\nThis is the easiest way to add `zig-sqlite` because it uses the bundled source code, avoiding all sorts of linking problems.\n\n## Git submodule\n\nIf you don't want to use a package manager you can simply add this repository as a git submodule.\n\nThen you need to chose if you want to use the system sqlite library or the bundled source code.\n\n## Using the system sqlite library\n\nIf you want to use the system sqlite library, add the following to your `build.zig` target(s):\n\n```zig\nexe.linkLibC();\nexe.linkSystemLibrary(\"sqlite3\");\nexe.addPackage(.{ .name = \"sqlite\", .path = \"third_party/zig-sqlite/sqlite.zig\" });\n```\n\n## Using the bundled sqlite source code file\n\nIf you want to use the bundled sqlite source code file, first you need to add it as a static library in your `build.zig` file:\n\n```zig\nconst sqlite = b.addStaticLibrary(\"sqlite\", null);\nsqlite.addCSourceFile(\"third_party/zig-sqlite/c/sqlite3.c\", &[_][]const u8{\"-std=c99\"});\nsqlite.linkLibC();\n```\n\nIf you need to define custom [compile-time options](https://www.sqlite.org/compile.html#overview) for sqlite, modify the flags (second argument to `addCSourceFile`).\n\nNow it's just a matter of linking your `build.zig` target(s) to this library instead of the system one:\n\n```zig\nexe.linkLibrary(sqlite);\nexe.addPackagePath(\"sqlite\", \"third_party/zig-sqlite/sqlite.zig\");\nexe.addIncludeDir(\"third_party/zig-sqlite/c\");\n```\n\nIf you're building with glibc you must make sure that the version used is at least 2.28.\n\nYou can do that in your `build.zig` file:\n```zig\nvar target = b.standardTargetOptions(.{});\ntarget.setGnuLibCVersion(2, 28, 0);\nexe.setTarget(target);\n```\n\nOr with `-Dtarget`:\n```\n$ zig build -Dtarget=native-linux-gnu.2.28\n```\n\n# Usage\n\nImport `zig-sqlite` like this:\n\n```zig\nconst sqlite = @import(\"sqlite\");\n```\n\n## Initialization\n\nYou must create and initialize an instance of `sqlite.Db`:\n\n```zig\nvar db = try sqlite.Db.init(.{\n    .mode = sqlite.Db.Mode{ .File = \"/home/vincent/mydata.db\" },\n    .open_flags = .{\n        .write = true,\n        .create = true,\n    },\n    .threading_mode = .MultiThread,\n});\n```\n\nThe `init` method takes a `InitOptions` struct which will be used to configure sqlite.\n\nOnly the `mode` field is mandatory, the other fields have sane default values.\n\n## Preparing a statement\n\n### Common use\n\nsqlite works exclusively by using prepared statements. The wrapper type is `sqlite.Statement`. Here is how you get one:\n\n```zig\nconst query =\n    \\\\SELECT id, name, age, salary FROM employees WHERE age > ? AND age < ?\n;\n\nvar stmt = try db.prepare(query);\ndefer stmt.deinit();\n```\n\nThe `Db.prepare` method takes a `comptime` query string.\n\n### Diagnostics\n\nIf you want failure diagnostics you can use `prepareWithDiags` like this:\n\n```zig\nvar diags = sqlite.Diagnostics{};\nvar stmt = db.prepareWithDiags(query, .{ .diags = &diags }) catch |err| {\n    std.log.err(\"unable to prepare statement, got error {}. diagnostics: {s}\", .{ err, diags });\n    return err;\n};\ndefer stmt.deinit();\n```\n\n## Executing a statement\n\nFor queries which do not return data (`INSERT`, `UPDATE`) you can use the `exec` method:\n\n```zig\nconst query =\n    \\\\UPDATE foo SET salary = ? WHERE id = ?\n;\n\nvar stmt = try db.prepare(query);\ndefer stmt.deinit();\n\ntry stmt.exec(.{\n    .salary = 20000,\n    .id = 40,\n});\n```\n\nSee the section \"Bind parameters and resultset rows\" for more information on the types mapping rules.\n\n## Reuse a statement\n\nYou can reuse a statement by resetting it like this:\n```zig\nconst query =\n    \\\\UPDATE foo SET salary = ? WHERE id = ?\n;\n\nvar stmt = try db.prepare(query);\ndefer stmt.deinit();\n\nvar id: usize = 0;\nwhile (id < 20) : (id += 1) {\n    stmt.reset();\n    try stmt.exec(.{\n        .salary = 2000,\n        .id = id,\n    });\n}\n```\n\n## Reading data\n\nFor queries which return data you have multiple options:\n* `Statement.all` which takes an allocator and can allocate memory.\n* `Statement.one` which does not take an allocator and cannot allocate memory (aside from what sqlite allocates itself).\n* `Statement.oneAlloc` which takes an allocator and can allocate memory.\n\n### Type parameter\n\nAll these methods take a type as first parameter.\n\nThe type represents a \"row\", it can be:\n* a struct where each field maps to the corresponding column in the resultset (so field 0 must map to column 1 and so on).\n* a single type, in that case the resultset must only return one column.\n\nThe type can be a pointer but only when using the methods taking an allocator.\n\nNot all types are allowed, see the section \"Bind parameters and resultset rows\" for more information on the types mapping rules.\n\n### Non allocating\n\nUsing `one`:\n\n```zig\nconst query =\n    \\\\SELECT name, age FROM employees WHERE id = ?\n;\n\nvar stmt = try db.prepare(query);\ndefer stmt.deinit();\n\nconst row = try stmt.one(\n    struct {\n        name: [128:0]u8,\n        age: usize,\n    },\n    .{},\n    .{ .id = 20 },\n);\nif (row) |row| {\n    std.log.debug(\"name: {}, age: {}\", .{std.mem.spanZ(&row.name), row.age});\n}\n```\nNotice that to read text we need to use a 0-terminated array; if the `name` column is bigger than 127 bytes the call to `one` will fail.\n\nIf the length of the data is variable then the sentinel is mandatory: without one there would be no way to know where the data ends in the array.\n\nHowever if the length is fixed, you can read into a non 0-terminated array, for example:\n\n```zig\nconst query =\n    \\\\SELECT id FROM employees WHERE name = ?\n;\n\nvar stmt = try db.prepare(query);\ndefer stmt.deinit();\n\nconst row = try stmt.one(\n    [16]u8,\n    .{},\n    .{ .name = \"Vincent\" },\n);\nif (row) |id| {\n    std.log.debug(\"id: {s}\", .{std.fmt.fmtSliceHexLower(&id)});\n}\n```\n\nIf the column data doesn't have the correct length a `error.ArraySizeMismatch` will be returned.\n\nThe convenience function `sqlite.Db.one` works exactly the same way:\n\n```zig\nconst query =\n    \\\\SELECT age FROM employees WHERE id = ?\n;\n\nconst row = try db.one(usize, query, .{}, .{ .id = 20 });\nif (row) |age| {\n    std.log.debug(\"age: {}\", .{age});\n}\n```\n\n### Allocating\n\nUsing `all`:\n\n```zig\nconst query =\n    \\\\SELECT name FROM employees WHERE age > ? AND age < ?\n;\n\nvar stmt = try db.prepare(query);\ndefer stmt.deinit();\n\nconst names = try stmt.all([]const u8, allocator, .{}, .{\n    .age1 = 20,\n    .age2 = 40,\n});\nfor (names) |name| {\n    std.log.debug(\"name: {s}\", .{ name });\n}\n```\n\nUsing `oneAlloc`:\n\n```zig\nconst query =\n    \\\\SELECT name FROM employees WHERE id = ?\n;\n\nvar stmt = try db.prepare(query);\ndefer stmt.deinit();\n\nconst row = try stmt.oneAlloc([]const u8, allocator, .{}, .{\n    .id = 200,\n});\nif (row) |name| {\n    std.log.debug(\"name: {}\", .{name});\n}\n```\n\n## Iterating\n\nAnother way to get the data returned by a query is to use the `sqlite.Iterator` type.\n\nYou can only get one by calling the `iterator` method on a statement.\n\nThe `iterator` method takes a type which is the same as with `all`, `one` or `oneAlloc`: every row retrieved by calling `next` or `nextAlloc` will have this type.\n\nIterating is done by calling the `next` or `nextAlloc` method on an iterator. Just like before, `next` cannot allocate memory while `nextAlloc` can allocate memory.\n\n`next` or `nextAlloc` will either return an optional value or an error; you should keep iterating until `null` is returned.\n\n### Non allocating\n\n```zig\nvar stmt = try db.prepare(\"SELECT age FROM user WHERE age < ?\");\ndefer stmt.deinit();\n\nvar iter = try stmt.iterator(usize, .{\n    .age = 20,\n});\n\nwhile (try iter.next(.{})) |age| {\n    std.debug.print(\"age: {}\\n\", .{age});\n}\n```\n\n### Allocating\n\n```zig\nvar stmt = try db.prepare(\"SELECT name FROM user WHERE age < ?\");\ndefer stmt.deinit();\n\nvar iter = try stmt.iterator([]const u8, .{\n    .age = 20,\n});\n\nwhile (true) {\n    var arena = std.heap.ArenaAllocator.init(allocator);\n    defer arena.deinit();\n\n    const name = (try iter.nextAlloc(arena.allocator(), .{})) orelse break;\n    std.debug.print(\"name: {}\\n\", .{name});\n}\n```\n\n## Bind parameters and resultset rows\n\nSince sqlite doesn't have many [types](https://www.sqlite.org/datatype3.html) only a small number of Zig types are allowed in binding parameters and in resultset mapping types.\n\nHere are the rules for bind parameters:\n* any Zig `Int` or `ComptimeInt` is treated as a `INTEGER`.\n* any Zig `Float` or `ComptimeFloat` is treated as a `REAL`.\n* `[]const u8`, `[]u8` is treated as a `TEXT`.\n* the custom `sqlite.Blob` type is treated as a `BLOB`.\n* the custom `sqlite.Text` type is treated as a `TEXT`.\n* the `null` value is treated as a `NULL`.\n* non-null optionals are treated like a regular value, null optionals are treated as a `NULL`.\n\nHere are the rules for resultset rows:\n* `INTEGER` can be read into any Zig `Int` provided the data fits.\n* `REAL` can be read into any Zig `Float` provided the data fits.\n* `TEXT` can be read into a `[]const u8` or `[]u8`.\n* `TEXT` can be read into any array of `u8` with a sentinel provided the data fits.\n* `BLOB` follows the same rules as `TEXT`.\n* `NULL` can be read into any optional.\n\nNote that arrays must have a sentinel because we need a way to communicate where the data actually stops in the array, so for example use `[200:0]u8` for a `TEXT` field.\n\n## Custom type binding and reading\n\nSometimes the default field binding or reading logic is not what you want, for example if you want to store an enum using its tag name instead of its integer value or\nif you want to store a byte slice as an hex string.\n\nTo accomplish this you must first define a wrapper struct for your type. For example if your type is a `[4]u8` and you want to treat it as an integer:\n```zig\npub const MyArray = struct {\n    data: [4]u8,\n\n    pub const BaseType = u32;\n\n    pub fn bindField(self: MyArray, _: std.mem.Allocator) !BaseType {\n        return std.mem.readIntNative(BaseType, &self.data);\n    }\n\n    pub fn readField(_: std.mem.Allocator, value: BaseType) !MyArray {\n        var arr: MyArray = undefined;\n        std.mem.writeIntNative(BaseType, &arr.data, value);\n        return arr;\n    }\n};\n```\n\nNow when you bind a value of type `MyArray` the value returned by `bindField` will be used for binding instead.\n\nSame for reading, when you select _into_ a `MyArray` row or field the value returned by `readField` will be used instead.\n\n_NOTE_: when you _do_ allocate in `bindField` or `readField` make sure to pass a `std.heap.ArenaAllocator`-based allocator.\n\nThe binding or reading code does not keep tracking of allocations made in custom types so it can't free the allocated data itself; it's therefore required\nto use an arena to prevent memory leaks.\n\n## Note about complex allocations\n\nDepending on your queries and types there can be a lot of allocations required. Take the following example:\n```zig\nconst User = struct {\n    id: usize,\n    first_name: []const u8,\n    last_name: []const u8,\n    data: []const u8,\n};\n\nfn fetchUsers(allocator: std.mem.Allocator, db: *sqlite.Db) ![]User {\n    var stmt = try db.prepare(\"SELECT id FROM user WHERE id > $id\");\n    defer stmt.deinit();\n\n    return stmt.all(User, allocator, .{}, .{ .id = 20 });\n}\n```\n\nThis will do multiple allocations:\n* one for each id field in the `User` type\n* one for the resulting slice\n\nTo facilitate memory handling, consider using an arena allocator like this:\n```zig\nvar arena = std.heap.ArenaAllocator.init(allocator);\ndefer arena.deinit();\n\nconst users = try fetchUsers(arena.allocator(), db);\n_ = users;\n```\n\nThis is especially recommended if you use custom types that allocate memory since, as noted above, it's necessary to prevent memory leaks.\n\n# Comptime checks\n\nPrepared statements contain _comptime_ metadata which is used to validate every call to `exec`, `one` and `all` _at compile time_.\n\n## Check the number of bind parameters.\n\nThe first check makes sure you provide the same number of bind parameters as there are bind markers in the query string.\n\nTake the following code:\n```zig\nvar stmt = try db.prepare(\"SELECT id FROM user WHERE age > ? AND age < ? AND weight > ?\");\ndefer stmt.deinit();\n\nconst rows = try stmt.all(usize, .{}, .{\n    .age_1 = 10,\n    .age_2 = 20,\n});\n_ = rows;\n```\nIt fails with this compilation error:\n```\n/home/vincent/dev/perso/libs/zig-sqlite/sqlite.zig:738:17: error: number of bind markers not equal to number of fields\n                @compileError(\"number of bind markers not equal to number of fields\");\n                ^\n/home/vincent/dev/perso/libs/zig-sqlite/sqlite.zig:817:22: note: called from here\n            self.bind(values);\n                     ^\n/home/vincent/dev/perso/libs/zig-sqlite/sqlite.zig:905:41: note: called from here\n            var iter = try self.iterator(Type, values);\n                                        ^\n./src/main.zig:19:30: note: called from here\n    const rows = try stmt.all(usize, allocator, .{}, .{\n                             ^\n./src/main.zig:5:29: note: called from here\npub fn main() anyerror!void {\n```\n\n## Assign types to bind markers and check them.\n\nThe second (and more interesting) check makes sure you provide appropriately typed values as bind parameters.\n\nThis check is not automatic since with a standard SQL query we have no way to know the types of the bind parameters, to use it you must provide theses types in the SQL query with a custom syntax.\n\nFor example, take the same code as above but now we also bind the last parameter:\n```zig\nvar stmt = try db.prepare(\"SELECT id FROM user WHERE age > ? AND age < ? AND weight > ?\");\ndefer stmt.deinit();\n\nconst rows = try stmt.all(usize, .{ .allocator = allocator }, .{\n    .age_1 = 10,\n    .age_2 = 20,\n    .weight = false,\n});\n_ = rows;\n```\n\nThis compiles correctly even if the `weight` field in our `user` table is of the type `INTEGER`.\n\nWe can make sure the bind parameters have the right type if we rewrite the query like this:\n```zig\nvar stmt = try db.prepare(\"SELECT id FROM user WHERE age > ? AND age < ? AND weight > ?{usize}\");\ndefer stmt.deinit();\n\nconst rows = try stmt.all(usize, .{ .allocator = allocator }, .{\n    .age_1 = 10,\n    .age_2 = 20,\n    .weight = false,\n});\n_ = rows;\n```\nNow this fails to compile:\n```\n/home/vincent/dev/perso/libs/zig-sqlite/sqlite.zig:745:25: error: value type bool is not the bind marker type usize\n                        @compileError(\"value type \" ++ @typeName(struct_field.field_type) ++ \" is not the bind marker type \" ++ @typeName(typ));\n                        ^\n/home/vincent/dev/perso/libs/zig-sqlite/sqlite.zig:817:22: note: called from here\n            self.bind(values);\n                     ^\n/home/vincent/dev/perso/libs/zig-sqlite/sqlite.zig:905:41: note: called from here\n            var iter = try self.iterator(Type, values);\n                                        ^\n./src/main.zig:19:30: note: called from here\n    const rows = try stmt.all(usize, allocator, .{}, .{\n                             ^\n./src/main.zig:5:29: note: called from here\npub fn main() anyerror!void {\n```\nThe syntax is straightforward: a bind marker `?` followed by `{`, a Zig type name and finally `}`.\n\nThere are a limited number of types allowed currently:\n * all [integer](https://ziglang.org/documentation/master/#Primitive-Types) types.\n * all [arbitrary bit-width integer](https://ziglang.org/documentation/master/#Primitive-Types) types.\n * all [float](https://ziglang.org/documentation/master/#Primitive-Types) types.\n * bool.\n * strings with `[]const u8` or `[]u8`.\n * strings with `sqlite.Text`.\n * blobs with `sqlite.Blob`.\n\nIt's probably possible to support arbitrary types if they can be marshaled to a sqlite type. This is something to investigate.\n\n**NOTE**: this is done at compile time and is quite CPU intensive, therefore it's possible you'll have to play with [@setEvalBranchQuota](https://ziglang.org/documentation/master/#setEvalBranchQuota) to make it compile.\n\nTo finish our example, passing the proper type allows it compile:\n```zig\nvar stmt = try db.prepare(\"SELECT id FROM user WHERE age > ? AND age < ? AND weight > ?{usize}\");\ndefer stmt.deinit();\n\nconst rows = try stmt.all(usize, .{}, .{\n    .age_1 = 10,\n    .age_2 = 20,\n    .weight = @as(usize, 200),\n});\n_ = rows;\n```\n\n# User defined SQL functions\n\nsqlite supports [user-defined SQL functions](https://www.sqlite.org/c3ref/create_function.html) which come in two types:\n* scalar functions\n* aggregate functions\n\nIn both cases the arguments are [sqlite3\\_values](https://www.sqlite.org/c3ref/value_blob.html) and are converted to Zig values using the following rules:\n* `TEXT` values can be either `sqlite.Text` or `[]const u8`\n* `BLOB` values can be either `sqlite.Blob` or `[]const u8`\n* `INTEGER` values can be any Zig integer\n* `REAL` values can be any Zig float\n\n## Scalar functions\n\nYou can define a scalar function using `db.createScalarFunction`:\n```zig\ntry db.createScalarFunction(\n    \"blake3\",\n    struct {\n        fn run(input: []const u8) [std.crypto.hash.Blake3.digest_length]u8 {\n            var hash: [std.crypto.hash.Blake3.digest_length]u8 = undefined;\n            std.crypto.hash.Blake3.hash(input, &hash, .{});\n            return hash;\n        }\n    }.run,\n    .{},\n);\n\nconst hash = try db.one([std.crypto.hash.Blake3.digest_length]u8, \"SELECT blake3('hello')\", .{}, .{});\n```\n\nEach input arguments in the function call in the statement is passed on to the registered `run` function.\n\n## Aggregate functions\n\nYou can define a scalar function using `db.createAggregateFunction`:\n```zig\nconst MyContext = struct {\n    sum: u32,\n};\nvar my_ctx = MyContext{ .sum = 0 };\n\ntry db.createAggregateFunction(\n    \"mySum\",\n    &my_ctx,\n    struct {\n        fn step(ctx: *MyContext, input: u32) void {\n            ctx.sum += input;\n        }\n    }.step,\n    struct {\n        fn finalize(ctx: *MyContext) u32 {\n            return ctx.sum;\n        }\n    }.finalize,\n    .{},\n);\n\nconst result = try db.one(usize, \"SELECT mySum(nb) FROM foobar\", .{}, .{});\n```\n\nEach input arguments in the function call in the statement is passed on to the registered `step` function.\nThe `finalize` function is called once at the end.\n\nThe context (2nd argument of `createAggregateFunction`) can be whatever you want; both `step` and `finalize` function must\nhave their first argument of the same type as the context.\n"
 },
 {
  "repo": "mortzdk/Websocket",
  "language": "C",
  "readme_contents": "# WSServer a C WebSocket Server\n\n[![Build Status](https://travis-ci.org/mortzdk/Websocket.svg?branch=master)](https://travis-ci.org/mortzdk/Websocket) [![Financial Contributors](https://opencollective.com/websocket/tiers/badge.svg)](https://opencollective.com/websocket/) \n\nWSServer is a fast, configurable, and extendable WebSocket Server for UNIX\nsystems written in C (C11).\n\nAs of version 2.0.0 the WSServer has been completely rewritten with many new\nfeatures, better support, better extendability and generally as a more stable\nWebSocket server.\n\nCurrent Version: **v2.1.0**\n\n### Early history\n\nThe original WebSocket server (v1.0.0 and before) started out as a hobby\nproject in 2012. The idea at the time, was to learn the C language and\nunderstand the basics of WebSockets. The initial version of the server worked\nfor some but not all aspects of the protocols present at that time. At that\ntime, different browsers implemented different versions of the WebSocket\nProtocol, which made it difficult to support all browsers. Today all major\nbrowsers support the [RFC6455](http://tools.ietf.org/html/rfc6455) protocol,\nwhich has been stable for many years. \n\n### Present (2020)\n\nWSServer now supports all aspects of the [RFC6455](http://tools.ietf.org/html/rfc6455) protocol, including the\n[RFC7692](https://tools.ietf.org/html/rfc7692) that enables the `permessage-deflate` extension. The \nimplementation is verified by the [Autobahn testsuite](https://github.com/crossbario/autobahn-testsuite) and by a lot of\nunit tests. It furthermore support the older protocols [HYBI10](https://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-10) and [HYBI07](https://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-07)\nas there are no noteable difference between those and the current stable one.\nCurrently there are no support for older versions of the protocol.\n\n### Donate\n\nTo support further development on this project and financially support the \ndeveloper having a nice cup of coffee, you can make a donation of your choice\n[here](https://opencollective.com/websocket).\n\n# How to get started\n\nTo build the WSServer for your UNIX system, simply run: `make release`. This\nwill compile extensions, subprotocols, and the binary that will be available\nfrom: `./bin/WSServer`.\n\nCurrently the WSServer support only one extension namely the `permessage-deflate`\nextension. Read more about this implementation [here](#Permessage-Deflate).\n\nFurthermore it supports two subprotocols: [echo](#Echo) and [broadcast](#Broadcast). \nThe [echo](#Echo) subprotocol is a simple protocol that sends whatever message\nreceived, back to the same client. This is also the default protocol chosen,\nif no subprotocol is provided by the client. The [broadcast](#Broadcast) subprotocol\nis slightly more advanced. It sends a message from one client to all other\nconnected clients. The behaviour is basically as a public chat room.\n\nThe server can be configured by providing a `-c [path_to_config_file.json]`\nflag. If no configuration is provided, the server will run with a default\nconfiguration, that support WebSocket over HTTP on port 80. You can read more\nabout the structure and description of the configuration file in the\n[configuration](#Configuration) section.\n\n### Log\n\nWSServer keeps a log file at `logs/WSServer.log`. The detail of the log is\ndefined by the `log_level` value in the configuration. Default for a release\nbuild is 3 (FATAL, ERROR, WARN, INFO). Default for a test build is 5 (FATAL,\nERROR, WARN, INFO, DEBUG, TRACE).\n\nThe log file is a good tool for discovering misbehavior of the server, such as\nwhen the server isn't able to start, since port 80 is already occupied by\nanother server instance.\n\n### Dependencies\n\nWSServer does in principle not rely on any third-party libraries in order to\nserve as a WebSocket server. However if you want the complete feature set the\n[**zlib**](https://zlib.net/) and one of the SSL libraries [**OpenSSL**](https://www.openssl.org/), \n[**WolfSSL**](https://www.wolfssl.com/), [**BoringSSL**](https://www.boringssl.googlesource.com/boringssl/), [**LibreSSL**](https://www.libressl.org/) must be installed \non your system in order to support the `permessage-deflate` extension and SSL (WSS).\n\n##### Ubuntu\n```\n$ sudo apt-get install zlib1g-dev\n$ sudo apt-get install openssl\n$ sudo apt-get install wolfssl\n```\n\n##### Arch\n```\n$ pacman -S zlib\n$ pacman -S wolfssl\n$ pacman -S libressl\n```\n\n##### MacOS\n```\n$ brew install zlib\n$ brew install openssl\n$ brew install wolfssl\n```\n\n##### FreeBSD\n```\n$ pkg install libressl\n$ pkg install boringssl\n$ pkg install openssl\n$ pkg install zlib\n```\n\nIf the installation method is not listed above or your package manager doesn't\ncontain the software, it can all be build from source.\n\nNo other dependencies are required with regards to building the server with the\nfull feature set.\n\nIf you want to run the [Autobahn testsuite](https://github.com/crossbario/autobahn-testsuite) and the unit tests yourself, further\ndependencies are required. These are [**docker**](https://www.docker.com/) and [**criterion**](https://github.com/Snaipe/Criterion).\n\n##### Ubuntu\n```\n$ sudo apt install docker.io\n$ sudo add-apt-repository ppa:snaipewastaken/ppa\n$ sudo apt-get update\n$ sudo apt-get install criterion-dev\n```\n\n##### Arch\n```\n$ pacman -S docker\n$ pacman -S criterion\n```\n\n##### MacOS\n```\n$ brew install docker\n$ brew install snaipe/soft/criterion\n```\n\n##### FreeBSD\n```\n$ pkg install docker\n```\n\nCriterion can be build for FreeBSD using the following [guide](https://criterion.readthedocs.io/en/latest/setup.html#installation).  \n\n### Configuration\n\nAn example of a configuration file can be found at [here](https://github.com/mortzdk/Websocket/blob/master/conf/wss.json). A lot of different\nthings are configurable through the configuration file. \n\n##### Origins\n\nThe `origins` key define a subset of allowed origins. It is always recommended\nto define this subset. In case no subset is defined a client can connect from\nanywhere.\n\n##### WebSocket URI\n\nA WebSocket URI consists of a *scheme*, *host*, *port*, *path* and a *query*.\nTake for example: \n\n**wss://mortz.dk:9011/websocket?csrf-token=asgjh48hs389hdla**.\n\nThe **wss** part of the URI is defined as the *scheme*, the **mortz.dk** part\nis defined as the host, the **9011** part is defined as the *port*, the\n**/websocket** part is defined as the *path* and the\n**?csrf-token=asgjh48hs389hdla** part is defined as the *query*.  The specific\nallowance of all 5 parts of the WebSocket URI are configurable.\n\nThe `hosts` key define a subset of strings allowed as the host. In the example\nabove, if we only want clients connecting to **mortz.dk**, we can add that\nstring to the subset.\n\nThe `paths` key define a subset of strings allowed as the connecting path. In\nthe example above, if we only want clients connecting through the path\n**/websocket**, we can add that string to the subset. The path **/** will\nalways be a valid connection path. The string values of the `paths` key are\nallowed to use POSIX Extended Regular Expressions syntax.\n\nThe `queries` key define a subset of strings allowed as the queries. In the\nexample above, if we only want clients to use a specific query\n__csrf-token=[^&]*__, we can add that string to the subset. The string values\nof the `paths` key are allowed to use POSIX Extended Regular Expressions\nsyntax. A WebSocket URI without any queries is always allowed.\n\nThe *scheme* and *port* part of the WebSocket URI is checked based on the ports\nchoosen for `http` and `https`.\n\n##### Port\n\nThe `port` key of the `setup` object is used to define the ports that http (ws)\nversion and the https (wss) should be listening to.\n\nA http (ws) version of the server will always be available. The https (wss)\nversion requires further configuration of SSL.\n\n##### Extensions\n\nThe `extensions` key of the `setup` object is used to define an array of\nsupported extensions. Each entry in the array is an object itself containing \na `file` and `config` key. The `file` key should point to the location of the\nshared object representing the extension. The basename of the `file` key is \nused as the extension name. The `config` key can be used to provide extra\nconfiguration to the extension.\n\n##### Subprotocols\n\nThe `subprotocols` key of the `setup` object is used to define an array of\nsupported subprotocols. Each entry in the array is an object itself containing \na `file` and `config` key. The `file` key should point to the location of the\nshared object representing the subprotocol. The basename of the `file` key is \nused as the subprotocol name. The `config` key can be used to provide extra\nconfiguration to the subprotocol. \n\n##### Favicon\n\nThe `favicon` key of the `setup` object is used to define a favicon to serve\nfor HTTP and HTTPS request. A lot of browsers do the request for favicons per\ndefault when performing HTTP and HTTPS requests. By defining this with the path\nto a valid ICO file, the WSServer will return a favicon.\n\n##### Timeouts\n\nThe `timeout` key of the `setup` object is used to define different timeouts of\nthe WSServer.\n\nThe `poll` key define a timeout for event polling. By setting it to a positive\ninteger **n**, the event loop will be interrupted every **n** milliseconds.\n\nThe `read` key define a timeout for the READ event. The timeout is checked\nwhenever the server requires to read more data from the client in order to\nsucceed. By setting it to a positive integer **n** the request will fail if the\nnext read took longer than **n** milliseconds.\n\nThe `write` key define a timeout for the WRITE event. The timeout is checked\nwhenever the server requires to write more data to the client than it is\ncurrently able to buffer. By setting it to a positive integer **n**\nthe request will fail if the next write took longer than **n** milliseconds.\n\nThe `client` key define a timeout for when the client was last active. By\nsetting it to a positive **n** integer, the client will be disconnected if it\nhas not been active the last **n** milliseconds.\n\nThe `pings` key defines the amount of pings performed within the span of the\n`client` timeout key. If this value is set, it is recommended to use a value\nstricly higher than 1, as the internal timing of the server is not 100%\naccurate.\n\n##### Size\n\nA lot of different sizes can be adjusted for the WSServer. All sizes but the\n`ringbuffer` are defined in bytes.\n\nThe `payload` size define how large a size of payload the server is willing to\naccept from the client.\n\nThe `header` size define how large a HTTP header the server will accept from\nthe client.\n\nThe `uri` size define how large a URI the server will accept from the client.\n\nThe `buffer` size define how large the internal read and write buffers should\nbe. \n\nThe `thread` size define how large each thread of the WSServer can maximally be.\n\nThe `ringbuffer` size define how many messages about to be written each client\ncan store in their ringbuffer.\n\nThe `frame` size define the maximal payload size of a single frame.\n\nThe `fragmented` size define how many fragments (frames) one single message can\nconsist of.\n\n##### Pool\n\nInternally the WSServer runs a threadpool to schedule IO work from the clients.\n\nThe `worker` key define the amount of threads the threadpool shall consist of.\nGenerally the rule of thumb is that the higher the load, the more threads.\nHowever the optimal amount of workers is probably system and hardware dependent.\n\n##### SSL (WSS)\n\nWSServer supports the *wss* scheme by the use of one of currently 4 SSL\nlibraries ([**OpenSSL**](https://www.openssl.org/), \n[**WolfSSL**](https://www.wolfssl.com/), [**BoringSSL**](https://www.boringssl.googlesource.com/boringssl/), [**LibreSSL**](https://www.libressl.org/)). \nThe default choice is OpenSSL, but the SSL library can be switched as follows:\n\n```\n# OpenSSL\n$ make\n\n# WolfSSL\n$ make SSL=WOLFSSL\n\n# LibreSSL\n$ make SSL_LIBRARY_PATH=/path/to/libressl\n\n# BoringSSL\n$ make SSL_LIBRARY_PATH=/path/to/boringssl\n```\n\nNote that if compiled with `SSL_LIBRARY_PATH` the binary must be executed with\n`LD_LIBRARY_PATH=/path/to/libressl/lib ./bin/WSServer`.\n\nIf LibreSSL or BoringSSL is installed in place of OpenSSL the compilation should\nwork out of the box by running `make`.\n\nIn order to activate SSL some configuration must be made.\n\nThe `key` key define the path to the SSL private key of the server. The private\nkey must be in the PEM format.\n\nThe `cert` key define the path to the SSL server certificate. The certificate\nmust be in the PEM format.\n\nThe `ca_file` key define the path to the root CA certificate.\n\nThe `ca_path` key define the path to a folder containing the trusted root CA\ncertifates.\n\nThe `dhparam` key define the path to the dhparam file. The dhparam file must be\nin the PEM format.\n\nThe `cipher_list` key define the ciphers that the server allows usage of.\n\nThe `cipher_suites` key define the cipher suites that the server allows usage\nof.\n\nThe `compression` key define whether compression should be used when\ncommunicating over SSL,\n\nThe `peer_cert` key define whether a peer certificate is required by the\nclient.\n\n# WebSocket Extensions\n\nThe WSServer enables usage of an arbitrary number of extensions. Extensions\nprovide a mechanism for implementations to opt-in to additional protocol\nfeatures.\n\nThe extensions themselves can be implemented in any language that is able to\ncompile into a shared object (*.so* file) with the following public\nfunctions:\n\n```\ntypedef void (*setAllocators)(WSS_malloc_t extmalloc, WSS_realloc_t extrealloc, WSS_free_t extfree);\ntypedef void (*onInit)(char *config);\ntypedef void (*onOpen)(int fd, char *param, char **accepted, bool *valid);\ntypedef void (*inFrame)(int fd, wss_frame_t *frame);\ntypedef void (*inFrames)(int fd, wss_frame_t **frames, size_t len);\ntypedef void (*outFrame)(int fd, wss_frame_t *frame);\ntypedef void (*outFrames)(int fd, wss_frame_t **frames, size_t len);\ntypedef void (*onClose)(int fd);\ntypedef void (*onDestroy)();\n```\n\nWhere `WSS_malloc_t`, `WSS_realloc_t`, and `WSS_free_t` are defined\nas:\n\n```\ntypedef void *(*WSS_malloc_t)(size_t size);\ntypedef void *(*WSS_realloc_t)(void *ptr, size_t size);\ntypedef void (*WSS_free_t)(void *ptr);\n```\n\nand `wss_frame_t` is defined as:\n\n```\ntypedef struct {\n    bool fin;\n    bool rsv1;\n    bool rsv2;\n    bool rsv3;\n    uint8_t opcode;\n    bool mask;\n    uint64_t payloadLength;\n    char maskingKey[4];\n    char *payload;\n    uint64_t extensionDataLength;\n    uint64_t applicationDataLength;\n} wss_frame_t;\n```\n\nFor the server to be able to use a custom extension one has to configure the\npath to the shared object in the configuration file as described [above](#Extensions).\n\nYou can have a look at the [extensions](https://github.com/mortzdk/websocket/blob/master/extensions) folder to see how to\nimplement your own extension.\n\n### Permessage-deflate\n\nThe WSServer comes with 1 build-in extension, namely the `permessage-deflate`\nextension defined in [RFC7692](https://tools.ietf.org/html/rfc7692). This\nextension enables compression and decompression of the frames between client\nand server.\n\n# WebSocket Subprotocols\n\nThe WSServer also enables usage of an arbitrary number of subprotocols.\nSubprotocols are application-level protocol layered over the WebSocket Protocol\nthat are used to define the behaviour of the websocket protocol.\n\nThe subprotocols themselves can be implemented in any language that is able to\ncompile into a shared object (*.so* file) with the following public\nfunctions:\n\n```\ntypedef void (*setAllocators)(WSS_malloc_t submalloc, WSS_realloc_t subrealloc, WSS_free_t subfree);\ntypedef void (*onInit)(char *config, WSS_send send);\ntypedef void (*onConnect)(int fd, char *ip, int port, char *path, char *cookies);\ntypedef void (*onMessage)(int fd, wss_opcode_t opcode, char *message, size_t message_length);\ntypedef void (*onWrite)(int fd, char *message, size_t message_length);\ntypedef void (*onClose)(int fd);\ntypedef void (*onDestroy)();\n```\n\nWhere `WSS_send`, `WSS_malloc_t`, `WSS_realloc_t`, and `WSS_free_t` are defined\nas:\n\n```\ntypedef void (*WSS_send)(int fd, wss_opcode_t opcode, char *message, uint64_t message_length);\ntypedef void *(*WSS_malloc_t)(size_t size);\ntypedef void *(*WSS_realloc_t)(void *ptr, size_t size);\ntypedef void (*WSS_free_t)(void *ptr);\n```\n\nand `wss_opcode_t` is defined as:\n\n```\ntypedef enum {\n    CONTINUATION_FRAME = 0x0,\n    TEXT_FRAME         = 0x1,\n    BINARY_FRAME       = 0x2,\n    CLOSE_FRAME        = 0x8,\n    PING_FRAME         = 0x9,\n    PONG_FRAME         = 0xA,\n} wss_opcode_t;\n```\n\nYou can have a look at the [subprotocols](https://github.com/mortzdk/websocket/blob/master/extensions) folder to see how to\nimplement your own subprotocol.\n\n### Client Authentication\n\nClient authentication is not implemented directly in WSServer, but is\nsupported through several means. The `onConnect` call of the subprotocol sends\ninformation about the connection to the subprotocol, this is information such\nas the *ip*, *port*, *path* and *cookies*. This can be used to do client\nauthentication using [cookies](https://coletiv.com/blog/using-websockets-with-cookie-based-authentication/), using query parameters of the path \nor simply by having an initial round of authentication messages between the\nclient and server.\n\nAs always it is strongly advised to use the [origins](#Origins) array of the\nconfiguration to only allow for certain origins to use the server.\n\n### Echo\n\nThe `echo` subprotocol is a very simple subprotocol that just echo's whatever\nthe client send back to the client. This subprotocol is especially useful for\ntesting and is used in the [Autobahn testsuite](https://github.com/crossbario/autobahn-testsuite).\n\n### Broadcast\n\nThe `broadcast` subprotocol is slightly more advanced. It keeps track of when a\nclient is connecting or closing in order to hold a map of those clients that\nshould be broadcastet to. Whenever a client sends a message, the message is\nbroadcastet to all other connected clients.\n\n# Documentation\n\nWSServer automatically generates documentation based on the comments in the\ncode. This documentation can be viewed [here](https://mortzdk.github.io/Websocket/documentation/).\n\nFurthermore one could take a look at the [RFC6455](http://tools.ietf.org/html/rfc6455) protocol and the\n[RFC7692](https://tools.ietf.org/html/rfc7692) protocol to understand how WebSockets and the permessage-deflate\nextension works.\n\n# Testing\n\nWSServer has been heavily tested by the use of unit tests, the \n[Autobahn testsuite](https://github.com/crossbario/autobahn-testsuite) and by having code coverage.\n\n### Unit tests\n\nA lot of unit tests has been written using the [Criterion](https://github.com/Snaipe/Criterion) unit testing library. As of right now the unit tests does not cover all files, and this is still work in progress.\n\nThe tests can be run by running `make test`.\n\n### Autobahn Testsuite\n\nThe Autobahn Testsuite is used to verify that the WSServer complies to the \n[RFC6455](http://tools.ietf.org/html/rfc6455) protocol. These tests can be used\nas both verification and as a measure of performance as a lot of the tests\nactually times the execution of a successful test.\n\nThe tests can be run by running `make autobahn`.\n\nYou can further see the current results of the tests [here](https://mortzdk.github.io/Websocket/autobahn/).\n\n### Code coverage\n\nThe coverage report can be generated by running `make test` and the latest can \nbe seen [here](https://mortzdk.github.io/Websocket/gcov/).\n\n# Further Work\n\nHere is a list of prioritized further work that currently can be done:\n\n1. Test on FreeBSD/MacOS\n2. Use Autoconf to check dependencies\n3. Rate limiting\n    - Rate limiting connections\n        - Count-Min Sketch (Sliding window)\n            * Belongs to the server object\n            * Allocate 60 count-min sketches (one per minute)\n            * Reset sketch if rotated\n    - Rate limiting messages\n        - Can be done by the subprotocols per message?\n    - Rate limiting frames\n        - Counting using Sliding window\n            * Belongs to the session object\n            * Allocate 60 integers (one per minute)\n            * Reset sketch if rotated\n4. Fuzz testing\n5. Support HTTP2\n6. Performance Improvements\n    - Look at 'khash' or 'tommy_hashdyn' instead of 'uthash' since these hashes\n      seems to be faster\n    - Realloc the double size of the current\n    - Refactor `wss_frame_t` structure away. Use the frames as byte strings\n      instead.\n    - Callgrind\n    - Cachegrind\n7. Backwards Specification Compability\n    - hybi-06\n    - hybi-05\n    - hybi-04\n    - hixie-76\n    - hixie-75\n\n# Contributors\n\nHere is a list of the contributors of v2.0.0 and above of the WSServer.\n\n[Morten Houm\u00f8ller Nygaard](https://github.com/mortzdk/)\n[Nicolas Mora](https://github.com/babelouest/)\n\n### Libraries \n\nWSServer makes use of other Open-Source libraries and code snippets. The links\nlisted below have all been used in some way.\n\n* [Fast Validation of UTF-8](https://github.com/lemire/fastvalidate-utf-8/)\n* [UTF-8 Decoder](http://bjoern.hoehrmann.de/utf-8/decoder/dfa/)\n* [SHA1](https://tools.ietf.org/html/rfc3174)\n* [Ringbuf](https://github.com/rmind/ringbuf)\n* [log.c](https://github.com/rxi/log.c)\n* [b64.c](https://github.com/littlstar/b64.c)\n* [rpmalloc](https://github.com/mjansson/rpmalloc)\n* [Threadpool](https://github.com/mbrossard/threadpool)\n* [json-parser](https://github.com/udp/json-parser)\n* [uthash](https://troydhanson.github.io/uthash/)\n* [Http Status Codes](https://github.com/j-ulrich/http-status-codes-cpp)\n\n# License\n\nWSServer is licenced under the [MIT license](https://github.com/mortzdk/Websocket/blob/master/LICENSE).\n"
 },
 {
  "repo": "mattdennewitz/playlist-to-vec",
  "language": "Python",
  "readme_contents": "# artistrecs\n\nA similar artist recommendation engine powered by Spotify\nplaylists and [word2vec](https://code.google.com/p/word2vec/).\n\nThis proof of concept was inspired by two pieces and my own\nlongstanding belief that the transitions between songs in playlists,\nwhen given enough, are valuable insights.\n\nSome bathroom reading:\n\n- [Playlist Harvesting](https://social.shorthand.com/huntedguy/3CfQA8mj2S/playlist-harvesting),\n  by [Stephen Phillips](https://social.shorthand.com/huntedguy)\n- [Distributional Similarity Music Recommendations Versus Spotify: A Comparison Based on User Evaluation](http://arno.uvt.nl/show.cgi?fid=136352), by Nevyana Boycheva\n\nAlso, quick reminder: this is a proof of concept! It's working and\npretty cool, but that doesn't mean the tools are complete, the project\nisn't without layout or design decision kinks, or that things won't change.\nIn fact, plan on things changing for as long as this message is here.\n\n----\n\nThis application consists of two major components:\n\n- A Celery-backed extraction setup for ingesting playlists from Spotify\n  en masse. Celery workers are responsible for importing playlists\n  and extracting artist names.\n- Helper scripts for training and querying data extracted\n  from said playlists\n\n**NOTE** - project layout will be changing shortly. See TODO for what's up.\n\n## Setup\n\nBefore getting started with this project, please ensure you have the\nfollowing installed:\n\n- a C compiler\n- Redis (up and running)\n- [`word2vec` bindings](#install-word2vec)\n\nPlease also have your OAuth client ID and secret ready from\nthe Spotify application you wish to use. If you need to register\na Spotify app, [do so here](https://developer.spotify.com/my-applications/#!/applications/create) before continuing.\n\n### Install `word2vec` bindings\n\n<a name=\"install-word2vec\"></a>\n\n#### OSX using Homebrew\n\n```shell\n$ brew install --HEAD homebrew/head-only/word2vec\n```\n\n### Setup\n\n### Install requirements\n\nCheck out this repo into a virtualenv,\nand then install its Python requirements using `pip`:\n\n```shell\n$ pip install -r requirements.txt\n```\n\n### Set up environment variables\n\nThis application's configuration data is taken from environment variables.\n\n- `SPOTIFY_CLIENT_ID`: Spotify OAuth client id\n- `SPOTIFY_CLIENT_SECRET`: Spotify OAuth client secret\n- `ARTISTRECS_BROKER_URL`: Celery broker URL.\n    Defaults to `localhost:6379`. [Read more](http://docs.celeryproject.org/en/latest/configuration.html#broker-url).\n- `ARTISTRECS_RESULT_BACKEND`: Celery result backend URL.\n    Defaults to `localhost:6379` [Read more](http://docs.celeryproject.org/en/latest/configuration.html#celery-result-backend)\n\nHow you set these variables is up to you.\n\n**TIP**: `envdir` is great for this.\n\n## Workflow and output\n\n### Extraction workflow\n\nFirst, a quick overview of what the Celery workers are doing,\nfrom start to finish:\n\n1. `playlist_generator` task receives a request to search Spotify's playlists\n   for a specific term. For each playlist found, `playlist_generator`\n   creates a new task: `resolve_playlist`.\n\n   If `recycle` is enabled, `playlist_generator` will respawn itself\n   to collect more information. This is controlled by the `max_recycles`\n   parameter - if left undefined, `playlist_generator` will collect\n   all playlists available for the given term.\n2. `resolve_playlist` receives a username and playlist id and fetches\n   the playlist's tracks. It then compiles into a list each artist name\n   from each track in the playlist [1]. Once all names have been collected,\n   `resolve_playlist` hands off the list to\n   `export_artist_sentence_from_playlist`.\n3. `export_artist_sentence_from_playlist` accepts a user id, playlist id,\n   and a list of artist names. It JSON-encodes them and then writes\n   the object to a file.\n\n   This task is run in a separate queue to avoid overlapping file writes.\n\nOnce all jobs have been processed, you should have a text file\nready to be processed by the `parser.py` script.\n\n[1] If the artist name about to be collected is also the last entry\n    in the current list, it is ignored. This is a naive way of protecting\n    against entire albums influencing transitional frequencies.\n\nRight now, there are many moving pieces. As this project matures\npast \"proof of concept\", noise will be reduced.\n\n### Output format\n\n`word2vec` works by analyzing sentences. That's a gross generalization.\nHere's something from the official docs:\n\n> The word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications.\n\n`gensim`, the `word2vec` implementation used here, expects sentences\nto be delivered as lists of words. In our particular use case,\nwe're constructing sentences from artist names. Wild, right?\n\nAt the end of a run thorugh the extraction workflow described above,\nyou'll have a file whose every line has a sentence constructed from artists\nin a playlist. The format for this output is:\n\n```json\n{\n    \"playlist_id\": \"<spotify playlist id>\",\n    \"user_id\": \"<spotify user id>\",\n    \"sentence\": [\n        \"<artist name>\",\n        \"<artist name>\",\n        ...\n    ]\n}\n```\n\nThis file can be found at `SENTENCE_OUTPUT_PATH`.\n\n## Running\n\nOnce you've completed the above, you're ready to begin.\n\n### Start Redis\n\nIf `redis-server` is not already running in the background, fire it up.\n\n```shell\n$ redis-server\n```\n\n### Start extraction queues\n\nIn one shell inside your virtualenv, fire up the extraction queue.\nThis queue is responsible for the tasks concerned with querying \nSpotify and compiling artist names from tracks inside of playlists.\n\n```shell\n$ celery worker -A artistrecs -l info -Q extraction\n```\n\nIn another shell, start the writer queue with 1 worker. This is a sloppy\nworkaround to prevent overlapping file writes by unlocked append access\nto a file.\n\n```shell\n$ celery worker -A artistrecs -l info -Q writer\n```\n\n### Insert a task\n\nTo query Spotify for playlists, insert a task using `insert_task.py`.\nThis helper script accepts two parameters, expecting one of them:\n\n```shell\n$ python insert_task.py -t <term>\n```\n\nSee `insert_task.py`'s `--help` print-out for extended usage details.\n\n### Parsing output\n\nData extracted from playlists may be parsed using `parser.py`.\n\n```shell\n$ python parser.py -i <path to output file> -t <artist name>\n```\n\nThis will emit a JSON-encoded ranked list of similar artists.\nMake sure that the artist name you're checking was, in fact,\na member of at least a few of the playlists brought in during\nextraction.\n\nSee `--help` for extended usage details.\n\n## TODO\n\n- Add `setup.py`, which should also install helper scripts as console scripts\n- Parser should default to using `SENTENCE_OUTPUT_PATH` env var\n  when `-i` is not given.\n\n## Issues\n\nWant to contribute? File an issue or a pull request.\n"
 },
 {
  "repo": "jordan-wright/rapportive",
  "language": "Python",
  "readme_contents": "![Travis-CI Build Status](https://travis-ci.org/jordan-wright/rapportive.png)\n\n**Note: It looks like Rapportive recently changed their API, breaking the functionality of this library. If I have time to look into this further, I'll see if I can get it back up and running. Until then - I'm accepting PR's :smile:**\n\nrapportive.py\n=============\n\nPython library to automate Rapportive queries.\n\nYou can refer to my blog post [here](http://jordan-wright.github.io/blog/2013/10/14/automated-social-engineering-recon-using-rapportive/) for more information.\n\n##Installation\nTo install rapportive, just run python setup.py install.\n\n##Usage\n```\nfrom rapportive import rapportive\nprofile = rapportive.request('email@domain.com')\n```\n\nResults are received as a Profile object, which contains the following attributes:\n\n* profile.name - Name of contact\n* profile.memberships - List of tuples of memberships (Linkedin, Facebook, Blogger, etc.)\n* profile.jobinfo - Tuple of (position, company)\n"
 },
 {
  "repo": "mobify/iterstuff",
  "language": "Python",
  "readme_contents": "iterstuff\n=========\n\n**Useful tools for working with iterators**\n\nIf the python2 `itertools` module is the Swiss Army Knife of functions for\niterables, `iterstuff` is the cut-down single-blade version that you can keep on\nyour keyring.\n\nYou can install iterstuff from pypi using pip:\n\n    pip install iterstuff\n\n## Lookahead\n\nThe Lookahead class is the main feature of iterstuff. It 'wraps' an iterable\nand allows:\n\n* Detection of the end of the generator using the `atend` property\n* 'Peeking' at the next item to be yielded using the `peek` property\n\nNote that 'wrapping' means that the Lookahead will advance the wrapped iterable (by calling\n`next`) as needed. As per the comments on the Lookahead `__init__`, creating\nthe Lookahead will advance to the first element of the wrapped iterable immediately.\nAfter that, iterating over the Lookahead will also iterate over the wrapped iterable.\n\nWe'll look at examples in a moment, but first here's a summary of usage:\n\n```python\n>>> # Create a generator that will yield three integers\n>>> g = xrange(3)\n>>> # Wrap it in a Lookahead\n>>> from iterstuff import Lookahead\n>>> x = Lookahead(g)\n```\n    \nNow we can use the properties of the Lookahead to check whether we're at the\nstart and/or end of the generator sequence, and to look at the next element\nthat would be yielded:\n\n```python\n>>> x.atstart\nTrue\n>>> x.atend\nFalse\n>>> x.peek\n0\n```\n\nLet's grab the first element and see how the properties change:\n\n```python\n>>> x.next()\n0\n>>> x.atstart\nFalse\n>>> x.atend\nFalse\nx.peek\n1\n```\n\nWe have two ways to iterate over a sequence wrapped in a Lookahead:\n\n```python\n>>> # The usual way\n>>> x = Lookahead(xrange(3))\n>>> for y in x: print y\n0\n1\n2\n\n>>> # By checking for the end of the sequence\n>>> x = Lookahead(xrange(3))\n>>> while not x.atend:\n...     y = x.next()\n...     print y\n...     \n0\n1\n2\n```\n\nAnd we can detect a completely empty Lookahead:\n\n```python\n>>> if x.atstart and x.atend:\n...    # x is an empty Lookahead\n```\n\nThe obvious question is: _how is this useful?_\n\n### Repeating a `takewhile`\n\nThe [itertools.takewhile](https://docs.python.org/2/library/itertools.html#itertools.takewhile) function\ncan yield items from an iterable while some condition is satisfied. However,\nit only yields items up until the condition is no longer satisfied, then it\nstops, **after** testing the next element. Let's see what happens if we\nwant to use it to break a sequence of characters into letters and digits.\n\n```python\n>>> from itertools import takewhile\n>>> # Build a generator that returns a sequence\n>>> data = iter('abcd123ghi')\n>>>\n>>> # Ok, let's get the characters that are not digits\n>>> print list(takewhile(lambda x: not x.isdigit(), data))\n['a', 'b', 'c', 'd']\n>>> \n>>> # Great, now let's get the digits\n>>> print list(takewhile(lambda x: x.isdigit(), data))\n['2', '3']\n```\n\nWhat happened to '1'? When we were processing the non-digits, the `takewhile`\nfunction read the '1' from `data`, passed it to the `lambda` and when that\nreturned False, terminated. But of course, by then the '1' had already been\nconsumed, so when we started the second `takewhile`, the first character it\ngot was '2'.\n\nWe can solve this with a Lookahead. Here's a repeatable `takewhile` equivalent\n(that's in the `iterstuff` module):\n\n```python\ndef repeatable_takewhile(predicate, iterable):\n    \"\"\"\n    Like itertools.takewhile, but does not consume the first\n    element of the iterable that fails the predicate test.\n    \"\"\"\n    \n    # Assert that the iterable is a Lookahead. The act of wrapping\n    # an iterable in a Lookahead consumes the first element, so we\n    # cannot do the wrapping inside this function.\n    if not isinstance(iterable, Lookahead):\n        raise TypeError(\"The iterable parameter must be a Lookahead\")\n    \n    # Use 'peek' to check if the next element will satisfy the\n    # predicate, and yield while this is True, or until we reach\n    # the end of the iterable.\n    while (not iterable.atend) and predicate(iterable.peek):\n        yield iterable.next()\n```\n\nLet's see how this behaves:\n\n```python\n>>> from iterstuff import repeatable_takewhile, Lookahead\n>>> data = Lookahead('abcd123ghi')\n>>> print list(repeatable_takewhile(lambda x: not x.isdigit(), data))\n['a', 'b', 'c', 'd']\n>>> print list(repeatable_takewhile(lambda x: x.isdigit(), data))\n['1', '2', '3']\n```\n\n### Examine data before it's used\n\nThe *pandas* library can build a `DataFrame` from almost any sequence of\nrecords. The `DataFrame` constructor checks the first record to determine the\ndata types of the columns. If we pass a generator `data` to the `DataFrame`\nconstructor, almost the first thing that happens is that `data` is turned into\na list, so that *pandas* can access `data[0]` to examine the data types. If\nyour generator yields many records, though, this is bad - it's just built a\nlist of those many records in memory, effectively doubling the amount of\nmemory used (memory to hold the list plus memory to hold the DataFrame).\n\nA Lookahead allows code to peek ahead at the next row. So we could do the\nsame job as *pandas* in a different way:\n\n```python\n# Wrap the data in a Lookahead so we can peek at the first row\npeekable = Lookahead(data)\n\n# If we're at the end of the Lookahead, there's no data\nif peekable.atend:\n    return\n    \n# Grab the first row so we can look at the data types\nfirst_row = peekable.peek\n\n# ...process the data types...\n```\n\n### Simple `pairwise`\n    \nThere's a beautiful recipe in the `itertools` documentation for yielding\npairs from an iterable:\n\n```python\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return izip(a, b)\n```\n\nBeautiful, but a little complex. We can make a simpler version with a\nLookahead:\n\n```python\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    it = Lookahead(iterable)\n    while not it.atend:\n        yield it.next(), it.peek\n```\n\nLet's try it:\n\n```python\n>>> data = iter('abcd123ghi')\n>>> print list(pairwise(data))\n[('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', '1'), ('1', '2'), ('2', '3'), ('3', 'g'), ('g', 'h'), ('h', 'i'), ('i', None)]\n```\n\n### Chunking\n\nChunking is like using the repeatable `takewhile`, but for a specific use-case.\n\nSuppose you're reading data from a database: the results of a big query over a\nLEFT OUTER JOIN between several tables. Let's create a simplified (but\nreal-world) example.\n\nWe store data that relates to timing of web pages. We store an `event` for\neach page, and for each event we store multiple `value`s. Our tables look\nsomething like:\n\n    Event\n    ID  Created             Session URL\n    01  2014-12-17 01:00:00 ab12f43 http://www.mobify.com/\n    02  2014-12-17 01:00:01 ab12f43 http://www.mobify.com/jobs\n    ...and so on for millions of events...\n    \n    Value\n    Event_ID  Name              Value\n    01        DOMContentLoaded     83\n    01        Load                122\n    02        DOMContentLoaded     64\n    02        Load                345\n    ...and so on for millions of values for millions of events...\n    \nAt the end of every day, we process the records for that day, by doing a\nquery like:\n\n```sql\nSELECT *\nFROM Event LEFT OUTER JOIN Value ON Event.ID = Value.Event_ID\nORDER BY Event.ID\n```\n\nWe'll probably end up with something like a SQLAlchemy `ResultProxy` or a\nDjango `QuerySet` - an iterable thing that yields records (and here we're\nassuming that your database will stream the results back to your Python\nclient so that you can process much more data than you could ever fit into\nmemory). Let's call that iterable thing `records`.\n\nWhat we want to do is to process each event. The problem is that if we just\niterate over the `records`:\n\n```python\nfor record in records:\n    print record.ID, record.Created, record.Name, record.Value\n```\n\n...we'll get one record per **value** - more than one record per **event**:\n\n    01 2014-12-17 01:00:00 DOMContentLoaded 83\n    01 2014-12-17 01:00:00 Load 122\n    02 2014-12-17 01:00:01 DOMContentLoaded 64\n    02 2014-12-17 01:00:01 Load 345\n\nIt's be better if we could handle all the records for one event together, then\nall the records for the next event, and so on.\n\nWe could use `repeatable_takewhile` to grab all the records belonging to the\nsame event:\n\n```python\nit = Lookahead(records)\n\nwhile not it.atend:\n    current_event_id = it.peek.ID\n    event_records = list(\n        repeatable_takewhile(\n            lambda r: r.ID == current_event_id,\n            it\n        )\n    )\n    \n    # Now we have just the records for the next event\n    ...process...\n```\n        \nBut because this is a common use case, Lookahead has a helper function to\nmake this even easier. The `chunked` function takes a function\nto extract a 'key' value from each element, and yields successive\niterables, each of which has records with the same key value.\n\n```python\nfrom iterstuff import chunked\nfor records_for_events in chunked(\n        records,\n        lambda r: r.ID\n    ):\n    # records_for_events is a sequence of records for\n    # one event.\n    ...process...\n```\n\nIn fact, we can use chunking in the character class problem we showed earlier:\n\n```python\n>>> data = (x for x in 'abcd123ghi')\n>>> for charset in chunked(data, lambda c: c.isdigit()):\n...     print list(charset)\n...     \n['a', 'b', 'c', 'd']\n['1', '2', '3']\n['g', 'h', 'i']\n```\n\n## Batching\n\nThe `batch` method is a simplification of a common use for `itertools.islice`.\n\nSuppose your generator yields records that you're reading from a file, or a\ndatabase. Suppose that there may be many hundreds of thousands of records, or\neven millions, so you can't fit them all into memory, and you need to do them in\nbatches of 1000.\n\nHere's one way to do this using `islice`:\n\n```python\nfrom itertools import islice\nCHUNK = 1000\nwhile True:\n    # Listify the records so that we can check if\n    # there were any returned.\n    chunk = list(islice(records, CHUNK))\n    if not chunk:\n        break\n    \n    # Process the records in this chunk\n    for record in chunk:\n        process(record)\n```\n\nOr the iterstuff `batch` function will do this for you in a simpler way:\n\n```python\nfrom iterstuff import batch\nCHUNK = 1000\nfor chunk in batch(records, CHUNK):\n    # Chunk is an iterable of up to CHUNK records\n    for record in chunk:\n        process(record)\n```\n    \nHere's an elegant `batch` solution provided by Hamish Lawson for ActiveState recipes:\n[http://code.activestate.com/recipes/303279-getting-items-in-batches/](http://code.activestate.com/recipes/303279-getting-items-in-batches/))\n\n```python\nfrom itertools import islice, chain\ndef batch(iterable, size):\n    sourceiter = iter(iterable)\n    while True:\n        batchiter = islice(sourceiter, size)\n        yield chain([batchiter.next()], batchiter)\n```\n        \nNote how this uses a call to `batchiter.next()` to cause `StopIteration` to be\nraised when the source iterable is exhausted. Because this consumes an element,\n`itertools.chain` needs to be used to 'push' that element back onto the head\nof the chunk. Using a Lookahead allows us to peek at the next element of the\niterable and avoid the push.  Here's how `iterstuff.batch` works:\n\n```python\ndef batch(iterable, size):\n    # Wrap an enumeration of the iterable in a Lookahead so that it\n    # yields (count, element) tuples\n    it = Lookahead(enumerate(iterable))\n\n    while not it.atend:\n        # Set the end_count using the count value\n        # of the next element.\n        end_count = it.peek[0] + size\n\n        # Yield a generator that will then yield up to\n        # 'size' elements from 'it'.\n        yield (\n            element\n            for counter, element in repeatable_takewhile(\n                # t[0] is the count part of each element\n                lambda t: t[0] < end_count,\n                it\n            )\n        )\n```\n\n## A Conclusion\n\nPython generators are a wonderful, powerful, flexible language feature. The\n`atend` and `peek` properties of the Lookahead class enable a whole set of \nsimple recipes for working with generators.\n\nYou can see examples of use in the unit tests for this package, and run them\nby executing the `tests.py` file directly.\n\n## Thanks\n...to the Engineering Gang at Mobify\n...to https://github.com/landonjross for Python3 support\n\n"
 },
 {
  "repo": "facebookarchive/StateService",
  "language": "Python",
  "readme_contents": "# StateService\n\nStateService is a state machine-as-a-service that reports the state of one or more machines, so that machines can decide the next best step to ensure stability. StateService works with configuration management software to provide self-healing and automated recovery capabilities.\n\n## Examples\n\nStateService can serve an explicit and/or implicit state machine.\n\n### Explicit State Machine\n\nStateService can read a state machine defined explicity in YAML.\n\n```yaml\n--- states.yaml ---\ncurrent_state: green_state\nstates:\n  - name: green_state\n    func: increment\n    current:\n      key: count\n      value: 0\n    target:\n      name: red_state\n      when:\n        key: count\n        value: 1\n  - name: red_state\n    func: increment\n    current:\n      key: count\n      value: 0\n    target:\n      name: blue_state\n      when:\n        key: count\n        value: 1\n  - name: blue_state\n```\n\nTo use, start an instance of StateService.\n\n```sh\n> ./state_service --machine states.yaml &> /dev/null &\n```\n\nUse cURL to query and update the state machine defined by `states.yaml`.\n\n```sh\n# Confirming initial state (green)...\n> curl -sIX GET http://localhost:5000/state?state=green_state | grep 200\n# Output is 200 OK\n# Transition from green state to new state (red)...\n> curl -sIX PUT http://localhost:5000/state?state=green_state\n# Confirming new state (red)...\n> curl -sIX GET http://localhost:5000/state?state=red_state | grep 200\n# Output is 200 OK\n# Confirming old state (green) is inactive...\n> curl -sIX GET http://localhost:5000/state?state=green_state | grep 406\n# Output is 406\n# Transitioning from red state to new state (blue)...\"\n> curl -sIX PUT http://localhost:5000/state?state=red_state\n# Confirming new state (blue)...\n> curl -sIX GET http://localhost:5000/state?state=blue_state | grep 200\n# Output is 200 OK\n```\n\nStateService provides two ways to update its state machine. The first is as above: external HTTP requests cause updates. The second uses a state machine that contains states whose transitions are described using time; in this case, StateService updates its state machine automatically (see 'Asynchronous State Machines' below).\n\n### Implicit State Machine\n\nStateService hosts machine-learning models and responds to requests from `POST /state` to predict the state of the machine making the request.\n\nStart an instance of StateService.\n\n```sh\n> ./state_service --config /path/to/conf --models /path/to/models &> /dev/null &\n```\n\nwhere the path to configuration files contains the following JSON file\n\n```json\n{\n    \"name\": \"colors\",\n    \"team\": \"state_service\",\n    \"model\": \"colors_v1.pkl\",\n    \"states\": [\n        \"green\",\n        \"red\",\n        \"blue\"\n    ]\n}\n```\n\nUse cURL to query the implicit state machine. Given a JSON file, `models.json`,\n\n```json\n{\n    \"name\": \"colors\", // References the configuration file and finds the target model\n    \"values\": [[500, 0]] // Suitable values for the target model's `predict` method\n}\n```\n\n```sh\n> curl -X POST -d @models.json http://localhost:5000/state\ngreen\n```\n\n## Requirements\n\nStateService requires:\n\n- Linux or macOS\n- Python 3.6+\n\n## Installing StateService\n\nClone this git repository and use `pip` to install its dependencies.\n\n```sh\n# Clone this git repository\n> git clone https://github.com/facebookincubator/StateService.git\n# Create a virtualenv\n> python -m venv state_service\n> . state_service/bin/activate\n# Use `pip` to install dependencies\n> cd StateService\n> pip install -r requirements.txt\n```\n\n## Building StateService\n\nWe recommend using [xar](https://github.com/facebookincubator/xar) to build StateService. Although StateService's dependencies as wheels are not available (namely, 'itsdangerous' and 'python-click'), `xar` files will provide a single binary for deploying StateService.\n\nCheck [Python Wheels](https://www.pythonwheels.com) for updates.\n\n## Testing StateService\n\nWe use `pytest` and included a `Makefile`, so that you can run\n\n```sh\n> make test\n```\n\nto test StateService.\n\n## How StateService works\n\nStateService is a Flask application that can be configured as an explicit and/or implicit state machine.\n\nStateService, as an explicit state machine, listens for GET and PUT requests and responds with HTTP status codes (200, 406, or 500). These status codes represent a YES/NO response when a machine queries the current state or wants to update the current state.\n\nAs an implicit state machine, StateService listens for POST requests and responds with a state value that is determined by a machine-learning model.\n\n## Full documentation\n\n### StateService and Explicit State Machines\n\nStateService reads a list of states from a YAML file and identifies an initial state for a machine (or machines). The state machine can be synchronous or asynchronous.\n\n#### Synchronous State Machines\n\nAn example of a synchronous state machine is:\n\n```yaml\ncurrent_state: green_state\nstates:\n  - name: green_state\n    func: increment\n    current:\n      key: count\n      value: 0\n    target:\n      name: red_state\n      when:\n        key: count\n        value: 1\n  - name: red_state\n    func: increment\n    current:\n      key: count\n      value: 0\n    target:\n      name: blue_state\n      when:\n        key: count\n        value: 1\n  - name: blue_state\n```\n\nIn this example, the current state is `green_state`. `green_state` is defined fully with its name, a `func` attribute (`increment`), its current attributes (a key/value pair that describes a `count` key with a value of 0), and a target state to which it transitions when `green_state`'s `count` becomes 1. The `func` key describes a method on the `State` class (see `state.py`).\n\n#### Asynchronous State Machines\n\nTo program an asynchronous state machine, describe a state machine as above, but assign the `func` key with a `time` value:\n\n```yaml\n...\nstates:\n  - name: green_state\n    func: time\n    target:\n      name: red_state\n      when:\n        key: clock\n        value: '3000-01-01T12:00:00'\n...\n```\n\ndescribes `green_state` that will transition to `red_state` after midday on January 1, 3000. A `current` key is not necessary when the `time` function is used (it is implied that the current `clock` value is the current time on the machine that's running StateService).\n\n---\n\nTwo `func` methods are defined: `increment` and `time`. In the case of `increment`, the method increments the current state's `key` by 1; `time` provides the state machine with the ability to transition states automatically depending on a specific time.\n\nThe final state of a state machine is described only by its name (more precisely, it's identified by the absence of a `func` attribute).\n\n#### Integrating StateService with Configuration Management Software\n\nStateService provides a state-machine-as-a-service. StateService reads a linear state machine (described using YAML, as above) and records the current state as one or more machines query and update its state machine.\n\nAfter each update, StateService persists its state, so failures in the service do not result in inconsistent state (by default, file storage is used).\n\nStateService uses HTTP to integrate with software automation tools like Chef to coordinate state across several machines. For example, if one machine requires its group to be in a certain state before performing an action, it can query StateService from a Chef resource:\n\n```rb\n# Assuming a synchronous state machine\nchange_command = './change.sh'\nexecute 'change_machine' do\n  cwd home_dir\n  command \"#{change_command} && curl -K didChangeMachine.curl\"\n  only_if 'curl -K canChangeMachine.curl', :cwd => home_dir\nend\n```\n\n`canChangeMachine.curl` describes a GET request from StateService, e.g., `https://state_service/state?state=canChangeMachineState`. If this request returns 200, the Chef resource will proceed to execute the command; otherwise, this resource will not execute. After executing the command, `didChangeMachine.curl` is used to update StateService using a PUT request, e.g., `https://state_service/state?state=canChangeMachineState`.\n\nWhen the `func` value is `time`, the Chef resource is defined as:\n\n```rb\n# Assuming an asynchronous state machine\nchange_command = './change.sh'\nexecute 'change_machine' do\n  cwd home_dir\n  command change_command\n  only_if 'curl -K canChangeMachine.curl', :cwd => home_dir\nend\n```\n\nwhere `canChangeMachine.curl` describes a GET request to StateService. This is consistent with the intention that, when state transitions are scheduled at a certain time, we only need to request the current state.\n\n### StateService and Implicit State Machines\n\nTo use StateService as an implicit state machine, create JSON files that describe the models available to StateService.\n\n```json\n{\n    \"name\": \"colors\",\n    \"team\": \"state_service\",\n    \"model\": \"colors_v1.pkl\",\n    \"states\": [\n        \"green\",\n        \"red\",\n        \"blue\"\n    ]\n}\n```\n\nThe above JSON file provides a `name` that will be provided in a `POST /state` request to identify the model, a `team` that is used to identify the subdirectory to search for the `model`, a serialized version of a ML model instance, and a list of `states`. The ML model instance must provide a `predict` method of the form:\n\n```py\ndef predict(self, values):\n    ...\n```\n\nThis method must return a single-element `list` containing an integer that references one of the states in the JSON file above.\n\n## Contributing\n\nSee the CONTRIBUTING file for how to help out and read our Code of Conduct (CODE\\_OF\\_CONDUCT.md).\n\n## License\n\nStateService is MIT licensed, as found in the LICENSE file.\n"
 },
 {
  "repo": "clintecker/django-google-analytics",
  "language": "Python",
  "readme_contents": "Google Analytics for Django Projects\n====================================\n\nI manage a lot of Django projects that present slightly-different forms to \nusers depending on the site/domain they're visiting.  There's also a bunch of \ncustom submission code that differs from form to form, but that's neither here\nnor there.\n\nI need different Google Analytics codes depending on the sites and after \nsticking these tags into every single template, I thought it would be cool to \nbe able to manage these Google analytics accounts from the Django admin page. \nI also added a mode of operation that excludes the admin interface altogether \n(you can just use the template tag)\n\n\n## Two modes of operation ##\n\n### Administering and associating codes with Django `Sites` framework ###\n\n1. Add the `google_analytics` application to your `INSTALLED_APPS` section of your `settings.py`.  This mode requires that you be using the Django sites framework too, so make sure you have that set up as well.\n2. Add `GOOGLE_ANALYTICS_MODEL = True` to your `settings.py` \n3. Run a `./manage.py syncdb` to add the database tables\n4. Go to your project's admin page (usually `/admin/`) and click into a site objects\n5. You'll now see a new field under the normal site information called \"Analytics Code\". In this box you put your unique analytics code for your project's domain.  It looks like `UA-xxxxxx-x` and save the site.\n6. In your base template (usually a `base.html`) insert this tag at the very top: `{% load analytics %}`\n7. In the same template, insert the following code right before the closing body tag: `{% analytics %}`\n\n### Just using the template tag ###\n\n\n1. Add the `google_analytics` application to your `INSTALLED_APPS` section of your `settings.py`.\n2. In your base template, usually a `base.html`, insert this tag at the very top: `{% load analytics %}`\n3. In the same template, insert the following code right before the closing body tag: `{% analytics \"UA-xxxxxx-x\" %}` the `UA-xxxxxx-x` is a unique Google Analytics code for you domain when you sign up for a new account.\n\n\n## Asynchronous Tracking ##\n\nGoogle's recent asynchronous tracking API is also supported, as of v0.2.  To use it,\nsimply call the templatetag `{% analytics_async %}` in the document head instead\nof calling `{% analytics %}` just before the end of the body.  You can also use\n`{% analytics_async \"UA-xxxxxx-x\" %}` as with the old templatetags.\n\nThis is being added as an option rather than a replacement for two reasons:\n\n1. Google recommends the Asynchronous Tracking snippet goes in the <head> tag, while\n   the old snippet goes at the end of the <body> tag, so as not to disrupt page loading.\n   Therefore it is not a drop in replacement in your template\n2. The new snippet is reported to [break sites that have a comment before the head tag](http://www.stevesouders.com/blog/2009/12/01/google-analytics-goes-async/#comment-1171). \n   Adding the asynchronous tracking to existing code would cause backwards \n   incompatiblity.\n\n## Tracking Page-Load Time ##\n\nGoogle has added [page-load tracking](http://www.google.com/support/analyticshelp/bin/answer.py?hl=en&answer=1205784&topic=1120718).\nTo use this feature add the following to your settings file:\n\n    GOOGLE_ANALYTICS_TRACK_PAGE_LOAD_TIME = True\n\n## Supporting other tracking methods ##\n\nSometimes, the built-in code snippets are not sufficient for what you want to\ndo with Google Analytics.  You might need to use different access methods,\nor to support more complex Google Analytics functionality.  Fortunately, using \ndifferent code snippets is dead easy, and there are two ways to do it.\n\n\n### Overriding the analytics template ###\n\nThe easiest way is to override the `'google_analytics/analytics_template.html'`\ntemplate in a template directory that gets loaded before the one in the \n`google_analytics` app.  \n\n\n### Registering a new analytics tag ###\n\nYou may want to keep the existing snippets around, while adding a new method.\nPerhaps some of your pages need one snippet, but other pages need a different\none.  In this case all you have to do is register a new tag in your tag \nlibrary using `do_get_analytics` like so:\n\n    from django import template\n    from google_analytics.templatetags import analytics\n\n    register = template.Library()\n    register.tag('my_analytics', analytics.do_get_analytics)\n    \nThen create a template at `'google_analytics/%(tag_name)s_template.html'`. \nIn this case the template name would be \n`'google_analytics/my_analytics_template.html'`.  Pass the variable \n`{{ analytics_code }}` to the template wherever you need it.\n\nThe new tag will have all the same properties as the default tag, supporting\nsite-based analytics codes, as well as explicitly defined codes.\n\nThe best way to do this is to create a tiny app just for this purpose, so \nyou don't have to modify the code in `google_analytics`.  Just put the above\ncode in `[app_name]/templatetags/[tag_library_name].py`.  Then put your \ntemplate in `[app_name]/templates/google_analytics/[template_name]`.  If your \napp is named `my_analytics_app`, your tag library is named `more_analytics`,\nand your tag is registered as `my_analytics`, the resulting app will have a \ndirectory structure like this:\n\n    my_analytics_app/\n    +-- templatetags/\n    |   +-- __init__.py\n    |   \\-- more_analytics.py\n    \\-- templates/\n         \\-- google_analytics/\n             \\-- my_analytics_template.html\n         \nFinally, add `'my_analytics_app'` to `INSTALLED_APPS` in your `settings.py` file.  Your new tag is \nready to go.  To use the tag, put `{% load more_analytics %}` at the head of \nyour template.  You can now access the `{% my_analytics %}` tag the same way \nyou would use `{% analytics %}`.\n\n\n## License ##\n\nThe MIT License\n\nCopyright (c) 2009 Clint Ecker\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
 },
 {
  "repo": "francoisgermain/SpeechDenoisingWithDeepFeatureLosses",
  "language": "Python",
  "readme_contents": "<a name=\"top\"></a>\n#  Speech Denoising with Deep Feature Losses ([arXiv](https://arxiv.org/abs/1806.10522), [sound examples](https://ccrma.stanford.edu/~francois/SpeechDenoisingWithDeepFeatureLosses/))\nThis is a Tensorflow implementation of our [Speech Denoising Convolutional Neural Network trained with Deep Feature Losses](https://arxiv.org/abs/1806.10522).\n\nContact: [Fran\u00e7ois Germain](mailto:francois@ccrma.stanford.edu)\n\n## Table of contents\n1. [Citation](#citation)\n2. [Setup](#setup)\n3. [Denoising scripts](#scripts)\n4. [Models](#models)\n5. [Noisy data](#data)\n6. [Deep feature loss](#feature-loss)\n7. [Notes](#notes)\n8. [SoX installation instructions](#sox-install)\n\n<a name=\"citation\"></a>\n## Citation\nIf you use our code for research, please cite our paper:\nFran\u00e7ois G. Germain, Qifeng Chen, and Vladlen Koltun. Speech Denoising with Deep Feature Losses. [arXiv:1806.10522](https://arxiv.org/abs/1806.10522). 2018.\n\n### License\nThe source code is published under the MIT license. See [LICENSE](./LICENCE) for details. In general, you can use the code for any purpose with proper attribution. If you do something interesting with the code, we'll be happy to know. Feel free to contact us.\n\n[Top](#top)\n\n<a name=\"setup\"></a>\n## Setup\n\n### Requirement\nRequired python libraries: Tensorflow with GPU support (>=1.4) + Scipy (>=1.1) + Numpy (>=1.14) + Tqdm (>=4.0.0). To install in your python distribution, run\n\n`pip install -r requirements.txt`\n\nWarning: Make sure your libraries (Cuda, Cudnn,...) are compatible with the tensorflow version you're using or the code will not run.\n\nRequired software (for resampling): [SoX](http://sox.sourceforge.net/) ([Installation instructions](#sox-install))\n\n**Important note:** _At the moment, this algorithm requires using **32-bit floating-point** audio files to perform correctly_. You can use sox to convert your file. To convert `audiofile.wav` to 32-bit floating-point audio at 16kHz sampling rate, run:\n\n`sox audiofile.wav -r 16000 -b 32 -e float audiofile-float.wav`\n\nTested in Ubuntu + Intel i7 CPU + Nvidia Titan X (Pascal) with Cuda (>=8.0) and CuDNN (>=5.0). CPU mode should also work with minor changes.\n\n### Quick start (testing)\n\nFor a quick testing, you can download the default validation data by running:\n\n`./download_sedata_onlyval.sh`\n\nfollowed by:\n\n`python senet_infer.py`\n\nThe denoised files will be stored in the folder _dataset/valset_noisy\\_denoised/_, with the same name as the corresponding source files in _dataset/valset_noisy/_.\n\n### Default data download\n\nIn order to run our algorithm with default parameters, you need to download the noisy dataset from Edinburgh DataShare (see below). The dataset can be automatically downloaded and pre-processed (i.e. resampled at 16kHz) by running the script\n\n`./download_sedata.sh`\n\nTo download only the testing data, you can run the reduced script:\n\n`./download_sedata_onlyval.sh`\n\n### Using custom data\n\nIf you want to use your own data for _testing_, you need to put all the .wav files in a single folder.\n\nIf you want to use your own data for _training_, you need to put your data in a single top folder. In that folder, you should have 4 individual folders:\n\n- _trainset\\_noisy/_ (for the noisy speech training files),\n- _trainset\\_clean/_ (for the ground truth clean speech training files),\n- _valset\\_noisy/_ (for the noisy validation files), and\n- _valset\\_clean/_ (for the noisy validation files).\n\nThe validation folders may be empty but they must exist. Matching files in the corresponding noisy and clean folders must have the same name.\n\nThe audio data *must be sampled at 16kHz* (you can resample your data using SoX - see download\\_data.sh for an example).\n\n[Top](#top)\n\n## Denoising scripts <a name=\"scripts\"></a>\n\n### Testing with default parameters\n\nOnce you've downloaded in the script download_data.sh, you can directly process the testing dataset by running\n\n`python senet_infer.py`\n\nThe denoised files will be stored in the folder _dataset/valset_noisy\\_denoised/_, with the same name as the corresponding source files in _dataset/valset_noisy/_.\n\nIn our configuration, the algorithm allocates ~5GB of memory on the GPU for training. Running the code as is on GPUs with less memory may fail.\n\n### Testing with custom data and/or denoising model\n\nIf you have custom testing data (_formatted as described above_) stored in a folder _foldername/_ and/or a custom denoising model *with names* _se\\_model.ckpt.*_ stored in a folder _model\\_folder/_, you can test that model on that data by running:\n\n`python senet_infer.py -d folder_name -m model_folder`\n\nThe denoised files will be stored in the folder _folder\\_name\\_denoised/_, with the same name as the corresponding source files.\n\nWarning: At this time, when using a custom model, you must make sure that the system parameters in senet\\_infer.py match the ones used in the stored denoising model or the code won't run properly (if running at all).\n\n### Training with default parameters\n\nOnce you've downloaded in the script download_data.sh, you can directly train a model using the training dataset by running\n\n`python senet_train.py`\n\nThe trained model will be stored in the root folder with the names _se\\_model.ckpt.*_.\n\nIn our configuration, the algorithm allocates ~5GB of memory on the GPU for training. Running the code as is on GPUs with less memory may fail.\n\n### Training with custome data and/or feature loss model\n\nIf you have custom training data (_formatted as described above_) stored in a folder _foldername/_ and/or a custom feature loss model *with names* _loss\\_model.ckpt.*_ stored in a folder _loss\\_folder/_, you can train a speech denoising model on that data using that feature loss model by running:\n\n`python senet_train.py -d folder_name -l loss_folder -o out_folder`\n\nThe trained model will be stored in folder _out\\_folder/_ (default is root folder) with the names _se\\_model.ckpt.*_.\n\nWarning: At this time, when using a custom loss model, you must make sure that the system parameters in senet\\_train.py match the ones used in the stored loss model or the code won't run properly (if running at all).\n\n[Top](#top)\n\n<a name=\"models\"></a>\n## Models\n\nThe deep feature loss network graph and parameters are stored in the models/loss\\_model.ckpt.* files.\n\nThe denoising network graph and parameters are stored in the models/se\\_model.ckpt.* files.\nThis model was trained following the procedure described in our associated paper. The current training script se\\_train.py is parameterized in such a way that an identical training procedure as in our associated paper would be performed on the specified training dataset.\n\n[Top](#top)\n\n <a name=\"data\"></a>\n## Noisy data\n\nThe data used to train and test our system is available publicly on the Edinburgh DataShare website at [https://datashare.is.ed.ac.uk/handle/10283/2791](https://datashare.is.ed.ac.uk/handle/10283/2791). Information on how the dataset is constructed can be found in [Valentini-Botinhao et al., 2016](https://www.research.ed.ac.uk/portal/files/26581510/SSW9_Cassia_1.pdf). The dataset was used without alteration except for resampling at 16kHz.\n\n[Top](#top)\n\n<a name=\"feature-loss\"></a>\n## Deep feature loss training\n\nWe also provide scripts to (re-)train the loss model. As of know, using the two classification tasks described in our paper is hard-coded.\n\n### Data\n\nOur feature loss network is trained on the _acoustic scene classification_ and _domestic audio tagging_ tasks of the [DCASE 2016 Challenge(https://www.cs.tut.fi/sgn/arg/dcase2016/). Downloading and pre-processing (i.e., downsampling to 16kHz) the corresponding data can be done by running the script:\n\n`./download_lossdata.sh`\n\nWarning: The training script expects the data at the locations set in the downloading script.\n\n[Top](#top)\n\n### Training script\n\nOnce the data is downloaded, you can (re-)train a deep feature loss model by running:\n\n`python lossnet_train.py`\n\nThe loss model is stored in the root folder by default. A custom output directory for loss model can be specified as:\n\n`python lossnet_train.py -o out_folder`\n\n[Top](#top)\n\n<a name=\"notes\"></a>\n## Notes\n\n * Currently, the download scripts are only provided for UNIX-like systems (Linux & Mac OSX). If you plan on running our algorithm on Windows, please contact us and/or download and resample the data \"by hand\".\n\n* Currently, dilation for 1-D layers is not properly implemented in the Tensorflow [slim library](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) we use. The functions _signal\\_to\\_dilated_ and _dilated\\_to\\_signal in helper.py allows to transform a 1-D layer into an interlaced 2-D layer such that undilated convolution on the 2-D layer is equivalent to dilated convolution on the 1-D layer.\n\n[Top](#top)\n\n<a name=\"sox-install\"></a>\n## SoX installation instructions\n\nThe latest version of SoX can be found on their SourceForge page at [https://sourceforge.net/projects/sox/files/sox/](https://sourceforge.net/projects/sox/files/sox/) (Go to the folder corresponding to the latest version). Below are additional details regarding the installations for many common operating systems.\n\n### Linux\n\n#### Ubuntu\n\nAs of June 13, 2018, SoX can be installed from the Ubuntu repositories by running in a terminal:\n\n`sudo apt-get install sox`\n\n#### Fedora\n\nAs of June 13, 2018, SoX can be installed from the Fedora repositories by running in a terminal:\n\n`sudo yum install sox`\n\n### Mac OSX\n\n#### Homebrew\n\nIf you have Homebrew installed, just run in a terminal:\n\n`brew install sox`\n\n#### Macports\n\nIf you have Macports installed, just run in a terminal:\n\n`port install sox`\n\nYou may need to run the command with root priviledges, in which case, run in a terminal:\n\n`sudo port install sox`\n\n#### Pre-compiled version\n\nSoX provides a pre-compiled executable for Mac OSX. You can download it at [https://sourceforge.net/projects/sox/files/sox/14.4.2/sox-14.4.2-macosx.zip/download](https://sourceforge.net/projects/sox/files/sox/14.4.2/sox-14.4.2-macosx.zip/download).\n\nThen unzip the downloaded archive and move the extracted folder to your _Applications_ folder.\n\nThe last step is to add that folder to your path. To do so, run in a terminal:\n\n```\ncd ~\necho \"\" >> .bash_profile\necho \"# Adding SoX to path\" >> .bash_profile\necho \"export PATH=\\$PATH:/Applications/sox-14.4.1\" >> .bash_profile\nsource .bash_profile\n```\n\nWarning: The executable hasn't been updated since 2015 so consider using one of the two options above instead or compile from sources if the executable fails\n\n### Install from sources (Unix-like systems)\n\nDownload sources from the terminal using:\n\n`wget https://sourceforge.net/projects/sox/files/sox/14.4.2/sox-14.4.2.tar.gz/download`\n\nUn-compress the archive:\n\n`tar -zxvf sox-14.4.2.tar.gz`\n\nGo into the folder with the extracted files:\n\n`cd sox-14.4.2`\n\nCompile and install SoX:\n\n```\n./configure\nmake -s\nmake install\n```\n\nWarning: Make sure there are no space in any of the folder name on the path of the source files or the building will fail.\n\n### Windows\n\nFollow instructions provided [here](https://github.com/JoFrhwld/FAVE/wiki/Sox-on-Windows). If you need additional assistance, please contact us.\n\n[Top](#top)\n"
 },
 {
  "repo": "thunlp/HATT-Proto",
  "language": "Python",
  "readme_contents": "# Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation Classification\n\nCode and data for AAAI2019 paper [Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation Classification](https://gaotianyu1350.github.io/assets/aaai2019_hatt_paper.pdf).\n\nAuthor: Tianyu Gao*, Xu Han*, Zhiyuan Liu, Maosong Sun. (\\* means equal contribution)\n\n## Dataset and Word Embedding\n\nWe evaluate our models on [FewRel](https://thunlp.github.io/fewrel), a large-scale dataset for few-shot relation classification. It has 100 relations and 700 instances for each relation. You can find some baseline models from [here](https://github.com/thunlp/fewrel).\n\nDue to the large size, we did not upload the glove file (pre-trained word embedding). Please download `glove.6B.50d.json` from [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b14bf0d3c9e04ead9c0a/?dl=1) or [Google Drive](https://drive.google.com/open?id=1UnncRYzDpezPkwIqhgkVW6BacIqz6EaB) and put it under `data/` folder.\n\n## Usage\n\nTo run our code, use this command for training\n```bash\npython train.py {MODEL_NAME} {N} {K} {NOISE_RATE}\n```\nand use this command for testing\n```bash\npython test.py {MODEL_NAME} {N} {K} {NOISE_RATE}\n```\nwhere {MODEL_NAME} could be `proto` or `proto_hatt`, `{N}` is the num of classes, `{K}` is the num of instances for each class and `{NOISE_RATE}` is the probability that one instance is wrong-labeled.\n"
 },
 {
  "repo": "avian2/jsonmerge",
  "language": "Python",
  "readme_contents": "Merge a series of JSON documents\n================================\n\nThis Python module allows you to merge a series of JSON documents into a\nsingle one.\n\nThis problem often occurs for example when different authors fill in\ndifferent parts of a common document and you need to construct a document\nthat includes contributions from all the authors. It also helps when\ndealing with consecutive versions of a document where different fields get\nupdated over time.\n\nConsider a trivial example with two documents::\n\n    >>> base = {\n    ...         \"foo\": 1,\n    ...         \"bar\": [ \"one\" ],\n    ...      }\n\n    >>> head = {\n    ...         \"bar\": [ \"two\" ],\n    ...         \"baz\": \"Hello, world!\"\n    ...     }\n\nWe call the document we are merging changes into *base* and the changed\ndocument *head*. To merge these two documents using *jsonmerge*::\n\n    >>> from pprint import pprint\n\n    >>> from jsonmerge import merge\n    >>> result = merge(base, head)\n\n    >>> pprint(result, width=40)\n    {'bar': ['two'],\n     'baz': 'Hello, world!',\n     'foo': 1}\n\nAs you can see, when encountering an JSON object, *jsonmerge* by default\nreturns fields that appear in either *base* or *head* document. For other\nJSON types, it simply replaces the older value. These principles are also\napplied in case of multiple nested JSON objects.\n\nIn a more realistic use case however, you might want to apply different\n*merge strategies* to different parts of the document. You can tell\n*jsonmerge* how to do that using a syntax based on `JSON schema`_.\n\nIf you already have schemas for your document, you can simply expand them\nwith some additional keywords. Apart from the custom keywords described\nbelow, *jsonmerge* by default uses the schema syntax defined in the `Draft\n4`_ of the JSON schema specification.\n\nYou use the *mergeStrategy* schema keyword to specify the strategy. The\ndefault two strategies mentioned above are called *objectMerge* for objects\nand *overwrite* for all other types.\n\nLet's say you want to specify that the merged *bar* field in the example\ndocument above should contain elements from all documents, not just the\nlatest one. You can do this with a schema like this::\n\n    >>> schema = {\n    ...             \"properties\": {\n    ...                 \"bar\": {\n    ...                     \"mergeStrategy\": \"append\"\n    ...                 }\n    ...             }\n    ...         }\n\n    >>> from jsonmerge import Merger\n    >>> merger = Merger(schema)\n    >>> result = merger.merge(base, head)\n\n    >>> pprint(result, width=40)\n    {'bar': ['one', 'two'],\n     'baz': 'Hello, world!',\n     'foo': 1}\n\nAnother common example is when you need to keep a versioned list of values\nthat appeared in the series of documents::\n\n    >>> schema = {\n    ...             \"properties\": {\n    ...                 \"foo\": {\n    ...                     \"type\": \"object\",\n    ...                     \"mergeStrategy\": \"version\",\n    ...                     \"mergeOptions\": { \"limit\": 5 }\n    ...                 }\n    ...             },\n    ...             \"additionalProperties\": False\n    ...         }\n    >>> from jsonmerge import Merger\n    >>> merger = Merger(schema)\n\n    >>> rev1 = {\n    ...     'foo': {\n    ...         'greeting': 'Hello, World!'\n    ...     }\n    ... }\n\n    >>> rev2 = {\n    ...     'foo': {\n    ...         'greeting': 'Howdy, World!'\n    ...     }\n    ... }\n\n    >>> base = None\n    >>> base = merger.merge(base, rev1, merge_options={\n    ...                     'version': {\n    ...                         'metadata': {\n    ...                             'revision': 1\n    ...                         }\n    ...                     }\n    ...                 })\n    >>> base = merger.merge(base, rev2, merge_options={\n    ...                     'version': {\n    ...                         'metadata': {\n    ...                             'revision': 2\n    ...                         }\n    ...                     }\n    ...                 })\n    >>> pprint(base, width=55)\n    {'foo': [{'revision': 1,\n              'value': {'greeting': 'Hello, World!'}},\n             {'revision': 2,\n              'value': {'greeting': 'Howdy, World!'}}]}\n\nNote that we use the *mergeOptions* keyword in the schema to supply\nadditional options to the merge strategy. In this case, we tell the\n*version* strategy to retain only 5 most recent versions of this field.\n\nWe also used the *merge_options* argument to supply some options that are\nspecific to each call of the *merge* method. Options specified this\nway are applied to all invocations of a specific strategy in a schema (in\ncontrast to *mergeOptions*, which applies only to the strategy invocation\nin that specific location in the schema). Options specified in\n*mergeOptions* schema keyword override the options specified in the\n*merge_options* argument.\n\nThe *metadata* option for the *version* strategy can contain some document\nmeta-data that is included for each version of the field. *metadata* can\ncontain an arbitrary JSON object.\n\nExample above also demonstrates how *jsonmerge* is typically used when\nmerging more than two documents. Typically you start with an empty *base*\nand then consecutively merge different *heads* into it.\n\nA common source of problems are documents that do not match the schema used\nfor merging. *jsonmerge* by itself does not validate input documents. It\nonly uses the schema to obtain necessary information to apply appropriate merge\nstrategies. Since the default strategies are used for parts of the\ndocument that are not covered by the schema it's easy to get unexpected\noutput without any obvious errors raised by *jsonmerge*.\n\nIn the following example, the property *Foo* (uppercase F) does not match\n*foo* (lowercase f) in the schema and hence the *version* strategy is not\napplied as with previous two revisions::\n\n    >>> rev3 = {\n    ...     'Foo': {\n    ...         'greeting': 'Howdy, World!'\n    ...     }\n    ... }\n\n    >>> base = merger.merge(base, rev3, merge_options={\n    ...                     'version': {\n    ...                         'metadata': {\n    ...                             'revision': 3\n    ...                         }\n    ...                     }\n    ...                 })\n\n    >>> pprint(base, width=55)\n    {'Foo': {'greeting': 'Howdy, World!'},\n     'foo': [{'revision': 1,\n              'value': {'greeting': 'Hello, World!'}},\n             {'revision': 2,\n              'value': {'greeting': 'Howdy, World!'}}]}\n\nHence it is recommended to validate the input documents against the schema\nbefore passing them to *jsonmerge*. This practice is even more effective if\nthe schema is filled in with more information than strictly necessary for\n*jsonmerge* (e.g. adding information about types, restrict valid object\nproperties with *additionalProperties*, etc.)::\n\n    >>> from jsonschema import validate\n    >>> validate(rev1, schema)\n    >>> validate(rev2, schema)\n    >>> validate(rev3, schema)\n    Traceback (most recent call last):\n        ...\n    jsonschema.exceptions.ValidationError: Additional properties are not allowed ('Foo' was unexpected)\n\nIf you care about well-formedness of your documents, you might also want to\nobtain a schema for the documents that the *merge* method creates.\n*jsonmerge* provides a way to automatically generate it from a schema for\nthe input document::\n\n    >>> result_schema = merger.get_schema()\n\n    >>> pprint(result_schema, width=80)\n    {'additionalProperties': False,\n     'properties': {'foo': {'items': {'properties': {'value': {'type': 'object'}}},\n                            'maxItems': 5,\n                            'type': 'array'}}}\n\nNote that because of the *version* strategy, the type of the *foo* field\nchanged from *object* to *array*.\n\n\nMerge strategies\n----------------\n\nThese are the currently implemented merge strategies.\n\noverwrite\n  Overwrite with the value in *base* with value in *head*. Works with any\n  type.\n\ndiscard\n  Keep the value in *base*, even if *head* contains a different value.\n  Works with any type.\n\n  By default, if *base* does not contain any value (i.e. that part of the\n  document is undefined), the value after merge is kept undefined. This can\n  be changed with the *keepIfUndef* option. If this option is *true*, then\n  the value from *head* will be retained in this case. This is useful if\n  you are merging a series of documents and want to keep the value that\n  first appears in the series, but want to discard further modifications.\n\nappend\n  Append arrays. Works only with arrays.\n\narrayMergeById\n  Merge arrays, identifying items to be merged by an ID field. Resulting\n  arrays have items from both *base* and *head* arrays.  Any items that\n  have identical an ID are merged based on the strategy specified further\n  down in the hierarchy.\n\n  By default, array items are expected to be objects and ID of the item is\n  obtained from the *id* property of the object.\n\n  You can specify an arbitrary *JSON pointer* to point to the ID of the\n  item using the *idRef* merge option. When resolving the pointer, document\n  root is placed at the root of the array item (e.g. by default, *idRef* is\n  '/id'). You can also set *idRef* to '/' to treat an array of integers or\n  strings as a set of unique values.\n\n  Array items in *head* for which the ID cannot be identified (e.g. *idRef*\n  pointer is invalid) are ignored.\n\n  You can specify an additional item ID to be ignored using the *ignoreId*\n  merge option.\n\n  A compound ID can be specified by setting *idRef* to an array of\n  pointers. In that case, if *any* pointer in the array is invalid for an\n  object in *head*, the object is ignored. If using an array for *idRef*\n  and if *ignoreId* option is also defined, *ignoreId* must be an array as\n  well.\n\narrayMergeByIndex\n  Merge array items by their index in the array. Similarly to\n  *arrayMergeById* strategy, the resulting arrays have items from both\n  *base* and *head* arrays. Items that occur at identical positions in both\n  arrays will be merged based on the strategy specified further down in the\n  hierarchy.\n\nobjectMerge\n  Merge objects. Resulting objects have properties from both *base* and\n  *head*. Any properties that are present both in *base* and *head* are\n  merged based on the strategy specified further down in the hierarchy\n  (e.g. in *properties*, *patternProperties* or *additionalProperties*\n  schema keywords).\n\n  The *objClass* option allows one to request a different dictionary class\n  to be used to hold the JSON object. The possible values are names that\n  correspond to specific Python classes. Built-in names include\n  *OrderedDict*, to use the collections.OrderedDict class, or *dict*,\n  which uses the Python's dict built-in. If not specified, *dict* is\n  used by default.\n\n  Note that additional classes or a different default can be configured via\n  the Merger() constructor (see below).\n\nversion\n  Changes the type of the value to an array. New values are appended to the\n  array in the form of an object with a *value* property. This way all\n  values seen during the merge are preserved.\n\n  You can add additional properties to the appended object using the\n  *metadata* option. Additionally, you can use *metadataSchema* option to\n  specify the schema for the object in the *metadata* option.\n\n  You can limit the length of the list using the *limit* option in the\n  *mergeOptions* keyword.\n\n  By default, if a *head* document contains the same value as the *base*,\n  document, no new version will be appended. You can change this by setting\n  *ignoreDups* option to *false*.\n\nIf a merge strategy is not specified in the schema, *objectMerge* is used\nfor objects and *overwrite* for all other values (but see also the section\nbelow regarding keywords that apply subschemas).\n\nYou can implement your own strategies by making subclasses of\njsonmerge.strategies.Strategy and passing them to Merger() constructor\n(see below).\n\n\nThe Merger Class\n----------------\n\nThe Merger class allows you to further customize the merging of JSON\ndata by allowing you to:\n\n- set the schema containing the merge strategy configuration,\n- provide additional strategy implementations,\n- set a default class to use for holding JSON object data and\n- configure additional JSON object classes selectable via the *objClass*\n  merge option.\n\nThe Merger constructor takes the following arguments (all optional, except\nschema):\n\nschema\n   The JSON Schema that contains the merge strategy directives\n   provided as a JSON object.  An empty dictionary should be provided\n   if no strategy configuration is needed.\n\nstrategies\n   A dictionary mapping strategy names to instances of Strategy\n   classes.  These will be combined with the built-in strategies\n   (overriding them with the instances having the same name).\n\nobjclass_def\n   The name of a supported dictionary-like class to hold JSON data by\n   default in the merged result. The name must match a built-in name or one\n   provided in the *objclass_menu* parameter.\n\nobjclass_menu\n   A dictionary providing additional classes to use as JSON object\n   containers.  The keys are names that can be used as values for the\n   *objectMerge* strategy's *objClass* option or the *objclass_def*\n   argument. Each value is a function or class that produces an instance of\n   the JSON object container. It must support an optional dictionary-like\n   object as a parameter which initializes its contents.\n\nvalidatorclass\n    A *jsonschema.Validator* subclass. This can be used to specify which\n    JSON Schema draft version will be used during merge. Some details such\n    as reference resolution are different between versions. By default, the\n    Draft 4 validator is used.\n\n\nSupport for keywords that apply subschemas\n------------------------------------------\n\nComplex merging of documents with schemas that use keywords *allOf*,\n*anyOf* and *oneOf* can be problematic. Such documents do not have a\nwell-defined type and might require merging of two values of different\ntypes, which will fail for some strategies. In such cases *get_schema()*\nmight also return schemas that never validate.\n\nThe *overwrite* strategy is usually the safest choice for such schemas.\n\nIf you explicitly define a merge strategy at the same level as *allOf*,\n*anyOf* or *oneOf* keyword, then *jsonmerge* will use the defined strategy\nand not further process any subschemas under those keywords. The\nstrategy however will descend as usual (e.g. *objectMerge* will take into\naccount subschemas under the *properties* keyword at the same level as\n*allOf*).\n\nIf a merge strategy is not explicitly defined and an *allOf* or *anyOf*\nkeyword is present, *jsonmerge* will raise an error.\n\nIf a merge strategy is not explicitly defined and an *oneOf* keyword is\npresent, *jsonmerge* will continue on the branch of *oneOf* that validates\nboth *base* and *head*. If no branch validates, it will raise an error.\n\nYou can define more complex behaviors by defining for your own strategy\nthat defines what to do in such cases. See docstring documentation for the\n*Strategy* class on how to do that.\n\n\nSecurity considerations\n-----------------------\n\nA JSON schema document can contain *$ref* references to external schemas.\n*jsonmerge* resolves URIs in these references using the mechanisms provided\nby the *jsonschema* module. External references can cause HTTP or similar\nnetwork requests to be performed.\n\nIf *jsonmerge* is used on untrusted input, this may lead to vulnerabilities\nsimilar to the XML External Entity (XXE) attack.\n\n\nRequirements\n------------\n\n*jsonmerge* supports Python 2 (2.7) and Python 3 (3.5 and newer).\n\nYou need *jsonschema* (https://pypi.python.org/pypi/jsonschema) module\ninstalled.\n\n\nInstallation\n------------\n\nTo install the latest *jsonmerge* release from the Python package index::\n\n    pip install jsonmerge\n\n\nSource\n------\n\nThe latest development version is available on GitHub:\nhttps://github.com/avian2/jsonmerge\n\nTo install from source, run the following from the top of the source\ndistribution::\n\n    pip install .\n\n*jsonmerge* uses `Tox`_ for testing. To run the test suite run::\n\n    tox\n\n\nReporting bugs and contributing code\n------------------------------------\n\nThank you for contributing to *jsonmerge*! Free software wouldn't be\npossible without contributions from users like you. However, please consider\nthat I maintain this project in my free time. Hence I ask you to follow\nthis simple etiquette to minimize the amount of effort needed to include\nyour contribution.\n\nPlease use `GitHub issues`_ to report bugs. Make sure that your report\nincludes:\n\n* A *minimal*, but complete, code example that reproduces the problem,\n  including any JSON data required to run it. It should be something I can\n  copy-paste into a .py file and run.\n* Relevant version of *jsonmerge* - either release number on PyPi or git\n  commit hash.\n* Copy of the traceback, in case you are reporting an unhandled exception.\n* Example of what you think should be the correct output, in case you are\n  reporting wrong result of a merge or schema generation.\n\nPlease use `GitHub pull requests`_ to contribute code. Make sure that your\npull request:\n\n* Passes all existing tests and includes new tests that cover added code.\n* Updates *README.rst* to document added functionality.\n\n\nLicense\n-------\n\nCopyright 2021, Tomaz Solc <tomaz.solc@tablix.org>\n\nThe MIT License (MIT)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n.. _JSON schema: http://json-schema.org\n.. _Draft 4: http://json-schema.org/specification-links.html#draft-4\n.. _Tox: https://tox.readthedocs.io/en/latest/\n.. _GitHub issues: https://github.com/avian2/jsonmerge/issues\n.. _GitHub pull requests: https://github.com/avian2/jsonmerge/pulls\n\n..\n    vim: tw=75 ts=4 sw=4 expandtab softtabstop=4\n"
 },
 {
  "repo": "shymonk/django-datatable",
  "language": "Python",
  "readme_contents": "django-datatable\n================\n\n|Build Status| |PyPI|\n\n.. figure:: https://www.shymonk.com/django-datatable/static/django_datatable_example.png\n   :alt: preview\n\n`online demo <https://www.shymonk.com/django-datatable/datasource/model/>`__\n\nOverview\n--------\n\ndjango-datatable is a simple Django app to organize data in tabular\nform based on `datatable <http://datatables.net>`__ and\n`bootstrap <http://getbootstrap.com/>`__.\n\nIt is worth mentioning that the design of this project makes reference\nto `django-table2 <https://github.com/bradleyayers/django-tables2>`__\nand is mainly for the purpose of learning. I really appreciate anyone\nmaking a pull-request to improve it.\n\nRequirements\n------------\n\n-  Python 2.x\n\n-  jQuery 1.6+\n\n-  Django 1.5+\n\n-  Bootstrap 3.0\n\nQuick start\n-----------\n\n-  Setup Django-datatable application in Python environment:\n\n   ::\n\n       $ pip install django-datatable\n\n-  Define a simple model named Person:\n\n   ::\n\n       # example/app/models.py\n       class Person(models.Model):\n           name = models.CharField(max_length=100)\n\n-  Add \"table\" to your INSTALLED\\_APPS setting like this:\n\n   ::\n\n       INSTALLED_APPS = (\n           ...,\n           'table',\n       )\n\n-  Add some data so you have something to display in the table. Now\n   define a PersonTable class without any options in the table file.\n\n   ::\n\n       # example/app/tables.py\n       from models import Person\n       from table import Table\n       from table.columns import Column\n\n       class PersonTable(Table):\n           id = Column(field='id')\n           name = Column(field='name')\n           class Meta:\n               model = Person\n\nAnd pass a table instance to the view.\n\n::\n\n        # example/app/views.py\n        from django.shortcuts import render\n        from app.tables import PersonTable\n\n        def people(request):\n            people = PersonTable()\n            return render(request, \"index.html\", {'people': people})\n\n-  Finally, implement the template:\n\n   ::\n\n       {# example/templates/index.html}\n       {% load static %}\n       {% load table_tags %}\n\n       <link href=\"{% static 'table/css/bootstrap.min.css' %}\" rel=\"stylesheet\">\n       <script src=\"{% static 'table/js/jquery.min.js' %}\"></script>\n       <script src=\"{% static 'table/js/bootstrap.min.js' %}\"></script>\n\n       <!DOCTYPE html>\n       <html>\n           <head>\n               <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n               <title>person</title>\n           </head>\n           <body>\n               <div class=\"container\" style=\"margin-top: 10px\">\n                   <h1>people</h1>\n                   <br />\n                   {% render_table people %}\n               </div>\n           </body>\n       </html>\n\nTag\n---\n\nRender the whole table by simple tag ``{% render_table %}``, pass\n``Table`` instance as single argument.\n\n::\n\n    {% render_table table %}\n\nDataSource\n----------\n\nModel\n`````\n\nUses a django MTV model as table data source, and queries all data in\ndatabase by default. See **model** in table options for details.\n\nQuerySet\n````````\n\nSimiliar to **Model**, but pass queryset when you initialize the table\ninstance instead of defining model option. Basically, it is used to\nfilter or sort data you want to display in table.\n\n::\n\n    Models:\n\n        # models.py\n        class Person(models.Model):\n            name = models.CharField(max_length=100)\n\n    Tables:\n\n        # tables.py\n        from models import Person\n        from table import Table\n            from table.columns import Column\n\n        class PersonTable(Table):\n            id = Column(field='id')\n            name = Column(field='name')\n\n    Views:\n\n        # views.py\n        from django.shortcuts import render\n        from models import Person\n        from app.tables import PersonTable\n\n        def people(request):\n            people = PersonTable(Person.objects.all())\n            return render(request, \"index.html\", {'people': people})\n\nDict-List\n`````````\n\nUse a list of dictionaries as table data source. Fields declared in\ncolumns correspond to the dictionary keys.\n\n::\n\n    Tables:\n\n        # tables.py\n        from table import Table\n        from table.columns import Column\n\n        class PersonTable(Table):\n            id = Column(field='id')\n            name = Column(field='name')\n\n    Views:\n\n        # views.py\n        from django.shortcuts import render\n        from app.tables import PersonTable\n\n        def people(request):\n            data = [{'id': 1, 'name': 'John'}, {'id': 2, 'name': 'Tom'}]\n            people = PersonTable(data)\n            return render(request, \"index.html\", {'people': people})\n\nBuilt-in Ajax\n`````````````\n\nFor large amounts of data, loading them on front-end entirely is\nimpossible. So, django-table provides a simle option 'ajax' to load data\nfrom the server-side asynchronously.\n\nNote that once toggling ``ajax``, the ``model`` option is necessary.\nDjango-table will do paging/searching/sorting based on\n``ModelClass.objects.all()``.\n\n::\n\n    Urls:\n\n        # urls.py\n        urlpatterns = patterns('',\n            url(r'^table/', include(table.urls')),\n        )\n\n    Tables:\n\n        # tables.py\n        from table import Table\n        from table.columns import Column\n\n        class PersonTable(Table):\n            id = Column(field='id')\n            name = Column(field='name')\n\n            class Meta:\n                model = Person\n                ajax = True\n\nCustom Ajax\n```````````\n\nIf you want to customize base data, use ``ajax_source`` option and\nimplement your own Class-based View by subclassing ``FeedDataView``.\n\n::\n\n    Tables:\n\n        # tables.py\n        class PersonTable(Table):\n            id = Column(field='id')\n            name = Column(field='name')\n\n            class Meta:\n                model = Person\n                ajax = True\n                ajax_source = reverse_lazy('table_data')\n\n    Urls:\n\n        # urls.py\n        urlpatterns = patterns('',\n            url(r'^table/data/$', MyDataView.as_view(), name='table_data'),\n        )\n\n    Views:\n\n        # views.py\n        from table.views import FeedDataView\n        from app.tables import PersonTable\n\n        class MyDataView(FeedDataView):\n\n            token = PersonTable.token\n\n            def get_queryset(self):\n                return super(MyDataView, self).get_queryset().filter(id__gt=5)\n\nColumns\n-------\n\n-  Column\n\n-  Link Column\n\n-  Datetime Column\n\n-  Checkbox Column\n\n-  Sequence Column\n\n-  Calendar Column\n\nWidgets\n-------\n\n-  search-box\n\n-  info-label\n\n-  pagination\n\n-  length-menu\n\n-  exten-button(deprecated)\n\nAPI Reference\n-------------\n\n-  `wiki <https://github.com/shymonk/django-datatable/wiki/API-Reference>`__\n\n.. |Build Status| image:: https://travis-ci.org/shymonk/django-datatable.svg?branch=master\n   :target: https://travis-ci.org/shymonk/django-datatable\n.. |PyPI| image:: https://img.shields.io/pypi/v/django-datatable.png\n   :target: https://pypi.python.org/pypi/django-datatable\n"
 },
 {
  "repo": "alexis-jacq/Pytorch-Sketch-RNN",
  "language": "Python",
  "readme_contents": "# Pytorch-Sketch-RNN\nA pytorch implementation of https://arxiv.org/abs/1704.03477\n\nIn order to draw other things than cats, you will find more drawing data here: https://github.com/googlecreativelab/quickdraw-dataset\n\nepoch 1900:\n\n![epoch_1900](images/1900_output_.jpg)\n\nepoch 2400:\n\n![epoch_2600](images/2600_output_.jpg)\n\nepoch 3400\n\n![epoch_3400](images/3400_output_.jpg)\n\nDefault hyperparameters for training has been found here: https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_rnn/README.md\n"
 },
 {
  "repo": "philiiiiiipp/Android-Screen-to-Face-Distance-Measurement",
  "language": "Java",
  "readme_contents": "Android-Screen-to-Face-Distance-Measurement\n===========================================\n\n#### Be aware that this code is unmaintained since a couple of years and will probably not work with current android versions. \n\nCalculate the distance between your smartphone screen and your face using the front facing camera. The idea is pretty simple and best explained in an image.\n\n![alt tag](Images/Idea.png)\n\nAn object or a \"distance\" will seem smaller once its further away from the camera than when its closer to it. So one should be able to calculate the distance between camera and face while using the eye distance at a reference length, in our case 29.7cm (the length of an A4 paper), and multiplying the reference length with the change of eye distance.\n\nClose: \n![alt tag](Images/Close.jpg)\nFar:\n![alt tag](Images/Far.jpg)\n\nFor a video  \nhttps://www.youtube.com/watch?v=-6_pGkPKAL4\n\nAnd the link to the paper with a more thorough analysation can be found at  \nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6825217  \nor [here](https://github.com/philiiiiiipp/Android-Screen-to-Face-Distance-Measurement/blob/master/A%20new%20context%20-%20Screen%20to%20Face%20distance%201%201.pdf). \n"
 },
 {
  "repo": "gary-rowe/hid4java",
  "language": "Java",
  "readme_contents": "# Project status\n\n[![Build Status](https://travis-ci.org/gary-rowe/hid4java.png?branch=master)](https://travis-ci.org/gary-rowe/hid4java) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.hid4java/hid4java/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.hid4java/hid4java) [![Javadocs](http://www.javadoc.io/badge/org.hid4java/hid4java.svg)](http://www.javadoc.io/doc/org.hid4java/hid4java)\n\n# \ud83c\udf1f Summary \n\nThe `hid4java` project supports USB HID devices through a common API which is provided here under the MIT license. The API is very simple but provides great flexibility such as support for feature reports and blocking reads with timeouts. Attach/detach events are provided to allow applications to respond instantly to device availability.\n\n## Telegram group\n\nIf you want to discuss `hid4java` in general please use [the Telegram chat](https://t.me/joinchat/CtU4ZBltWCAFBAjwM5KLLw). I can't guarantee\nan instant response but I'm usually active on Telegram during office hours in the GMT timezone.\n\nRemember to [check the Wiki first](https://github.com/gary-rowe/hid4java/wiki/Home) before asking questions to avoid causing frustration!\n\n## Technologies\n\n* [hidapi](https://github.com/libusb/hidapi) - Native USB HID library for multiple platforms\n* [JNA](https://github.com/twall/jna) - to remove the need for Java Native Interface (JNI) and greatly simplify the project\n* [dockcross](https://github.com/dockcross/dockcross) - Cross-compilation environments for multiple platforms to create hidapi libraries\n* Java 7+ - to remove dependencies on JVMs that have reached end of life\n\n## Maven dependency\n\n```xml\n\n<dependencies>\n\n  <!-- hid4java for cross-platform HID USB -->\n  <dependency>\n    <groupId>org.hid4java</groupId>\n    <artifactId>hid4java</artifactId>\n    <version>0.7.0</version>\n  </dependency>\n\n</dependencies>\n\n```\n\n## Gradle dependency\n\n```gradle\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation('org.hid4java:hid4java')\n}\n\n```\n\n## \ud83d\ude80 Code example\n\nTaken from [UsbHidEnumerationExample](https://github.com/gary-rowe/hid4java/blob/develop/src/test/java/org/hid4java/examples/UsbHidEnumerationExample.java) which\nprovides more details. \n\n```java\n// Configure to use custom specification\nHidServicesSpecification hidServicesSpecification = new HidServicesSpecification();\n\n// Use the v0.7.0 manual start feature to get immediate attach events\nhidServicesSpecification.setAutoStart(false);\n\n// Get HID services using custom specification\nHidServices hidServices = HidManager.getHidServices(hidServicesSpecification);\nhidServices.addHidServicesListener(this);\n\n// Manually start the services to get attachment event\nhidServices.start();\n\n// Provide a list of attached devices\nfor (HidDevice hidDevice : hidServices.getAttachedHidDevices()) {\n  System.out.println(hidDevice);\n}\n    \n```\n\n# \u2699 Local build\n\nIf you're unfamiliar with Maven and git the wiki provides [an easy guide to creating a development environment](https://github.com/gary-rowe/hid4java/wiki/How-to-set-up-a-build-environment-from-scratch).\n\nThe project uses the standard Maven build process and can be used without having external hardware attached. Just do the usual\n\n```\ncd <workspace>\ngit clone https://github.com/gary-rowe/hid4java.git\ncd hid4java\nmvn clean install\n```\n\nand you're good to go. \n\n# \ud83e\udd14 More information\n\nMuch of the information previously in this README has been migrated to the project Wiki as it was getting rather long. Here are some useful jumping off points that should help:\n\n* [Home](https://github.com/gary-rowe/hid4java/wiki/Home) - The wiki Home page with lots of useful launch points\n* [FAQ](https://github.com/gary-rowe/hid4java/wiki/FAQ) - Frequently asked questions\n* [Examples](https://github.com/gary-rowe/hid4java/wiki/Examples) - Using the examples to kickstart your own project\n* [Troubleshooting](https://github.com/gary-rowe/hid4java/wiki/Troubleshooting) - A comprehensive troubleshooting guide\n\n# \ud83d\udcd5 Closing notes\n\nAll trademarks and copyrights are acknowledged.\n\nMany thanks to `victorix` who provided the basis for this library. Please [see the inspiration on the mbed.org site](http://developer.mbed.org/cookbook/USBHID-bindings-).\n\nThanks also go to everyone who has contributed their knowledge and advice during the creation and subsequent improvement of this library.\n"
 },
 {
  "repo": "mediamonks/tilt-game-android",
  "language": "Java",
  "readme_contents": "#Android Tilt Game\n\nTilt is an Android adaptation of the classic handheld maze genre. While the concept of the game is the same, the fun lies in the features introduced by the player\ufffds smartphone. This Android Experiment lets players accurately navigate a variety of mazes by utilizing the motion sensors of their Android devices. With multiplayer gameplay supported by a Bluetooth connection, players can make use of an automatic scoring system, leaderboards, and a precise timer for those split second time differences.\n\nCreated as an open source Android experiment, the app showcases the capabilities of Android devices by highlighting the reliability of their Bluetooth connection and the responsive accuracy of their motion sensors.\n\nThe code utilizes an NDK port of the Box2D library to simulate the physics of the maze board and ball. The And Engine is used for display of game graphics using OpenGL. The OrientationProvider classes by hitlabnz.com are used to retrieve motion sensor data.\n\nCode is made available under the MIT license.\n\nCopyright (c) 2015 MediaMonks\n\n### **this is an [android experiment](http://androidexperiments.com)**\n"
 },
 {
  "repo": "alexmojaki/s3-stream-upload",
  "language": "Java",
  "readme_contents": "# S3 Stream Upload\n\nThis library allows you to efficiently stream large amounts of data to AWS S3 in Java without having to store the whole object in memory or use files. The S3 API requires that a content\nlength be set before starting uploading, which is a problem when you want to calculate a large amount of data on the fly.\nThe standard Java AWS SDK will simply buffer all the data in memory so that it can calculate the length, which consumes\nRAM and delays the upload. You can write the data to a temporary file but disk IO is slow (if your data is already in a file, using this library is pointless). This library provides\nan `OutputStream` that packages data written to it into chunks which are sent in a multipart upload. You can also use\nseveral streams and upload the data in parallel.\n\nThe entrypoint is the class `StreamTransferManager`. Read more in the\n[javadoc](http://alexmojaki.github.io/s3-stream-upload/javadoc/apidocs/alex/mojaki/s3upload/StreamTransferManager.html),\nincluding a usage example.\n\nThis is available from [maven central](https://mvnrepository.com/artifact/com.github.alexmojaki/s3-stream-upload/latest).\n\n## Changelog\n\n### 2.2.4\n\n- [Abort multipart upload when content is empty](https://github.com/alexmojaki/s3-stream-upload/pull/41) thanks to @kcalcagno\n\n\n### 2.2.3\n\n- [Use String.valueOf to protect against null uploadId in toString](https://github.com/alexmojaki/s3-stream-upload/pull/38) thanks to @kcalcagno\n\n### 2.2.2\n\n- [Sort PartETag list to avoid an InvalidPartOrder error on complete (#34)](https://github.com/alexmojaki/s3-stream-upload/pull/34) thanks to @dana-katzenelson-livongo\n\n### 2.2.1\n\n- Exceptions during upload lead to an exception in writers instead of them getting stuck trying to put on the queue.\n- Bump `aws-java-sdk` and `slf4j-api` dependency versions.\n\n### 2.2.0\n\n- [Add `checkIntegrity()` method](https://github.com/alexmojaki/s3-stream-upload/pull/26) thanks\nto **@gkolakowski-ias**. This allows verifying the upload with MD5 hashes.\n\n### 2.1.0\n\n- Support uploading empty objects. In this case the user may want to override `customisePutEmptyObjectRequest`.\n- Bump `aws-java-sdk` and `slf4j-api` dependency versions.\n\n### 2.0.3\n\n- Bump `aws-java-sdk` dependency version\n\n### 2.0.2\n\n- Avoid race condition in big uploads causing some parts to be missing from final completed upload. \n\n### 2.0.1\n\n- Allow thrown `Error`s (e.g. OOM) to bubble up unchanged.\n\n### 2.0.0\n\n- The `checkSize()` method is now private as the user no longer needs to call it. You can remove all calls to it.\n- The constructor from version 1 is now deprecated. Instead you should call the 3 parameter constructor which has the essentials, and optionally chain the desired builder style setters to configure.\n"
 },
 {
  "repo": "easilycoder/GuideView",
  "language": "Java",
  "readme_contents": "[![](https://jitpack.io/v/easilycoder/GuideView.svg)](https://jitpack.io/#easilycoder/GuideView)\n\n# FEATURE UPDATE\n\n* \u5728\u663e\u793a\u5f15\u5bfc\u56fe\u7684\u60c5\u51b5\u4e0b\uff0ctargetView\u53ef\u4ee5\u8bbe\u7f6e\u652f\u6301\u70b9\u51fb\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cdialogFragment\u7684\u4e8b\u4ef6\u4f1a\u88ab\u900f\u4f20\u5230targetView\u4e0a\uff0c\u89e6\u53d1\u5176\u539f\u6709\u7684\u70b9\u51fb\u903b\u8f91\uff1b\n\n  ```Java\n  // \u8bbe\u7f6etargetView\u662f\u5426\u53ef\u4ee5\u63a5\u53d7\u70b9\u51fb\u4e8b\u4ef6\n  GuideViewBundle.Builder#setTargetViewClickable();\n  //\u8bbe\u7f6etargetView\u70b9\u51fb\u4e4b\u540e\u662f\u5426\u5c55\u793a\u4e0b\u4e00\u9875\u7684\u5f15\u5bfc\u89c6\u56fe\uff08\u5982\u679c\u6ca1\u6709\u4e0b\u4e00\u9875\u5219\u5173\u95ed\u6574\u4e2a\u5f15\u5bfc\u89c6\u56fe\uff09\n  GuideViewBundle.Builder#setDismissOnTouchInTargetView();\n  ```\n\n* \u652f\u6301\u8bbe\u7f6eGuideView(\u5373\u6bcf\u4e00\u9875\u7684\u5f15\u5bfc\u89c6\u56fe)\u5173\u95ed\u7684\u76d1\u542c\uff1b\n\n  ```Java\n  GuideViewBundle.Builder#setGuideViewHideListener();\n  ```\n\n* \u652f\u6301\u8bbe\u5b9a\u663e\u793a\u6761\u4ef6\u7684\u5e03\u5c14\u503c\uff0c\u65b9\u4fbf\u94fe\u5f0f\u8c03\u7528API\n\n  ```java\n  // \u5982\u679c\u4f20\u5165\u7684condition\u503c\u4e3afalse\uff0c\u90a3\u4e48\u5373\u4f7f\u5bf9\u5e94\u7684GuideViewBundle\u88abadd\u8fdbGuideViewFragment\u4e2d\u4e5f\u4e0d\u4f1a\u88ab\u663e\u793a\n  GuideViewBundle.Builder#condition(condition)\n  ```\n\n\n# GuideView:\u57fa\u4e8eDialogFragment\u5b9e\u73b0\n\n\u57fa\u4e8e`DialogFragment`\u5b9e\u73b0\u7684\u5f15\u5bfc\u906e\u7f69\u6d6e\u5c42\u89c6\u56fe\uff0c\u5177\u5907\u4ee5\u4e0b\u7684\u7279\u6027\uff1a\n\n* \u54cd\u5e94\u5bfc\u822a\u6309\u94ae\u7684\u52a8\u4f5c\uff08\u56e0\u4e3a\u900f\u660e\u7684\u6d6e\u5c42\u672c\u8d28\u662f\u4e00\u4e2adialog\uff09\n* \u94fe\u5f0f\u5f15\u5bfc\u5c42\uff0c\u652f\u6301\u8bbe\u5b9a\u4e00\u7ec4\u7684\u5f15\u5bfc\u906e\u7f69\u89c6\u56fe\uff0c\u70b9\u51fb\u5207\u6362\u4e0b\u4e00\u4e2a\u8bd5\u56fe\n* \u81ea\u52a8\u7ed8\u5236\u534a\u900f\u660e\u6d6e\u5c42\u3001\u900f\u660e\u6838\u5fc3\u533a\u4ee5\u53ca\u786e\u4fdd\u76ee\u6807\u89c6\u56fe\u548c\u5f15\u5bfc\u89c6\u56fe\u7684\u4f4d\u7f6e\n\n\u6548\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff1a\n\n![](/assets/guideview.gif)\n\n## \u5b9e\u73b0\u8bf4\u660e\n\n\u9875\u9762\u7684\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a\n\n![ic_guideview](/assets/ic_guideview.png)\n\n### \u6838\u5fc3\u7c7b\n\n#### GuideViewBundle\n\n\u5f15\u5bfc\u89c6\u56fe\u7684\u914d\u7f6e\u9879\u7c7b\uff0c\u6bcf\u4e00\u9875\u5f15\u5bfc\u89c6\u56fe\u5bf9\u5e94\u4e00\u4e2a\u914d\u7f6e\u9879\u3002\u5728**GuideView**\u5185\u90e8\u901a\u8fc7\u8fd9\u4e2a\u914d\u7f6e\u9879\u53bb\u6784\u9020`GuideView`\u7684\u5b9e\u4f8b\uff0c\u5e76\u901a\u8fc7`GuideViewFragment`\u663e\u793a\u5728\u754c\u9762\u4e0a\u3002\n\n\u5176\u4e2d\u7684\u5c5e\u6027\u90fd\u901a\u8fc7\u6784\u9020\u5668\u7684\u6a21\u5f0f\uff0c\u901a\u8fc7\u9759\u6001\u5185\u90e8\u7c7b`Builder`\u8fdb\u884c\u6784\u5efa\uff0c\u5c5e\u6027\u8bf4\u660e\u5982\u4e0b\uff1a\n\n* targetView\n\n  \u5f15\u5bfc\u89c6\u56fe\u9700\u8981\u663e\u793a\u9644\u7740\u7684\u76ee\u6807\u89c6\u56fe\n\n* hintView\n\n  \u5f15\u5bfc\u89c6\u56fe\uff08\u4e0d\u5305\u542b\u534a\u900f\u660e\u6d6e\u5c42\u4ee5\u53ca\u900f\u660e\u7126\u70b9\u533a\uff09\n\n* transparentSpaceXXX\n\n  \u9ed8\u8ba4\u7684\u60c5\u51b5\u4e0b\uff0c\u900f\u660e\u7126\u70b9\u533a\u7684\u5927\u5c0f\u8ddf\u76ee\u6807\u89c6\u56fe\u7684\u5927\u5c0f\u4fdd\u6301\u4e00\u81f4\uff0c\u5982\u679c\u9700\u8981\u52a0\u5927\u900f\u660e\u533a\u57df\u7684\u5927\u5c0f\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u8fd9\u7ec4\u5c5e\u6027\uff0c\u6307\u5b9a\u4e0a\u4e0b\u5de6\u53f3\u7684\u989d\u5916\u7684\u7a7a\u767d\u533a\u57df\n\n* hintViewMarginXXX\n\n  \u5f15\u5bfc\u89c6\u56fe\uff08hintView\uff09\u76f8\u5bf9\u4e8e\u76ee\u6807\u89c6\u56fe\uff08targetView\uff09\u7684\u8fb9\u8ddd\n\n* hasTransparentLayer\n\n  \u662f\u5426\u663e\u793a\u900f\u660e\u7126\u70b9\u533a\u57df\uff0c\u9ed8\u8ba4\u663e\u793a\u3002\u53ef\u4ee5\u9009\u62e9\u4e0d\u7ed8\u5236\u900f\u660e\u7126\u70b9\u533a\u57df\u800c\u53ea\u6709\u534a\u900f\u660e\u7684\u6d6e\u5c42\n\n* hintViewDirection\n\n  \u5f15\u5bfc\u89c6\u56fe\uff08hintView\uff09\u76f8\u5bf9\u4e8e\u76ee\u6807\u89c6\u56fe\uff08targetView\uff09\u7684\u4f4d\u7f6e\u65b9\u5411\uff0c\u76ee\u524d\u53ef\u4ee5\u5b9a\u4e49\u4e0a\uff08\u4e0a\u65b9\u5de6\u5bf9\u9f50\uff09\u3001\u4e0b\uff08\u4e0b\u65b9\u5de6\u5bf9\u9f50\uff09\u3001\u5de6\uff08\u5de6\u65b9\u4e0a\u5bf9\u9f50\uff09\u3001\u53f3\uff08\u53f3\u65b9\u4e0a\u5bf9\u9f50\uff09\u56db\u4e2a\u65b9\u5411\u3002\u5982\u679c\u9700\u8981\u5728\u4f4d\u7f6e\u4e4b\u4f59\u6709\u4e0d\u4e00\u6837\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u53ef\u4ee5\u4f7f\u7528hintViewMarginXXX\u5c5e\u6027\n\n* outlineType\n\n  \u900f\u660e\u7126\u70b9\u533a\u7684\u8f6e\u5ed3\u7c7b\u578b\uff0c\u6709\u5706\u5f62\uff08\u692d\u5706\uff09\u8f6e\u5ed3\u548c\u65b9\u5f62\u8f6e\u5ed3\u4e24\u79cd\n\n* maskColor\n\n  \u534a\u900f\u660e\u906e\u7f69\u6d6e\u5c42\u7684\u989c\u8272\n\n* isDismissOnClicked\n\n  \u5168\u5c40\u70b9\u51fb\u53ef\u4ee5\u5173\u95ed\u5f15\u5bfc\u89c6\u56fe\uff0c\u9ed8\u8ba4\u4e3atrue\u3002\u5982\u679c\u8bbe\u7f6efalse\uff0c\u5219\u9700\u8981\u624b\u52a8\u8bbe\u7f6e\u70b9\u51fbhintView\u7684\u7279\u5b9a\u4f4d\u7f6e\u5173\u95ed\u89c6\u56fe\n\n#### GuideView\n\n\u754c\u9762\u5b9e\u9645\u5c55\u793a\u7684\u89c6\u56fe\u5bf9\u8c61\uff0c\u6839\u636e`GuideViewBundle`\u8bbe\u7f6e\u7684\u5c5e\u6027\uff0c\u7531`GuideViewFragment`\u521b\u5efa\u5e76\u6dfb\u52a0\u5230\u9f50\u89c6\u56fe\u5bb9\u5668\u4e2d\uff0c\u5bf9\u5916\u90e8\u4e1a\u52a1\u5b8c\u5168\u900f\u660e\u65e0\u611f\u77e5\u5230\u4e00\u4e2a\u7c7b\n\n#### GuideViewFragment\n\n\u5b9e\u9645\u663e\u793a\u5f15\u5bfc\u89c6\u56fe\u7684\u5f39\u7a97\u3002\u5176\u5185\u90e8\u52a0\u8f7d\u4e86\u4e00\u4e2a`FrameLayout`\u5bb9\u5668\uff0c\u901a\u8fc7\u5728\u5bb9\u5668\u4e2d\u6dfb\u52a0`GuideView`\u7684\u5b9e\u4f8b\u5b9e\u73b0\u663e\u793a\u5f15\u5bfc\u89c6\u56fe\u5c42\u3002\u4e00\u4e2a`GuideViewFragment`\u53ef\u4ee5\u8bbe\u5b9a\u4e00\u7ec4\u5f15\u5bfc\u89c6\u56fe\uff0c\u5b8c\u6210\u4e00\u7ec4\u5f15\u5bfc\u5e8f\u5217\u3002\u8bf7\u4f7f\u7528\u5176\u9759\u6001\u5185\u90e8\u7c7b`Builder`\u6784\u5efa\u5176\u5b9e\u4f8b\uff0c\u5e76\u4f7f\u7528`Builder#addGuidViewBundle(bundle)`\u65b9\u6cd5\u6dfb\u52a0\u5f15\u5bfc\u89c6\u56fe\u7684\u914d\u7f6e\u9879\u3002\n\n\u5982\u679c\u9700\u8981\u81ea\u5b9a\u4e49\u70b9\u51fb\u5173\u95ed\u7684\u52a8\u4f5c\uff08`GuideViewBundle.Builder#setDismissOnClicked(false)`\u7684\u60c5\u51b5\u4e0b\uff09\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u65b9\u6cd5\n\n```Java\nvoid onNext()\n```\n\n\u5982\u679c\u8fd8\u5b58\u5728\u6ca1\u6709\u663e\u793a\u7684\u5f15\u5bfc\u89c6\u56fe\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u7ee7\u7eed\u663e\u793a\u4e0b\u4e00\u5f20\uff0c\u5426\u5219\u4f1a\u5173\u95ed\u5f39\u7a97\n\n## \u6dfb\u52a0\u4f9d\u8d56\n\n1. \u5728\u6839\u76ee\u5f55\u7684build.gradle\u6587\u4ef6\u4e2d\u6dfb\u52a0jitpack\u4ed3\u5e93\n\n   ```groovy\n   allprojects {\n   \t\trepositories {\n   \t\t\t...\n   \t\t\tmaven { url 'https://jitpack.io' }\n   \t\t}\n   }\n   ```\n\n2. \u6dfb\u52a0**GuideView**\u4f9d\u8d56\n\n   ```groovy\n   dependencies {\n   \tcompile 'com.github.easilycoder:GuideView:0.0.1'\n   }\n   ```\n\n### \u4f7f\u7528\u793a\u4f8b\n\n```kotlin\nGuideViewFragment.Builder()\n                    .addGuidViewBundle(GuideViewBundle.Builder()\n                            .setTargetView(tvContent)\n                            .setHintView(hintViewLeft)\n                            .setDismissOnClicked(false)\n                            .setHintViewMargin(0, -160, 0, 0)\n                            .setTransparentSpace(space, space, space, space)\n                            .setOutlineType(TYPE_RECT)\n                            .setHintViewParams(params)\n                            .setHintViewDirection(LEFT).build())\n                    .addGuidViewBundle(GuideViewBundle.Builder()\n                            .setTargetView(tvContent)\n                            .setOutlineType(TYPE_OVAL)\n                            .setHintView(hintViewTop)\n                            .setDismissOnClicked(false)\n                            .setHintViewParams(params)\n                            .setHintViewMargin(-dp2px(this, 55f), 0, 0, 0)\n                            .setTransparentSpace(space, space, space, space)\n                            .setHintViewDirection(TOP)\n                            .build())\n                    .addGuidViewBundle(GuideViewBundle.Builder()\n                            .setTargetView(tvContent)\n                            .setOutlineType(TYPE_OVAL)\n                            .setHintView(hintViewRight)\n                            .setDismissOnClicked(false)\n                            .setHintViewParams(params)\n                            .setHintViewMargin(0, -160, 0, 0)\n                            .setTransparentSpace(space, space, space, space)\n                            .setHintViewDirection(RIGHT)\n                            .build())\n                    .addGuidViewBundle(GuideViewBundle.Builder()\n                            .setTargetView(tvContent)\n                            .setOutlineType(TYPE_OVAL)\n                            .setHintViewParams(params)\n                            .setHintViewMargin(-dp2px(this, 55f), 0, 0, 0)\n                            .setHintView(hintViewBottom)\n                            .setTransparentSpace(space, space, space, space)\n                            .setHintViewDirection(BOTTOM)\n                            .build())\n                    .setCancelable(false)\n                    .build().show(supportFragmentManager, \"hit\")\n```\n\n\n\n"
 },
 {
  "repo": "liaohuqiu/android-ClipboardManagerCompat",
  "language": "Java",
  "readme_contents": "Compatibility for ClipboardManager, from API level 1.\n\nRelated post: [\u518d\u8c08 Android API \u517c\u5bb9\u6027\u5904\u7406](http://www.liaohuqiu.net/cn/posts/android-api-compat-guide/)\n\nABOUT ME / \u5173\u6ce8\u6211:  [Github](https://github.com/liaohuqiu) | [twitter](https://twitter.com/liaohuqiu) | [\u5fae\u535a](http://weibo.com/liaohuqiu)\n\n#### Interface\n\n```java\npublic interface ClipboardManagerCompat {\n\n    void addPrimaryClipChangedListener(OnPrimaryClipChangedListener listener);\n\n    void removePrimaryClipChangedListener(OnPrimaryClipChangedListener listener);\n\n    CharSequence getText();\n\n    void setText(CharSequence text);\n\n    boolean hasText();\n}\n```\n\n* Screen snapshot\n\n<div><img src='https://raw.githubusercontent.com/liaohuqiu/android-ClipboardManagerCompat/master/art/clipboard-manager-compat.gif' width=\"300px\" style='border: #f1f1f1 solid 1px'/></div>\n\n#### Import\n\nRepositories:\n\n```groovy\nallprojects {\n    repositories {\n        mavenCentral()\n        maven {\n            url \"https://oss.sonatype.org/content/repositories/snapshots\"\n        }\n        jcenter()\n    }\n}\n```\n\nAdd to dependencies:\n\n```groovy\ncompile 'in.srain.cube:clipboard-manager-compat:1.0.3'\n```\n\n### LICENSE\n\nMIT\n"
 },
 {
  "repo": "quemb/QMBForm",
  "language": "Java",
  "readme_contents": "QMBForm\n=======\n![Travis-CI](https://api.travis-ci.org/quemb/QMBForm.svg?branch=master)\n![Coverage](https://img.shields.io/badge/coverage-31%25-yellow.svg)\n![Technical Dept](https://img.shields.io/badge/tech%20dept-2%2C3%25-green.svg)\n\nCreate simple Android forms\n\n![Demo](resources/sample_cast.gif)\n\n## Basic Usage\n\n1.  Create a \"FromDescriptor\" - It's the holder for the form\n\n    ```java\n    FormDescriptor descriptor = FormDescriptor.newInstance();\n    descriptor.setOnFormRowValueChangedListener(this); //Listen for changes\n    ```\n\n1.  Create sections with \"SectionDescriptor\" and add rows - Sections can have titles and rows (Form Elements)\n\n    ```java\n    SectionDescriptor sectionDescriptor = SectionDescriptor.newInstance(\"tag\",\"Title\");\n    \n    //Add rows - form elements\n    sectionDescriptor.addRow( RowDescriptor.newInstance(\"text\",RowDescriptor.FormRowDescriptorTypeText, \"Text\", new Value<String>(\"test\")) );\n    sectionDescriptor.addRow( RowDescriptor.newInstance(\"dateDialog\",RowDescriptor.FormRowDescriptorTypeDate, \"Date Dialog\") );\n    ```\n    \n1.  To render your form, use the \"FormManager\" helper. You will need a ListView instance.\n  \n    ```java\n        FormManager formManager = new FormManager();\n        formManager.setup(descriptor, mListView, getActivity());\n        formManager.setOnFormRowClickListener(this);    \n    ```\n\n\n    \n1.  Attention! If you use EditText based form elements, make sure you set \"windowSoftInputMode\" to \"adjustPan\"\n\n    Do it in your manifest:\n    ```xml\n    <activity\n\t\t\t      android:windowSoftInputMode=\"adjustPan\"/>\n            \n    ```\n\n    Or programmatically in the onCreate method of your activity\n    ```java\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n\n        super.onCreate(savedInstanceState);\n\n        getWindow().setSoftInputMode(WindowManager.LayoutParams.SOFT_INPUT_ADJUST_PAN);\n        \n    }\n    ```\n    \n1.  Create elements with custom type identifier and a FormCell class. You can also submit configs to use objects in your cell\n\n    ```java\n    String customType = \"customRowIdentifier\";\n    CellViewFactory.getInstance().setRowTypeMap(customType, CustomFormCell.class);\n\n    RowDescriptor customRow = RowDescriptor.newInstance(\"custom\",customType, \"title\", new Value<String>(\"value\"));\n    HashMap<String, Object> cellConfig = new HashMap();\n    cellConfig.put(\"config\",configObject);\n    customRow.setCellConfig(cellConfig);\n    section.addRow(customRow);\n    ```\n\n## Use annotations for models\n\n1.  You can create a form from a POJO by adding annotations to your model properties\n  \n    ```java\n    public class Entry {\n\n        @FormElement(\n                label = R.string.lb_title,\n                rowDescriptorType = RowDescriptor.FormRowDescriptorTypeText,\n                sortId = 1,\n                section = R.string.section_general\n        )\n        public String title;\n    \n        @FormElement(\n                label = R.string.lb_description,\n                rowDescriptorType = RowDescriptor.FormRowDescriptorTypeTextView,\n                sortId = 2,\n                section = R.string.section_general\n        )\n        public String description;\n    \n        @FormElement(\n                label = R.string.lb_date_dialog,\n                rowDescriptorType = RowDescriptor.FormRowDescriptorTypeDate,\n                sortId = 4,\n                section = R.string.section_date\n        )\n        public Date date;\n    \n        @FormElement(\n                label = R.string.lb_date_inline,\n                rowDescriptorType = RowDescriptor.FormRowDescriptorTypeDateInline,\n                tag = \"customDateInlineTag\",\n                sortId = 3,\n                section = R.string.section_date\n        )\n        public Date dateInline;\n\n    }\n    \n    ```\n2.  Use \"FormDescriptorAnnotationFactory\" to create a FormDescriptor from your model\n  \n    ```java\n    FormDescriptorAnnotationFactory factory = new FormDescriptorAnnotationFactory(getActivity());\n    FormDescriptor descriptor = factory.createFormDescriptorFromAnnotatedClass(entry);\n    ```\n\n## Validation\n1. Define custom validators by implementing ```FormValidator```\n    ```java\n    public class EmailValidator implements FormValidator {\n\t    private static final String EMAIL_PATTERN =\n\t            \"^[_A-Za-z0-9-\\\\+]+(\\\\.[_A-Za-z0-9-]+)*@\"\n\t                    + \"[A-Za-z0-9-]+(\\\\.[A-Za-z0-9]+)*(\\\\.[A-Za-z]{2,})$\";\n\t\n\t    @Override\n\t    public RowValidationError validate(RowDescriptor descriptor) {\n\t        Value value = descriptor.getValue();\n\t        if (value.getValue() != null && value.getValue() instanceof String) {\n\t            String val = (String) value.getValue();\n\t            return (val.matches(EMAIL_PATTERN)) ? null : new RowValidationError(descriptor,  R.string.validation_invalid_email);\n\t        }\n\t        return new RowValidationError(descriptor, R.string.validation_invalid_email);\n\t    }\n}\n    ```\n\n2. With annotations\n    Specify validator classes as an array of .class items \n    ```java\n    @FormElement(\n            label = R.string.email,\n            rowDescriptorType = RowDescriptor.FormRowDescriptorTypeEmailInline,\n            sortId = 2,\n            validatorClasses = {EmailValidator.class, BlankStringValidator.class}\n    )\n    public String email;\n    ```\n\n3. Or add validators to rowdescriptors manually\n    ```java\n    RowDescriptor rowDescriptor = RowDescriptor.newInstance(\"valid\",\n\t    RowDescriptor.FormRowDescriptorTypeEmail,\n\t    \"Email Test\",\n\t    new Value<String>(\"notavalidemail\"));\n\n    rowDescriptor.addValidator(new EmailValidator());\n    ```\n    \n## Installation\n\nQMBForm is not available at the maven repository yet. But it's a gradly based android-library project. \n\n1. Include the QMBFrom directory as a library module in your application\n2. see sample application: \n\nAdd this to your build.gradle dependencies section\n  ```\n  compile project(\":lib:QMBForm\")\n  ```\nAdd this to your settings.gradle\n  ```\n  include ':app', ':lib:QMBForm'\n  ```\n\nQMBForm **has no** dependencies to other third party libs (but compile 'com.android.support:appcompat-v7:19.+') is needed\n\n## Supported Form Elements\nMost elements have an inline version (label on the same line as the displayed value) and a normal version (label on separate line above displayed value).\n\n- Available elements:\n  ```java\n    public static final String FormRowDescriptorTypeName = \"name\";\n  \n    public static final String FormRowDescriptorTypeText = \"text\";\n    public static final String FormRowDescriptorTypeTextInline = \"textInline\";\n    public static final String FormRowDescriptorTypeDetail = \"detail\";\n    public static final String FormRowDescriptorTypeDetailInline = \"detailInline\";\n    public static final String FormRowDescriptorTypeTextView = \"textView\";\n    public static final String FormRowDescriptorTypeTextViewInline = \"textViewInline\";\n    public static final String FormRowDescriptorTypeURL = \"url\";\n    public static final String FormRowDescriptorTypeEmail = \"email\";\n    public static final String FormRowDescriptorTypeEmailInline = \"emailInline\";\n    public static final String FormRowDescriptorTypePassword = \"password\";\n    public static final String FormRowDescriptorTypePasswordInline = \"passwordInline\";\n    public static final String FormRowDescriptorTypeNumber = \"number\";\n    public static final String FormRowDescriptorTypeNumberInline = \"numberInline\";\n    public static final String FormRowDescriptorTypeCurrency = \"currency\";\n    public static final String FormRowDescriptorTypePhone = \"phone\";\n    \n    public static final String FormRowDescriptorTypeInteger = \"integer\";\n    public static final String FormRowDescriptorTypeIntegerInline = \"integerInline\";\n   \n    public static final String FormRowDescriptorTypeSelectorSpinner = \"selectorSpinner\";\n    public static final String FormRowDescriptorTypeSelectorSpinnerInline = \"selectorSpinnerInline\";\n    public static final String FormRowDescriptorTypeSelectorPickerDialog = \"selectorPickerDialog\";\n    \n    public static final String FormRowDescriptorTypeDateInline = \"dateInline\";\n    public static final String FormRowDescriptorTypeTimeInline = \"timeInline\";\n    public static final String FormRowDescriptorTypeDate = \"date\";\n    public static final String FormRowDescriptorTypeTime = \"time\";\n    \n    public static final String FormRowDescriptorTypeBooleanCheck = \"booleanCheck\";\n    public static final String FormRowDescriptorTypeBooleanSwitch = \"booleanSwitch\";\n    \n    public static final String FormRowDescriptorTypeButton = \"button\";\n    public static final String FormRowDescriptorTypeButtonInline = \"buttonInline\";\n\n    public static final String FormRowDescriptorTypeSelectorSegmentedControl = \"selectorSegmentedControl\";\n    public static final String FormRowDescriptorTypeSelectorSegmentedControlInline = \"selectorSegmentedControlInline\";\n    \n  ```\n- Coming elements: (Avaiable at XLForm)\n  ```java\n  \n    public static final String FormRowDescriptorTypeTwitter = \"twitter\";\n    public static final String FormRowDescriptorTypeAccount = \"account\";\n  \n    public static final String FormRowDescriptorTypeSelectorPush = \"selectorPush\";\n    public static final String FormRowDescriptorTypeSelectorActionSheet = \"selectorActionSheet\";\n    public static final String FormRowDescriptorTypeSelectorAlertView = \"selectorAlertView\";\n    public static final String FormRowDescriptorTypeSelectorPickerView = \"selectorPickerView\";\n    public static final String FormRowDescriptorTypeSelectorPickerViewInline = \"selectorPickerViewInline\";\n    \n    public static final String FormRowDescriptorTypeMultipleSelector = \"multipleSelector\";\n    public static final String FormRowDescriptorTypeSelectorLeftRight = \"selectorLeftRight\";\n    \n    public static final String FormRowDescriptorTypeDateTimeInline = \"datetimeInline\";\n    public static final String FormRowDescriptorTypeDateTime = \"datetime\";\n    \n    public static final String FormRowDescriptorTypePicker = \"picker\";\n    \n    public static final String FormRowDescriptorTypeImage = \"image\";\n    public static final String FormRowDescriptorTypeStepCounter = \"stepCounter\";\n    \n  ```\n\n## To do\n\n- Simpler cell customisation\n- More form elements\n- Form model \n\n## Credits\nQMBForm is based on the ideas of [XLForm](https://github.com/xmartlabs/XLForm) - the most flexible and powerful iOS library to create dynamic table-view forms.\n\nThanks guys!\n"
 },
 {
  "repo": "limedroid/ARecyclerView",
  "language": "Java",
  "readme_contents": "# ARecyclerView\n\n\u5bf9RecyclerView\u7684\u5c01\u88c5\uff0c\u529f\u80fd\u5f3a\u5927\u3001\u4f7f\u7528\u7b80\u5355\u3001\u6269\u5c55\u6027\u5f3a\u3002\u8be5\u5e93\u4e3b\u8981\u5206\u6210\u4e09\u90e8\u5206\uff1a**RecyclerAdapter**\u3001**XRecyclerView**\u3001**XRecyclerContentLayout**\n\n\n>\u8be5\u5e93\u5728\u5546\u4e1a\u9879\u76ee\u4e2d\u5386\u7ecf\u4e00\u5e74\u591a\u65f6\u95f4\u6253\u78e8\uff0c\u6b22\u8fcestar\u3001fork\uff0c\u540e\u671f\u4f1a\u6709\u66f4\u591a\u5206\u4eab\uff0c\u671f\u5f85\u60a8\u7684\u5efa\u8bae\u548c\u5173\u6ce8\u3002\n\n**\u672c\u5e93\u5df2\u7ecf\u8fc1\u79fb\u5230JitPack**\n\n[![](https://jitpack.io/v/limedroid/ARecyclerView.svg)](https://jitpack.io/#limedroid/ARecyclerView)\n\n\n## \u8bf4\u660e\n\n\u5173\u4e8eRecyclerView\uff0c\u6709\u5f88\u591a\u5e93\u3002ARecyclerView\u4e0e\u5176\u4ed6\u5e93\u6709\u8fd9\u51e0\u4e2a\u533a\u522b\uff0c\u4e5f\u8bb8\u80fd\u66f4\u597d\u7684\u8fdb\u884c\u6269\u5c55\uff0c\u65b9\u4fbf\u60a8\u7684\u4f7f\u7528\u3002\n\n* **ARecyclerView\u7ee7\u627f\u81eaRecyclerView\uff0c\u5b83\u5c31\u662f\u4e00\u4e2a\u5c01\u88c5\u4e86\u5e38\u89c1\u529f\u80fd\u7684RecyclerView\uff0c\u800c\u4e0d\u662f\u7ee7\u627fFrameLayout**\n* **ARecyclerView\u4e2d\u5b9e\u73b0\u4e86Header\u3001Footer\uff0cheader\u548cFooter\u53ef\u4ee5\u6709\u591a\u4e2a**\n* **ARecyclerView\u7684\u6bcf\u4e00\u4e2aheader\u3001footer\u7684viewType\u662f\u4e0d\u540c\u7684\uff0c\u800c\u5927\u90e8\u5206\u5f00\u6e90\u5e93\u7684header\u3001footer\u7684viewtype\u662f\u76f8\u540c\u7684\uff0c\u5176\u76f4\u63a5\u540e\u679c\u662f\u754c\u9762\u5361\u987f**\n* **ARecyclerView\u53ef\u4ee5\u505a\u51fa\u51e0\u4e4e\u4efb\u4f55\u7684\u754c\u9762\u6548\u679c\uff0c\u53ef\u4ee5\u53d6\u4ee3ScrollView\uff0c\u4f60\u53ea\u9700\u8981\u4f7f\u7528header\u6216\u8005footer**\n* **ARecyclerView\u4e2d\u5b9e\u73b0\u4e86\u4e0a\u62c9\u52a0\u8f7d\u66f4\u591a\uff0c\u53ef\u4ee5\u81ea\u5b9a\u4e49\u52a0\u8f7d\u66f4\u591a\u7684\u6548\u679c\uff0c\u53ea\u9700\u8981\u5b9e\u73b0LoadMoreUIHandler\u63a5\u53e3\u5373\u53ef**\n* **ARecyclerView\u5e76\u672a\u5b9e\u73b0\u4e0b\u62c9\u5237\u65b0\u529f\u80fd\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9SwipeRefreshLayout\u6216\u8005\u5176\u4ed6\u7684\u4e0b\u62c9\u5237\u65b0viewGroup\u5305\u88f9\uff0c\u5373\u4f60\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u4e0b\u62c9\u5237\u65b0\u529f\u80fd\u7684\u5b9e\u73b0\u3002**\n* **\u4e3a\u4e86\u65b9\u4fbf\u81ea\u5b9a\u4e49\u4f7f\u7528\uff0c\u7279\u522b\u96c6\u6210\u4e86XRecyclerContentLayout\u63a7\u4ef6\uff0c\u4f60\u53ef\u4ee5\u6839\u636e\u4e1a\u52a1\u8fdb\u884c\u6269\u5c55\uff0cXRecyclerContentLayout\u53ea\u662f\u4e00\u4e2a\u793a\u4f8b\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u6ee1\u8db3\u7edd\u5927\u90e8\u5206\u9700\u6c42\u4e86**\n* **\u6b22\u8fce\u60a8\u63d0\u51fa\u5b9d\u8d35\u7684\u610f\u89c1**\n\n\n<p align=\"center\">\n  <img src=\"art/xrecyclerview.gif\" alt=\"XRecyclerView\" />\n</p>\n\n\n## \u4f7f\u7528\n\n* Github \uff1a [**ARecyclerView**](https://github.com/limedroid/ARecyclerView)\n\n### step1 \n\n\u5728\u6839\u9879\u76ee\u7684`build.gradle`\u6587\u4ef6\u4e2d\u6dfb\u52a0\n\n```groovy\nallprojects {\n\t\trepositories {\n\t\t\t...\n\t\t\tmaven { url 'https://jitpack.io' }\n\t}\n}\n```\n\n### step2\n\n\u6dfb\u52a0\u4f9d\u8d56\n\n```groovy\ndependencies {\n\t   compile 'com.github.limedroid:ARecyclerView:v1.2.1'\n}\n```\n\n\n## \u66f4\u65b0\u65e5\u5fd7\n\n* v1.2.1\n\t* \u6dfb\u52a0setGridSpanLookUp\u65b9\u6cd5\n\n* v1.1.0  2017\u5e741\u670822\u65e5\n\t* \u589e\u52a0refresh\u4e0eloadmore\u7684\u4e92\u65a5\u5224\u65ad\n\t* \u4fee\u590d\u7981\u6b62\u5237\u65b0\u7684bug\n\n\n## RecyclerAdapter\n\nRecyclerAdapter\u7b80\u5316\u4e86Adapter\u7684\u5f00\u53d1\uff0c\u5c01\u88c5\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u903b\u8f91\uff0c\u5305\u62ec\u6570\u636e\u96c6\u5408\u64cd\u4f5c\u3001\u63a5\u53e3\u76d1\u542cRecyclerItemCallback\u53ef\u4ee5\u6ee1\u8db399%\u7684\u9700\u6c42\u3002\n\n### \u4f7f\u7528\u793a\u4f8b\n\n```java\npublic class TestRecAdapter extends RecyclerAdapter<TestRecAdapter.Item, TestRecAdapter.ViewHolder> {\n\n\t@Override\n    public ViewHolder onCreateViewHolder(ViewGroup parent, int viewType) {\n      \n    }\n\n\t@Override\n    public void onBindViewHolder(ViewHolder holder, int position) {\n        \n    }\n}\n```\n\n\n## XRecyclerView\n\nXRecyclerView\u662f\u5bf9RecyclerView\u7684\u5c01\u88c5\uff0c\u5176\u4e3b\u8981\u7279\u6027\u5305\u62ec:\n\n* \u4e00\u884c\u4ee3\u7801\u6dfb\u52a0\u3001\u5220\u9664\u3001\u4fee\u6539Header\u6216\u8005Footer\n* \u4e00\u884c\u4ee3\u7801\u6dfb\u52a0\u9ed8\u8ba4\u7684\u4e0a\u62c9\u52a0\u8f7d\u6548\u679c\n* \u4e00\u884c\u4ee3\u7801\u5207\u6362\u81ea\u5b9a\u4e49\u4e0a\u62c9\u52a0\u8f7d\u6548\u679c\n* \u4e00\u884c\u4ee3\u7801\u8f7b\u677e\u6dfb\u52a0LayoutManager\n* \u4e00\u884c\u4ee3\u7801\u6dfb\u52a0divider\n* Adapter\u89c4\u8303\u53ca\u5c01\u88c5**RecyclerAdapter**\n\n### \u4f7f\u7528\u793a\u4f8b\n\n```java\nrecyclerView.verticalLayoutManager(this)        //\u8bbe\u7f6elayoutManager\n            .setAdapter(adapter);                   //\u8bbe\u7f6eAdapter\nrecyclerView.horizontalDivider(R.color.x_red, R.dimen.divider_height);  //\u8bbe\u7f6edivider\nrecyclerView.setOnRefreshAndLoadMoreListener(new XRecyclerView.OnRefreshAndLoadMoreListener() { //\u8bbe\u7f6e\u5237\u65b0\u548c\u4e0a\u62c9\u52a0\u8f7d\u76d1\u542c\n            @Override\n            public void onRefresh() {\n                loadData(1);\n            }\n\n            @Override\n            public void onLoadMore(int page) {\n                loadData(page);\n            }\n        });\nrecyclerView.useDefLoadMoreView();      //\u4f7f\u7528\u9ed8\u8ba4\u7684\u4e0a\u62c9\u5237\u65b0\u6837\u5f0f\nrecyclerView.addHeaderView(headView);       //\u6dfb\u52a0header\nrecyclerView.addFooterView(footview);       //\u6dfb\u52a0footer\nrecyclerView.removeHeaderView(headview);    //\u5220\u9664header\nrecyclerView.removeFooterView(footview);    //\u5220\u9664footer\nrecyclerView.setRefreshEnabled(true);\t//\u8bbe\u7f6e\u662f\u5426\u53ef\u4e0b\u62c9\u5237\u65b0\n```\n\n## XRecyclerContentLayout\n\nXRecyclerContentLayout\u7ee7\u627f\u4e86[**XStateController**](https://github.com/limedroid/XStateController)\uff0c\u53ef\u81ea\u5b9a\u4e49Loading\u3001Error\u3001Empty\u3001Content\u56db\u79cd\u663e\u793a\u72b6\u6001\uff0c\u6ee1\u8db3\u4e86\u7edd\u5927\u90e8\u5206\u9700\u6c42.\n\n### \u4f7f\u7528\u793a\u4f8b\n\n```xml\n<cn.droidlover.xrecyclerview.XRecyclerContentLayout\n        android:id=\"@+id/contentLayout\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        app:recyclerBackgroundColor=\"#f5f5f5\"\n        app:recyclerScrollbarNone=\"true\"\n        app:recyclerScrollbarStyle=\"outsideOverlay\" />\n```\n\n```java\ncontentLayout.loadingView(new LoadingView(this));\n             .errorView(new ErrorView(this));\n             .emptyView(new EmptyView(this));\n\ncontentLayout.showLoading();\ncontentLayout.showError();\ncontentLayout.showEmpty();\ncontentLayout.showContent();\n```\n\n### \u81ea\u5b9a\u4e49LoadMoreView\u7684\u5b9e\u73b0\n\n\u81ea\u5b9a\u4e49LoaderMoreView\u53ea\u9700\u5b9e\u73b0LoadMoreUIHandler\u63a5\u53e3\uff0c\u7136\u540e\u8c03\u7528xrecyclerView\u7684loadMoreFooterView(loadMoreView)\u65b9\u6cd5\u5373\u53ef\u3002\n\n```java\n @Override\n    public void onLoading() {\n        setVisibility(VISIBLE);\n        tvMsg.setText(\"\u52a0\u8f7d\u4e2d\");\n        progressBar.setVisibility(VISIBLE);\n    }\n\n    @Override\n    public void onLoadFinish(boolean hasMore) {\n        if (hasMore) {\n            setVisibility(GONE);\n        } else {\n            setVisibility(VISIBLE);\n            tvMsg.setText(\"\u6ca1\u6709\u66f4\u591a\u6570\u636e\");\n            progressBar.setVisibility(GONE);\n        }\n    }\n```\n\u8bbe\u7f6eloadMoreView\n```java\nrecyclerView.loadMoreFooterView(loadMoreView);\n```\n\n### \u81ea\u5b9a\u4e49\u5b9e\u73b0XRecyclerContentLayout\n\n\u5728XRecyclerContentLayout\u4e2d\u5185\u7f6e\u4e86SwipeRefreshLayout\u4e0b\u62c9\u5237\u65b0\u6837\u5f0f\uff0c\u60a8\u4e5f\u53ef\u4ee5\u81ea\u5b9a\u4e49\u5b9e\u73b0XRecyclerContentLayout\uff0c\u53ea\u9700\u5b9e\u73b0XRecyclerView.StateCallback\u63a5\u53e3\u5373\u53ef.\n\n```java\npublic interface StateCallback {\n        void notifyEmpty();     //\u6570\u636e\u4e3a\u7a7a\n\n        void notifyContent();   //\u663e\u793acontentview\n\n        void refreshState(boolean isRefresh);   //\u66f4\u65b0\u5237\u65b0\u72b6\u6001\n\n        void refreshEnabled(boolean isEnabled); //\u5237\u65b0\u662f\u5426\u53ef\u7528\n    }\n```\n\n***\u8be6\u60c5demo\u53ef\u89c1app module\u3002***\n\n## \u76f8\u5173\u5e93\n\n* [XStateController](https://github.com/limedroid/XStateController) \u6700\u5b8c\u7f8e\u7684\u72b6\u6001\u63a7\u5236\u673a\uff0c\u652f\u6301loading\u3001error\u3001empty\u3001content\u56db\u79cd\u72b6\u6001\u7684\u81ea\u5b9a\u4e49\u52a8\u753b\u5207\u6362\n\n## \u5173\u4e8e\u6211\n\n**Email** : droidlover@126.com\n\n \n\n"
 },
 {
  "repo": "zalando/opentracing-toolbox",
  "language": "Java",
  "readme_contents": "# OpenTracing Toolbox\n\n> \u26a0\n>  \n> This project is in maintenance mode - because OpenTracing has been [archived](https://www.cncf.io/blog/2022/01/31/cncf-archives-the-opentracing-project/).\n> \n> Please [migrate your OpenTracing applications to OpenTelemetry](https://opentelemetry.io/docs/migration/opentracing/).\n> \n> We will continue to support this project with bugfixes until most applications in Zalando have been migrated to OpenTelemetry.\n\n[![Stability: Active](https://masterminds.github.io/stability/active.svg)](https://masterminds.github.io/stability/active.html)\n![Build Status](https://github.com/zalando/opentracing-toolbox/workflows/build/badge.svg)\n[![Coverage Status](https://img.shields.io/coveralls/zalando/opentracing-toolbox/main.svg)](https://coveralls.io/r/zalando/opentracing-toolbox)\n[![Code Quality](https://img.shields.io/codacy/grade/69e173024eec403797466e147a2051a3/main.svg)](https://www.codacy.com/app/whiskeysierra/opentracing-toolbox)\n[![Release](https://img.shields.io/github/release/zalando/opentracing-toolbox.svg)](https://github.com/zalando/opentracing-toolbox/releases)\n[![Maven Central](https://img.shields.io/maven-central/v/org.zalando/opentracing-toolbox.svg)](https://maven-badges.herokuapp.com/maven-central/org.zalando/opentracing-toolbox)\n[![OpenTracing](https://img.shields.io/badge/OpenTracing-enabled-blue.svg)](http://opentracing.io)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/zalando/opentracing-toolbox/main/LICENSE)\n\n*OpenTracing Toolbox* is a collection of libraries that build on top of [OpenTracing](https://opentracing.io/) and provide extensions and plugins to existing instrumentations.\n\n- **Status**: Under development and used in production\n\n## Features\n\n- Legacy [FlowID](#flowid) compatibility support \n- JDBC support\n- Extensible [OpenTracing Proxy](#proxy) (wrapper) w/ support for listeners and interceptors\n- Servlet support\n- Spring Web and Webflux support\n- Sensible defaults\n\n## Modules\n\n### FlowID\n\nThe `opentracing-flowid` module replaces the former [`zalando/tracer`](https://github.com/zalando/tracer) library by providing support for the `X-Flow-ID` header propagation on top of OpenTracing.\n\nGo checkout out its [README](opentracing-flowid) for details. \n\n### JDBC\n\nThe `opentracing-jdbc` module is a direct **competitor** to [opentracing-contrib/java-jdbc](https://github.com/opentracing-contrib/java-jdbc). \n\nGo checkout out its [README](opentracing-jdbc) for details. \n\n### Proxy\n\nThe `opentracing-proxy` module is a direct **competitor** to [opentracing-contrib/java-api-extensions](https://github.com/opentracing-contrib/java-api-extensions). \n\nGo checkout out its [README](opentracing-proxy) for details. \n\n### Servlet Extension\n\nThe `opentracing-servlet-extension` module is an **extension** to `io.opentracing.contrib:opentracing-web-servlet-filter` and only useful if used in conjunction.\n\nGo checkout out its [README](opentracing-servlet-extension) for details. \n\n### Spring Web Extension\n\nThe `opentracing-spring-web-extension` module is an **extension** to `io.opentracing.contrib:opentracing-spring-web` and only useful if used in conjunction.\n\nGo checkout out its [README](opentracing-spring-extension/opentracing-spring-web-extension) for details. \n\n### Spring Webflux Extension\n\nThe `opentracing-spring-webflux-extension` module is an **extension** to `io.opentracing.contrib:opentracing-spring-web` and only useful if used in conjunction.\n\nGo checkout out its [README](opentracing-spring-extension/opentracing-spring-webflux-extension) for details. \n\n## Getting Help\n\nIf you have questions, concerns, bug reports, etc., please file an issue in this repository's [Issue Tracker](../../issues).\n\n## Getting Involved/Contributing\n\nTo contribute, simply make a pull request and add a brief description (1-2 sentences) of your addition or change. For\nmore details, check the [contribution guidelines](.github/CONTRIBUTING.md).\n\n## Alternatives\n\nTracer, by design, does not provide sampling, metrics or annotations. Neither does it use the semantics of spans as\nmost of the following projects do. If you require any of these, you're highly encouraged to try them.\n\n- [The OpenTracing Project](http://opentracing.io/)\n- [Apache HTrace](http://htrace.incubator.apache.org/)\n- [Spring Cloud Sleuth](http://cloud.spring.io/spring-cloud-sleuth/)\n- [Zipkin](http://zipkin.io/)\n"
 },
 {
  "repo": "liaohuqiu/android-ActionQueue",
  "language": "Java",
  "readme_contents": "[![Android Gems](http://www.android-gems.com/badge/liaohuqiu/android-ActionQueue.svg?branch=master)](http://www.android-gems.com/lib/liaohuqiu/android-ActionQueue)\n\r\nActionQueue allows you run action one by one.\n\n<div><img src='https://raw.githubusercontent.com/liaohuqiu/android-ActionQueue/master/images/screen-snapshot.gif' width=\"300px\" style='border: #f1f1f1 solid 1px'/></div>\n\n#### Import\n\nRepositories:\n\n```groovy\nallprojects {\n    repositories {\n        mavenCentral()\n        maven {\n            url \"https://oss.sonatype.org/content/repositories/snapshots\"\n        }\n        jcenter()\n    }\n}\n```\n\nAdd to dependencies:\n\n```groovy\ncompile 'in.srain.cube:action-queue:1.0.1'\n```\n\n#### Usage\n\n* create actions\n\n    ```java\n    String[] messageList = new String[]{\n            \"message 1\",\n            \"message 2\",\n            \"message 3\",\n    };\n    for (int i = 0; i < messageList.length; i++) {\n        String message = messageList[i];\n        PopDialogAction action = new PopDialogAction(message);\n        mActionQueue.add(action);\n    }\n    ```\n\n* process action \n\n    ```java\n    class PopDialogAction extends ActionQueue.Action<String> {\n    \n        public PopDialogAction(String badge) {\n            super(badge);\n        }\n    \n        @Override\n        public void onAction() {\n            AlertDialog.Builder builder = new AlertDialog.Builder(MainActivity.this);\n            Dialog dialog = builder.setMessage(getBadge()).show();\n            // notify action is done, and next aciton will be executed\n            dialog.setOnDismissListener(mOnDismissListener);\n        }\n    }\n    ```\n\n* notify when action is done\n\n    ```java\n    DialogInterface.OnDismissListener mOnDismissListener = new DialogInterface.OnDismissListener() {\n        @Override\n        public void onDismiss(DialogInterface dialog) {\n            mActionQueue.notifyActionDoneThenTryToPopNext();\n        }\n    };\n    ```\n\n* LICENSE: MIT\n\n\n"
 },
 {
  "repo": "wuhaiwei/SSZipArchive",
  "language": "C",
  "readme_contents": "# SSZipArchive\n\nSSZipArchive is a simple utility class for zipping and unzipping files. Features:\n\n* Unzipping zip files\n* Unzipping password protected zip files\n* Creating zip files\n* Appending to zip files\n* Zipping files\n* Zipping NSData with a filename\n\n## Adding to your project\n\n1. Add the `SSZipArchive` and `minizip` folders to your project.\n2. Add the `libz` library to your target\n\nSSZipArchive requires ARC.\n\n## Usage\n\n``` objective-c\n// Unzipping\nNSString *zipPath = @\"path_to_your_zip_file\";\nNSString *destinationPath = @\"path_to_the_folder_where_you_want_it_unzipped\";\n[SSZipArchive unzipFileAtPath:zipPath toDestination:destinationPath];\n\n// Zipping\nNSString *zippedPath = @\"path_where_you_want_the_file_created\";\nNSArray *inputPaths = [NSArray arrayWithObjects:\n                       [[NSBundle mainBundle] pathForResource:@\"photo1\" ofType:@\"jpg\"],\n                       [[NSBundle mainBundle] pathForResource:@\"photo2\" ofType:@\"jpg\"]\n                       nil];\n[SSZipArchive createZipFileAtPath:zippedPath withFilesAtPaths:inputPaths];\n```\n\n## Tests\n\nSimply, open the Xcode 5 or higher project in the Tests directory and press Command-U to run the tests.\n\n## License\n\nSSZipArchive is licensed under the [MIT license](https://github.com/samsoffes/ssziparchive/raw/master/LICENSE).  A slightly modified version of [Minizip](http://www.winimage.com/zLibDll/minizip.html) 1.1 is also included and is licensed under the [Zlib license](http://www.zlib.net/zlib_license.html).\n\n## Thanks\n\nThanks [aish](http://code.google.com/p/ziparchive) for creating [ZipArchive](http://code.google.com/p/ziparchive) which SSZipArchive is based on, Johnnie Walker ([@randomsequence](https://github.com/randomsequence)) for implementing creation support, and John Engelhart ([@johnezang](https://github.com/johnezang)) for all his amazing help along the way.\n"
 },
 {
  "repo": "rliou92/umonitor",
  "language": "C",
  "readme_contents": "# umonitor\nManage monitor configuration automatically\n\nThe goal of this project is to implement *desktop environment independent* dynamic monitor\nmanagement. Dynamic monitor management means that the positions and resolutions\nof the monitors will automatically be updated whenever monitors are\nhotplugged. This program is written in C using XCB to directly communicate with the X11 server and consists of only one binary. This program is targeted at users who are using a window manager on a laptop who hotplug monitors frequently.\n\nI encourage you to look at a rewrite of this program in Cython/Python here called [python-umonitor](https://github.com/rliou92/python-umonitor). A higher level language such as Python allows quicker development times and easier maintenance, and using Cython allows me to continue using XCB's API. The program also has several additional features.\n\n# Installation\nRun `make`. `umonitor` binary will be created in `bin`.\n\nFor Arch Linux users there is an AUR package [here](https://aur.archlinux.org/packages/umonitor-git/).\n\n# Usage\n\n<!--\n1. I have renamed the configuration file from `umon2.conf` to `umon.conf` because that extra '2' is unnecessary.\n1. The program daemonizes itself when called with `--listen`, so do not run the program in the background while called with the `--listen` flag anymore.\n-->\n\n\n* Setup your monitor resolutions and positions using `xrandr` or related tools (`arandr` is a good one).\n* Run `umonitor --save <profile_name>`.\n* Run `umonitor --listen` to daemonize the program and begin automatically applying monitor setup.\n\nThe configuration file is stored in `~/.config/umon.conf`. You can load a\nprofile manually by executing `umonitor --load <profile_name>`. Profiles can be deleted `umonitor --delete <profile_name>`.\n\nExample scenario: You are working on a laptop. You want to save just the monitor\nconfiguration of just the laptop screen into the profile name called 'home'. At\nhome you plug in an external monitor, and you want to save that configuration as\n'docked'.\n\n```\n# With only the laptop screen (no external monitors)\n$ umonitor --save home\nProfile home saved!\n\n# Plug in external monitor\n\n# Setup your desired configuration\n$ xrandr --output HDMI-1 --mode 1920x1080 --pos 1600x0\n$ xrandr --output eDP1 --mode 1600x900 --pos 0x0\n\n# Save the current configuration into a profile\n$ umonitor --save docked\nProfile docked saved!\n\n# Begin autodetecting changes in monitor\n$ umonitor --listen\nhome\ndocked*\n---------------------------------\n# Monitor is unplugged\nhome*\ndocked\n---------------------------------\n```\n\nProgram help can also be viewed through `umonitor --help`.\n\nIf you would like to auto start this program, you can add the program to your .xinitrc:\n```\n$ cat ~/.xinitrc\n#!/bin/sh\n...\n...\n...\numonitor --listen --quiet\nexec i3 # your window manager of choice\n```\n\n# Features\nGive me some feedback!\n\n* What is saved and applied dynamically:\n  * Monitor vendor name + model number\n  * Crtc x and y position\n  * Resolution\n  * Primary output\n  * Rotation\n* Daemonizes when called with `--listen`\n* Prevents saving of duplicate profiles\n* Valgrind clean\n\nFuture improvements:\n\n<!-- TODO only load when configuration matches profile  -->\n<!-- TODO view profile contents -->\n<!--* Implement debug as compile option -->\n<!-- * Encoding of resolution -->\n<!-- * More commandline options\n  * Alternate configuration file location? -->\n<!-- * Handling the case when multiple outputs are connected to same crtc? -->\n* Bugs:\n  * Tell me! Run umonitor with the `--verbose` flag to get debugging output\n  <!-- * After umonitor -a the output displays an extra unknown character -->\n<!--* Updating Doxygen documentation? -->\n\nI'm open for any feature requests!\n\n# Inspiration\nI drew inspiration of this program from [udiskie](https://github.com/coldfix/udiskie). I enjoy using only window managers. For me, udiskie is one essential program to automount removable storage media. I would just include it in my .xinitrc and not have to worry about mounting anything manually again. I thought to myself, \"Why not have the same program but for managing monitor hotplugging?\"\n\nAt first, I found that the most common solutions to managing monitor hotplugging were using xrandr scripts. These scripts seemed really hacky to me, involving hardcoding of the DISPLAY environment variable. The reason why the DISPLAY environment variable needed to be hardcoded is because these scripts would be run by udev. udev runs your desired script as root and has no idea of the desired user's DISPLAY environment variable when it detects monitors being hotplugged.\n\nThe most popular program that manages monitor setups autorandr also has this problem, as it is setup to be run from a udev rule. It solves it by checking all processes not owned by root and seeing if the user of that process has a DISPLAY variable. For *all users* that do, it forks itself and changes its uid/guid to that user. Autorandr does not know which user you are, it just runs for all users!\n\nI believed a better solution existed. By using the XCB library, I can communicate directly with the X11 server with this program running as just a single user. I do not need to rely on udev directly because the X11 server sends signals when monitors are hotplugged. Furthermore, using this program is as simple as including it in the .xinitrc, just like udiskie. For a Linux laptop user who uses a window manager only like me, I believe this software is almost a necessity for a good user experience!\n\n# Related Projects\n\n* [autorandr](https://github.com/phillipberndt/autorandr)\n* [srandrd](https://github.com/jceb/srandrd) and [screenconfig](https://github.com/jceb/screenconfig)\n\nThis program is different from existing projects because it is written entirely\nin C using xcb calls and accomplishes dynamic monitor management in a single\nbinary. Unlike autorandr, there is no need to install rules for udev or hooks\nfor pmutils. Srandrd + screenconfig seems pretty promising to me, but there are\nfeatures missing such as setting the crtc xy positions. Maybe if you combine srandrd with autorandr I would view the solution as not hacky.\n\n# About\nThis is my personal project. My motivation for writing this program comes from\nusing i3 wm on my laptop. From writing this program I have learned a great\ndeal about how to interact with the X11 server, trying out OOP in C, and hope to continue learning even more in the future. I want\nto work on this project until I deem that it is \"complete\", fufilling its\npurpose of dynamic monitor management for those who do not use a desktop\nenvironment.\n\n# Credits\nI borrowed the edid parsing code from [eds](https://github.com/compnerd/eds).\n"
 },
 {
  "repo": "kkoenig/wimproved.vim",
  "language": "C",
  "readme_contents": "## :sparkles: wimproved.vim [![Build status](https://ci.appveyor.com/api/projects/status/8qn9sgwhmc4ppqx0/branch/master?svg=true)](https://ci.appveyor.com/project/kkoenig/wimproved-vim/branch/master)\n\n### Introduction\nAn effort to create a better editing experience for Vim on Windows.<br>\nSupports fullscreen while taking care to fix visual glitches seen in\nother plugins.\n\nPut the following in your .vimrc and enjoy!\n```VimL\nautocmd GUIEnter * silent! WToggleClean\n```\n\n### Commands\n`:WCenter {scale}`\n  - Centers the window on the current monitor.  If scale is non-zero, resizes<br>\n    the window as a percentage (scale / 100) of the current monitor size.\n\n`:WSetAlpha {alpha}`\n  - Sets the alpha of the window to the given value.\n\n`:WToggleFullscreen`\n  - Toggles full-screen support.\n\n`:WToggleClean`\n  - Toggles between the default and 'clean' window styling.\n\n### Installation\nInstall using your favorite plugin manager.  The plugin expects wimproved.dll to exist in the root plugin folder.\nIf you have `cmake` and Visual Studio installed run the following:\n\n```shell\ncmake -G \"NMake Makefiles\" . && nmake\n```\n\n### Running tests \n```shell\nnpm install\nnpm test\n```\n\n### Contributions\nContributions and pull requests are welcome.\n\n### License\n\nThis software is licensed under the [MIT license](http://en.wikipedia.org/wiki/MIT_License).\n\u00a9 2015 Killian Koenig &lt;<killiankoenig@gmail.com>&gt;.\n\n------\nLike this plugin? Follow this repository on\n[GitHub](https://github.com/kkoenig/wimproved.vim) and vote for it on\n[vim.org](http://www.vim.org/scripts/script.php?script_id=5265).  \n\n"
 },
 {
  "repo": "MobileForensicsResearch/mem",
  "language": "C",
  "readme_contents": "# mem\nTool used for dumping memory from Android devices. Root access is required.\n\n./mem pid out_path\n - where pid is the target PID to capture\n - and out_path is the local dir to write output\n If out_path is not there, writes to stdout\n\nTo ensure forensic soundness, mem should be copied into memory (/dev or another tmpfs location), and netcat should be used to write data out over ADB to avoid writing to the device. Netcat versions compiled for Android can be found at https://github.com/MobileForensicsResearch/netcat\n\nEg:\n\n1: On local machine run:\nadb forward tcp:9999 tcp:9999\n\n2: From adb shell run:\n./mem pid | nc -l -p 9999\n\n3: On local machine run:\nnc 127.0.0.1 9999 > output_file"
 },
 {
  "repo": "cyfdecyf/hanzi2pinyin",
  "language": "C",
  "readme_contents": "This library converts Chinese character to pinyin.\n\nLimitations:\n\n1. Does not handle polyphone (now).\n2. The pinyin data is obtained from Unihan database. Maybe not complete or\n   incorrect for some characters. But I can't find more accurate data from other\n   resources.\n"
 },
 {
  "repo": "rsms/fontkit",
  "language": "C",
  "readme_contents": "# FontKit\n\nFontKit is a JavaScript/WebAssembly library for working with fonts,\nbacked by industry-strength\n[Freetype](https://www.freetype.org/) and\n[Harfbuzz](http://harfbuzz.org/).\n\n> This is a project in its early stages of development\n\n## Building\n\nThe only requirement for building fontkit is\n[Docker](https://www.docker.com/get-docker)\n\n```\n./build.sh\n```\n\nOutput is generated in the `build` directory.\n\nFor available options, run `./build.sh -help`\n\n"
 },
 {
  "repo": "zsa/reactor",
  "language": "C",
  "readme_contents": "# Reactor\n\nReactor is the firmware generator part of [Fusion](https://github.com/ErgoDox-EZ/fusion).\n\nIt takes the JSON's exported by the Fusion project and process them in to ready-to-be-downloaded firmware.\n\nReactor uses the awesome [qmk_firmware](http://github.com/jackhumbert/qmk_firmware) by Jack Humbert. The firmware is included through a git  subtree. To update the firmware, check out [updating the firmware](#updating-the-firmware).\n\n\n## General process\n\n- Take JSON input\n- Generate a .c template file based on the JSON and the liquid template. Name it something unique\n- Create a hex file by compiling that c file. Name it something unique too\n- Read .hex file and return it\n\n# Local development\n\nYou can run the tests with `be guard` - which will watch for changes in ruby files and run whenever they do.\n\nTesting is done in ruby 2.2.2\n\n## Updating the firmware\n\nThe qmk_firmware is included in this repository as a git subtree. Any **changes to qmk_firmware should be made to the [qmk_firmware repository](http://github.com/jackhumbert/qmk_firmware)** not this repository.\n\nTo update the subtree from the qmk_firmware repository, pull the updates from qmk_firmware into this repository.\n\n**If you don't have the remote and the subtree setup:**\n\n1. Add the remote for qmk_firmware\n    \n    `git remote add -f  qmk_firmware git@github.com:jackhumbert/qmk_firmware.git`\n\n2. Add the subtree\n    \n    `git subtree add --prefix lib/firmware qmk_firmware master --squash`\n\n**If you have the remote and subtree setup:**\n\n1. Fetch the updates\n\n    `git fetch qmk_firmware master`\n\n2. Update the subtree\n    \n    `git subtree pull --prefix lib/firmware qmk_firmware master --squash`\n\n<sub>[Read more about git subtrees here](http://blogs.atlassian.com/2013/05/alternatives-to-git-submodule-git-subtree/)</sub>\n\n### Compiling firmware\n\n#### Linux\n\nUsing the instructions from [the teensy loader build environment](https://www.pjrc.com/teensy/gcc.html):\n\n    sudo apt-get install gcc-avr binutils-avr avr-libc\n\n#### Mac OSX\n\nUsing homebrew it's easy to compile the firmware:\n\n    brew tap osx-cross/avr\n    brew install avr-libc\n    \n\nNext cd into the appropriate folder of the qmk_firmware, say keyboard/ergodox_ez\n \n    cd keyboard/ergodox_ez\n    \nThen run `make` for the default keymap, or\n    \n    make KEYMAP=\"yourown\"\n    \nfor your own keymap. This requires a files keymap_yourown.c in the subfolder keymaps.\nYou should end up with a `ergodox_ez.hex` file, which you can use with the [Teensy loader](http://www.pjrc.com/teensy/loader_cli.html) or [Teensy GUI](https://www.pjrc.com/teensy/loader.html)\n"
 },
 {
  "repo": "justinethier/node-kdtree",
  "language": "C",
  "readme_contents": "[<img src=\"https://raw.githubusercontent.com/justinethier/node-kdtree/master/doc/node-kdtree-logo.png\" alt=\"\">](https://github.com/justinethier/node-kdtree) \n#node-kdtree\n\nnode-kdtree is a node.js addon that defines a wrapper to libkdtree, allowing one to work with KD trees directly in node. A [KD tree](http://en.wikipedia.org/wiki/Kd-tree) is a data structure that organizes points in a multi-dimensional space, and in particular is useful for performing efficient nearest neighbor searches.\n\n## Dependencies\nThe [kdtree](https://github.com/jtsiomb/kdtree) C library is required. In order to install, get the latest version from [here](https://github.com/jtsiomb/kdtree/releases) and run the following commands:\n\n    ./configure\n    make\n    sudo make install PREFIX=/usr \n\n## Installation\nThe easiest way to install node-kdtree is to use the [npm](https://github.com/isaacs/npm) package manager:\n\n    npm install kdtree\n\n## Usage\n\n###Creating a tree\nYou may create a tree by instantiating a new `KDTree` object:\n\n    var kd = require('kdtree');\n    var tree = new kd.KDTree(3); // A new tree for 3-dimensional points\n\nWhen creating a new tree we can specify the dimensions of the data. For example, a three-dimensional tree will contain points of the form (x, y, z). If a dimension is not specified, the tree defaults to three dimensions.\n\n###Adding data to a tree\nData may be added to the tree using the `insert` method:\n\n    tree.insert(1, 2, 3);\n    tree.insert(10, 20, 30);\n\nThere must be one argument for each dimension of the data - for example, a three dimensional tree would have three arguments to `insert`. An optional data parameter may also be specified to store a data value alongside the point data:\n\n    tree.insert(39.285785, -76.610262, \"USS Constellation\");\n\n###Nearest neighbor searches\nThe `nearest` method is used to find the point in the tree that is closest to a target point. For example:\n\n    > tree.nearest(39.273889, -76.738056);\n    [39.272051, -76.731917, \"Bill's Music, Inc.\"]\n\n`nearest` will return an array containing closest point, or an empty array if no points were found. As shown above, if the point contains a data value, that value will also be returned at the end of the array.\n\n\nA `nearestRange` method is also provided, which allows us to find all of the points within a given range. For example:\n\n    > tree.nearestRange(0, 0, 3);\n    [ [ 1, 1 ],\n      [ 0, 2 ],\n      [ 2, 0 ],\n      [ 1, 0 ],\n      [ 0, 1 ],\n      [ 0, 0 ] ]\n\nThe first arguments to `nearestRange` are the components of the point to begin searching at. The last argument is the search range.\n\n##API\n\n[API documentation](https://github.com/justinethier/node-kdtree/blob/master/doc/API.markdown)\n\n##Credits\n\nnode-kdtree is developed by [Justin Ethier](http://github.com/justinethier).\n\nThanks to John Tsiombikas for developing libkdtree!\n\nPatches are welcome; please send via pull request on github.\n"
 },
 {
  "repo": "tang3w/CocoaSugar",
  "language": "C",
  "readme_contents": "Introduction\n============\n\nCocoaSugar is an Objective-C library that can make developing apps easier. It includes a collection of runtime and Cocoa Touch improvements to solve some practical problem.\n\nDocumentation\n=============\n\n## `COSLayout`\n\n`COSLayout` is yet another layout library. It's neither a wrapper nor a replacement for Auto Layout. It dose not handle circular references of constraints and constraint priority. Besides that, `COSLayout` can solve all layout cases. What's more, `COSLayout` provides some additional benefits: smaller memory footprint, better performance and more intuitive expression.\n\n`COSLayout` is an abstraction of layout of view. With `COSLayout`, you can specify view's layout relative to it's superview, sibling views and non-sibling views. Following example specifies a 10-points constraint from view's bottom to superview's bottom:\n\n```objc\nUIView *view = [[UIView alloc] init];\n\nCOSLayout *layout = [COSLayout layoutOfView:view];\n\n[layout addRule:@\"bb = 10\"];\n```\n\nIn the example above, a rule has been added into layout by method `addRule:`. A rule is expressed in Sample Layout Language or \"SLL\", which can specify constraints intuitively. The syntax of SLL is very simple, just comma-separated assignment expressions. Each assignment expression specifies a constraint, l-value is constraint name, r-value is constraint value.\n\n`COSLayout` supports 18 constraints:\n\nConstraint | Direction  | Description\n-----------|------------|------------\n`w`        | Horizontal | View's width\n`h`        | Vertical   | View's height\n`tt`       | Vertical   | Space from view's top to superview's top\n`tb`       | Vertical   | Space from view's top to superview's bottom\n`ll`       | Horizontal | Space from view's left to superview's left\n`lr`       | Horizontal | Space from view's left to superview's right\n`bb`       | Vertical   | Space from view's bottom to superview's bottom\n`bt`       | Vertical   | Space from view's bottom to superview's top\n`rr`       | Horizontal | Space from view's right to superview's right\n`rl`       | Horizontal | Space from view's right to superview's left\n`ct`       | Vertical   | Space from view's center to superview's top\n`cl`       | Horizontal | Space from view's center to superview's left\n`cb`       | Vertical   | Space from view's center to superview's bottom\n`cr`       | Horizontal | Space from view's center to superview's right\n`minw`     | Horizontal | Minimal width of view\n`maxw`     | Horizontal | Maximal width of view\n`minh`     | Vertical   | Minimal height of view\n`maxh`     | Vertical   | Maximal height of view\n\n`COSLayout` supports 5 constraint value types:\n\nConstraint Value Type | Example             | Description\n----------------------|---------------------|------------\nCGFloat               | `5` `-10` `20.0`    | Fixed length on screen\nPercentage            | `5%` `-10%` `20.0%` | Percentage of superview's width or height\nFormat specifier      | `%tt` `%w` `%f`     | Constraint value given by additional argument\nConstraint            | `tt` `maxw`         | Constraint value of current layout\nNil                   | `nil`               | Used to reset a constraint\n\nNote that the percentage has different means for different constraint directions. If current constraint direction is horizontal, the percentage represents the percentage of superview's width, otherwise, the percentage of superview's height. You can also specify direction of percentage by `H:` (horizontal) or `V:` (vertical) prefix. For example, `H:50%` means 50% width of superview, `V:30%` means 30% height of superview.\n\nFormat specifier represents a constraint value given by additional argument. For example, `%tt` is the space from other view's top to superview's top. Here, the other view is given by additional argument, and the superview is the superview of layout's view. It means that `COSLayout` can specify constraints between non-sibling views.\n\n`COSLayout` support 20 format specifiers:\n\nFormat | Type                     | Description\n-------|--------------------------|------------\n`%tt`  | `UIView *`               | Space from view's top to superview's top\n`%tb`  | `UIView *`               | Space from view's top to superview's bottom\n`%ll`  | `UIView *`               | Space from view's left to superview's left\n`%lr`  | `UIView *`               | Space from view's left to superview's right\n`%bb`  | `UIView *`               | Space from view's bottom to superview's bottom\n`%bt`  | `UIView *`               | Space from view's bottom to superview's top\n`%rr`  | `UIView *`               | Space from view's right to superview's right\n`%rl`  | `UIView *`               | Space from view's right to superview's left\n`%ct`  | `UIView *`               | Space from view's center to superview's top\n`%cl`  | `UIView *`               | Space from view's center to superview's left\n`%cb`  | `UIView *`               | Space from view's center to superview's bottom\n`%cr`  | `UIView *`               | Space from view's center to superview's right\n`%w`   | `UIView *`               | Width of view\n`%h`   | `UIView *`               | Height of view\n`%f`   | `CGFloat`                | Fixed length on screen\n`%p`   | `CGFloat`                | Percentage of superview's width or superview's height\n`%^f`  | `CGFloat(^)(UIView *)`   | Space provided by a block\n`%^p`  | `CGFloat(^)(UIView *)`   | Percentage provided by a block\n`%@f`  | `id<COSCGFloatProtocol>` | Space provided by an object\n`%@p`  | `id<COSCGFloatProtocol>` | Percentage provided by an object\n\nLike percentage constraint value, you can also use `H:` and `V:` prefix to specify direction of percentage. For example, `H:%p`, `H:%^p`, `H:%@p` means percentage is horizontal, `V:%p`, `V:%^p`, `V:%@p` means percentage is vertical.\n\nIt is worth mentioning that, format specifier also create a dependency between two views: the layout view and the other view given by additional argument. In `COSLayout`, the dependencies is presented by DAG. So `COSLayout` do not support the circular dependencies. When superview needs layout, all layouts of subviews will solve it's constraints according to the dependencies.\n\n### Constraint value expression\n\nYou can apply arithmetic operator between constraint values. Like other languages, SLL supports 5 basic arithmetic operators:\n\nOperator name | Priority | Associativity | Code examples\n--------------|----------|---------------|--------------\n`=`           | 1        | right         | `tt = 20` `ct = 50%`\n`+=`          | 1        | right         | `tt += 20` `ct += 50%`\n`-=`          | 1        | right         | `tt -= 20` `ct -= 50%`\n`*=`          | 1        | right         | `tt *= 20` `ct *= 50%`\n`/=`          | 1        | right         | `tt /= 20` `ct /= 50%`\n`+`           | 2        | left          | `10 + 20` `50% + 10` `%w + 5`\n`-`           | 2        | left          | `20 - 10` `50% - 10` `%h - 5`\n`*`           | 3        | left          | `50 * 2` `80% * 0.5` `%h * 2`\n`/`           | 3        | left          | `100 / 2` `100% / 2` `%h / 2`\n\nYou can also use `()` to group sub-expression to change the evaluation order of expression.\n\n### Examples\n\nIn the following example, `COSLayout` aligns view's top-right corner to superview's top-right corner with 5-points space.\n\n```objc\nUIView *view = [[UIView alloc] init];\n\nCOSLayout *layout = [COSLayout layoutOfView:view];\n\n[layout addRule:@\"tt = rr = 5\"];\n```\n\nIn the following example, `COSLayout` aligns view's left/bottom/right to superview's left/bottom/right with 10-points space, and make the view's top aligned to superview's center.\n\n```objc\nUIView *view = [[UIView alloc] init];\n\nCOSLayout *layout = [COSLayout layoutOfView:view];\n\n[layout addRule:@\"ll = bb = rr = 10, tt = 50%\"];\n```\n\n## `COSObserver`\n\n`COSObserver` is an improvement of KVO. Using `COSObserver`, you can use block for KVO notification. It eliminates some inconvenience of KVO. After making an observation by `COSObserver`, there's no need to remove observer manually, `COSObserver` can remove observer for target automatically when either observer or target is dealloced.\n\nIn the following example, `observer` is the observer for `label1`. Then, `label2` is added to `observer` as target. When the text of `label2` changed, `observer` observes this change immediately and call the block, which makes text of `label1` the same as text of `label2`.\n\n```objc\nUILabel *label1 = [[UILabel alloc] init];\nUILabel *label2 = [[UILabel alloc] init];\n\nCOSObserver *observer = [COSObserver observerForObject:label1];\n\n[observer\n addTarget:label2\n forKeyPath:@\"text\"\n options:NSKeyValueObservingOptionNew\n block:^(id object, id target, NSDictionary *change) {\n     [target setText:change[NSKeyValueChangeNewKey]];\n }];\n```\n"
 },
 {
  "repo": "unosquare/pigpio-dotnet",
  "language": "C",
  "readme_contents": "[![Build status](https://ci.appveyor.com/api/projects/status/n5xt8b07j65a65tb/branch/master?svg=true)](https://ci.appveyor.com/project/geoperez/pigpio-dotnet/branch/master)\n\n# <img src=\"https://raw.githubusercontent.com/unosquare/pigpio-dotnet/master/Support/pigpio-dotnet.png\" alt=\"pgipio-dotnet\" style=\"width:16px; height:16px\" /> Raspbery Pi - libpigpio for .net\n\n**WE ARE LOOKING FOR A NEW HOME FOR THIS PROJECT. APPLY AT:** https://adoptoposs.org/p/d3470190-942b-44ac-84fc-90259cbdee43\n\n*:star: Please star this project if you find it useful!*\n\nProvides complete managed access to the popular pigpio C library.\n\nThe premise is simple: using the powerful features of C# to control the ARM peripherals of the Raspberry Pi. This library provides a comprehensive way to access the hardware of the Pi. It uses the fantastic C library [pigpio](https://github.com/joan2937/pigpio/). The [documentation of the library can be found here](http://abyz.me.uk/rpi/pigpio/).\n\nAs a programmer, the choice is yours. You can call the native methods either directly or via the comprehensive API of PiGpio.net.\n\n## Example of blinking an LED with direct native calls\n\n```csharp\nSetup.GpioInitialise();\nvar pin = SystemGpio.Bcm18;\nIO.GpioSetMode(pin, PinMode.Output);\n\nwhile (true)\n{\n    IO.GpioWrite(pin, true);\n    Thread.Sleep(500);\n    IO.GpioWrite(pin, false);\n    Thread.Sleep(500);\n}\n```\n\n## Example of blinking an LED with the PiGpio.net Managed API\n\n```csharp\nvar pin = Board.Pins[18];\n\nwhile (true)\n{\n    pin.Value = !pin.Value;\n    Thread.Sleep(500);\n}\n```\n\n## Related Projects and Nugets\n| Name | Author | Description |\n| ---- | ------ | ----------- |\n| [RaspberryIO](https://github.com/unosquare/raspberryio) | [Unosquare](https://github.com/unosquare) | The Raspberry Pi's IO Functionality in an easy-to-use API for .NET (Mono/.NET Core). |\n| [PiGpio.net](https://github.com/unosquare/pigpio-dotnet) | [Unosquare](https://github.com/unosquare) | Provides complete managed access to the popular pigpio C library |\n| [Raspberry Abstractions](https://www.nuget.org/packages/Unosquare.Raspberry.Abstractions) | [Unosquare](https://www.nuget.org/profiles/Unosquare) | Allows you to implement your own provider for RaspberryIO. |\n| [Raspberry# IO](https://github.com/raspberry-sharp/raspberry-sharp-io) | [raspberry-sharp](https://github.com/raspberry-sharp) | Raspberry# IO is a .NET/Mono IO Library for Raspberry Pi. This project is an initiative of the [Raspberry#](http://www.raspberry-sharp.org/) Community. |\n| [WiringPi.Net](https://github.com/danriches/WiringPi.Net) | [Daniel Riches](https://github.com/danriches) | A simple C# wrapper for Gordon's WiringPi library. |\n| [PiSharp](https://github.com/andycb/PiSharp) |[Andy Bradford](https://github.com/andycb) | Pi# is a library to expose the GPIO functionality of the Raspberry Pi computer to the C# and Visual Basic.Net languages |\n"
 },
 {
  "repo": "seomoz/pyreBloom",
  "language": "Python",
  "readme_contents": "Python + Redis + Bloom Filter = pyreBloom\n=====================================================\n[![Build Status](https://travis-ci.org/seomoz/pyreBloom.svg)](https://travis-ci.org/seomoz/pyreBloom)\n![Status: Production](https://img.shields.io/badge/status-production-green.svg?style=flat)\n![Team: Big Data](https://img.shields.io/badge/team-big_data-green.svg?style=flat)\n![Scope: External](https://img.shields.io/badge/scope-external-green.svg?style=flat)\n![Open Source: MIT](https://img.shields.io/badge/open_source-MIT-green.svg?style=flat)\n![Critical: Yes](https://img.shields.io/badge/critical-yes-red.svg?style=flat)\n\nOne of Salvatore's suggestions for Redis' GETBIT and SETBIT commands is to\nimplement bloom filters. There was an existing python project that we used\nfor inspiration.\n\nNotice\n======\n__Important__ -- The most recent version uses different seed values from all\nprevious releases. Previous releases were using `srand` and `rand`, though they\nare not guaranteed to yield the same values on different systems. For example,\ntwo clients compiled on different platforms with different C implementations\nmay not necessarily agree on what's in the filters. This latest version fixes\nthis, but will also be incompatible with filters constructed with any previous\nversions.\n\nInstallation\n============\n\nYou will need `hiredis` installed, and a C compiler (probably GCC). You can\noptionally have `Cython` installed, which will generate the C extension code.\nWith those things installed, it's pretty simple:\n\n```bash\npip install -r requirements.txt\npython setup.py install\n```\n\nHiredis\n-------\nIn order to install `hiredis`, you can:\n\n```bash\n# From https://github.com/paulasmuth/recommendify/issues/6#issuecomment-4496616\n# via https://github.com/seomoz/pyreBloom/issues/7#issuecomment-21182063\n#\n# On Mac:\nbrew install hiredis\n\n# With Ubuntu:\napt-get install libhiredis-dev\n\n# From source:\ngit clone https://github.com/redis/hiredis\ncd hiredis && make && sudo make install\n```\n\nUsage\n=====\n\nThere are serial and batch forms for both `add` and `contains`. The batch \nmodes are about 4-5 times faster than their serial equivalents, so use them\nwhen you can. When you instantiate a pyreBloom, you should give it a redis\nkey name, a capacity, and an error rate:\n\n```python\nimport pyreBloom\np = pyreBloom.pyreBloom('myBloomFilter', 100000, 0.01)\n# You can find out how many bits this will theoretically consume\np.bits\n# And how many hashes are needed to satisfy the false positive rate\np.hashes\n```\n\nFrom that point, you can add elements quite easily:\n\n```python\ntests = ['hello', 'how', 'are', 'you', 'today']\np.extend(tests)\n```\n\nThe batch mode of `contains` differs from the serial version in that it actually\nreturns which elements are in the bloom filter:\n\n```python\np.contains('hello')\n# True\np.contains(['hello', 'whats', 'new', 'with', 'you'])\n# ['hello', 'you']\n'hello' in p\n# True\n```\n\nThe Story\n=========\n\nWe needed to keep track of sets of urls that we had seen when crawling web\npages, and had previously been keeping track of them in redis sets. Redis \nsets are, after all, extremely fast. As you can see in the benchmarks, set\ninsertions can handle about 500k 20-character insertions per second. _That_\nis performant.\n\nHowever, these sets of urls got to be prohibitively large. But, since we \ndidn't really need to know _which_ urls we had seen but merely whether or\nnot we had seen a given url, we started inserting hashes of urls into redis\nsets. Unfortunately, even these got to be prohibitively large. We tried a\nlot of things, including limiting the number of discovered urls, but we \nalso thought about using bloom filters.\n\nThere was an existing library to use redis strings as bloom filters, but it\nwasn't inserting elements fast enough for our liking. By implementing our\nhash functions in pure C we were able to double our performance. Using the \nC bindings for redis (hiredis), we were able to squeeze another 5x performance\nboost, for a total of about 10x over the original implementation.\n\nRough Bench\n===========\n\nHere are numbers from the benchmark script run on a 2011-ish MacBook Pro\nand Redis 2.4.0, inserting 10k 20-character psuedo-random words:\n\n\tGenerating 20000 random test words\n\tGenerated random test words in 0.365890s\n\tFilter using 4 hash functions and 95850 bits\n\tBatch insert : 0.209492s (47734.526951 words / second)\n\tSerial insert: 0.770047s (12986.217154 words / second)\n\tBatch test   : 0.170484s (58656.590137 words / second)\n\tSerial test  : 0.728285s (13730.886920 words / second)\n\tFalse positive rate: 0.012300 (0.100000 expected)\n\tRedis set add  : 0.023647s (422885.373502 words / second)\n\tRedis pipe chk : 0.244068s (40972.163611 words / second)\n\tRedis pipe sadd: 0.241150s (41467.941791 words / second)\n\tRedis pipe chk : 0.240877s (41514.979051 words / second)\n\nWhile set insertions are __much__ faster than our bloom filter insertions\n(this is mostly do to the fact that there's not a 'SETMBIT' command), the\npipelined versions of 'sadd' and checking for membership in the set are\nactually a little slower than the bloom filter implementation. Win some, \nlose some.\n\n"
 },
 {
  "repo": "mitmul/chainer-faster-rcnn",
  "language": "Python",
  "readme_contents": "# Faster R-CNN\n\n# **This repo has been deprecated. [Here](https://github.com/pfnet/chainercv) is the complete codes for training Faster-RCNN on your data and using the pre-trained Faster-RCNN model for new data: [ChainerCV](https://github.com/pfnet/chainercv)**\n\nThis is an experimental implementation of Faster R-CNN in Chainer based on Ross Girshick's work: [py-faster-rcnn codes](https://github.com/rbgirshick/py-faster-rcnn).\n\n## Requirement\n\nUsing anaconda is strongly recommended.\n\n- Python 2.7.6+, 3.4.3+, 3.5.1+\n\n  - [Chainer](https://github.com/pfnet/chainer) 1.22.0+\n  - NumPy 1.9, 1.10, 1.11\n  - Cython 0.25+\n  - OpenCV 2.9+, 3.1+\n\n### Installation of dependencies\n\n```\npip install numpy\npip install cython\npip install chainer\npip install chainercv\n# for python3\nconda install -c https://conda.binstar.org/menpo opencv3\n# for python2\nconda install opencv\n```\n\n### For Windows users\n\nThere's a known problem in cpu_nms.pyx. But a workaround has been posted [here](https://github.com/mitmul/chainer-faster-rcnn/issues/1) (and see also [the issue posted to the original py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn/issues/36)).\n\n## Setup\n\n### 1\\. Build extensions\n\n```\npython setup.py build_ext -i\n```\n\n## Inference\n\n### 1\\. Download pre-trained model\n\n```\nif [ ! -d data ]; then mkdir data; fi\ncurl https://dl.dropboxusercontent.com/u/2498135/faster-rcnn/VGG16_faster_rcnn_final.model?dl=1 -o data/VGG16_faster_rcnn_final.model\n```\n\n**NOTE:** The model definition in `faster_rcnn.py` has been changed, so if you already have the older pre-trained model file, please download it again to replace the older one with the new one.\n\n### 2\\. Use forward.py\n\n```\ncurl -O http://vision.cs.utexas.edu/voc/VOC2007_test/JPEGImages/004545.jpg\npython forward.py --img_fn 004545.jpg --gpu 0\n```\n\n`--gpu 0` turns on GPU. When you turn off GPU, use `--gpu -1` or remove `--gpu` option.\n\n![](https://raw.githubusercontent.com/wiki/mitmul/chainer-faster-rcnn/images/result.png)\n\n#### Layers\n\nSummarization of Faster R-CNN layers used during inference\n\n##### RPN\n\nThe region proposal layer (RPN) is consisted of `AnchorTargetLayer` and `ProposalLayer`. RPN takes feature maps from trunk network like VGG-16, and performs 3x3 convolution to it. Then, it applies two independent 1x1 convolutions to the output of the first 3x3 convolution. Resulting outputs are `rpn_cls_score` and `rpn_bbox_pred`.\n\n- The shape of `rpn_cls_score` is `(N, 2 * n_anchors, 14, 14)` because each pixel on the feature map has `n_anchors` bboxes and each bbox should have 2 values that mean object/background.\n- The shape of `rpn_bbox_pred` is `(N, 4 * n_anchors, 14, 14)` because each pixel on the feature map has `n_anchors` bboxes, and each bbox is represented with 4 values that mean left top `x` and `y`, `width` and `height`.\n\n## Training\n\n### 1\\. Make sure `chainercv` has been installed\n\n[ChainerCV](https://github.com/pfnet/chainercv) is a utility library enables Chainer to treat various datasets easily. It also provides some transformation utility for data augmentation, and includes some standard algorithms for some comptuer vision tasks. Check the repo to know details. Here I use (`VOCDetectionDataset`)[http://chainercv.readthedocs.io/en/latest/reference/datasets.html#vocdetectiondataset] of ChainerCV. Anyway, before starting training of FasterRCNN, please install ChainerCV via pip.\n\n```\npip install chainercv\n```\n\n### 2\\. Start training\n\n```\npython train_rpn.py\n```\n\n## Faster R-CNN Architecture\n\n**Note that it is a visualization of the workflow DURING INFERENCE**\n\n![](https://raw.githubusercontent.com/wiki/mitmul/chainer-faster-rcnn/images/Faster%20R-CNN.png)\n"
 },
 {
  "repo": "gcobb321/icloud3",
  "language": "Python",
  "readme_contents": "# Welcome to iCloud3 v2.4!\n\n[![CurrentVersion](https://img.shields.io/badge/Current_Version-v2.4.7-blue.svg)](https://github.com/gcobb321/icloud3)\n[![Released](https://img.shields.io/badge/Released-December,_2021-blue.svg)](https://github.com/gcobb321/icloud3)\n[![ProjectStage](https://img.shields.io/badge/Project_Stage-General_Availability-red.svg)](https://github.com/gcobb321/icloud3)\n[![Type](https://img.shields.io/badge/Type-Custom_Component-orange.svg)](https://github.com/gcobb321/icloud3)\n[![HACS](https://img.shields.io/badge/HACS-Default-orange.svg)](https://github.com/gcobb321/icloud3)\n\niCloud3 is a device_tracker custom_component for iPhones, iPads and Apple Watches. It is tightly integrated with the Home Assistant IOS App, uses the Waze Route Tracker for distance and time information, creates a Stationary Zone when you are stationary, lets you monitor distance and time information for the home zone and other zones (work, school, etc.), minimizes battery usage, and much more.\n\niCloud3 is a Home Assistant device tracker custom component that greatly expands the capabilities of the iCloud HA component. It exposes many new attributes, provides many new features, is based on enhanced route tracking methods, is much more accurate, and includes additional service calls\n\n\n\n<div  align=\"center\"><a href=\"https://gcobb321.github.io/icloud3/#/\"><img src=\"https://github.com/gcobb321/icloud3/raw/master/docs/images/button_documentation.jpg\"></a><a href=\"https://github.com/gcobb321/icloud3/releases\"><img src=\"https://github.com/gcobb321/icloud3/raw/master/docs/images/button_download_long.jpg\"></a></div>\n\n------\n\n### Features\n\n* Supports the Apple iCloud 2-factor-authentication Verification you are familiar with.\n* Supports iCloud Location Services and the HA iOS App tracking methods. The most efficient method is automatically assigned to the devices you are tracking on a per device basis.\n  * Family Sharing locates and tracks your family members that are in your iCloud Account's Family Sharing List.\n  * Find-my-Friends locates and tracks the people who are sharing their location in the *FindMy App*.\n  * Home Assistant iOS App is responsible for zone enter and exit notifications and location update triggers. It is also used if the others are not available, you do not want to use your iCloud account or there are problems with authentication or verifying trusted device access to your account.\n* A variable polling interval that is based on the Waze Route Mapping Service (drive time and distance) rather than just a calculated straight line distance.\n* Monitors the IOS App device_tracker and sensors to immediately capture zone enter, exit and location change notifications.\n* Sensor templates are created that can be used in automations, in scripts and on Lovelace cards. The sensors that are created can be customized to suit your needs.\n* Calculate distance and travel time for more than one zone.\n* The iCloud3 Event Log Lovelace card lets you review tracking history, update transactions that have been discarded because of old locations or poor GPS, operational errors and the ability to restart iCloud3 without restarting Home Assistant.\n* GPS wandering that randomly changes the device's state from home to not_home is eliminated.\n* Short 15-second polling interval when you are less than 1 mi/km from home lets you reliably trigger automations based on an accurate distance.\n* A Stationary Zone is created when you are in the same location for while (doctors office, mall, school, restaurant, etc.) to  reduce device polling and conserve battery life.\n* Old location data and GPS inaccuracy locations are automatically discarded.\n* Additional service call commands (setting intervals, pausing and resuming polling, zone assignment, etc.)\n* No other tracking program (other than the HA iOS app) are needed to handle device tracking and presence detection. You will not need Nmap, OwnTracks, router based tracking components.\n* New Configuration variables and Attributes let you customize how you want to use iCloud3.\n* The iCloud3 configuration file (config_ic3.yaml) can be used for most of the configuration parameters. Change the iCloud3 parameters, restart iCloud3 on the iCloud3 Event Log card and they take effect immediately. Without restarting Home Assistant.\n* Extensive documentation on how to use iCloud3, how to set it up and how to customize it to meet your needs. It includes many sample automations and scripts that you can use to set your own device tracking and presence detection.\n\nAnd much more ...\n\n### iCloud3 Information Card & Event Log Custom Card\n\n![readme](docs/images/readme.jpg)\n\n*Gary Cobb, aka GeeksterGary*\n"
 },
 {
  "repo": "naldeborgh7575/brain_segmentation",
  "language": "Python",
  "readme_contents": "# Automatic Brain Tumor Segmentation\n\nNote: This project is not currently active. It is likely outdated and buggy. I unfortunately do not have the time to update it or keep up with pull requests. \n\nBrain tumor segmentation seeks to separate healthy tissue from tumorous regions such as the advancing tumor, necrotic core and surrounding edema. This is an essential step in diagnosis and treatment planning, both of which need to take place quickly in the case of a malignancy in order to maximize the likelihood of successful treatment. Due to the slow and tedious nature of manual segmentation, there is a high demand for computer algorithms that can do this quickly and accurately.\n\n## Table of Contents\n1. [Dataset](#dataset)\n2. [MRI Background](#mri-background)\n    * [MRI Pre-Processing](#mri-pre-processing)\n    * [Pulse Sequences](#pulse-sequences)\n    * [Segmentation](#segmentation)\n3. [High Grade Gliomas](#high-grade-gliomas)\n4. [Convolutional Neural Networks](#convolutional-neural-networks)\n    * [Model Architecture](#model-architecture)\n    * [Training the Model](#training-the-model)  \n    * [Patch Selection](#patch-selection)\n    * [Results](#results)\n5. [Future Directions](#future-directions)\n\n## Dataset\n\nAll MRI data was provided by the [2015 MICCAI BraTS Challenge](http://www.braintumorsegmentation.org), which consists of approximately 250 high-grade glioma cases and 50 low-grade cases. However, due to the limited time  Each dataset contains four different MRI [pulse sequences](#pulse-sequences), each of which is comprised of 155 brain slices, for a total of 620 images per patient. Professional segmentation is provided as ground truth labels for each case. Figure 1 is an example of a scan with the ground truth segmentation. The segmentation labels are represented as follows:  \n\n<img alt=\"Example of tumor segmentation overlay on T2\" src=\"images/segmented_slice.png\" width='400'>  \n<sub><b>Figure 1: </b> Ground truth segmentation overlay on a T2 weighted scan. </sub>   \n\n\n## MRI Background\n\nMagnetic Resonance Imaging (MRI) is the most common diagnostic tool brain tumors due primarily to it's noninvasive nature and ability to image diverse tissue types and physiological processes. MRI uses a magnetic gradient and radio frequency pulses to take repetitive axial slices of the brain and construct a 3-dimensional representation(Figure 2). Each brain scan 155 slices, with each pixel representing a 1mm<sup>3</sup> voxel.  \n\n<img alt=\"Basic MRI Workflow\" src=\"images/MRI_workflow.png\" width=450>\n<img alt=\"3D rendering produced by T2 MRI scan\" src=\"images/t29_143.gif\" width=250>  \n<sub> <b> Figure 2: </b> (Left) Basic MRI workflow. Slices are taken axially at 1mm increments, creating the 3-dimensional rendering (right). Note that this is only one of four commonly-used pulse sequences used for tumor segmentation. </sub>\n\n### MRI pre-processing ([code](https://github.com/naldeborgh7575/brain_segmentation/blob/master/code/brain_pipeline.py))\n\nOne of the challenges in working with MRI data is dealing with the artifacts produced either by inhomogeneity in the magnetic field or small movements made by the patient during scan time. Oftentimes a bias will be present across the resulting scans (Figure 3), which can effect the segmentation results particularly in the setting of computer-based models.\n\n<img alt=\"Bias correction before and after\" src=\"images/n4_correction.png\" width=200>  \n<sub><b>Figure 3:</b> Brain scans before and after n4ITK bias correction. Notice the higher intensity at the bottom of the image on the right. This can be a source of false positives in a computer segmentation. </sub>  \n\nI employed an [n4ITK bias correction](http://www.ncbi.nlm.nih.gov/pubmed/20378467) on all T1 and T1C images in the dataset ([code](https://github.com/naldeborgh7575/brain_segmentation/blob/master/code/n4_bias_correction.py)), which removed the intensity gradient on each scan. Additional image pre-processing requires standardizing the pixel intensities, since MRI intensities are expressed in arbitrary units and may differ significantly between machines used and scan times.\n\n### Pulse sequences\nThere are multiple radio frequency pulse sequences that can be used to illuminate different types of tissue. For adequate segmentation there are often four different unique sequences acquired: Fluid Attenuated Inversion Recovery (FLAIR), T1, T1-contrasted, and T2 (Figure 4). Each of these pulse sequences exploits the distinct chemical and physiological characteristics of various tissue types, resulting in contrast between the individual classes. Notice the variability in intensities among the four images in Figure 4, all of which are images of the same brain taken with different pulse sequences.\n\n<img alt=\"The four MRI sequences used in brain tumor segmentation: Flair, T1, T1-contrasted and T2\" src=\"images/modalities.png\" width=200>  \n<sub><b> Figure 4: </b> Flair (top left), T1, T1C and T2 (bottom right) pulse sequences. </sub>\n\n### Segmentation\nNotice now that a single patient will produce upwards of 600 images from a single MRI, given that all four sequences produce 155 slices each (Figure 5). To get a satisfactory manual segmentation a radiologist must spend several hours tediously determining which voxels belong to which class. In the setting of malignant brain tumors, an algorithmic alternative would give clinicians more time focusing on the wellbeing of the patient, allowing for more immediate patient care and higher throughput treatment times.\n\n<img alt=\"All images produced from a single patient brain scan.\" src=\"images/brain_grids.png\" width=800>  \n\n<img alt=\"Results of the complete segmentation of a single brain\" src='images/segment.png' width=800>  \n<sub> <b>Figure 5:</b> (Top) Representative scans from each tumor imaging sequence. Approximately 600 images need to be analyzed per brain for a segmentation. (Bottom) The results of a complete tumor segmentation.</sub>\n\nAutomatic tumor segmentation has the potential to decrease lag time between diagnostic tests and treatment by providing an efficient and standardized report of tumor location in a fraction of the time it would take a radiologist to do so.\n\n## High Grade Gliomas\n\n<b>Glioblastoma cases each year (US)</b><sup>[5](#references)</sup>: 12,000  \n<b>Median survival</b>: 14.6 months  \n<b>Five-year survival rate</b>: < 10%\n\nHigh-grade malignant brain tumors are generally associated with a short life expectancy and limited treatment options. The aggressive nature of this illness necessitates efficient diagnosis and treatment planning to improve quality of and extend patient life. This urgency reinforces thee need for reliable and fast automatic segmentation methods in clinical settings. Unfortunately, algorithmic segmentation of these particular tumors has proven to be a very challenging task, due primarily to the fact that they tend to be very structurally and spatially diverse (Figure 6).\n\n<img alt=\"Diversity of tumor size, shape and location\" src=\"images/tumor_diversity.png\" width='400'>  \n<sub><b>Figure 6: </b> Three different examples of high grade gliomas, tumor segmentations are outlined on the bottom images. Notice the variation in size, shape and location in the brain, a quality of these tumors that makes them difficult to segment. </sub>\n\n## Convolutional Neural Networks\n\nConvolutional Neural Networks(CNNs) are a powerful tool in the field of image recognition. They were inspired in the late 1960s by the elucidation of how the [mammalian visual cortex works](https://en.wikipedia.org/wiki/Receptive_field): many networks neurons sensitive to a given 'receptive field' tiled over the entire visual field<sup>[2](#references)</sup>. This aspect of CNNs contributes to their high flexibility and spatial invariance, making them ideal candidates for semantic segmentatiaon of images with high disparity in locations of objects of interest. CNNs are a powerful tool in machine learning that are well suited for the challenging problem tackled in this project.\n\n### Model Architecture ([code](https://github.com/naldeborgh7575/brain_segmentation/blob/master/code/Segmentation_Models.py))\n\nI use a four-layer Convolutional Neural Network (CNN) model that, besides [n4ITK](#mri-pre-processing) bias correction, requires minimal [pre-processing](https://github.com/naldeborgh7575/brain_segmentation/blob/master/brain_pipeline.py). The model can distinguish between and predict healthy tissue, actively enhancing tumor and non-advancing tumor regions (Figure 7).  The local invariant nature of CNNs allows for abstraction of token features for classification without relying on large-scale spatial information that is inconsistent in the case of tumor location.\n\n<img alt=\"Basic ConvNet model architecture\" src=\"images/model_architecture.png\" width=600>  \n<sub><b>Figure 6: </b> Basic model architecture of my segmentation model. Input is four 33x33 patches from a randomly selected slice. Each imaging pulse sequence is input as a channel into the net, followed by four convolution/max pooling layers (note- the last convolutional layer is not followed by max pooling). </sub>\n\n\n### Training the Model\n\nI created the model using Keras and ran it on an Amazon AWS GPU-optimized EC2 instance. I tested several models, but elected to use the 4-layer sequential model shown in Figure 6 due to the two-week time constraint of the project, as it had best initial results and fastest run time.\n\nThe model was trained on randomly selected 33x33 patches of MRI images to classify the center pixel. Each input has 4 channels, one for each imaging sequence, so the net can learn what relative pixel intensities are hallmarks of each given class. The model is trained on approximately 50,000 patches for six epochs. The model generally begins to overfit after six epochs, and the validation accuracy on balanced classes reaches approximately 55 percent. [Future directions](#future-directions) will include more training phases and updated methods for patch selection.\n\n### Patch Selection ([code](https://github.com/naldeborgh7575/brain_segmentation/blob/master/code/patch_library.py))\nThe purpose of training the model on patches (Figure 8) is to exploit the fact that a class of any given voxel is highly dependent on the class of it's surrounding voxels. Patches give the net access to information about the pixel's local environment, which influences the final prediction of the patch.\n\n<img alt=\"Examples of T1c patches used as input.\" src=\"images/patches.png\" width=800>  \n<sub><b> Figure 8: </b> Examples of 33 x 33 pixel patches used as input for the neural network. These particular patches were acquired with a T1C pulse sequence, but the actual input includes all pulse sequences. </sub>\n\nAnother important factor in patch selection is to make sure the classes of the input data are balanced. Otherwise, the net will be overwhelmed with background images and fail to classify any of the minority classes. Approximately 98% of the data belongs to the background class (healthy tissue or the black surrounding area), with the remaining 2% of pixels divided among the four tumor classes.\n\nI tried out several different methods for sampling patches, which had a large impact on the results. I began randomly selecting patches of a given class from the data and repeating this for all five classes. However, with this sampling method approximately half of the background patches were just the zero-intensity area with no brain, so the model classified most patches with brain tissue as tumor, and only the black areas as background (Figure 9).\n\n<img alt='Results of initial segmentation' src='images/bad_example.png' height=200>\n<img alt=\"Improved segmentation results after restricting the amount of \" src=\"images/improved_seg.png\" height=200>  \n<sub><b> Figure 9: </b> (Left) results of segmentation without excluding exclusively zero-intensity patches. Notice that even healthy tissue is classified as tumor. (Right) results of segmentation after restricting the amount of zero-intensity pixels allowed in a given patch. The tumor prediction is now restricted mostly to the actual area of the lesion </sub>  \n\nI then restricted the selection process to exclude patches in which more than 25% of the pixels were of zero-intensity. This greatly improved the results, one of which can be seen on the right in Figure 9.\n\nUnfortunately the model still struggles with class boundary segmentation. The boundaries in my results are quite smooth, while the ground truth tends to have more detail. This is a downside to working with patch-based prediction, since the predicted identity of boundary pixels is influenced by neighbors of a different class. A method I've played to fix this involves selecting a certain subset of the training data from the highest entropy patches in the ground truth segmentation. High entropy patches have more classes represented in them, so the model will have more boundary examples to learn from. I am still fine tuning this process and will be updating the results accordingly.\n\n\n### Results\n\nBelow is a summary of how well the current model is predicting. As more advances are made this section will be updated. A representative example of a tumor segmentation on test data is displayed in Figure 10. The model can identify each of the four classes with a good amount of accuracy, with the exception of class boundaries, which are smoother in my prediction than the ground truth.\n\n<img alt=\"Result Frame\" src=\"images/results.png\" width=404>  \n<img alt='Ground Truth: Professional Segmentation' src='images/gt.gif' width=200>\n<img alt='Results of CNN Model' src='images/my_res.gif' width=200>  \n<sub><b> Figure 10: </b> Results of CNN model segmentation on a single slice (top) with respect to the ground truth, and a 3D representation of the segmentation (bottom). </sub>\n\nNotice that towards the top of the 3-dimensional network results representation some of the cerebrospinal fluid (CSF) is incorrectly classified as tumor. This is unsurprising, considering that the CSF has similar features to parts of the tumor in some pulse sequences. There are several potential solutions for this:\n\n1. Pre-process the images by masking CSF (much easier than tumors to extract)\n2. Train the model on more CSF-containing patches so it can learn to distinguish between CSF and tumor\n3. Add more nodes to the model, which may cause it to learn more features from the current patches.\n\n\n\n## Future Directions\n\nWhile my model yields promising results, an application such as this leaves no room for errors or false positives. In a surgical setting it is essential to remove as much of the tumor mass as possible without damaging any surrounding healthy tissue. There are countless ways to improve this model, ranging from the overall architecture, to adjusting how we sample the data.\n\nWhen I began building the model I built an architecture based on one built by [Havaei et al](http://arxiv.org/pdf/1505.03540.pdf), which uses a cascaded, two-pathway architecture and looks at both local and global features of patches. I elected to use the simpler model to meet the two-week deadline for this project, but in the future I will work on tuning models similar to this to improve upon the accuracy of this model.\n\n## References\n\n    1. Havaei, M. et. al, Brain Tumor Segmentation with Deep Neural Networks. arXiv preprint arXiv:1505.03540, 2015.\n    2. Hubel, D. and Wiesel, T. Receptive fields and functional architecture of monkey striate cortex. Journal of Physiology 1968.\n    3. Kistler et. al, The virtual skeleton database: an open access repository for biomedical research and collaboration. JMIR, 2013.\n    4. Menze et al., The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS), IEEE Trans. Med. Imaging, 2015.\n    5. Stupp et al., Effects of radiotherapy with concomitant and adjuvant temozolomide versus radiotherapy alone on survival in glioblastoma in a randomised phase III study: 5-year analysis of the EORTC-NCIC trial. The Lancet Onc., 2009.\n    6. Tustison, NJ. et. al, N4ITK: improved N3 bias correction. IEEE Trans Med Imaging, 2010.\n"
 },
 {
  "repo": "SmallMunich/nutonomy_pointpillars",
  "language": "Python",
  "readme_contents": "# PointPillars Pytorch Model Convert To ONNX, And Using TensorRT to Load this IR(ONNX) for Fast Speeding Inference\n\nWelcome to PointPillars(This is origin from nuTonomy/second.pytorch ReadMe.txt).\n\nThis repo demonstrates how to reproduce the results from\n[_PointPillars: Fast Encoders for Object Detection from Point Clouds_](https://arxiv.org/abs/1812.05784) (to be published at CVPR 2019) on the\n[KITTI dataset](http://www.cvlibs.net/datasets/kitti/) by making the minimum required changes from the preexisting\nopen source codebase [SECOND](https://github.com/traveller59/second.pytorch). \n\nMeanwhile, This part of the code also refers to the open source k0suke-murakami (https://github.com/k0suke-murakami/train_point_pillars) this code. \n\nThis is not an official nuTonomy codebase, but it can be used to match the published PointPillars results.\n\n**WARNING: This code is not being actively maintained. This code can be used to reproduce the results in the first version of the paper, https://arxiv.org/abs/1812.05784v1. For an actively maintained repository that can also reproduce PointPillars results on nuScenes, we recommend using [SECOND](https://github.com/traveller59/second.pytorch). We are not the owners of the repository, but we have worked with the author and endorse his code.**\n\n![Example Results](https://github.com/SmallMunich/nutonomy_pointpillars/blob/master/images/pointpillars_kitti_results.png)\n\n\n## Getting Started\n\nThis is a fork of [SECOND for KITTI object detection](https://github.com/traveller59/second.pytorch) and the relevant\nsubset of the original README is reproduced here.\n\n\n### Docker Environments\n\nIf you do not waste time on pointpillars envs, please pull my docker virtual environments :\n\n```bash\ndocker pull smallmunich/suke_pointpillars:v1 \n```\n\nAttention: when you launch this docker envs, please run this command :\n\n```bash \nconda activate pointpillars \n```\n\nAnd Then, you can run train or evaluation or onnx model generate command line.\n\n\n### Install\n\n#### 1. Clone code\n\n```bash\ngit clone https://github.com/SmallMunich/nutonomy_pointpillars.git\n```\n\n#### 2. Install Python packages\n\nIt is recommend to use the Anaconda package manager.\n\nFirst, use Anaconda to configure as many packages as possible.\n```bash\nconda create -n pointpillars python=3.6 anaconda\nsource activate pointpillars\nconda install shapely pybind11 protobuf scikit-image numba pillow\nconda install pytorch torchvision -c pytorch\nconda install google-sparsehash -c bioconda\n```\n\nThen use pip for the packages missing from Anaconda.\n```bash\npip install --upgrade pip\npip install fire tensorboardX\n```\n\nFinally, install SparseConvNet. This is not required for PointPillars, but the general SECOND code base expects this\nto be correctly configured. However, I suggest you install the spconv instead of SparseConvNet.\n```bash\ngit clone git@github.com:facebookresearch/SparseConvNet.git\ncd SparseConvNet/\nbash build.sh\n# NOTE: if bash build.sh fails, try bash develop.sh instead\n```\n\nAdditionally, you may need to install Boost geometry:\n\n```bash\nsudo apt-get install libboost-all-dev\n```\n\n\n#### 3. Setup cuda for numba\n\nYou need to add following environment variables for numba to ~/.bashrc:\n\n```bash\nexport NUMBAPRO_CUDA_DRIVER=/usr/lib/x86_64-linux-gnu/libcuda.so\nexport NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so\nexport NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice\n```\n\n#### 4. PYTHONPATH\n\nAdd nutonomy_pointpillars/ to your PYTHONPATH.\n\n```bash \nexport PYTHONPATH=$PYTHONPATH:/your_root_path/nutonomy_pointpillars/\n```\n\n### Prepare dataset\n\n#### 1. Dataset preparation\n\nDownload KITTI dataset and create some directories first:\n\n```plain\n\u2514\u2500\u2500 KITTI_DATASET_ROOT\n       \u251c\u2500\u2500 training    <-- 7481 train data\n       |   \u251c\u2500\u2500 image_2 <-- for visualization\n       |   \u251c\u2500\u2500 calib\n       |   \u251c\u2500\u2500 label_2\n       |   \u251c\u2500\u2500 velodyne\n       |   \u2514\u2500\u2500 velodyne_reduced <-- empty directory\n       \u2514\u2500\u2500 testing     <-- 7580 test data\n           \u251c\u2500\u2500 image_2 <-- for visualization\n           \u251c\u2500\u2500 calib\n           \u251c\u2500\u2500 velodyne\n           \u2514\u2500\u2500 velodyne_reduced <-- empty directory\n```\n\nNote: PointPillar's protos use ```KITTI_DATASET_ROOT=/data/sets/kitti_second/```.\n\n#### 2. Create kitti infos:\n\n```bash\npython create_data.py create_kitti_info_file --data_path=KITTI_DATASET_ROOT\n```\n\n#### 3. Create reduced point cloud:\n\n```bash\npython create_data.py create_reduced_point_cloud --data_path=KITTI_DATASET_ROOT\n```\n\n#### 4. Create groundtruth-database infos:\n\n```bash\npython create_data.py create_groundtruth_database --data_path=KITTI_DATASET_ROOT\n```\n\n#### 5. Modify config file\n\nThe config file needs to be edited to point to the above datasets:\n\n```bash\ntrain_input_reader: {\n  ...\n  database_sampler {\n    database_info_path: \"/path/to/kitti_dbinfos_train.pkl\"\n    ...\n  }\n  kitti_info_path: \"/path/to/kitti_infos_train.pkl\"\n  kitti_root_path: \"KITTI_DATASET_ROOT\"\n}\n...\neval_input_reader: {\n  ...\n  kitti_info_path: \"/path/to/kitti_infos_val.pkl\"\n  kitti_root_path: \"KITTI_DATASET_ROOT\"\n}\n```\n\n\n### Train\n\n```bash\ncd ~/second.pytorch/second\npython ./pytorch/train.py train --config_path=./configs/pointpillars/car/xyres_16.proto --model_dir=/path/to/model_dir\n```\n\n* If you want to train a new model, make sure \"/path/to/model_dir\" doesn't exist.\n* If \"/path/to/model_dir\" does exist, training will be resumed from the last checkpoint.\n* Training only supports a single GPU. \n* Training uses a batchsize=2 which should fit in memory on most standard GPUs.\n* On a single 1080Ti, training xyres_16 requires approximately 20 hours for 160 epochs.\n\n\n### Evaluate\n\n\n```bash\ncd ~/second.pytorch/second/\npython pytorch/train.py evaluate --config_path= configs/pointpillars/car/xyres_16.proto --model_dir=/path/to/model_dir\n```\n\n* Detection result will saved in model_dir/eval_results/step_xxx.\n* By default, results are stored as a result.pkl file. To save as official KITTI label format use --pickle_result=False.\n\n### ONNX IR Generate\n\n### pointpillars pytorch model convert to IR onnx, you should verify some code as follows:\n\nthis python file is : second/pyotrch/models/voxelnet.py\n\n```bash\n        voxel_features = self.voxel_feature_extractor(pillar_x, pillar_y, pillar_z, pillar_i,\n                                                      num_points, x_sub_shaped, y_sub_shaped, mask)\n\n        ###################################################################################\n        # return voxel_features ### onnx voxel_features export\n        # middle_feature_extractor for trim shape\n        voxel_features = voxel_features.squeeze()\n        voxel_features = voxel_features.permute(1, 0)\n```  \n\nUNCOMMENT this line: return voxel_features \n\nAnd Then, you can run convert IR command.\n\n```bash\ncd ~/second.pytorch/second/\npython pytorch/train.py onnx_model_generate --config_path= configs/pointpillars/car/xyres_16.proto --model_dir=/path/to/model_dir\n```\n\n### Compare ONNX model With Pytorch Origin model predicts \n\n* If you want to check this convert model about pfe.onnx and rpn.onnx model, please refer to this py-file: check_onnx_valid.py \n\n* Now, we can compare onnx results with pytorch origin model predicts as follows : \n\n* the pfe.onnx and rpn.onnx predicts file is located: \"second/pytorch/onnx_predict_outputs\", you can see it carefully.\n```bash\n    eval_voxel_features.txt \n    eval_voxel_features_onnx.txt \n    eval_rpn_features.txt \n    eval_rpn_onnx_features.txt \n```\n\n* pfe.onnx model compare with origin pfe-layer : \n![Example Results](https://github.com/SmallMunich/nutonomy_pointpillars/blob/master/images/voxel_features.jpg)\n\n* rpn.onnx model compare with origin rpn-layer : \n![Example Results](https://github.com/SmallMunich/nutonomy_pointpillars/blob/master/images/rpn_features.jpg)\n\n### Compare ONNX with TensorRT Fast Speed Inference \n\n* First you needs this environments(onnx_tensorrt envs):\n\n```bash\n      docker pull smallmunich/onnx_tensorrt:latest\n```\n\n* If you want to use pfe.onnx and rpn.onnx model for tensorrt inference, please refer to this py-file: tensorrt_onnx_infer.py \n\n* Now, we can compare onnx results with pytorch origin model predicts as follows : \n\n* the pfe.onnx and rpn.onnx predicts file is located: \"second/pytorch/onnx_predict_outputs\", you can see it carefully.\n```bash\n    pfe_rpn_onnx_outputs.txt \n    pfe_tensorrt_outputs.txt \n    rpn_onnx_outputs.txt \n    rpn_tensorrt_outputs.txt \n```\n\n* pfe.onnx model compare with tensorrt pfe-layer : \n![Example Results](https://github.com/SmallMunich/nutonomy_pointpillars/blob/master/images/pfe_trt.jpg)\n\n* rpn.onnx model compare with tensorrt rpn-layer : \n![Example Results](https://github.com/SmallMunich/nutonomy_pointpillars/blob/master/images/rpn_trt.jpg)\n\n### Blog Address\n\n* More Details will be update on my chinese blog:\n* export from pytorch to onnx IR blog : https://blog.csdn.net/Small_Munich/article/details/101559424  \n* onnx compare blog : https://blog.csdn.net/Small_Munich/article/details/102073540\n* tensorrt compare blog : https://blog.csdn.net/Small_Munich/article/details/102489147\n* wait for update & best wishes.\n\n"
 },
 {
  "repo": "umautobots/vod-converter",
  "language": "Python",
  "readme_contents": "# Visual Object Dataset converter\n\nConverts between object dataset formats. Requires Python 3.6.\n\nExample: convert from data in [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php) format to\n[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html) format:\n\n```\n$ python3.6 vod_converter/main.py --from kitti --from-path datasets/mydata-kitti --to voc --to-path datasets/mydata-voc\n```\n\nSee `main.py` for documentation on how to easily plug in additional data formats; you can define a function\nthat can read in your data into a common format, and it will be then ready to convert to any supported format.\n\nSimilarly, you can implement a single function that takes the common format and outputs to the filesystem in\nyour format and you will be ready to convert from e.g VOC to yours.\n\nCurrently support conversion from:\n\n- [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php)\n- [KITTI tracking](http://www.cvlibs.net/datasets/kitti/eval_tracking.php)\n- [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html)\n- [Udacity CrowdAI and AUTTI](https://github.com/udacity/self-driving-car/tree/master/annotations)\n\nto:\n\n- [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html)\n- [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php)\n\n## That 'train.txt' file for KITTI\n\nWhen reading in KITTI, the script expects a `train.txt` file that isn't part of the original dataset. This is simply a file with the name of each datapoint you wish to capture. [Here's an example with everything in the training set](https://github.com/umautobots/vod-converter/files/1139276/train.txt). You can also create it like so:\n\n```\n$ cd datasets/kitti && ls -1 training/image_2 | cut -d. -f1 > train.txt && cd -\n$ head datasets/kitti/train.txt\n000000\n000001\n000002\n000003\n000004\n000005\n000006\n000007\n000008\n000009\n```\n\n## Python2 support\n\nThis project is written using features requiring Python3.6+, but there is [a fork](https://github.com/nghiattran/vod-converter) that has been updated to work in Python2 if you need it.\n\n"
 },
 {
  "repo": "r0oth3x49/lynda-dl",
  "language": "Python",
  "readme_contents": "[![GitHub release](https://img.shields.io/badge/release-v0.3-brightgreen.svg?style=flat-square)](https://github.com/r0oth3x49/lynda-dl/releases/tag/v0.3)\n[![GitHub stars](https://img.shields.io/github/stars/r0oth3x49/lynda-dl.svg?style=flat-square)](https://github.com/r0oth3x49/lynda-dl/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/r0oth3x49/lynda-dl.svg?style=flat-square)](https://github.com/r0oth3x49/lynda-dl/network)\n[![GitHub issues](https://img.shields.io/github/issues/r0oth3x49/lynda-dl.svg?style=flat-square)](https://github.com/r0oth3x49/lynda-dl/issues)\n[![GitHub license](https://img.shields.io/github/license/r0oth3x49/lynda-dl.svg?style=flat-square)](https://github.com/r0oth3x49/lynda-dl/blob/master/LICENSE)\n\n# lynda-dl\n**A cross-platform python based utility to download courses from lynda for personal offline use.**\n\n## ***Note***\n - i don't provide any exe file for this repository use the exe files on your own risk i 'm not responsible for anything\n that happens to your account or system, use this repository from source with python installed.\n\n[![lynda-dl.png](https://i.postimg.cc/NFqdCyxH/lynda-dl.png)](https://postimg.cc/341jdxV8)\n\n## ***Features***\n\n- Resume capability for a course video.\n- Supports organization and individual lynda users both.\n- Added support for cookie based login. (option: `-k / --cookies`).\n- List down course contents and video resolution, suggest the best resolution (option: `--info`).\n- Download/skip all available subtitles for a video (options: `--skip-sub, --skip-sub`).\n- Download lecture(s) requested resolution (option: `-q / --quality`).\n- Download course to user requested path (option: `-d / --directory`).\n\n## ***Issue Reporting Guideline***\n\nTo maintain an effective bugfix workflow and make sure issues will be solved, I ask reporters to follow some simple guidelines.\n\nBefore creating an issue, please do the following:\n\n1. **Use the GitHub issue search** &mdash; check if the issue has already been reported.\n2. **Check if the issue has been fixed** &mdash; try to reproduce it using the latest `master` in the repository.\n3. Make sure, that information you are about to report is related to this repository \n   and not the one available ***Python's repository***, Because this repository cannot be downloaded via pip.\n\nA good bug report shouldn't leave others needing to chase you up for more\ninformation. Please try to be as detailed as possible in your report. What is\nyour environment? What was the course url? What steps will reproduce the issue? What OS\nexperience the problem? All these details will help to fix any potential bugs as soon as possible.\n\n### ***Example:***\n\n> Short and descriptive example bug report title\n>\n> A summary of the issue and the OS environment in which it occurs. If\n> suitable, include the steps required to reproduce the bug.\n>\n> 1. This is the first step\n> 2. This is the second step\n> 3. Further steps, etc.\n>\n> `<url>` - a lynda course link to reproduce the error.\n>\n> Any other information you want to share that is relevant to the issue being reported.\n\n## ***Extracting Cookies***\n\n - Login to your lynda account via browser.\n - Once you are logged in right click on page the search for option called **Inspect Element** and click on that.\n - Under that look for **Network Tab** and click on that. Under that **Network Tab** click on Requests type **XHR** .\n - Now Visit the **Course URL** you want to download, You will see some requests under **Network Tab XHR**.\n - Right click on any of the Requests which links to **lynda.com**. Simply copy **Request Headers** and save to text file.\n - Done run the lynda-dl against that text file it will start downloading the course.\n\n## ***Requirements***\n\n- Python (2 or 3)\n- Python `pip`\n- Python module `requests`\n- Python module `colorama`\n- Python module `unidecode`\n- Python module `six`\n- Python module `requests[security]` or `pyOpenSSL`\n\n## ***Module Installation***\n\n\tpip install -r requirements.txt\n\t\n## ***Tested on***\n\n- Windows 7/8/8.1/10\n- Kali linux (2017.2)\n- Ubuntu-LTS (64-bit) (tested with super user)\n- Mac OSX 10.9.5 (tested with super user)\n \n## ***Download lynda-dl***\n\nYou can download the latest version of lynda-dl by cloning the GitHub repository.\n\n\tgit clone https://github.com/r0oth3x49/lynda-dl.git\n\n\n## ***Usage***\n\n***Download course using user credentials***\n\n    python lynda-dl.py COURSE_URL\n  \n***OR***\n\n    python lynda-dl.py -u user@domain.com -p p4ssw0rd COURSE_URL\n  \n***Download course using organization's library card***\n\n    python lynda-dl.py -o organization COURSE_URL\n  \n***OR***\n\n    python lynda-dl.py -u library_card_num -p library_card_pin -o organization COURSE_URL\n  \n  \n***Download course to a specific location using user credentials***\n\n    python lynda-dl.py COURSE_URL -d \"/path/to/directory/\"\n  \n***OR***\n\n    python lynda-dl.py -u user@domain.com -p p4ssw0rd COURSE_URL -d \"/path/to/directory/\"\n\n  \n***Download course to a specific location using organization's library card***\n\n    python lynda-dl.py -o organization COURSE_URL -d \"/path/to/directory/\"\n  \n***OR***\n\n    python lynda-dl.py -u library_card_num -p library_card_pin -o organization COURSE_URL  -d \"/path/to/directory/\"\n\n***list down course information***\n\n    python lynda-dl.py COURSE_URL --info\n  \n***Download with specific resolution/ quality***\n\n    python lynda-dl.py COURSE_URL -q 720\n\n## **Advanced Usage**\n\n<pre><code>\nAuthor: Nasir khan (<a href=\"http://r0oth3x49.herokuapp.com/\">r0ot h3x49</a>)\n\nusage: lynda-dl.py [-h] [-v] [-k] [-u] [-p] [-o] [-d] [-q] [--info]\n                   [--sub-only] [--skip-sub]\n                   course\n\nA cross-platform python based utility to download courses from lynda for\npersonal offline use.\n\npositional arguments:\n  course                Lynda course or file containing list of courses.\n\nGeneral:\n  -h, --help            Shows the help.\n  -v, --version         Shows the version.\n\nAuthentication:\n  -k , --cookies        Cookies to authenticate with.\n  -u , --username       Username or Library Card Number.\n  -p , --password       Password or Library Card Pin.\n  -o , --organization   Organization, registered at Lynda.\n\nAdvance:\n  -d , --directory      Download to specific directory.\n  -q , --quality        Download specific video quality.\n\nOthers:\n  --info                List all lectures with available resolution.\n  --sub-only            Download captions/subtitle only.\n  --skip-sub            Download course but skip captions/subtitle.\n\nExample:\n  python lynda-dl.py  COURSE_URL\n  python lynda-dl.py -o organization COURSE_URL\n  python lynda-dl.py -k cookies.txt COURSE_URL\n\n</code></pre>\n"
 },
 {
  "repo": "initialstate/pi-process-dashboard",
  "language": "Python",
  "readme_contents": "# Remotely Monitor Your Pi Processes and IP Addresses\n---\n_by Jamie Bailey_\n\n![Process Dashboard Hero](https://github.com/InitialState/pi-process-dashboard/wiki/img/pi_process_dashboard.jpg)\n\nIf you are using one or more Raspberry Pis to run a dedicated task (such as monitoring [who's at home](https://github.com/initialstate/pi-sensor-free-presence-detector/wiki) or [the weather](https://github.com/initialstate/wunderground-sensehat/wiki) or [your beer fridge](https://github.com/initialstate/beerfridge/wiki)), you need those processes to run uninterrupted. A task that exits unexpectedly may need your immediate attention to avoid lost data, project delays, or a system failure. It is impractical to manually babysit a bunch of Pis to make sure everything keeps running. A better way to ensure continuous operation is to be alerted when a process exits and be able to pull up a single dashboard at anytime to see the status of every important process running on every one of your deployed Pis. If your Pi is running headless, having the IP address of that Pi in the same dashboard will also come in handy. \n\nIn this tutorial, we will use a couple of simple scripts to:\n- create a web-based dashboard that monitors the status of multiple processes and IP addresses of each device\n- configure our Pi to launch the dedicated process and its monitor on boot\n- create an email/SMS notification when a process exits\n\n[Read More ...](https://github.com/initialstate/pi-process-dashboard/wiki)\n"
 },
 {
  "repo": "zhongwen/predictron",
  "language": "Python",
  "readme_contents": "# The Predictron\n\nA TensorFlow implementation of\n```\nThe Predictron: End-To-End Learning and Planning\nDavid Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, et al.\n```\n[arXiv](https://arxiv.org/abs/1612.08810)\n\n## Run\nTry it with\n```\npython ./train_multigpu.py --max_depths=<2,4,8,16> --batch_size=128 \\\n  --num_gpus=<your available number of GPUs>\n```\n\nWe assume `batch_size` can be divided by `num_gpus`.\n\n## Requirements:\ntensorflow=0.12.1\n\n## Some Results\n`max_step = 16` and Training on `8` NVIDIA GTX TITAN X GPUs. It takes quite a long time to achieve the same numbers of steps reported in the paper. Here are some results during the training procedure.\n```\nINFO:multigpu_train:2017-01-09 01:11:36.704713: step 159760, loss = 0.0043, loss_preturns = 0.0018, loss_lambda_preturns = 0.0004 (2679.7 examples/sec; 0.048 sec/batch)\nINFO:multigpu_train:2017-01-09 01:11:39.854633: step 159770, loss = 0.0038, loss_preturns = 0.0017, loss_lambda_preturns = 0.0002 (2888.7 examples/sec; 0.044 sec/batch)\nINFO:multigpu_train:2017-01-09 01:11:43.026452: step 159780, loss = 0.0067, loss_preturns = 0.0031, loss_lambda_preturns = 0.0002 (2848.1 examples/sec; 0.045 sec/batch)\nINFO:multigpu_train:2017-01-09 01:11:46.252385: step 159790, loss = 0.0099, loss_preturns = 0.0035, loss_lambda_preturns = 0.0014 (3272.3 examples/sec; 0.039 sec/batch)\nINFO:multigpu_train:2017-01-09 01:11:49.477405: step 159800, loss = 0.0032, loss_preturns = 0.0013, loss_lambda_preturns = 0.0003 (3051.7 examples/sec; 0.042 sec/batch)\nINFO:multigpu_train:2017-01-09 01:11:52.570256: step 159810, loss = 0.0046, loss_preturns = 0.0020, loss_lambda_preturns = 0.0003 (3314.3 examples/sec; 0.039 sec/batch)\nINFO:multigpu_train:2017-01-09 01:11:55.710512: step 159820, loss = 0.0040, loss_preturns = 0.0017, loss_lambda_preturns = 0.0003 (3374.0 examples/sec; 0.038 sec/batch)\n```\n"
 },
 {
  "repo": "spesmilo/electrum-server",
  "language": "Python",
  "readme_contents": "IRC is used by Electrum server to find 'peers' - other Electrum servers. The\ncurrent list can be seen by running:\n\n  electrum-server peers\n\nThe following config file options are used by the IRC part of Electrum server:\n\n    [server]\n    irc = yes\n    irc_nick = server_nickname\n    host = fqdn.host.name.tld\n    # report_host = fqdn.host.name.tld\n    # report_stratum_tcp_port = 50001\n\n`irc` is used to determine whether the IRC thread will be started or\nthe Electrum server will run in private mode (default). In private\nmode, `electrum-server peers` will always return an empty list.\n\n`host` is a fully-qualified domain name (FQDN) of your Electrum\nserver. It is used both when binding the listener for incoming client\nconnections and as part of the realname field in IRC (see below).\n\n`report_host` is a an optional fully-qualified domain name (FQDN) of\nyour Electrum server instead of `host`. It is used as part of the name\nfield in IRC for incoming client connections.  This is useful in a NAT\nsetup where you bind to a private IP locally but have an external IP\nset up at your router and external DNS.\n\n`report_stratum_tcp_port` and `report_stratum_tcp_ssl_port` are\noptional settings for a port number to be reported in the IRC name\nfield without actually binding this port locally. This is useful in a\nNAT setup where you might want to bind to a high port locally but DNAT\na different possibly privileged port for inbound connections\n\n`irc_nick` is a nick name that will be appended to the E_ suffix when\ncomposing the IRC nickname to identify your server on #electrum.\n\nPlease note the IRC name field can only contain 50 chars and will be\ncomposed of `host` + protocol version number + Port numbers for the\nvarious protocols.  Please check whether port numbers are cut off at\nthe end\n\n\nExample of port forwarding using iptables:\niptables -t nat -A PREROUTING -p tcp --dport 110 -j REDIRECT --to-ports 50002\n\n"
 },
 {
  "repo": "rybalkinsd/atom",
  "language": "Java",
  "readme_contents": "# Java Technoatom/Technosphere\n\n\n[![Build Status](https://travis-ci.org/rybalkinsd/atom.png?branch=master)](https://travis-ci.org/rybalkinsd/atom)\n\n[![](http://icons.iconarchive.com/icons/dakirby309/simply-styled/32/YouTube-icon.png) Youtube channel](https://www.youtube.com/playlist?list=PLrCZzMib1e9pnFbVV3u4s7ki5NTnm7WgT) - watch course videos\n\n[![](http://icons.iconarchive.com/icons/alecive/flatwoken/32/Apps-Telegram-icon.png) Join chat](https://t.me/joinchat/AAISfEF63F-alTwJsD3sfQ) - feel free to contact us  \n  \nCourse repository. All course lectures/sources/homeworks are available here.  \n  \n## Lectures list:\n[Lecture 1. Basics, git](https://gitpitch.com/rybalkinsd/atom/lecture01?grs=github&t=white&p=lecture01%2Fpresentation#/)  \n[Lecture 2. Classes, gradle](https://gitpitch.com/rybalkinsd/atom/lecture02?grs=github&t=white&p=lecture02%2Fpresentation#/)  \n[Lecture 3. Collections, Exceptions, Generics](https://gitpitch.com/rybalkinsd/atom/lecture03?grs=github&t=white&p=lecture03%2Fpresentation#/)  \n[Lecture 4. Web client, HTTP, REST](https://gitpitch.com/rybalkinsd/atom/lecture04?grs=github&t=white&p=lecture04%2Fpresentation#/)  \n[Lecture 5. Web server, Spring, Annotations](https://gitpitch.com/rybalkinsd/atom/lecture05?grs=github&t=white&p=lecture05%2Fpresentation#/)  \n[Lecture 6. Databases, SQL, JDBC](https://gitpitch.com/rybalkinsd/atom/lecture06?grs=github&t=white&p=lecture06%2Fpresentation#/)  \n[Lecture 7. ORM, Hibernate](https://gitpitch.com/rybalkinsd/atom/lecture07?grs=github&t=white&p=lecture07%2Fpresentation#/)  \n[Lecture 8. WebSocket](https://gitpitch.com/rybalkinsd/atom/lecture08?grs=github&t=white&p=lecture08%2Fpresentation#/)  \n[Lecture 9. Game Server architecture](https://gitpitch.com/rybalkinsd/atom/lecture09?grs=github&t=white&p=lecture09%2Fpresentation#/)  \n[Lecture 10. Basic concurrency, Critical section](https://gitpitch.com/rybalkinsd/atom/lecture10?grs=github&t=white&p=lecture10%2Fpresentation#/)  \n[Lecture 11. Basic concurrency, Concurrent data structures](https://gitpitch.com/rybalkinsd/atom/lecture11?grs=github&t=white&p=lecture11%2Fpresentation#/)  \n[Lecture 12. GC, Heap](https://gitpitch.com/rybalkinsd/atom/lecture12?grs=github&t=white&p=lecture12%2Fpresentation#/)  \n\n"
 },
 {
  "repo": "maxyou/CalendarPicker",
  "language": "Java",
  "readme_contents": "# Calendar Picker\n\nCalendar Picker\n\n---\n\nCan preset a selected day. Can customize almost all text size, color, bg color, and month title.\n\nUpdate:<br>\n    v1.1: If no preset day, hide Selected button. Show it when a day be clicked.<br>\n    v1.1.2: Remove no needed example code and intent filter which create no needed launch icon.<br>\n\n\n[demo apk](https://github.com/maxyou/CalendarPicker/blob/master/example-release.apk?raw=true)<br>\n\n![pic][1]\n\n  [1]: https://raw.githubusercontent.com/maxyou/CalendarPicker/master/ezgif.com-resize.gif\n\n\n  Add it in your root build.gradle at the end of repositories:\n\n    allprojects {\n      repositories {\n        ...\n        maven { url 'https://jitpack.io' }\n      }\n    }\n\n  Add the dependency:\n\n    dependencies {\n        compile 'com.github.maxyou:CalendarPicker:v1.1.2'\n    }\n\n\n  Only new a dialog:<br>\n\n    Builder builder = new Builder(MainActivity.this, new Builder.CalendarPickerOnConfirm() {\n                @Override\n                public void onComplete(YearMonthDay yearMonthDay) {\n                    MyConfig.uiToast(\"You pick \"+yearMonthDay.year+\"-\"+yearMonthDay.month+\"-\"+yearMonthDay.day);\n                }\n            })\n                    .setPromptText(\"\u9009\u62e9\u4e00\u4e2a\u65e5\u671f\")\n                    .setPromptSize(18)\n                    .setPromptColor(Color.RED)\n                    .setPromptBgColor(0xFFFFFFFF)\n\n                    .setSelectedText(\"\u5df2\u9009\")\n                    .setSelectedSize(14)\n                    .setSelectedColor(Color.WHITE)\n                    .setSelectedBgColor(0xFF1E90FF)\n\n                    .setTodayText(\"\u4eca\u5929\")\n                    .setTodaySize(14)\n                    .setTodayColor(Color.DKGRAY)\n                    .setTodayBgColor(Color.YELLOW)\n\n                    .setMonthTitle(new Builder.FormatMonthTitle() {\n                        @Override\n                        public String setMonthTitle(int year, int month) {\n                            return \"\"+year+\"\u5e74\"+month+\"\u6708\";\n                        }\n                    })\n                    .setMonthTitleSize(16)\n                    .setMonthTitleColor(0xFFB22222)\n                    .setMonthTitleBgColor(0x00000000)\n\n                    .setWeekIndex(new String[]{\"\u4e00\", \"\u4e8c\", \"\u4e09\", \"\u56db\", \"\u4e94\", \"\u516d\", \"\u65e5\"})\n                    .setWeekIndexSize(16)\n                    .setWeekIndexColor(0xFFFF00FF)\n                    .setWeekIndexBgColor(0x00000000)\n\n                    .setCancelText(\"\u53d6\u6d88\")\n                    .setCancelSize(14)\n                    .setCancelColor(Color.RED)\n                    .setCancelBgColor(0xFFFFFFFF)\n\n                    .setConfirmText(\"\u786e\u5b9a\")\n                    .setConfirmSize(14)\n                    .setConfirmColor(Color.RED)\n                    .setConfirmBgColor(0xFFB0E0E6)\n\n                    .setPreset(new YearMonthDay(2017, 11, 4))\n                    .setDaySize(16)\n                    .setDayColor(Color.BLUE)\n                    .setDayOtherMonthColor(0xFF87CEFA)\n                    .setMonthBaseBgColor(0xFFD0EED0)\n\n                    .setJump2Preset(true)\n                    //.restoreDefault()\n                    ;\n\n            builder.show();\n\n\n## License<br>\nunder [MIT License](http://www.opensource.org/licenses/MIT).\n"
 },
 {
  "repo": "futurepress/react-native-static-server",
  "language": "Java",
  "readme_contents": "\n# react-native-static-server\n\nA cross platform component for serving static assets with React Native.\n\n## Getting started\n\n`$ npm install react-native-static-server --save`\n\n### Installation\n\nFrom react-native 0.60 autolinking will take care of the link step but don't forget to run pod install\n\n`$ react-native link react-native-static-server`\n\n## Usage\n\nDeclare the `StaticServer` with a port or use the default `0` to pick a random available port.\n\n```javascript\nimport StaticServer from 'react-native-static-server';\n\nlet server = new StaticServer(8080);\n\n// Start the server\nserver.start().then((url) => {\n  console.log(\"Serving at URL\", url);\n});\n\n// Stop the server\nserver.stop();\n\n// Check if native server running\nconst isRunning = await server.isRunning()\n// isRunning - true/false\n```\n\n`StaticServer` serves from the document directory (default) or takes an optional absolute path to serve from.\n\nFor instance, using [react-native-fs](https://github.com/johanneslumpe/react-native-fs) you can get the document directory and specify a directory from there.\n\n#### Default (document directory)\n\n```javascript\nimport StaticServer from 'react-native-static-server';\nimport RNFS from 'react-native-fs';\n\n// create a path you want to write to\nlet path = RNFS.DocumentDirectoryPath + '/www';\n\nlet server = new StaticServer(8080, path);\n```\n\n#### Custom folder (iOS)\n\n##### Create the folder for static files\n\nCreate a folder in your project's top-level directory (usually next to your node_modules and index.js file), and put the files you want to access over http in there.\n\n##### Add folder (static files) to XCode\n\nThis folder **must be added to XCode** so it gets bundled with the app.\n\nIn XCode, `Project Navigator` right click in the folder project \u2192 `Add files to \"<project>\"` \u2192 Select the static folder **and clic options (Uncheck copy items if needed, Create folder references)** so don't duplicate files \u2192 Clic Add.\n\nWhen the app gets bundled, this folder will be next to the compiled app, so using `MainBundlePath` property from `react-native-fs` you can access to the directory.\n\n```javascript\nimport StaticServer from 'react-native-static-server';\nimport RNFS from 'react-native-fs';\n\n// path where files will be served from (index.html here)\nlet path = RNFS.MainBundlePath + '/www';\n\nlet server = new StaticServer(8080, path);\n```\n\nIf the server should only be accessible from within the app, set `localOnly` to `true`\n\n```javascript\nimport StaticServer from 'react-native-static-server';\n\n// Just set options with defaults\nlet server = new StaticServer({localOnly : true });\n// Or also valid are:\nlet server = new StaticServer(8080, {localOnly : true });\nlet server = new StaticServer(8080, path, {localOnly : true });\n\n```\n\nIf the server should not pause when the app is in the background, set `keepAlive` to `true`\n\n```javascript\nlet server = new StaticServer({keepAlive : true });\n```\n\nPassing `0` as the port number will cause a random port to be assigned every time the server starts.\nIt will reset to a new random port each time the server unpauses, so this should only be used with `keepAlive`.\n\n```javascript\nlet server = new StaticServer(0, {keepAlive : true });\n```\n\n## Credits\n\n* iOS server: [GCDWebServer](https://github.com/swisspol/GCDWebServer)\n* Android server: [NanoHttpd Webserver](https://github.com/NanoHttpd/nanohttpd)\n\nThanks to [CorHttpd](https://github.com/floatinghotpot/cordova-httpd) and [react-native-httpserver](https://gitlab.com/base.io/react-native-httpserver#README) for the basis of this library.\n"
 },
 {
  "repo": "androidessence/PinchZoomTextView",
  "language": "Java",
  "readme_contents": "PinchZoomTextView Library\n=============\n\nThis library allows you to have a TextView that will grow/shrink the font size using gestures from the user.\n\nUsage\n-----\n\nTo have access to the library, add the dependency to your build.gradle:\n\n```java\n\tcompile 'com.androidessence:pinchzoomtextview:1.0.1'\n```\n\nDeveloper Notes\n---------------\n\nBy default, the zoom feature is always enabled. If for any reason you want to disable it, simply call the `PinchZoomTextView#setZoomEnabled(boolean enabled)` method.\n\nSample\n-----\n\nTo see the library in action, here is a sample of the text size changing:\n\n<img src='sample.gif' width='400' height='640' />\n\nCredits & Contact\n-----------------\n\nPinchZoomTextView was created by:\n\n- [Adam McNeilly](https://github.com/AdamMc331)\n\nWith special thanks to [Eric Cugota](https://github.com/tryadelion) for helping me get this into Bintray and make it available for you.\n\nAnd it's released under [Android Essence blog](http://androidessence.com/).\n\nLicense\n-------\n\nPinchZoomTextView is available under the [MIT License](https://opensource.org/licenses/MIT). You are free to do with it what you want. If you submit a pull request, please add your name to the credits section. :)\n"
 },
 {
  "repo": "waylau/mongodb-file-server",
  "language": "Java",
  "readme_contents": "# MongoDB File Server\uff08\u57fa\u4e8e MongoDB \u7684\u6587\u4ef6\u670d\u52a1\u5668\uff09\r\n\r\nMongoDB File Server is a file server system based on MongoDB. MongoDB File Server is committed to the storage of small files, such as pictures in the blog, ordinary documents and so on.\r\n\r\nIt's using some very popular technology like:\r\n\r\n* MongoDB 3.6.4\r\n* Spring Boot 2.0.3.RELEASE\r\n* Spring Data MongoDB 2.0.8.RELEASE\r\n* Spring 5.0.7.RELEASE\r\n* Thymeleaf 3.0.9.RELEASE\r\n* Thymeleaf Layout Dialect 2.2.0\r\n* Embedded MongoDB 2.0.2\r\n* Gradle 4.5.1\r\n\r\n\u57fa\u4e8e MongoDB \u7684\u6587\u4ef6\u670d\u52a1\u5668\u3002MongoDB File Server \u81f4\u529b\u4e8e\u5c0f\u578b\u6587\u4ef6\u7684\u5b58\u50a8\uff0c\u6bd4\u5982\u535a\u5ba2\u4e2d\u56fe\u7247\u3001\u666e\u901a\u6587\u6863\u7b49\u3002\u7531\u4e8eMongoDB \u652f\u6301\u591a\u79cd\u6570\u636e\u683c\u5f0f\u7684\u5b58\u50a8\uff0c\u5bf9\u4e8e\u4e8c\u8fdb\u5236\u7684\u5b58\u50a8\u81ea\u7136\u4e5f\u662f\u4e0d\u8bdd\u4e0b\uff0c\u6240\u4ee5\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u7528\u4e8e\u5b58\u50a8\u6587\u4ef6\u3002\u7531\u4e8e  MongoDB \u7684 BSON \u6587\u6863\u5bf9\u4e8e\u6570\u636e\u91cf\u5927\u5c0f\u7684\u9650\u5236\uff08\u6bcf\u4e2a\u6587\u6863\u4e0d\u8d85\u8fc716M\uff09\uff0c\u6240\u4ee5\u672c\u6587\u4ef6\u670d\u52a1\u5668\u4e3b\u8981\u9488\u5bf9\u7684\u662f\u5c0f\u578b\u6587\u4ef6\u7684\u5b58\u50a8\u3002\u5bf9\u4e8e\u5927\u578b\u6587\u4ef6\u7684\u5b58\u50a8\uff08\u6bd4\u5982\u8d85\u8fc716M\uff09\uff0cMongoDB \u5b98\u65b9\u5df2\u7ecf\u63d0\u4f9b\u4e86\u6210\u719f\u7684\u4ea7\u54c1  [GridFS](https://docs.mongodb.com/manual/core/gridfs/)\uff0c\u8bfb\u8005\u670b\u53cb\u53ef\u4ee5\u81ea\u884c\u4e86\u89e3\u3002\r\n\r\n\u672c\u6587\u4e0d\u4f1a\u5bf9 MongoDB \u7684\u6982\u5ff5\u3001\u57fa\u672c\u7528\u6cd5\u505a\u8fc7\u591a\u7684\u4ecb\u7ecd\uff0c\u6709\u5174\u8da3\u7684\u670b\u53cb\u53ef\u81ea\u884c\u67e5\u9605\u5176\u4ed6\u6587\u732e\uff0c\u6bd4\u5982\uff0c\u7b14\u8005\u6240\u8457\u7684[\u300a\u5206\u5e03\u5f0f\u7cfb\u7edf\u5e38\u7528\u6280\u672f\u53ca\u6848\u4f8b\u5206\u6790\u300b](https://github.com/waylau/distributed-systems-technologies-and-cases-analysis)\u4e00\u4e66\uff0c\u5bf9 MongoDB \u65b9\u9762\u4e5f\u6709\u6240\u7740\u58a8\u3002 \r\n\r\n\r\n## Features \u7279\u6027\r\n\r\n* Easy to use.\uff08\u6613\u4e8e\u4f7f\u7528\uff09\r\n* RESTful API.\r\n* Chinese characters friendly.\uff08\u4e2d\u6587\u53cb\u597d\uff09\r\n* ...\r\n\r\n## APIs\r\n\r\nHere are useful APIs.\r\n\r\n* GET  /files/{pageIndex}/{pageSize} : Paging query file list.(\u5206\u9875\u67e5\u8be2\u6587\u4ef6\u5217\u8868)\r\n* GET  /files/{id} : Download file.(\u4e0b\u8f7d\u67d0\u4e2a\u6587\u4ef6)\r\n* GET  /view/{id} : View file online.(\u5728\u7ebf\u9884\u89c8\u67d0\u4e2a\u6587\u4ef6\u3002\u6bd4\u5982\uff0c\u663e\u793a\u56fe\u7247)\r\n* POST /upload : Upload file.(\u4e0a\u4f20\u6587\u4ef6)\r\n* DELETE /{id} : Delete file.(\u5220\u9664\u6587\u4ef6)\r\n\r\n\r\n## How to \uff08\u5982\u4f55\u4f7f\u7528\uff09\r\n\r\nIt's so easy to start up the MongoDB File Server with 2 steps.\r\n\r\n\u53ea\u9700\u8981\u4e24\u6b65\u3002\r\n\r\n### 1. Get source\uff08\u83b7\u53d6\u6e90\u7801\uff09\r\n\r\n```shell\r\n$ git clone https://github.com/waylau/mongodb-file-server.git\r\n```\r\n\r\n### 2. Run\uff08\u8fd0\u884c\uff09\r\n\r\n```shell\r\n$ gradlew bootRun\r\n```\r\n\r\nthen, you can visit the application at <http://localhost:8081>.\r\n\r\n## Configuration \uff08\u914d\u7f6e\uff09\r\n\r\n\r\nThe default configuration is \uff08\u9ed8\u8ba4\u914d\u7f6e\u5982\u4e0b\uff09 :\r\n\r\n```\r\nserver.address=localhost\r\nserver.port=8081\r\n\r\n# Thymeleaf \r\nspring.thymeleaf.encoding=UTF-8\r\nspring.thymeleaf.cache=false\r\nspring.thymeleaf.mode=HTML5\r\n\r\n# limit upload file size\r\nspring.servlet.multipart.max-file-size=1024KB\r\nspring.servlet.multipart.max-request-size=1024KB\r\n```\r\n\r\n`spring.http.multipart.max-file-size` and `spring.http.multipart.max-request-size` limit upload file never larger than 1MB.\r\n\r\nNOTE: default configuration will use a embedded Mongo, that means data will never persist when the MongoDB File Server restart.\r\n\r\nYou can set `spring.data.mongodb.uri` property to configure additional settings such as the replica set.\uff08\u652f\u6301\u914d\u7f6e\u72ec\u7acb\u8fd0\u884c\u7684 MongoDB \u7684\u8fde\u63a5\u65b9\u5f0f\uff09:\r\n\r\n```shell\r\nspring.data.mongodb.uri=mongodb://user:secret@mongo1.example.com:12345,mongo2.example.com:23456/test\r\n```\r\n\r\nIf you want to use a stanlne MongoDB server, comment out Embedded MongoDB dependencies in `build.gradle` file.\uff08\u5982\u679c\u9700\u8981\u4f7f\u7528\u72ec\u7acb\u8fd0\u884c\u7684 MongoDB\uff0c\u5c31\u628a\u4e0b\u9762\u7684\u4f9d\u8d56\u6ce8\u91ca\u6389\uff09:\r\n\r\n```\r\ndependencies {\r\n\t...\r\n\t// compile('de.flapdoodle.embed:de.flapdoodle.embed.mongo')\r\n\t...\r\n}\r\n```\r\n\r\n## Detail \uff08\u8be6\u7ec6\u8bbe\u8ba1\u8bf4\u660e\uff09\r\n\r\nSee detail <https://waylau.com/mogodb-file-server-with-spring-boot>.\r\n\r\n## Host\uff08\u6258\u7ba1\uff09\r\n\r\n* GitHub\uff1a<https://github.com/waylau/mongodb-file-server>\r\n* \u7801\u4e91\uff1a<https://gitee.com/waylau/mongodb-file-server>\r\n\r\n## Contact \u8054\u7cfb\u4f5c\u8005\r\n\r\n* Blog: [waylau.com](https://waylau.com)\r\n* Gmail: [waylau521(at)gmail.com](mailto:waylau521@gmail.com)\r\n* Weibo: [waylau521](http://weibo.com/waylau521)\r\n* Twitter: [waylau521](https://twitter.com/waylau521)\r\n* Github : [waylau](https://github.com/waylau)\r\n\r\n## Donate \u6350\u8d60\r\n\r\nSupport me!\r\n\r\n\u611f\u8c22\u60a8\u5bf9\u8001\u536b[\u5f00\u6e90\u5de5\u4f5c](https://github.com/waylau)\u7684\u652f\u6301!\r\n\r\n![\u5f00\u6e90\u6350\u8d60](https://waylau.com/images/showmethemoney-sm.jpg)\r\n\r\n\u6350\u8d60\u6240\u5f97\u6240\u6709\u6b3e\u9879\u5c06\u7528\u4e8e\u5f00\u6e90\u4e8b\u4e1a\uff01"
 },
 {
  "repo": "Amterson/AlginProject",
  "language": "Java",
  "readme_contents": "\n### 1.\u975e\u4e2d\u6587\u5355\u8bcd\u4e0d\u591f\u4e00\u884c\u4f1a\u81ea\u52a8\u622a\u65ad\uff0c\u7528\u7b26\u53f7\u201c-\u201d\u8fde\u63a5\u8d77\u6765\uff1b\n\n### 2.\u9002\u914d\u5e03\u5c40\u7684\u65b9\u5411\uff0c\u4f7f\u7528\u539f\u751fTextView\u7684\u5c5e\u6027\uff1aandroid:gravity=\"\"\u548candroid:textAlignment=\"\"\uff0cgravity\u7684\u4f18\u5148\u7ea7\u8f83\u9ad8\uff0c\u5982\u679c\u540c\u65f6\u8bbe\u7f6e\u8fd9\u4e24\u4e2a\u5c5e\u6027\u5219\u4ee5textAlignment\u7684\u5c5e\u6027\u4e3a\u51c6\uff1b\n```\n<com.example.testdemo1.XQJustifyTextView\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:layout_margin=\"10dp\"\n        android:textSize=\"16sp\"\n        android:gravity=\"start\"\n        android:textAlignment=\"textStart\"/>\n```\n\n### 3.\u82f1\u6587\u60c5\u51b5\u4e0b\u4f7f\u7528\u5143\u97f3\u5b57\u6bcd\u8fdb\u884c\u622a\u65ad\uff0c\u5982\u679c\u6ca1\u6709\u627e\u5230\u5143\u97f3\u5b57\u6bcd\u5219\u4f7f\u7528\u9ed8\u8ba4\u89c4\u5219\u622a\u65ad\uff1b\n\n### 4.\u4f9d\u8d56Library\n\u5728\u4e3b\u9879\u76eeapp\u7684build.gradle\u4e2d\u4f9d\u8d56\n```\ndependencies {\n    ...\n    implementation 'com.text:alginlib:1.0.1'\n}\n```\n"
 },
 {
  "repo": "twitch4j/twitch4j",
  "language": "Java",
  "readme_contents": "<p align=\"center\"><a href=\"https://twitch4j.github.io/\"><img src=\".github/logo.png?raw=true\" width=\"150\"></a></p>\n\n# Java API for [Twitch](https://www.twitch.tv/)\n\nSupport:\n\n[![Discord Server](https://discordapp.com/api/guilds/143001431388061696/embed.png?style=banner2)](https://discord.gg/FQ5vgW3)\n[![Twitch API Server](https://discordapp.com/api/guilds/325552783787032576/embed.png?style=banner2)](https://discord.gg/8NXaEyV)\n\nBadges:\n\n[![Latest](https://img.shields.io/github/release/twitch4j/twitch4j/all.svg?style=flate&label=latest)](https://search.maven.org/search?q=g:com.github.twitch4j)\n[![pipeline status](https://gitlab.com/twitch4j/twitch4j/badges/master/pipeline.svg)](https://gitlab.com/twitch4j/twitch4j/commits/master)\n[![Docs](https://img.shields.io/badge/documentation-grey.svg?style=flat)](https://twitch4j.github.io/docs/)\n[![Docs](https://img.shields.io/badge/javadoc-brightgreen.svg?style=flat)](https://twitch4j.github.io/javadoc/)\n\n--------\n\n## A quick note:\n\nThis project provides multiple standalone modules you can use to interact with twitch and related services.\n\n## Quick Start\n\nCheck out the [Documentation](https://twitch4j.github.io/docs/getting-started/installation/)!\n\n--------\n\n## Modules\n\nShared\n* [Event4J](https://github.com/PhilippHeuer/events4j)\n* [Credential Manager](https://github.com/PhilippHeuer/credential-manager)\n\nProject\n* Auth\n* API - Extensions\n* API - Kraken (Deprecated)\n* API - Helix\n* Chat\n* PubSub\n* GraphQL\n* Kotlin\n\n## Problems\n\nIf you have problems using the *Twitch Java API*, then you are welcome to join the [discord server](https://discord.gg/FQ5vgW3) to talk about it.\n\nIf you discover any issues/have feature requests, then please [open an issue here](https://github.com/twitch4j/twitch4j/issues/new).\n\n## Contributing\n\nWe welcome contributions to the library, be it new features, bug fixes, or even small enhancements.\nPlease do read the [contributing guide](https://twitch4j.github.io/docs/contribution/) on the documentation site as it provides code guidelines and helpful tips for getting started.\nBy contributing, you are expected to abide by our [code of conduct](https://github.com/twitch4j/.github/blob/main/CODE_OF_CONDUCT.md) and agree to the license below.\nThank you again for your interest in improving Twitch4J!\n\n## License\n\nReleased under the [MIT license](./LICENSE).\n"
 },
 {
  "repo": "ittianyu/POCenter",
  "language": "Java",
  "readme_contents": "## \u5916\u5305\u96c6\u4e2d\u8425 ##\n\n\u6574\u5408\u591a\u4e2a\u8f6f\u4ef6\u5916\u5305\u5e73\u53f0\u9879\u76ee\u4fe1\u606f\uff0c\u66ff\u4f60\u7b5b\u9009\u4f18\u8d28\u9879\u76ee\n\n![MIT License](https://img.shields.io/github/license/mashape/apistatus.svg) ![api 15+](https://img.shields.io/badge/API-14%2B-green.svg)\n\n## \u5173\u4e8e\u6211\u4eec ##\n[![\u5929\u5b87\u5de5\u4f5c\u5ba4](https://github.com/ittianyu/MobileGuard/blob/master/read_me_images/logo-transparent.png?raw=true)](http://www.ittianyu.com)\n\n## \u529f\u80fd ##\n- ### \u9996\u9875 ###\n\u5c55\u793a\u4f60\u5173\u6ce8\u7684\u5916\u5305\u4fe1\u606f\uff0c\u6bd4\u5982\uff1a \u79fb\u52a8app\u3001\u7f51\u7ad9\u5f00\u53d1\u3001\u5fae\u4fe1/\u5c0f\u7a0b\u5e8f\u3002\n\n- ### \u53d1\u73b0 ###\n\u5c55\u793a\u6240\u6709\u7684\u5916\u5305\u4fe1\u606f\u3002\n\n- ### \u641c\u7d22 ###\n\u5c55\u793a\u6807\u9898\u6216\u63cf\u8ff0\u4e2d\u542b\u6709\u6307\u5b9a\u5173\u952e\u8bcd\u7684\u5916\u5305\u4fe1\u606f\u3002\n\n\n## \u622a\u56fe ##\n\n![\u6b22\u8fce\u754c\u9762](/screenshots/splash.jpg) ![\u4e3b\u754c\u9762](/screenshots/home.jpg)\n\n![\u4e3b\u754c\u9762\u6ed1\u52a8](/screenshots/home_scroll.jpg) ![\u53d1\u73b0\u754c\u9762](/screenshots/find.jpg)\n\n![\u641c\u7d22\u754c\u9762](/screenshots/search.jpg) ![\u6211\u7684\u754c\u9762](/screenshots/mime.jpg)\n\n![\u6ca1\u6709\u6570\u636e\u754c\u9762](/screenshots/empty.jpg) ![\u9519\u8bef\u754c\u9762](/screenshots/error.jpg)\n\n\n## \u4e0b\u8f7d ##\n\n\u5df2\u5728 [\u767e\u5ea6\u624b\u673a\u52a9\u624b](http://shouji.baidu.com/software/10867391.html)\u3001[91\u5e02\u573a](http://apk.91.com/Soft/Android/com.ittianyu.pocenter-1.html)\u3001[\u5b89\u5353\u5e02\u573a](http://apk.hiapk.com/appinfo/com.ittianyu.pocenter/1) \u4e0a\u7ebf\u3002\n\n\u8d34\u51fa\u4e8c\u7ef4\u7801\u65b9\u4fbf\u4e0b\u8f7d\n\n![](http://d.hiphotos.bdimg.com/wisegame/pic/item/72dfa9ec8a136327b9c91278988fa0ec08fac752.jpg)\n\n## \u6280\u672f\u70b9 ##\n\n- \u6846\u67b6\uff1aMVP\n- \u7f51\u7edc\u8bbf\u95ee\uff1aRetrofit2 + OkHttp3\n- \u7f51\u7edc\u7f13\u5b58\uff1aRxCache\n- \u6570\u636e\u53ca\u89e3\u6790\uff1aGson \u89e3\u6790 json\n- \u5f02\u6b65\u6846\u67b6\uff1aRxJava2\n- \u8bbe\u8ba1\u89c4\u8303\uff1aMaterial Design\n- \u754c\u9762\u5e03\u5c40\uff1aBottomNavigationView + ViewPager + Fragment\n- \u7b2c\u4e09\u65b9\u63a5\u5165\uff1aShareSdk\u3001\u53cb\u76df app \u7edf\u8ba1\u3001TinkerPatch \u70ed\u66f4\u65b0\n\n## \u4eae\u70b9 ##\n\n- ViewPager \u61d2\u52a0\u8f7d\uff1a\u5728 Activity \u751f\u547d\u5468\u671f\u5185\u4ec5\u52a0\u8f7d\u4e00\u6b21\uff0c\u4e14\u7b2c\u4e00\u6b21\u53ef\u89c1\u65f6\u52a0\u8f7d\u3002\n- LCEE \u754c\u9762\u903b\u8f91\uff1a\u4e5f\u5c31\u662f\u52a0\u8f7d\u3001\u5185\u5bb9\u3001\u9519\u8bef\u3001\u7a7a\u89c6\u56fe\u3002\n\n## \u9879\u76ee\u5305\u7ed3\u6784 ##\n\n- com.ittianyu.pocenter\n\t- common &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u516c\u7528\u4ee3\u7801\n\t\t- api &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u7f51\u7edc\u8bbf\u95ee\u4ee3\u7801\n\t\t- base &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \u57fa\u7c7b\n\t\t- bean &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \u5b9e\u4f53\u7c7b\n\t\t- utils&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u5de5\u5177\u7c7b\n\t- features&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u529f\u80fd\n\t\t- detail&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u8be6\u60c5\n\t\t- find &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \u53d1\u73b0\n\t\t- home&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u4e3b\u9875\n\t\t- mime&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u6211\u7684\n\t\t- search &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \u641c\u7d22\n\t\t- type &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u6807\u7b7e\u7ba1\u7406\n\t\t- version&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \u7248\u672c\u66f4\u65b0\n\t- MainActivity&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \u4e3b\u754c\u9762\n\t- SplashActivity&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\u6b22\u8fce\u754c\u9762\n\n\n## \u8e29\u5751\u7ecf\u5386 ##\n\n- \u6df7\u6dc6\uff1a\u5982\u679c\u9879\u76ee\u4e2d\u6709\u7528\u5230 Gson \u89e3\u6790\u6570\u636e\uff0c\u4e0d\u8981\u6df7\u6dc6\u5b9e\u4f53\u7c7b\u3002\u6700\u597d\u517b\u6210\u7528\u4e00\u4e2a\u5e93\uff0c\u5c31\u52a0\u4e0a\u4e00\u4e2a\u5e93\u7684\u6df7\u6dc6\u4ee3\u7801\u7684\u4e60\u60ef\u3002\n\n\n## \u81f4\u8c22 ##\n\n\u611f\u8c22\u5de5\u4f5c\u5ba4\u7684 UI\u5927\u5927 \u7ed9\u4e0e\u4e86\u5173\u4e8e\u914d\u8272\u65b9\u9762\u7684\u5efa\u8bae\u3002\n\n\n## \u6388\u6743 ##\n\n\tMIT License\n\t\n\tCopyright (c) 2017 ittianyu\n\t\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n\tof this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\t\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n\t\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n"
 },
 {
  "repo": "acloudyh/springCloud",
  "language": "Java",
  "readme_contents": "## springCloud \u5b66\u4e60\n\n[springCloud-config\u914d\u7f6e\u4e2d\u5fc3](https://github.com/acloudyh/springCloud-config)\n\n## \u535a\u5ba2\n\n[\u4e2a\u4eba\u4e3b\u9875](https://acloudyh.github.io/categories/)\n\n[CSDN](https://blog.csdn.net/yanghao937170)\n\n\u8111\u56fe\u94fe\u63a5:\n\u94fe\u63a5: https://pan.baidu.com/s/1NqN2yP0FfsrozJ6bIANTNQ \u63d0\u53d6\u7801: k7v9\n\n![Alt text](image/cloud\u7ec4\u4ef6\u56fe.png)\n\n## Eureka\u57fa\u7840\u77e5\u8bc6\n\n#### \u4ec0\u4e48\u662f\u670d\u52a1\u6ce8\u518c\u4e0e\u53d1\u73b0\n\n    Eureka\u91c7\u7528\u4e86CS\u7684\u8bbe\u8ba1\u67b6\u6784\uff0cEureka Server\u4f5c\u4e3a\u670d\u52a1\u6ce8\u518c\u529f\u80fd\u7684\u670d\u52a1\u5668\uff0c\u5b83\u662f\u670d\u52a1\u6ce8\u518c\u4e2d\u5fc3\u3002\u800c\u7cfb\u7edf\u4e2d\u7684\u5176\u4ed6\u5fae\u670d\u52a1,\u4f7f\u7528Eureka\u7684\u5ba2\u6237\u7aef\u8fde\u63a5\u5230Eureka Server\u5e76\u7ef4\u6301\u5fc3\n    \u8df3\u8fde\u63a5\u3002\u8fd9\u6837\u7cfb\u7edf\u7684\u7ef4\u62a4\u4eba\u5458\u5c31\u53ef\u4ee5\u901a\u8fc7Eureka Server\u6765\u76d1\u63a7\u7cfb\u7edf\u4e2d\u5404\u4e2a\u5fae\u670d\u52a1\u662f\u5426\u6b63\u5e38\u8fd0\u884c\u3002\n \n    \u5728\u670d\u52a1\u6ce8\u518c\u4e0e\u53d1\u73b0\u4e2d\uff0c\u6709\u4e00\u4e2a\u6ce8\u518c\u4e2d\u5fc3\u3002 \u5f53\u670d\u52a1\u5668\u542f\u52a8\u7684\u65f6\u5019\uff0c\u4f1a\u628a\u5f53\u524d\u81ea\u5df1\u670d\u52a1\u5668\u7684\u4fe1\u606f\u6bd4\u5982\u670d\u52a1\u5730\u5740\u901a\u8baf\u5730\u5740\u7b49\u4ee5\u522b\u540d\u65b9\u5f0f\u6ce8\u518c\u5230\u6ce8\u518c\u4e2d\u5fc3\u4e0a\u3002\u53e6-\u65b9(\u6d88\u8d39\u8005|\u670d\u52a1\u63d0\u4f9b\n    \u8005)\uff0c\u4ee5\u8be5\u522b\u540d\u7684\u65b9\u5f0f\u53bb\u6ce8\u518c\u4e2d\u5fc3\u4e0a\u83b7\u53d6\u5230\u5b9e\u9645\u7684\u670d\u52a1\u901a\u8baf\u5730\u5740,\u7136\u540e\u518d\u5b9e\u73b0\u672c\u5730RPC\u8c03\u7528RPC\u8fdc\u7a0b\u8c03\u7528\u6846\u67b6\u6838\u5fc3\u8bbe\u8ba1\u601d\u60f3:\u5728\u4e8e\u6ce8\u518c\u4e2d\u5fc3\uff0c\u56e0\u4e3a\u4f7f\u7528\u6ce8\u518c\u4e2d\u5fc3\u7ba1\u7406\u6bcf\u4e2a\u670d\u52a1\u4e0e\n    \u670d\u52a1\u4e4b\u95f4\u7684\u4e00\u4e2a\u4f9d\u8d56\u5173\u7cfb(\u670d\u52a1\u6cbb\u7406\u6982\u5ff5)\u3002\u5728\u4efb\u4f55rpc\u8fdc\u7a0b\u6846\u67b6\u4e2d\uff0c\u90fd\u4f1a\u6709-\u4e2a\u6ce8\u518c\u4e2d\u5fc3(\u5b58\u653e\u670d\u52a1\u5730\u5740\u76f8\u5173\u4fe1\u606f(\u63a5\u53e3\u5730\u5740)\n\n## consul\n\n#### Mac\u5b89\u88c5consul\n\n- \u542f\u52a8\n    ```\n    brew install consul\n    ```\n- \u8fd0\u884c\n    ```shell script\n    consul agent -dev\n    ```\n- \u9875\u9762\u67e5\u770b\n    ```html\n    http://localhost:8500/\n    ```\n\n## \u7ecf\u5178CAP\u56fe\n\n```text\n    C: Consistency(\u5f3a\u4e00\u81f4\u6027)\n    A: Availability(\u53ef\u7528\u6027)\n    P: Partition tolerance(\u5206\u533a\u5bb9\u9519)\n    CAP\u7406\u8bba\u5173\u6ce8\u7c92\u5ea6\u662f\u6570\u636e\uff0c\u800c\u4e0d\u662f\u6574\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7b56\u7565\n```\n\n    AP(Eureka)\n    \n    CP(Zookeeper/Consul)\n\n- CAP\u7406\u8bba\u7684\u6838\u5fc3\u662f: -\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e0d\u53ef\u80fd\u540c\u65f6\u5f88\u597d\u7684\u6ee1\u8db3\u4e00\u81f4\u6027\uff0c\u53ef\u7528\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u8fd9\u4e09\u4e2a\u9700\u6c42,\n- \u6700\u591a\u53ea\u80fd\u540c\u65f6\u8f83\u597d\u7684\u6ee1\u8db3\u4e24\u4e2a\n\n\u56e0\u6b64\uff0c\u6839\u636eCAP\u539f\u7406\u5c06NoSQL\u6570\u636e\u5e93\u5206\u6210\u4e86\u6ee1\u8db3CA\u539f\u5219\u3001\u6ee1\u8db3CP\u539f\u5219\u548c\u6ee1\u8db3AP\u539f\u5219\u4e09\u5927\u7c7b:\n\n- CA-\u5355\u70b9\u96c6\u7fa4\uff0c\u6ee1\u8db3\u4e00\u81f4\u6027\uff0c\u53ef\u7528\u6027\u7684\u7cfb\u7edf\uff0c\u901a\u5e38\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u4e0d\u592a\u5f3a\u5927\u3002\n- CP-\u6ee1\u8db3\u4e00 \u81f4\u6027,\u5206\u533a\u5bb9\u5fcd\u5fc5\u7684\u7cfb\u7edf\uff0c\u901a\u5e38\u6027\u80fd\u4e0d\u662f\u7279\u522b\u9ad8\u3002\n- AP -\u6ee1\u8db3\u53ef\u7528\u6027,\u5206\u533a\u5bb9\u5fcd\u6027\u7684\u7cfb\u7edf\uff0c\u901a\u5e38\u53ef\u80fd\u5bf9\u4e00 \u81f4\u6027\u8981\u6c42\u4f4e\u4e00 \u4e9b\u3002\n\n  ![Alt text](image/CAP.png)\n\n## Ribbon\n\n- \u7b80\u4ecb:\n    - Spring Cloud Ribbon \u662f\u57fa\u4e8eNetflix\u5b9e\u73b0\u7684\u4e00\u5957\u5ba2\u6237\u7aef\u8d1f\u8f7d\u5747\u8861\u7684\u5de5\u5177;\n    - \u7b80\u5355\u7684\u8bf4,Ribbon\u662fNetflix\u53d1\u5e03\u7684\u5f00\u6e90\u9879\u76ee,\u4e3b\u8981\u529f\u80fd\u662f\u63d0\u4f9b\u5ba2\u6237\u7aef\u7684\u8f6f\u4ef6\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u548c\u670d\u52a1\u8c03\u7528. Ribbon\u5ba2\u6237\u7aef\u7ec4\u4ef6\u63d0\u4f9b\u4e00\u7cfb\u5217\u5b8c\u5584\u7684\u914d\u7f6e\u9879,\u5982\u8fde\u63a5\u8d85\u65f6,\u91cd\u8bd5\u7b49. \u7b80\u5355\u7684\u8bf4,\u5c31\u662f\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u5217\u51faLoad\n      Balancer (\u7b80\u79f0LB)\u540e\u9762\u6240\u6709\u7684\u673a\u5668, Ribbon\u4f1a\u81ea\u52a8\u7684\u5e2e\u52a9\u4f60\u57fa\u4e8e\u67d0\u79cd\u89c4\u5219(\u7b80\u5355\u8f6e\u8be2,\u968f\u673a\u8fde\u63a5\u7b49)\u53bb\u8fde\u63a5\u8fd9\u4e9b\u673a\u5668.\n\n- Ribbon \u672c\u5730\u8d1f\u8f7d\u5747\u8861\u5ba2\u6237\u7aef VS Nginx \u670d\u52a1\u7aef\u8d1f\u8f7d\u5747\u8861\u533a\u522b\n    - Nginx\u662f\u670d\u52a1\u5668\u8d1f\u8f7d\u5747\u8861,\u5ba2\u6237\u7aef\u6240\u6709\u8bf7\u6c42\u90fd\u4f1a\u4ea4\u7ed9Nginx,\u7136\u540e\u7531Nginx\u5b9e\u73b0\u8f6c\u53d1\u8bf7\u6c42,\u5373\u8d1f\u8f7d\u5747\u8861\u662f\u7531\u670d\u52a1\u7aef\u5b9e\u73b0\u7684\n    - Ribbon\u672c\u5730\u8d1f\u8f7d\u5747\u8861,\u5728\u8c03\u7528\u5fae\u670d\u52a1\u63a5\u53e3\u65f6\u5019,\u4f1a\u5728\u6ce8\u518c\u4e2d\u5fc3\u4e0a\u83b7\u53d6\u6ce8\u518c\u4fe1\u606f\u670d\u52a1\u5217\u8868\u4e4b\u540e\u7f13\u5b58\u5230JVM\u672c\u5730,\u4ece\u800c\u5728\u672c\u5730\u5b9e\u73b0RPC\u8fdc\u7a0b\u670d\u52a1\u8c03\u7528\u6280\u672f.\n\n## OpenFeign\n\n    Feign\u662f\u4e00\u4e2a\u58f0\u660e\u5f0f\u7684web\u670d\u52a1\u5ba2\u6237\u7aef\uff0c\u8ba9\u7f16\u5199web\u670d\u52a1\u5ba2\u6237\u7aef\u53d8\u5f97\u975e\u5e38\u5bb9\u6613\uff0c\u53ea\u9700\u521b\u5efa\u4e00\u4e2a\u63a5\u53e3\u5e76\u5728\u63a5\u53e3\u4e0a\u6dfb\u52a0\u6ce8\u89e3\u5373\u53ef\n\n```java\n\n//\u542f\u7528feign\u5ba2\u6237\u7aef\n@EnableFeignClients\n\n//\u5b9a\u4e49feign\u5ba2\u6237\u7aef\n@FeignClient(value = \"CLOUD-PAYMENT-SERVICE\")\n```\n\n## Hystrix\n\n#### \u670d\u52a1\u964d\u7ea7\n\n- \u6982\u5ff5: \u670d\u52a1\u964d\u7ea7\uff0c\u5f53\u670d\u52a1\u5668\u538b\u529b\u5267\u589e\u7684\u60c5\u51b5\u4e0b\uff0c\u6839\u636e\u5f53\u524d\u4e1a\u52a1\u60c5\u51b5\u53ca\u6d41\u91cf\u5bf9\u4e00\u4e9b\u670d\u52a1\u548c\u9875\u9762\u6709\u7b56\u7565\u7684\u964d\u7ea7\uff0c\u4ee5\u6b64\u91ca\u653e\u670d\u52a1\u5668\u8d44\u6e90\u4ee5\u4fdd\u8bc1\u6838\u5fc3\u4efb\u52a1\u7684\u6b63\u5e38\u8fd0\u884c; (\u7b80\u5355\u6765\u8bf4: \u670d\u52a1\u5668\u5fd9\uff0c\u8bf7\u7a0d\u5019\u518d\u8bd5\uff0c\u4e0d\u8ba9\u5ba2\u6237\u7aef\u7b49\u5f85\u5e76\u7acb\u523b\u8fd4\u56de\u4e00\u4e2a\u53cb\u597d\u63d0\u793a\uff0cfallback)\n- \u54ea\u4e9b\u60c5\u51b5\u89e6\u53d1\u964d\u7ea7\n    - \u7a0b\u5e8f\u8fd0\u884c\u5f02\u5e38\n    - \u8d85\u65f6\n    - \u670d\u52a1\u7194\u65ad\u89e6\u53d1\u670d\u52a1\u964d\u7ea7\n    - \u7ebf\u7a0b\u6c60/\u4fe1\u53f7\u91cf\u6253\u6ee1\u4e5f\u4f1a\u5bfc\u81f4\u670d\u52a1\u964d\u7ea7\n- \u6ce8\u89e3\n    ```java\n      @HystrixCommand(fallbackMethod = \"PaymentTimeOutFallbackMethod\", commandProperties = {\n                @HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\", value = \"1500\")\n        })\n    ```\n- \u6ce8\u610f\u7edf\u4e00fallback\n\n#### \u670d\u52a1\u7194\u65ad\n\n    \u670d\u52a1\u7684\u964d\u7ea7->\u8fdb\u800c\u7194\u65ad->\u6062\u590d\u8c03\u7528\u94fe\u8def\n\n[https://martinfowler.com/bliki/CircuitBreaker.html](https://martinfowler.com/bliki/CircuitBreaker.html)\n\n![Alt text](image/CircuitBreaker.jpg)\n\n- \u6982\u5ff5: \u5982\u679c\u67d0\u4e2a\u76ee\u6807\u670d\u52a1\u8c03\u7528\u6162\u6216\u8005\u6709\u5927\u91cf\u8d85\u65f6\uff0c\u6b64\u65f6\uff0c\u7194\u65ad\u8be5\u670d\u52a1\u7684\u8c03\u7528\uff0c\u5bf9\u4e8e\u540e\u7eed\u8c03\u7528\u8bf7\u6c42\uff0c\u4e0d\u5728\u7ee7\u7eed\u8c03\u7528\u76ee\u6807\u670d\u52a1\uff0c\u76f4\u63a5\u8fd4\u56de\uff0c\u5feb\u901f\u91ca\u653e\u8d44\u6e90; \u5982\u679c\u76ee\u6807\u670d\u52a1\u60c5\u51b5\u597d\u8f6c\u5219\u6062\u590d\u8c03\u7528\u3002(\u7b80\u5355\u6765\u8bf4:\n  \u7c7b\u6bd4\u4fdd\u9669\u4e1d\u8fbe\u5230\u6700\u5927\u670d\u52a1\u8bbf\u95ee\u540e\uff0c\u76f4\u63a5\u62d2\u7edd\u8bbf\u95ee\uff0c\u62c9\u95f8\u9650\u7535\uff0c\u7136\u540e\u8c03\u7528\u670d\u52a1\u964d\u7ea7\u7684\u65b9\u6cd5\u5e76\u8fd4\u56de\u53cb\u597d\u63d0\u793a)\n- \u7194\u65ad\u8bbe\u8ba1\n\n  \u4e09\u4e2a\u6a21\u5757\uff1a\u7194\u65ad\u8bf7\u6c42\u5224\u65ad\u7b97\u6cd5\u3001\u7194\u65ad\u6062\u590d\u673a\u5236\u3001\u7194\u65ad\u62a5\u8b66\n\n    - \u7194\u65ad\u8bf7\u6c42\u5224\u65ad\u673a\u5236\u7b97\u6cd5\uff1a\u4f7f\u7528\u65e0\u9501\u5faa\u73af\u961f\u5217\u8ba1\u6570\uff0c\u6bcf\u4e2a\u7194\u65ad\u5668\u9ed8\u8ba4\u7ef4\u62a410\u4e2abucket\uff0c\u6bcf1\u79d2\u4e00\u4e2abucket\uff0c\u6bcf\u4e2ablucket\u8bb0\u5f55\u8bf7\u6c42\u7684\u6210\u529f\u3001\u5931\u8d25\u3001\u8d85\u65f6\u3001\u62d2\u7edd\u7684\u72b6\u6001\uff0c\u9ed8\u8ba4\u9519\u8bef\u8d85\u8fc750%\u4e1410\u79d2\u5185\u8d85\u8fc720\u4e2a\u8bf7\u6c42\u8fdb\u884c\u4e2d\u65ad\u62e6\u622a\u3002\n\n    - \u7194\u65ad\u6062\u590d\uff1a\u5bf9\u4e8e\u88ab\u7194\u65ad\u7684\u8bf7\u6c42\uff0c\u6bcf\u96945s\u5141\u8bb8\u90e8\u5206\u8bf7\u6c42\u901a\u8fc7\uff0c\u82e5\u8bf7\u6c42\u90fd\u662f\u5065\u5eb7\u7684\uff08RT<250ms\uff09\u5219\u5bf9\u8bf7\u6c42\u5065\u5eb7\u6062\u590d\u3002\n\n    - \u7194\u65ad\u62a5\u8b66\uff1a\u5bf9\u4e8e\u7194\u65ad\u7684\u8bf7\u6c42\u6253\u65e5\u5fd7\uff0c\u5f02\u5e38\u8bf7\u6c42\u8d85\u8fc7\u67d0\u4e9b\u8bbe\u5b9a\u5219\u62a5\u8b66\n\n- \u6ce8\u89e3 \u914d\u7f6e\u53c2\u6570\n    ```java\n   @HystrixCommand(fallbackMethod = \"xxx_method\",\n            groupKey = \"strGroupCommand\",\n            commandKey = \"strCommarld\",\n            threadPoolKey = \"strThreadPool\",\n            commandProperties = {\n                    //\u8bbe\u7f6e\u9694\u79bb\u7b56\u7565\uff0cTHREAD \u8868\u793a\u7ebf\u7a0b\u5979SEMAPHORE:\u4fe1\u53f7\u4ed6\u9694\u79bb\n                    @HystrixProperty(name = \"execution.isolation.strategy\", value = \"THREAD\"),\n                    //\u5f53\u9694\u79bb\u7b56\u7565\u9009\u62e9\u4fe1\u53f7\u4ed6\u9694\u79bb\u7684\u65f6\u5019\uff0c\u7528\u6765\u8bbe\u7f6e\u4fe1\u53f7\u5730\u7684\u5927\u5c0f(\u6700\u5927\u5e76\u53d1\u6570)\n                    @HystrixProperty(name = \"execution.isolation.semaphore.maxConcurrentRequests\", value = \"10\"),\n                    //\u914d\u7f6e\u547d\u4ee4\u6267\u884c\u7684\u8d85\u65f6\u65f6\u95f4\n                    @HystrixProperty(name = \"execution.isolation.thread.timeoutinMilliseconds\", value = \"10\"),\n                    //\u662f\u5426\u542f\u7528\u8d85\u65f6\u65f6\u95f4\n                    @HystrixProperty(name = \"execution.timeout.enabled\", value = \"true\"),\n                    //\u6267\u884c\u8d85\u65f6\u7684\u65f6\u5019\u662f\u5426\u4e2d\u65ad\n                    @HystrixProperty(name = \"execution.isolation.thread.interruptOnTimeout\", value = \"true\"),\n                    //\u6267\u884c\u88ab\u53d6\u6d88\u7684\u65f6\u5019\u662f\u5426\u4e2d\u65ad\n                    @HystrixProperty(name = \"execution.isolation.thread.interruptOnCancel\", value = \"true\"),\n                    //\u5141\u8bb8\u56de\u8c03\u65b9\u6cd5\u6267\u884c\u7684\u6700\u5927\u5e76\u53d1\u6570\n                    @HystrixProperty(name = \"fallback.isolation.semaphore.maxConcurrentRequests\", value = \"10\"),\n                    //\u670d\u52a1\u964d\u7ea7\u662f\u5426\u542f\u7528\uff0c\u662f\u5426\u6267\u884c\u56de\u8c03\u51fd\u6570\n                    @HystrixProperty(name = \"fallback.enabled\", value = \"true\"),\n                    @HystrixProperty(name = \"circuitBreaker.enabled\", value = \"true\"),\n                    //\u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u5728\u6eda\u52a8\u65f6\u95f4\u7a97\u4e2d\uff0c\u65ad\u8def\u5668\u7194\u65ad\u7684\u6700\u5c0f\u8bf7\u6c42\u6570\u3002\u4f8b\u5982\uff0c\u9ed8\u8ba4\u8be5\u503c\u4e3a20\u7684\u65f6\u5019\uff0c\n                    //\u5982\u679c\u6eda\u52a8\u65f6\u95f4\u7a97(\u9ed8\u8ba410\u79d2)\u5185\u4ec5\u6536\u5230\u4e8619\u4e2a\u8bf7\u6c42\uff0c\u5373\u4f7f\u8fd919\u4e2a\u8bf7\u6c42\u90fd\u5931\u8d25\u4e86\uff0c \u65ad\u8def\u5668\u4e5f\u4e0d\u4f1a\u6253\u5f00\u3002\n                    @HystrixProperty(name = \"circuitBreaker.requestVolumeThreshold\", value = \"20\"),\n                    // \u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u5728\u7194\u52a8\u65f6\u95f4\u7a97\u4e2d\u8868\u793a\u5728\u6eda\u52a8\u65f6\u95f4\u7a97\u4e2d\uff0c\u5728\u8bf7\u6c42\u6570\u91cf\u8d85\u8fc7\n                    // circuitBreaker.requestVolumeThreshold \u7684\u60c5\u51b5\u4e0b,\u5982\u679c\u9519\u8bef\u8bf7\u6c42\u6570\u7684\u767e\u5206\u6bd4\u8d85\u8fc750,\n                    //\u5c31\u628a\u65ad\u8def\u5668\u8bbe\u7f6e\u4e3a\u201c\u6253\u5f00\u201d\u72b6\u6001\uff0c\u5426\u5219\u5c31\u8bbe\u7f6e\u4e3a\u201c\u5173\u95ed\u201d\u72b6\u6001\u3002\n                    @HystrixProperty(name = \"circuitBreaker.errorThresholdPercentage\", value = \"50\"),\n                    // \u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u5f53\u65ad\u8def\u5668\u6253\u5f00\u4e4b\u540e\u7684\u4f11\u7720\u65f6\u95f4\u7a97\u3002\u4f11\u7720\u65f6\u95f4\u7a97\u7ed3\u675f\u4e4b\u540e,\n                    //\u4f1a\u5c06\u65ad\u8def\u5668\u7f6e\u4e3a\"\u534a\u5f00\u201d\u72b6\u6001\uff0c\u5c1d\u8bd5\u7194\u65ad\u7684\u8bf7\u6c42\u547d\u4ee4\uff0c\u5982\u679c\u4f4e\u7136\u5931\u8d25\u5c31\u5c06\u65ad\u8def\u5668\u7ee7\u7eed\u8bbe\u7f6e\u4e3a\"\u6253\u5f00\u201d\u72b6\u6001\uff0c\n                    //\u5982\u679c\u6210\u529f\u5c31\u8bbe\u7f6e\u4e3a\"\u5173\u95ed\u201d\u72b6\u6001\u3002\n                    @HystrixProperty(name = \"circuitBreaker.sleepWindowinMilliseconds\", value = \"5009\"),\n                    //\u65ad\u8def\u5668\u5f3a\u5236\u6253\u5f00\n                    @HystrixProperty(name = \"circuitBreaker.force0pen\", value = \"false\"),\n                    // \u65ad\u8def\u5668\u5f3a\u5236\u5173\u95ed\n                    @HystrixProperty(name = \"circuitBreaker.forceClosed\", value = \"false\"),\n                    //\u6eda\u52a8\u65f6\u95f4\u7a97\u8bbe\u7f6e\uff0c\u8be5\u65f6\u95f4\u7528\u4e8e\u65ad\u8def\u5668\u5224\u65ad\u5065\u5eb7\u5ea6\u65f6\u9700\u8981\u6536\u96c6\u4fe1\u606f\u7684\u6301\u7eed\u65f6\u95f4\n                    @HystrixProperty(name = \"metrics.rollingStats.timeinMilliseconds\", value = \"10000\"),\n                    //\u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u6eda\u52a8\u65f6\u95f4\u7a97\u7edf\u8ba1\u6307\u6807\u4fe1\u606f\u65f6\u5212\u5206\u201d\u6876\"\u7684\u6570\u91cf\uff0c\u65ad\u8def\u5668\u5728\u6536\u96c6\u6307\u6807\u4fe1\u606f\u7684\u65f6\u5019\u4f1a\u6839\u636e\u8bbe\u7f6e\u7684\u65f6\u95f4\u7a97\u957f\u5ea6\u62c6\u5206\u6210\u591a\u4e2a\"\u76f8\"\u6765\u7d2f\u8ba1\u5404\u5ea6\u91cf\u503c\uff0c\u6bcf\u4e2a\u201d\u6876\"\u8bb0\u5f55\u4e86-\u6bb5\u65f6\u95f4\u5185\u7684\u91c7\u96c6\u6307\u6807\u3002\n                    //\u6bd4\u598210\u79d2\u5185\u62c6\u5206\u621010\u4e2a\u201d\u6876\"\u6536\u96c6\u8fd9\u6837\uff0c\u6240\u4ee5timeinMilliseconds \u5fc5\u987b\u80fd\u88abnumBuckets \u6574\u9664\u3002\u5426\u5219\u4f1a\u629b\u5f02\u5e38\n                    @HystrixProperty(name = \"metrics.rollingStats.numBuckets\", value = \"10\"),\n                    //\u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u5bf9\u547d\u4ee4\u6267\u884c\u7684\u5ef6\u8fdf\u662f\u5426\u4f7f\u7528\u767e\u5206\u4f4d\u6570\u6765\u8ddf\u8e2a\u548c\u8ba1\u7b97\u3002\u5982\u679c\u8bbe\u7f6e\u4e3afalse,\u90a3\u4e48\u6240\u6709\u7684\u6982\u8981\u7edf\u8ba1\u90fd\u5c06\u8fd4\u56de-1.\n                    @HystrixProperty(name = \"metrics .rollingPercentile.enabled\", value = \"false\"),\n                    //\u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u767e\u5206\u4f4d\u7edf\u8ba1\u7684\u6eda\u52a8\u7a97\u53e3\u7684\u6301\u7eed\u65f6\u95f4\uff0c \u5355\u4f4d\u4e3a\u6beb\u79d2\u3002\n                    @HystrixProperty(name = \"metrics.rollingPercentile.timeInMilliseconds\", value = \"60000\"),\n                    //\u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u767e\u5206\u4f4d\u7edf\u8ba1\u6f14\u52a8\u7a97\u53e3\u4e2d\u4f7f\u7528\u201c\u6876\u201d\u7684\u6570\u91cf\u3002\n                    @HystrixProperty(name = \"metrics.rollingPercentile.numBuckets\", value = \"60000\"),\n                    // \u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u6bcf\u4e2a \u201c\u6876\u201d\u4e2d\u4fdd\u7559\u7684\u6700\u5927\u6267\u884c\u6b21\u6570\u3002\u5982\u679c\u5728\u6eda\u52a8\u65f6\u95f4\u7a97\u5185\u53d1\u751f\u8d85\u8fc7\u8be5\u8bbe\u5b9a\u503c\u7684\u6267\u884c\u6b21\u6570\uff0c\u5c31\u4ece\u6700\u521d\u7684\u4f4d\u7f6e\u5f00\u59cb\u91cd\u5199\u3002\u4f8b\u5982\uff0c\u5c06\u8be5\u503c\u8bbe\u7f6e\u4e3a100,\u71ce\u52a8\u7a97\u53e3\u4e3a10\u79d2\uff0c \u82e5\u572810\u79d2\u5185\u4e00 \u4e00\u4e2a\u201c\u6876 \u201d \u4e2d\u53d1\u751f7500\u6b21\u6267\u884c\uff0c\n                    //\u90a3\u4e48\u8be5\u201c\u6876\u201d\u4e2d\u53ea\u4fdd\u7559\u6700\u540e\u7684100\u6b21\u6267\u884c\u7684\u7edf\u8ba1\u3002\u53e6\u5916,\u589e\u52a0\u8be5\u503c\u7684\u5927\u5c0f\u5c06\u4f1a\u589e\u52a0\u5185\u5b58\u91cf\u7684\u6d88\u8017\uff0c \u5e76\u589e\u52a0\u6392\u5e8f\u767e\u5206\u4f4d\u6570\u6240\u9700\u7684\u8ba1\u7b97\n                    @HystrixProperty(name = \"metrics.rollingPercentile.bucketSize\", value = \"100\"),\n                    //\u8be5\u5c5e\u6027\u7528\u6765\u8bbe\u7f6e\u91c7\u96c6\u5f71\u54cd\u65ad\u8def\u5668\u72b6\u6001\u7684\u5065\u5eb7\u5feb\u7167(\u8bf7\u6c42\u7684\u6210\u529f\u3001\u9519\u8bef\u767e\u5206\u6bd4) \u7684\u95f4\u9694\u7b49\u5f85\u65f6\u95f4\u3002\n                    @HystrixProperty(name = \"metrics.healthSnapshot.intervalinMilliseconds\", value = \"500\"),\n                    //\u662f\u5426\u5f00\u542f\u8bf7\u6c42\u7f13\u5b58\n                    @HystrixProperty(name = \"requestCache.enabled\", value = \"true\"),\n                    // HystrixCommand\u7684\u6267\u884c\u548c\u65f6\u95f4\u662f\u5426\u6253\u5370\u65e5\u5fd7\u5230HystrixRequestLog\u4e2d\n                    @HystrixProperty(name = \"requestLog.enabled\", value = \"true\"),\n            },\n            threadPoolProperties = {\n                    //\u8be5\u53c2\u6570\u7528\u6765\u8bbe\u7f6e\u6267\u884c\u547d\u4ee4\u7ebf\u7a0b\u4ed6\u7684\u6838\u5fc3\u7ebf\u7a0b\u6570\uff0c\u8be5\u503c \u4e5f\u5c31\u662f\u547d\u4ee4\u6267\u884c\u7684\u6700\u5927\u5e76\u53d1\u91cf\n                    @HystrixProperty(name = \"coreSize\", value = \"10\"),\n                    //\u8be5\u53c2\u6570\u7528\u6765\u8bbe\u7f6e\u7ebf\u7a0b\u5979\u7684\u6700\u5927\u961f\u5217\u5927\u5c0f\u3002\u5f53\u8bbe\u7f6e\u4e3a-1\u65f6\uff0c\u7ebf\u7a0b\u6c60\u5c06\u4f7f\u7528SynchronousQueue \u5b9e\u73b0\u7684\u961f\u5217\uff0c\n                    // \u5426\u5219\u5c06\u4f7f\u7528LinkedBlocakingQueue\u5b9e\u73b0\u961f\u5217\n                    @HystrixProperty(name = \"maxQueueSize\", value = \"-1\"),\n                    // \u8be5\u53c2\u6570\u7528\u6765\u4e3a\u961f\u5217\u8bbe\u7f6e\u62d2\u7edd\u9600\u503c\u3002 \u901a\u8fc7\u8be5\u53c2\u6570\uff0c \u5373\u4f7f\u961f\u5217\u6ca1\u6709\u8fbe\u5230\u6700\u5927\u503c\u4e5f\u80fd\u62d2\u7edd\u8bf7\u6c42\u3002\n                    //\u8a72\u53c2\u6570\u4e3b\u8981\u662f\u5bfelinkedBlockingQueue \u961f\u5217\u7684\u6734\u5145,\u56e0\u4e3alinkedBlockingQueue\n                    //\u961f\u5217\u4e0d\u80fd\u52a8\u6001\u4fee\u6539\u5b83\u7684\u5bf9\u8c61\u5927\u5c0f\uff0c\u800c\u901a\u8fc7\u8be5\u5c5e\u6027\u5c31\u53ef\u4ee5\u8c03\u6574\u62d2\u7edd\u8bf7\u6c42\u7684\u961f\u5217\u5927\u5c0f\u4e86\u3002\n                    @HystrixProperty(name = \"queueSizeRejectionThreshold\", value = \"5\"),\n            }\n    )\n    ```\n\n#### \u670d\u52a1\u9650\u6d41\n\n- \u6982\u5ff5: \u9650\u6d41\u6a21\u5f0f\u4e3b\u8981\u662f\u63d0\u524d\u5bf9\u5404\u4e2a\u7c7b\u578b\u7684\u8bf7\u6c42\u8bbe\u7f6e\u6700\u9ad8\u7684QPS\u9608\u503c\uff0c\u82e5\u9ad8\u4e8e\u8bbe\u7f6e\u7684\u9608\u503c\u5219\u5bf9\u8be5\u8bf7\u6c42\u76f4\u63a5\u8fd4\u56de\uff0c\u4e0d\u518d\u8c03\u7528\u540e\u7eed\u8d44\u6e90(\u7b80\u5355\u6765\u8bf4: \u79d2\u6740\u9ad8\u5e76\u53d1\u7b49\u64cd\u4f5c\uff0c\u4e25\u7981\u4e00\u7a9d\u8702\u7684\u8fc7\u6765\u62e5\u6324\uff0c\u5927\u5bb6\u6392\u961f\uff0c\u4e00\u79d2\u949fN\u4e2a\uff0c\u6709\u5e8f\u8fdb\u884c)\n\n***\u53c2\u8003alibaba\u7684Sentinel***\n\n#### Hystrix\u5de5\u4f5c\u6d41\u7a0b\n\n[b\u7ad9\u89c6\u9891P62](https://www.bilibili.com/video/BV18E411x7eT?p=62)\n\n![Alt text](image/hystrix-command-flow-chart.png)\n\n### SpringCloud Gateway\n\n#### SpringCloud Gateway \u7279\u6027\n\n- \u57fa\u4e8eSpring Framework 5, Project Relactor\u548cSpring Boot 2.0\u8fdb\u884c\u6784\u5efa;\n- \u52a8\u6001\u8def\u7531:\u80fd\u591f\u5339\u914d\u4efb\u4f55\u8bf7\u6c42\u5c5e\u6027;\n- \u53ef\u4ee5\u5bf9\u8def\u7531\u6307\u5b9aPredicate (\u65ad\u8a00)\u548cFilter (\u8fc7\u6ee4\u5668) ;\n- \u96c6\u6210Hystrix\u7684\u65ad\u8def\u5668\u529f\u80fd;\n- \u96c6\u6210Spring Cloud\u670d\u52a1\u53d1\u73b0\u529f\u80fd;\n- \u6613\u4e8e\u7f16\u5199\u7684Predicate (\u65ad\u8a00)\u548cFilter (\u8fc7\u6ee4\u5668) ;\n- \u6e05\u6c42\u9650\u6d41\u529f\u80fd;\n- \u652f\u6301\u8def\u5f84\u91cd\u5199\u3002\n\n#### SpringCloud Gateway \u548czuul\u533a\u522b\n\n\u5728SpringCloud Finchley\u6b63\u5f0f\u7248\u4e4b\u524d\uff0cSpring Cloud\u63a8\u8350\u7684\u7f51\u5173\u662fNetflix\u63d0\u4f9b\u7684Zuul:\n\n1. Zuul 1.x, \u662f-\u4e2a\u57fa\u4e8e\u963b\u585eI/ 0\u7684API Gateway\n2. Zuul 1.x\u57fa\u4e8eServlet 2. 5\u4f7f\u7528\u963b\u585e\u67b6\u6784\u5b83\u4e0d\u652f\u6301\u4efb\u4f55\u957f\u8fde\u63a5(\u5982WebSocket) Zuul\u7684\u8bbe\u8ba1\u6a21\u5f0f\u548cNginx\u8f83\u50cf\uff0c\u6bcf\u6b21|/ 0\u64cd\u4f5c\u90fd\u662f\u4ece\u5de5\u4f5c\u7ebf\u7a0b\u4e2d\u9009\u62e9\u4e00\u4e2a\u6267\u884c,\n   \u8bf7\u6c42\u7ebf\u7a0b\u88ab\u963b\u585e\u5230\u5de5\u4f5c\u7ebf\u7a0b\u5b8c\u6210\uff0c\u4f46\u662f\u5dee\u522b\u662fNginx\u7528C++\u5b9e\u73b0\uff0cZuul \u7528Java\u5b9e\u73b0\uff0c\u800cJVM\u672c\u8eab\u4f1a\u6709\u7b2c-\u6b21\u52a0\u8f7d\u8f83\u6162\u7684\u60c5\u51b5\uff0c\u4f7f\u5f97Zuul\u7684\u6027\u80fd\u76f8\u5bf9\u8f83\u5dee\u3002\n3. Zuul 2.x\u7406\u5ff5\u66f4\u5148\u8fdb,\u60f3\u57fa\u4e8eNetty\u975e\u963b\u585e\u548c\u652f\u6301\u957f\u8fde\u63a5,\u4f46SpringCloud\u76ee\u524d\u8fd8\u6ca1\u6709\u6574\u5408\u3002Zuul 2.x\u7684\u6027\u80fd\u8f83Zuul 1.x \u6709\u8f83\u5927\u63d0\u5347\u3002\u5728\u6027\u80fd\u65b9\u9762\uff0c\u6839\u636e\u5b98\u65b9\u63d0\u4f9b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cSpring Cloud\n   Gateway\u7684RPS (\u6bcf\u79d2\u8bf7\u6c42\u6570)\u662fZuul \u76841.6\u500d\u3002\n4. Spring Cloud Gateway\u5efa\u7acb\u5728Spring Framework 5\u3001Project Reactor\u548cSpring Boot2\u4e4b\u4e0a\uff0c\u4f7f\u7528\u975e\u963b\u585eAPI\u3002\n5. Spring Cloud Gateway\u8fd8\u652f\u6301WebSocket,\u5e76\u4e14\u4e0eSpring\u7d27\u5bc6\u96c6\u6210\u62e5\u6709\u66f4\u597d\u7684\u5f00\u53d1\u4f53\u9a8c\n\n### \u6d88\u606f\u603b\u7ebf\n\n#### Rabbit MQ\n\n1. \u5b89\u88c5\n\n[\u5b98\u7f51\u5b89\u88c5\u65b9\u5f0f](https://www.rabbitmq.com/install-homebrew.html)\n\n ```shell script\n brew install rabbitmq\n ```\n\n2. \u4fee\u6539\u914d\u7f6e\n\nvim  ~/.zshrc\n\n ```shell script\n # RabbitMQ Config\n export PATH=$PATH:/usr/local/sbin\n ```\n\n3. \u542f\u52a8\n\n```shell script\n\u279c  ~ rabbitmq-server\nConfiguring logger redirection\n\n  ##  ##      RabbitMQ 3.8.8\n  ##  ##\n  ##########  Copyright (c) 2007-2020 VMware, Inc. or its affiliates.\n  ######  ##\n  ##########  Licensed under the MPL 2.0. Website: https://rabbitmq.com\n\n  Doc guides: https://rabbitmq.com/documentation.html\n  Support:    https://rabbitmq.com/contact.html\n  Tutorials:  https://rabbitmq.com/getstarted.html\n  Monitoring: https://rabbitmq.com/monitoring.html\n\n  Logs: /usr/local/var/log/rabbitmq/rabbit@localhost.log\n        /usr/local/var/log/rabbitmq/rabbit@localhost_upgrade.log\n\n  Config file(s): (none)\n\n  Starting broker... completed with 6 plugins.\n```\n\n4. \u767b\u5f55\u67e5\u770b\n   http://localhost:15672\n   \u8d26\u53f7\u5bc6\u7801:guest\n\n## Spring Cloud Alibaba\n\n[\u5b98\u7f51](https://spring.io/projects/spring-cloud-alibaba)\n\n[github\u4e2d\u6587\u6587\u6863](https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md)\n\n[github\u82f1\u6587\u6587\u6863](https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html)\n\n### Nacos\n\n1. \u7b80\u4ecb\n    - Nacos \u76f8\u5f53\u4e8e\u6ce8\u518c\u4e2d\u5fc3+\u914d\u7f6e\u4e2d\u5fc3\u7684\u7ec4\u5408 ===> Nacos=Eureka+Config+Bus\n    - [\u5b98\u7f51](https://nacos.io/zh-cn/)\n\n2. \u5b89\u88c5\n   [\u53c2\u8003\u5b98\u7f51](https://nacos.io/zh-cn/docs/quick-start.html)\n\n    - \u5355\u673a\u542f\u52a8\n\n    ```shell\n    sh startup.sh -m standalone\n    ```\n\n    - \u9047\u89c1\u62a5\u9519\n    ```\n   Caused by: org.apache.derby.iapi.error.StandardException: Failed to start database '/Users/neo/code/nacos/data/derby-data' with class loader   org.springframework.boot.loader.LaunchedURLClassLoader@7daf6ecc, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 15 common frames omitted\n    Caused by: org.apache.derby.iapi.error.StandardException: Another instance of Derby may have already booted the database /Users/neo/code/nacos/data/derby-data.\n    at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n    at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n    at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n    at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n    at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n    at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n    ```\n    - \u5220\u9664derby-data\u6587\u4ef6\u5939\u91cd\u65b0\u542f\u52a8\n      ```/Users/neo/code/nacos/data/derby-data```\n\n3. nacos\u4e0e\u5176\u5b83\u6ce8\u518c\u4e2d\u5fc3\u7279\u65b0\u5bf9\u6bd4\n\n   ![Alt text](image/nacos.jpg)\n\n4. Nacos\u652f\u6301AP\u548cCP\u6a21\u5f0f\u7684\u5207\u6362\n\n- C\u662f\u6240\u6709\u8282\u70b9\u5728\u540c\u4e00\u65f6\u95f4\u770b\u5230\u7684\u6570\u636e\u662f\u4e00\u81f4\u7684; \u800cA\u7684\u5b9a\u4e49\u662f\u6240\u6709\u7684\u8bf7\u6c42\u90fd\u4f1a\u6536\u5230\u54cd\u5e94\u3002\n- \u4f55\u65f6\u9009\u62e9\u4f7f\u7528\u4f55\u79cd\u6a21\u5f0f?\n    - \u5982\u679c\u4e0d\u9700\u8981\u5b58\u50a8\u670d\u52a1\u7ea7\u522b\u7684\u4fe1\u606f\u4e14\u670d\u52a1\u5b9e\u4f8b\u662f\u901a\u8fc7nacos- cdient\u6ce8\u518c\uff0c\u5e76\u80fd\u591f\u4fdd\u6301\u5fc3\u8df3\u4e0a\u62a5,\u90a3\u4e48\u5c31\u53ef\u4ee5\u9009\u62e9AP\u6a21\u5f0f\u3002\u5f53\u524d\u4e3b\u6d41\u7684\u670d\u52a1\u5982Spring cloud\u548cDubbo\u670d\u52a1,\u90fd\u9002\n      \u7528\u4e8eAP\u6a21\u5f0f\uff0cAP\u6a21\u5f0f\u4e3a\u4e86\u670d\u52a1\u7684\u53ef\u80fd\u6027\u800c\u51cf\u5f31\u4e86\u4e00\u81f4\u6027, \u56e0\u6b64AP\u6a21\u5f0f\u4e0b\u53ea\u652f\u6301\u6ce8\u518c\u4e34\u65f6\u5b9e\u4f8b\u3002\n    - \u5982\u679c\u9700\u8981\u5728\u670d\u52a1\u7ea7\u522b\u7f16\u8f91\u6216\u8005\u5b58\u50a8\u914d\u7f6e\u4fe1\u606f\uff0c\u90a3\u4e48CP\u662f\u5fc5\u987b\uff0cK8S\u670d\u52a1\u548cDNS\u670d\u52a1\u5219\u9002\u7528\u4e8eCP\u6a21\u5f0f\u3002\n      CP\u6a21\u5f0f\u4e0b\u5219\u652f\u6301\u6ce8\u518c\u6301\u4e45\u5316\u5b9e\u4f8b\uff0c\u6b64\u65f6\u5219\u662f\u4ee5Raft\u534f\u8bae\u4e3a\u96c6\u7fa4\u8fd0\u884c\u6a21\u5f0f\uff0c\u8be5\u6a21\u5f0f\u4e0b\u6ce8\u518c\u5b9e\u4f8b\u4e4b\u524d\u5fc5\u987b\u5148\u6ce8\u518c\u670d\u52a1\uff0c\u5982\u679c\u670d\u52a1\u4e0d\u5b58\u5728\uff0c\u5219\u4f1a\u8fd4\u56de\u9519\u8bef\u3002\n- \u5982\u4f55\u5207\u6362\n\n  ```shell script\n    curl -X PUT '$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP'\n\t``` \n\n### Nacos\u6301\u4e45\u5316\u914d\u7f6e\n\n1. \u8bf4\u660e\n\n- [\u5b98\u7f51\u94fe\u63a5](https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html)\n- \u67b6\u6784\u56fe\u7b49\u4ef7\u4e8e\n  ![Alt text](image/nacos\u96c6\u7fa4.png)\n\n2. derby\u5207\u6362mysql\u811a\u672c\n   [nacos-mysql.sql\u811a\u672c](nacos-mysql.sql)\n\n3. \u4fee\u6539application.properties,\u5e76\u91cd\u542fnacos\n\n```\n    spring.datasource.platform=mysql\n    \n    db.num=1\n    db.url.0=jdbc:mysql://11.162.196.16:3306/nacos_devtest?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true\n    db.user=nacos_devtest\n    db.password=youdontknow\n```\n\n## Nacos\u96c6\u7fa4\u914d\u7f6e\u53caNginx\u53cd\u5411\u4ee3\u7406\n\n## \u5b89\u88c5Nacos\n\n**\u672c\u6587\u57fa\u4e8eCentOS 7**\n\n[CSDN\u540c\u6b65\u66f4\u65b0](https://blog.csdn.net/yanghao937170/article/details/108750824)\n\n1. \u4e0b\u8f7dNacos\n\n   [\u4e0b\u8f7d\u5730\u5740](https://github.com/alibaba/nacos/releases/tag/1.3.2)\n2. \u914d\u7f6eNacos\n    - \u521b\u5efamynacos\u6587\u4ef6\u5939\uff1b\u5c06tar\u5305\u89e3\u538b\u5230mynacos\u6587\u4ef6\u5939\u4e2d\n    - \u914d\u7f6ecluster.conf\n\n        ```powershell\n       cp cluster.conf.example cluster.conf\n       vim cluster.conf\n         ```\n    - \u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9(\u6839\u636e\u81ea\u5df1\u7684**\u4e3b\u673aip**\u6765\u586b)\n\n        ```\n        192.168.81.129:3333\n        192.168.81.129:4444\n        192.168.81.129:5555\n        ```\n\n    - \u4fee\u6539\u6570\u636e\u5e93\u6587\u4ef6,**vim application.properties**\n\n        ```\n       spring.datasource.platform=mysql\n       db.num=1\n       db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true\n       db.user=root\n       db.password=6yhn^YHN\n        ```\n\n    - \u5173\u95ed\u9632\u706b\u5899\u6216\u8005\u5f00\u653e\u7aef\u53e3\uff08\u4e3a\u4e86\u7701\u4e8b\uff0c\u6211\u5173\u95ed\u4e86\u548c\u9632\u706b\u5899\uff09\n        ```powershell\n        systemctl stop firewalld\n        ```\n\n3. \u542f\u52a8Nacos\n   **TIPS\uff1a\u56e0\u4e3a\u662f\u4e00\u53f0\u673a\u5668\uff0c\u6240\u4ee5\u4ee5\u591a\u4e2a\u5b9e\u4f8b\u6765\u8bbe\u7f6e\u96c6\u7fa4\uff0c\u4ee5\u7aef\u53e3\u533a\u5206\uff0c\u9700\u8981\u542f\u52a8\u4e09\u4e2anacos\uff1b\u5982\u679c\u662f\u591a\u53f0\u673a\u5668\u90a3\u5c31\u6b63\u5e38\u4e00\u4e2a\u673a\u5668\u8d77\u4e00\u4e2anacos\u5373\u53ef**\n    - \u590d\u5236**startup.sh**\n\n         ```powershell\n          cp startup.sh.bak startup-3333.sh\n          cp startup.sh.bak startup-4444.sh\n          cp startup.sh.bak startup-5555.sh\n         ```\n    - \u4fee\u6539**vim startup-3333.sh**\uff08\u4e00\u4e2a\u793a\u4f8b\uff0c4444\u548c5555 \u90fd\u6309\u7167\u6b64\u4fee\u6539\uff09,\u589e\u52a0**-Dserver.port=3333**\u542f\u52a8\u65f6\u6307\u5b9a\u7aef\u53e3\u53f7\n\n        ```powershell\n         # start\n         echo \"$JAVA ${JAVA_OPT}\" > ${BASE_DIR}/logs/start.out 2>&1 &\n         nohup $JAVA -Dserver.port=3333 ${JAVA_OPT} nacos.nacos >> ${BASE_DIR}/logs/start.out 2>&1 &\n         echo \"nacos is starting\uff0cyou can check the ${BASE_DIR}/logs/start.out\"\n        ```\n\n    - \u542f\u52a8nacos\n\n        ```powershell\n       ./startup-3333.sh\n       ./startup-4444.sh\n       ./startup-5555.sh\n        ```\n\n## \u5b89\u88c5Nginx\n\n\u8d34\u4e2a\u5b89\u88c5nginx\u6559\u7a0b\uff0c\u5e76\u4e14\u914d\u7f6estream\u8d1f\u8f7d\u5747\u8861 [\u8f6c\u5411\u7f51\u5740](http://xiaohost.com/2754.html)\n\n1. \u914d\u7f6eNginx \u7531\u4e8e\u6211\u662fyum\u5b89\u88c5\u7684Nginx \u914d\u7f6e\u6587\u4ef6 **vim /etc/nginx/nginx.confg**\n    - \u8bbe\u7f6eupstream cluster\n    - \u8bbe\u7f6elocation\u6839\u8def\u5f84\u3002**proxy_pass http://cluster;**\n\n    ``` \n    #gzip  on;\n    upstream cluster{\n        server 192.168.81.129:3333;\n        server 192.168.81.129:4444;\n        server 192.168.81.129:5555;\n    }\n    server {\n        listen       1111;\n        server_name  localhost;\n\n        #charset koi8-r;\n\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n            root   html;\n            index  index.html index.htm;\n            proxy_pass http://cluster;\n        }\n    ```\n\n1. \u542f\u52a8Nginx\n\n    ```\n    systemctl start nginx.service\n    ```\n2. \u767b\u5f55\u67e5\u770b[http://192.168.81.129:1111/nacos/#/login](http://192.168.81.129:1111/nacos/#/login)\n\n## Seata\n\n1. \u6240\u9700\u7684\u5efa\u8868\u8bed\u53e5\u4ee5\u53ca\u9879\u76ee\u4e2d\u7684[\u521d\u59cb\u5316\u811a\u672csql](seata-project-undo_log.sql)\n2. \u5b9e\u73b0\u793a\u4f8b\n   ![Alt text](image/seata\u5b9e\u73b0\u8fc7\u7a0b.png)\n   \n   \n   \n   \n   \n   \n"
 },
 {
  "repo": "baoyongzhang/ParcelableGenerator",
  "language": "Java",
  "readme_contents": "ParcelableGenerator\n===================\n[ ![Download](https://api.bintray.com/packages/baoyongzhang/maven/ParcelableGenerator/images/download.svg) ](https://bintray.com/baoyongzhang/maven/ParcelableGenerator/_latestVersion)\n## \u4ecb\u7ecd\nParcelableGenerator\u53ef\u4ee5\u5c06\u4efb\u610f\u5bf9\u8c61\u8f6c\u6362\u4e3aParcelable\u7c7b\u578b\uff0c\u65b9\u4fbf\u5bf9\u8c61\u4f20\u8f93\u3002\n\n\u5728Android\u4e2d\uff0c\u5bf9\u8c61\u7684\u5e8f\u5217\u5316\u4e00\u822c\u6709\u4e24\u79cd\u65b9\u5f0f\uff0c\u4e00\u79cd\u662fSerializable\uff0c\u4e00\u79cd\u662fParcelable\u3002\n\n* Serializable \u5728Java\u4e2d\u5c31\u5b58\u5728\uff0c\u6548\u7387\u8f83\u4f4e\u3002\n* Parcelable \u662fAndroid\u4e2d\u63d0\u4f9b\u7684\uff0c\u4e5f\u662f\u5b98\u65b9\u63a8\u8350\u7684\u65b9\u5f0f\uff0c\u6548\u7387\u6bd4Serializable\u9ad8\u5f88\u591a\u3002\n\n\u867d\u7136Parcelable\u6548\u7387\u9ad8\uff0c\u4f46\u662f\u4f7f\u7528\u8d77\u6765\u6bd4Serializable\u9ebb\u70e6\u5f88\u591a\uff0c\u5f88\u591a\u4eba\u4e0d\u4f7f\u7528Parcelable\u5c31\u662f\u56e0\u4e3a\u5199\u6cd5\u592a\u9ebb\u70e6\uff0c\u5c24\u5176\u662f\u5c5e\u6027\u7279\u522b\u591a\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8981\u5c06\u6bcf\u4e2a\u5c5e\u6027Parcel.write()\u7136\u540e\u5728Parcel.read()\u56de\u6765\uff0c\u76f8\u5f53\u7e41\u7410\uff0c\u4e0d\u5982Serializable\u7b80\u5355\u7c97\u66b4\uff0c\u76f4\u63a5\u6709\u6548\u3002\n\nParcelableGenerator\u53ef\u4ee5\u89e3\u51b3Parcelable\u4f7f\u7528\u9ebb\u70e6\u7684\u95ee\u9898\uff0c\u8ba9\u4f7f\u7528Parcelable\u7684\u7b80\u5355\u6027\u53ef\u4ee5\u548c\u4f7f\u7528Serializable\u76f8\u5ab2\u7f8e\u3002\n\n\n## \u4f7f\u7528\u65b9\u6cd5\n\n\u4f8b\u5982\u6211\u4eec\u6709\u4e00\u4e2aUser\u7c7b\uff0c\u7528\u6765\u4fdd\u5b58\u7528\u6237\u7684\u4e00\u4e9b\u4fe1\u606f\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528@Parcelable\u4fee\u9970\u8be5\u7c7b\uff0c\u6ce8\u610f@Parcelable\u4fee\u9970\u7684\u7c7b\u5fc5\u987b\u6709\u516c\u6709\u65e0\u53c2\u6784\u9020\u65b9\u6cd5\u3002\n\n```java\nimport com.baoyz.pg.Parcelable;\n\n@Parcelable\npublic class User {\n\n\tprivate String name;\n\tprivate int age;\n\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\n}\n```\n\n\u6211\u4eec\u8981\u5c06\u4e00\u4e2aUser\u5bf9\u8c61\u901a\u8fc7Intent\u4f20\u9012\u7ed9\u4e00\u4e2aActivity\u53eb\u505aShowUserActivity\u3002\u6211\u4eec\u9700\u8981\u8c03\u7528Intent.putExtra()\u65b9\u6cd5\u5c06\u5bf9\u8c61\u4f20\u5165\uff0c\u8fd9\u65f6\u5019\u76f4\u63a5\u4f20\u9012\u80af\u5b9a\u662f\u4e0d\u884c\u7684\uff0c\u6211\u4eec\u9700\u8981\u8c03\u7528PG.createParcelable()\u65b9\u6cd5\u5c06\u5bf9\u8c61\u8f6c\u6362\u4e3aParcelable\u5728\u4f20\u5165Intent\u4e2d\u3002\n\n```java\nimport com.baoyz.pg.PG;\n\n// \u6a21\u62df\u521b\u5efa\u5bf9\u8c61\uff0c\u5e76\u8bbe\u7f6e\u5c5e\u6027\u503c \nUser user = new User();\nuser.setName(\"zhangsan\");\nuser.setAge(18);\n\t\t\nIntent intent = new Intent(this, ShowUserActivity.class);\n// \u8c03\u7528PG\u5c06\u5bf9\u8c61\u8f6c\u6362\u6210Parcelable\uff0c\u4f20\u5165Intent\u4e2d\nintent.putExtra(\"user\", PG.convertParcelable(user));\nstartActivity(intent);\n```\n\n\u5728ShowUserActivity\u4e2d\u83b7\u53d6User\u5bf9\u8c61\uff0c\u65e0\u9700\u5199\u4efb\u4f55\u8f6c\u6362\u7684\u4ee3\u7801\uff0c\u76f4\u63a5getIntent().getParcelableExtra()\u8d4b\u503c\u7ed9\u539f\u5bf9\u8c61\u7c7b\u578b\u53d8\u91cf\u5373\u53ef\u3002\n\n```java\npublic class ShowUserActivity extends Activity {\n\n\t@Override\n\tprotected void onCreate(Bundle savedInstanceState) {\n\t\tsuper.onCreate(savedInstanceState);\n\t\t\n\t\t// \u76f4\u63a5\u83b7\u53d6\u539f\u5bf9\u8c61\u7c7b\u578b\n\t\tUser user = getIntent().getParcelableExtra(\"user\");\n\t\t\n\t\t// \u83b7\u53d6\u5c5e\u6027\u503c\n\t\tuser.getName();\n\t\tuser.getAge();\n\t\t\n\t}\n\n}\n```\n\n#### \u5bf9\u4e8e\u7ee7\u627f: `@Parcelable` \u81ea\u52a8\u4f5c\u7528\u4e8e\u88ab\u7ee7\u627f\u7684\u7c7b, \u5b50\u7c7b\u9700\u4fee\u9970, \u4f46\u7236\u7c7b\u65e0\u9700\u4fee\u9970. \u6bd4\u5982:\n\n```java\npublic class Base {\n    private String str;\n\n    public String getStr() {\n        return str;\n    }\n\n    public void setStr(String str) {\n        this.str = str;\n    }\n}\n```\n\n\u6ce8\u610f, \u4e0a\u9762\u7684 `Base` \u662f\u6ca1\u6709\u4fee\u9970\u7684 POD.\n\n```java\n@Parcelable\npublic class Child extends Base {\n    private int i;\n\n    public int getI() {\n        return i;\n    }\n\n    public voiid setI(int i) {\n        this.i = i;\n    }\n}\n```\n\n\u6ce8\u610f, \u5b50\u7c7b `Child` \u662f\u88ab `@Parcelable` \u4fee\u9970\u7684.\n\n\u6b64\u65f6, \u6211\u4eec\u6709\u5982\u4e0b\u4ee3\u7801:\n\n```java\n        Intent intent = new Intent(this, MainActivity.class);\n        Child child = new Child();\n        child.setStr(\"child\");      // \u57fa\u7c7b\u6210\u5458\n        child.setI(1234);           // \u5b50\u7c7b\u6210\u5458\n        intent.putExtra(\"bean\", PG.convertParcelable(child));\n```\n\u57fa\u7c7b `str` \u548c\u5b50\u7c7b `i` \u4e24\u4e2a\u5b57\u6bb5\u5747\u53ef\u88ab Parcel.\n#### \u5bf9\u4e8e\u7ec4\u5408: \u9700\u8981\u88ab\u7ec4\u5408\u7684\u5bf9\u8c61\u9700 `@Parcelable` \u4fee\u9970. \u4f8b\u5982:\n```java\n@Parcelable\npublic class X {\n    private String str;\n\n    public String getStr() {\n        return str;\n    }\n\n    public void setStr(String str) {\n        this.str = str;\n    }\n}\n```\n\n```java\n@Parcelable\npublic class Y {\n    private X x;\n\n    public X getX() {\n        return x;\n    }\n\n    public void setX(X x) {\n        this.x = x;\n    }\n}\n```\n\n\u6ce8\u610f, `X` \u548c `Y` \u90fd\u9700\u8981 `@Parcelable` \u4fee\u9970.\n\n</br>\n\n## \u66f4\u65b0\u4ecb\u7ecd\n\n#### Version 2.0\n\n* \u4fee\u590dBUG\uff0c\u4f7f\u7528\u57fa\u672c\u6570\u636e\u7c7b\u578b\u5305\u88c5\u7c7b\u4f1a\u51fa\u73b0\u95ee\u9898\u7b49\u3002\n* \u589e\u52a0 @ParcelIgnore \u6ce8\u89e3\uff0c\u4fee\u9970\u5728Model\u7684Field\u4e0a\u9762\uff0c\u53ef\u4ee5\u5ffd\u7565\u8be5\u5b57\u6bb5\u4e0d\u8fdb\u884c\u5e8f\u5217\u5316\u3002\n* \u4f7f\u7528\u66f4\u52a0\u65b9\u4fbf\uff0c\u5f53Model\u4e2d\u7684\u5c5e\u6027\u662f\u5176\u4ed6\u5bf9\u8c61\uff0c\u6216\u8005List\u4e2d\u5305\u542b\u5176\u4ed6\u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u7684\u7c7b\u7528 @Parcelable \u58f0\u660e\u4e4b\u540e\u65e0\u9700\u52a0\u8f6c\u6362\u4ee3\u7801\u3002\n\n```java\n// \u5f53\u4f20\u9012\u5bf9\u8c61\u7684\u5c5e\u6027\u5305\u542b\u5176\u4ed6\u5bf9\u8c61\uff0c\u6216\u8005\u662fList\uff0c\u800c\u8be5\u5bf9\u8c61\u6216List\u4e2d\u7684\u5bf9\u8c61\u4e0d\u652f\u6301\u5e8f\u5217\u5316\uff0c\u90a3\u4e48\u76f4\u63a5\u4f20\u9012\u5c06\u4f1a\u51fa\u73b0null\n// \u89e3\u51b3\u529e\u6cd5\uff0c\u5c06\u4e0d\u652f\u6301\u5e8f\u5217\u5316\u7684\u7c7b\u7528@Parcelable\u4fee\u9970\n// \u4f8b\u5982\u4e00\u4e2a\u6559\u5ba4\u5bf9\u8c61\nClassroom room = new Classroom();\n// \u6559\u5ba4\u4e2d\u5305\u542b\u4e00\u4e2a\u8001\u5e08\uff0cTeacher\u7c7b\u7528@Parcelable\u4fee\u9970\nTeacher teacher = new Teacher(\"teacherName\");\n// \u5c06\u8001\u5e08\u5bf9\u8c61\u76f4\u63a5\u8d4b\u503c\u7ed9\u6559\u5ba4\nroom.setTeacher(teacher);\n// \u518d\u4f8b\u5982\uff0c\u6559\u5ba4\u4e2d\u5305\u542b\u5f88\u591a\u5b66\u751f\uff0c\u4f7f\u7528List\u4fdd\u5b58\uff0cStudent\u7c7b\u7528@Parcelable\u4fee\u9970\nList<Student> students = new ArrayList<Student>();\n// \u76f4\u63a5\u521b\u5efaStudent\u5bf9\u8c61\u6dfb\u52a0\u5230List\u4e2d\nstudents.add(new Student(\"stu1\"));\nstudents.add(new Student(\"stu2\"));\nstudents.add(new Student(\"stu3\"));\nroom.setStudents(students);\n// \u4f20\u9012\u6559\u5ba4\u5bf9\u8c61\uff0c\u8c03\u7528\u8f6c\u6362\u65b9\u6cd5\uff0c\u6b64\u65f6\u5185\u90e8\u4f1a\u81ea\u52a8\u5c06Teacher\u3001\u548cList\u4e2d\u7684Student\u5bf9\u8c61\u8f6c\u4e3aParcelable\u7c7b\u578b\u5e76\u4f20\u9012\nintent.putExtra(\"classroom\", PG.convertParcelable(room));\n```\n\n#### Version 1.1\n\n* Sample\u7a0b\u5e8f\u8868\u8fbe\u66f4\u52a0\u6e05\u695a\u3002\n* \u4fee\u6539\u65b9\u6cd5\u540dcreateParcelable()\u4e3aconvertParcelable()\uff0c\u539f\u65b9\u6cd5@Deprecated \u4e0d\u5f71\u54cd\u539f\u6709\u4ee3\u7801\u3002\n* \u589e\u52a0PG.convert(Object)\u65b9\u6cd5\uff0c\u4e0ecreateParcelable()\u529f\u80fd\u7c7b\u4f3c\uff0c\u53ea\u662f\u8fd4\u56de\u503c\u4e0d\u540c\uff0cconvertParcelable()\u8fd4\u56deParcelable\u7c7b\u578b\uff0cconvert()\u8fd4\u56de\u7c7b\u578b\u4e0e\u4f20\u5165\u7684\u5bf9\u8c61\u7c7b\u578b\u4e00\u81f4\uff0c\u53ea\u662f\u8be5\u5bf9\u8c61\u5df2\u7ecf\u652f\u6301\u5e8f\u5217\u5316\u3002\n\n#### Version 1.0\n\n* \u5c06\u4efb\u610f\u5bf9\u8c61\u8f6c\u6362\u4e3aParcelable\u7c7b\u578b\n\n\n## \u5728\u4f60\u7684\u9879\u76ee\u4e2d\u4f7f\u7528\n\n### Gradle\n\n```\ndependencies {\n    provided 'com.baoyz.pg:compiler:2.1.1'\n    compile 'com.baoyz.pg:core:2.1.1'\n}\n```\n"
 }
]