{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pdf plumber\n",
    "#csv.preview\n",
    "import pandas as pd\n",
    "#import unicode character database\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#import our aquire\n",
    "from acquire import *\n",
    "\n",
    "\n",
    "#import our stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_apply_join(funct,listobj):\n",
    "    'helperfuction letters'\n",
    "\n",
    "    mapped=map(funct, listobj)\n",
    "    mapped=list(mapped)\n",
    "    mapped=''.join(mapped)\n",
    "  \n",
    "    return mapped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "    'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "    #create oujr english stopwords list\n",
    "\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "    stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "    # stops=(stops|stop_words_extend)-exclude_words\n",
    "    #another way\n",
    "    \n",
    "    filtered=list(filter((lambda x: x not in stops), text.split()))\n",
    "    filtered=' '.join(filtered)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''   \n",
    "    Filters out all special characters if you need to edit then supply a new regex filter \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #make a copy and begin to transform it\n",
    "    newtext = text.lower()\n",
    "\n",
    "    #encode into ascii then decode\n",
    "    newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "\n",
    "    #use re.sub to remove special characters\n",
    "    newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return newtext\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def tokenizer(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    ''' \n",
    "    For a large file just save it locally\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "    #make ready tokenizer object\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    newtext = tokenize.tokenize(newtext, return_str=True)\n",
    "    return newtext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def stemmed(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''    \n",
    "    Takes text, tokenizes it, stems it\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    total=list(pd.read_pickle('words.pkl'))\n",
    "\n",
    "\n",
    "    \n",
    "    #make ready porter stemmer object\n",
    "    newtext=tokenizer(text,regexfilter=regexfilter)\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stemmedlist=split_apply_join(ps.stem,newtext)\n",
    "    # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "    # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "    stemfiltered=list(filter(lambda x:(len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), stemmedlist.split()))\n",
    "    stemfiltered=' '.join(stemfiltered)\n",
    " \n",
    "    stemfiltered=basic_clean(stemfiltered,regexfilter=regexfilter)\n",
    "\n",
    "    return stemfiltered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''    \n",
    "    \n",
    "      Takes text, tokenizes it, lemmatizes it\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    total=list(pd.read_pickle('words.pkl'))\n",
    "    \n",
    "\n",
    "    #make ready the lemmatizer object\n",
    "    newtext=tokenizer(text,regexfilter=regexfilter)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "\n",
    "    # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "    # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "    lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "    lemmafiltered=' '.join(lemmafiltered)\n",
    "  \n",
    "    lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "\n",
    "    return lemmafiltered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dictlist_super_NLP_comp(dictlist,regexfilter=r'[^a-z0-9\\'\\s]',stop_words_extend_reduce=[\"'\"]):\n",
    "    ''\n",
    "    ndictlist=deepcopy(dictlist)\n",
    "    mapper=[]\n",
    "    interestingkeys=list(ndictlist.keys())\n",
    "    for i in range(0,len(ndictlist)):           \n",
    "            k=interestingkeys[i]\n",
    "            text=ndictlist.get(k)         \n",
    "            org={f'org':text}\n",
    "            clean=basic_clean(text,regexfilter=regexfilter)\n",
    "            cleaned=({f'cleaned':clean})\n",
    "            lemmatized=lemmatizor(text,regexfilter)\n",
    "            stopfilteredlemitezed=stopfilter(lemmatized,stop_words_extend_reduce=stop_words_extend_reduce)\n",
    "            lemma={f'lemmatized':stopfilteredlemitezed}\n",
    "            stem=stemmed(text,regexfilter)\n",
    "            stopfilteredstem=stopfilter(stem,stop_words_extend_reduce=stop_words_extend_reduce)\n",
    "            stemma={f'stemmed':stopfilteredstem}\n",
    "            mapper.append({k:dict(**org,**cleaned,**lemma,**stemma)})              \n",
    "               \n",
    "\n",
    "\n",
    "  \n",
    "       \n",
    "    return mapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf=pd.read_json('data.json')\n",
    "\n",
    "len(datadf)\n",
    "\n",
    "# df_C=datadf[datadf['language']=='C'];df_C.head(n=30)\n",
    "\n",
    "\n",
    "# df_Python=datadf[datadf['language']=='Python'];df_Python.head(n=30)\n",
    "\n",
    "\n",
    "# df_Java=datadf[datadf['language']=='Java'];df_Java.head(n=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_articles=(datadf['readme_contents']).to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=dictlist_super_NLP_comp(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abort',\n",
       " 'abstract',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'account',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'achieve',\n",
       " 'acorn',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adapt',\n",
       " 'adaptor',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'additive',\n",
       " 'address',\n",
       " 'adjust',\n",
       " 'adoption',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'ae',\n",
       " 'affect',\n",
       " 'affine',\n",
       " 'affinity',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'agnostic',\n",
       " 'ago',\n",
       " 'agora',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aim',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'alert',\n",
       " 'alias',\n",
       " 'align',\n",
       " 'alive',\n",
       " 'allocate',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'alpha',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'although',\n",
       " 'altitude',\n",
       " 'always',\n",
       " 'amateur',\n",
       " 'amazing',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'amplify',\n",
       " 'anaconda',\n",
       " 'analogy',\n",
       " 'analysis',\n",
       " 'analyze',\n",
       " 'android',\n",
       " 'angular',\n",
       " 'animated',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'anti',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apache',\n",
       " 'apart',\n",
       " 'apogee',\n",
       " 'apparent',\n",
       " 'append',\n",
       " 'appendix',\n",
       " 'apple',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'approach',\n",
       " 'apsides',\n",
       " 'apt',\n",
       " 'ar',\n",
       " 'arc',\n",
       " 'arch',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'arena',\n",
       " 'argument',\n",
       " 'arise',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'array',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'article',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'aspect',\n",
       " 'assemble',\n",
       " 'assembly',\n",
       " 'assert',\n",
       " 'assets',\n",
       " 'assign',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'ast',\n",
       " 'atom',\n",
       " 'atomic',\n",
       " 'attach',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacker',\n",
       " 'attempt',\n",
       " 'attest',\n",
       " 'audio',\n",
       " 'author',\n",
       " 'auto',\n",
       " 'autobahn',\n",
       " 'avenger',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'await',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'axes',\n",
       " 'axis',\n",
       " 'azimuth',\n",
       " 'azure',\n",
       " 'back',\n",
       " 'backdoor',\n",
       " 'backed',\n",
       " 'bad',\n",
       " 'badge',\n",
       " 'bake',\n",
       " 'baked',\n",
       " 'baking',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'bank',\n",
       " 'banked',\n",
       " 'banner',\n",
       " 'bar',\n",
       " 'bare',\n",
       " 'barrier',\n",
       " 'base',\n",
       " 'based',\n",
       " 'bases',\n",
       " 'bash',\n",
       " 'basic',\n",
       " 'basis',\n",
       " 'basque',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'battery',\n",
       " 'beacon',\n",
       " 'beam',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bee',\n",
       " 'beetle',\n",
       " 'begin',\n",
       " 'begun',\n",
       " 'behave',\n",
       " 'behavior',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'ben',\n",
       " 'bench',\n",
       " 'bending',\n",
       " 'benefit',\n",
       " 'benign',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'bespoke',\n",
       " 'best',\n",
       " 'beta',\n",
       " 'better',\n",
       " 'beyond',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'bin',\n",
       " 'binary',\n",
       " 'bind',\n",
       " 'binding',\n",
       " 'bit',\n",
       " 'bitwise',\n",
       " 'black',\n",
       " 'blank',\n",
       " 'blazing',\n",
       " 'blending',\n",
       " 'blind',\n",
       " 'blinding',\n",
       " 'blinking',\n",
       " 'blinky',\n",
       " 'blob',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blocking',\n",
       " 'bloom',\n",
       " 'blue',\n",
       " 'blur',\n",
       " 'board',\n",
       " 'body',\n",
       " 'bogus',\n",
       " 'boiler',\n",
       " 'bold',\n",
       " 'bonus',\n",
       " 'book',\n",
       " 'bookworm',\n",
       " 'bool',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'boots',\n",
       " 'border',\n",
       " 'bottom',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'bounded',\n",
       " 'bounty',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'brake',\n",
       " 'bran',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'break',\n",
       " 'breaking',\n",
       " 'brew',\n",
       " 'bridge',\n",
       " 'bridging',\n",
       " 'brief',\n",
       " 'bring',\n",
       " 'broken',\n",
       " 'broker',\n",
       " 'browse',\n",
       " 'browser',\n",
       " 'brute',\n",
       " 'bu',\n",
       " 'buffer',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'builder',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bulk',\n",
       " 'bulky',\n",
       " 'bully',\n",
       " 'bump',\n",
       " 'bundle',\n",
       " 'bundler',\n",
       " 'burden',\n",
       " 'bureau',\n",
       " 'burnout',\n",
       " 'burrito',\n",
       " 'bus',\n",
       " 'button',\n",
       " 'buttons',\n",
       " 'buy',\n",
       " 'bypass',\n",
       " 'ca',\n",
       " 'cable',\n",
       " 'cache',\n",
       " 'cal',\n",
       " 'calendar',\n",
       " 'call',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calmer',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'campaign',\n",
       " 'cannot',\n",
       " 'canvas',\n",
       " 'canyon',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'capstone',\n",
       " 'caption',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'carbon',\n",
       " 'card',\n",
       " 'care',\n",
       " 'careful',\n",
       " 'caret',\n",
       " 'cargo',\n",
       " 'carman',\n",
       " 'carnage',\n",
       " 'carried',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'category',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'causing',\n",
       " 'caution',\n",
       " 'ce',\n",
       " 'cee',\n",
       " 'ceil',\n",
       " 'cell',\n",
       " 'cellular',\n",
       " 'center',\n",
       " 'centered',\n",
       " 'certain',\n",
       " 'chained',\n",
       " 'change',\n",
       " 'channel',\n",
       " 'chaos',\n",
       " 'chapter',\n",
       " 'char',\n",
       " 'charge',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'cheat',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'chee',\n",
       " 'child',\n",
       " 'chip',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'choosing',\n",
       " 'chose',\n",
       " 'chosen',\n",
       " 'chrome',\n",
       " 'chromium',\n",
       " 'chunk',\n",
       " 'cipher',\n",
       " 'circuit',\n",
       " 'circular',\n",
       " 'cirrus',\n",
       " 'citation',\n",
       " 'cite',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'clang',\n",
       " 'clarify',\n",
       " 'clarity',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'clause',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'cleanup',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'click',\n",
       " 'client',\n",
       " 'clone',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'cloud',\n",
       " 'cluster',\n",
       " 'cobalt',\n",
       " 'coco',\n",
       " 'cocoa',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'col',\n",
       " 'cold',\n",
       " 'college',\n",
       " 'color',\n",
       " 'colored',\n",
       " 'colorful',\n",
       " 'colors',\n",
       " 'colour',\n",
       " 'column',\n",
       " 'combine',\n",
       " 'combined',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comet',\n",
       " 'comma',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'commit',\n",
       " 'common',\n",
       " 'commonly',\n",
       " 'compact',\n",
       " 'company',\n",
       " 'compare',\n",
       " 'compile',\n",
       " 'compiler',\n",
       " 'complete',\n",
       " 'complex',\n",
       " 'compo',\n",
       " 'composed',\n",
       " 'composer',\n",
       " 'compound',\n",
       " 'compress',\n",
       " 'compute',\n",
       " 'computer',\n",
       " 'concept',\n",
       " 'concern',\n",
       " 'concise',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'confused',\n",
       " 'connect',\n",
       " 'consider',\n",
       " 'consist',\n",
       " 'console',\n",
       " 'constant',\n",
       " 'consult',\n",
       " 'consume',\n",
       " 'consumer',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'content',\n",
       " 'contents',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'contract',\n",
       " 'control',\n",
       " 'convert',\n",
       " 'cool',\n",
       " 'copied',\n",
       " 'copy',\n",
       " 'core',\n",
       " 'corner',\n",
       " 'corpora',\n",
       " 'corpus',\n",
       " 'correct',\n",
       " 'cortex',\n",
       " 'cosmic',\n",
       " 'costly',\n",
       " 'could',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'counting',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'covid',\n",
       " 'cracked',\n",
       " 'crash',\n",
       " 'crate',\n",
       " 'crawl',\n",
       " 'create',\n",
       " 'creation',\n",
       " 'creative',\n",
       " 'creator',\n",
       " 'credit',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'critical',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'cry',\n",
       " 'crystal',\n",
       " 'cubic',\n",
       " 'culling',\n",
       " 'culture',\n",
       " 'cup',\n",
       " 'cur',\n",
       " 'curl',\n",
       " 'current',\n",
       " 'curve',\n",
       " 'custom',\n",
       " 'customer',\n",
       " 'cutoff',\n",
       " 'cyan',\n",
       " 'cycle',\n",
       " 'cypress',\n",
       " 'daemon',\n",
       " 'dag',\n",
       " 'daily',\n",
       " 'damage',\n",
       " 'damages',\n",
       " 'dare',\n",
       " 'darken',\n",
       " 'dash',\n",
       " 'data',\n",
       " 'date',\n",
       " 'dawn',\n",
       " 'day',\n",
       " 'de',\n",
       " 'deal',\n",
       " 'dealing',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'declare',\n",
       " 'declared',\n",
       " 'decrease',\n",
       " 'deem',\n",
       " 'deep',\n",
       " 'deer',\n",
       " 'default',\n",
       " 'defer',\n",
       " 'deferred',\n",
       " 'define',\n",
       " 'defined',\n",
       " 'deflate',\n",
       " 'deform',\n",
       " 'defunct',\n",
       " 'degree',\n",
       " 'delay',\n",
       " 'delaying',\n",
       " 'delegate',\n",
       " 'delete',\n",
       " 'deletion',\n",
       " 'delta',\n",
       " 'demand',\n",
       " 'demos',\n",
       " 'denial',\n",
       " 'denote',\n",
       " 'density',\n",
       " 'depend',\n",
       " 'deploy',\n",
       " 'depth',\n",
       " 'derive',\n",
       " 'derived',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'desired',\n",
       " 'despite',\n",
       " 'detach',\n",
       " 'detail',\n",
       " 'detailed',\n",
       " 'detect',\n",
       " 'dev',\n",
       " 'develop',\n",
       " 'device',\n",
       " 'devoted',\n",
       " 'differ',\n",
       " 'digest',\n",
       " 'digital',\n",
       " 'dilation',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'director',\n",
       " 'disable',\n",
       " 'disabled',\n",
       " 'disc',\n",
       " 'disclose',\n",
       " 'discord',\n",
       " 'discover',\n",
       " 'disk',\n",
       " 'disown',\n",
       " 'dispatch',\n",
       " 'display',\n",
       " 'distance',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'division',\n",
       " 'doc',\n",
       " 'docker',\n",
       " 'document',\n",
       " 'dolphin',\n",
       " 'dom',\n",
       " 'domain',\n",
       " 'donate',\n",
       " 'donation',\n",
       " 'done',\n",
       " 'door',\n",
       " 'dos',\n",
       " 'dose',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'doubling',\n",
       " 'draft',\n",
       " 'dragged',\n",
       " 'dramatic',\n",
       " 'draw',\n",
       " 'drawing',\n",
       " 'dream',\n",
       " 'drew',\n",
       " 'drive',\n",
       " 'driven',\n",
       " 'driver',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'dual',\n",
       " 'due',\n",
       " 'dumb',\n",
       " 'dump',\n",
       " 'dumping',\n",
       " 'duration',\n",
       " 'dusk',\n",
       " 'dutch',\n",
       " 'duty',\n",
       " 'dynamic',\n",
       " 'eager',\n",
       " 'eagle',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'ease',\n",
       " 'easier',\n",
       " 'easiest',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'echo',\n",
       " 'eclipse',\n",
       " 'ecliptic',\n",
       " 'edge',\n",
       " 'edit',\n",
       " 'edition',\n",
       " 'editor',\n",
       " 'effect',\n",
       " 'effects',\n",
       " 'effort',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'el',\n",
       " 'elastic',\n",
       " 'elegant',\n",
       " 'element',\n",
       " 'elevated',\n",
       " 'elf',\n",
       " 'elicit',\n",
       " 'elite',\n",
       " 'elixir',\n",
       " 'elliptic',\n",
       " 'else',\n",
       " 'embed',\n",
       " 'empire',\n",
       " 'employer',\n",
       " 'empty',\n",
       " 'emu',\n",
       " 'emulate',\n",
       " 'emulator',\n",
       " 'en',\n",
       " 'enable',\n",
       " 'encode',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'endorsed',\n",
       " 'energy',\n",
       " 'engage',\n",
       " 'engine',\n",
       " 'enhanced',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'ensue',\n",
       " 'ensure',\n",
       " 'entail',\n",
       " 'enter',\n",
       " 'entering',\n",
       " 'entire',\n",
       " 'entirely',\n",
       " 'entry',\n",
       " 'epic',\n",
       " 'episode',\n",
       " 'equal',\n",
       " 'equality',\n",
       " 'equation',\n",
       " 'equator',\n",
       " 'er',\n",
       " 'erase',\n",
       " 'erased',\n",
       " 'erotica',\n",
       " 'err',\n",
       " 'error',\n",
       " 'es',\n",
       " 'escape',\n",
       " 'essence',\n",
       " 'evaluate',\n",
       " 'evasion',\n",
       " 'even',\n",
       " 'event',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'evident',\n",
       " 'evil',\n",
       " 'ex',\n",
       " 'exact',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'exceed',\n",
       " 'except',\n",
       " 'exchange',\n",
       " 'excited',\n",
       " 'exclude',\n",
       " 'execute',\n",
       " 'executed',\n",
       " 'exist',\n",
       " 'existent',\n",
       " 'exit',\n",
       " 'exotic',\n",
       " 'expand',\n",
       " 'expanded',\n",
       " 'expect',\n",
       " 'expense',\n",
       " 'explain',\n",
       " 'explicit',\n",
       " 'explore',\n",
       " 'explorer',\n",
       " 'exponent',\n",
       " 'export',\n",
       " 'expose',\n",
       " 'exposed',\n",
       " 'express',\n",
       " 'extend',\n",
       " 'extended',\n",
       " 'extent',\n",
       " 'external',\n",
       " 'extra',\n",
       " 'extract',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'factor',\n",
       " 'fade',\n",
       " 'fail',\n",
       " 'failure',\n",
       " 'faint',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fairy',\n",
       " 'falling',\n",
       " 'false',\n",
       " 'fame',\n",
       " 'familiar',\n",
       " 'family',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fatal',\n",
       " 'favorite',\n",
       " 'fear',\n",
       " 'feasible',\n",
       " 'feature',\n",
       " 'featured',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'felt',\n",
       " 'fetch',\n",
       " 'fi',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'fill',\n",
       " 'filter',\n",
       " 'fin',\n",
       " 'final',\n",
       " 'finalize',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'fire',\n",
       " 'firefly',\n",
       " 'firing',\n",
       " 'first',\n",
       " 'fit',\n",
       " 'fitness',\n",
       " 'fix',\n",
       " 'fixed',\n",
       " 'fixing',\n",
       " 'flag',\n",
       " 'flash',\n",
       " 'flashing',\n",
       " 'flat',\n",
       " 'flavored',\n",
       " 'flexible',\n",
       " 'flight',\n",
       " 'flip',\n",
       " 'float',\n",
       " 'floating',\n",
       " 'flood',\n",
       " 'floppy',\n",
       " 'flow',\n",
       " 'flux',\n",
       " 'focus',\n",
       " 'fog',\n",
       " 'folder',\n",
       " 'foliage',\n",
       " 'follow',\n",
       " 'folly',\n",
       " 'font',\n",
       " 'foo',\n",
       " 'force',\n",
       " 'forcibly',\n",
       " 'foreign',\n",
       " 'forensic',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'fork',\n",
       " 'forked',\n",
       " 'form',\n",
       " 'format',\n",
       " 'formed',\n",
       " 'former',\n",
       " 'formula',\n",
       " 'formulae',\n",
       " 'forum',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'frame',\n",
       " 'free',\n",
       " 'freed',\n",
       " 'freely',\n",
       " 'friction',\n",
       " 'friendly',\n",
       " 'frozen',\n",
       " 'fruit',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'function',\n",
       " 'fund',\n",
       " 'funky',\n",
       " 'fusion',\n",
       " 'future',\n",
       " 'fuzz',\n",
       " 'gain',\n",
       " 'gains',\n",
       " 'galactic',\n",
       " 'gale',\n",
       " 'gallery',\n",
       " 'game',\n",
       " 'gamma',\n",
       " 'garbage',\n",
       " 'gargoyle',\n",
       " 'garland',\n",
       " 'gather',\n",
       " 'gear',\n",
       " 'gem',\n",
       " 'gen',\n",
       " 'general',\n",
       " 'generate',\n",
       " 'generic',\n",
       " 'generous',\n",
       " 'genesis',\n",
       " 'german',\n",
       " 'get',\n",
       " 'getter',\n",
       " 'getting',\n",
       " 'ghost',\n",
       " 'gib',\n",
       " 'gid',\n",
       " 'gif',\n",
       " 'git',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'glib',\n",
       " 'global',\n",
       " 'gnome',\n",
       " 'gnu',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'golf',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'grab',\n",
       " 'grade',\n",
       " 'grained',\n",
       " 'grammar',\n",
       " 'grand',\n",
       " 'graph',\n",
       " 'graphic',\n",
       " 'graphics',\n",
       " 'grapple',\n",
       " 'gray',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatly',\n",
       " 'green',\n",
       " 'grey',\n",
       " 'grid',\n",
       " 'gross',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'guard',\n",
       " 'guess',\n",
       " 'guide',\n",
       " 'guile',\n",
       " 'gun',\n",
       " 'gym',\n",
       " 'gyp',\n",
       " 'hack',\n",
       " 'hacked',\n",
       " 'hacking',\n",
       " 'hacky',\n",
       " 'half',\n",
       " 'halt',\n",
       " 'halting',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'handled',\n",
       " 'handler',\n",
       " 'handling',\n",
       " 'happen',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'harness',\n",
       " 'hash',\n",
       " 'hawk',\n",
       " 'head',\n",
       " 'headache',\n",
       " 'header',\n",
       " 'health',\n",
       " 'heap',\n",
       " 'heaps',\n",
       " 'heart',\n",
       " 'heaven',\n",
       " 'heavens',\n",
       " 'heavily',\n",
       " 'heavy',\n",
       " 'hedging',\n",
       " 'height',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'helper',\n",
       " 'helpful',\n",
       " 'hence',\n",
       " 'hereby',\n",
       " 'hertz',\n",
       " 'hex',\n",
       " 'hidden',\n",
       " 'hide',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'highly',\n",
       " 'hill',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hobby',\n",
       " 'hold',\n",
       " 'holding',\n",
       " 'hole',\n",
       " 'home',\n",
       " 'honest',\n",
       " 'honeypot',\n",
       " 'honk',\n",
       " 'honor',\n",
       " 'hood',\n",
       " 'hook',\n",
       " 'hope',\n",
       " 'horizon',\n",
       " 'host',\n",
       " 'hostile',\n",
       " 'hosting',\n",
       " 'hotness',\n",
       " 'however',\n",
       " 'hub',\n",
       " 'hud',\n",
       " 'hue',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'humble',\n",
       " 'hungry',\n",
       " 'hunt',\n",
       " 'hunting',\n",
       " 'hyper',\n",
       " 'icon',\n",
       " 'id',\n",
       " 'ide',\n",
       " 'idea',\n",
       " 'ideal',\n",
       " 'identify',\n",
       " 'ides',\n",
       " 'idiot',\n",
       " 'ignore',\n",
       " 'illegal',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=list(pd.read_pickle('words.pkl'))\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=pd.DataFrame(newdf[0]).T\n",
    "for i in range(1,len(newdf)):\n",
    "    b=pd.DataFrame(newdf[i]).T\n",
    "    a=pd.merge(a,b,how='outer',right_index=False,left_index=False)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>org</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cstack/db_tutorial</td>\n",
       "      <td>C</td>\n",
       "      <td># Let's Build a Simple Database\\n\\n[View rende...</td>\n",
       "      <td>let's build a simple database\\n\\n view rende...</td>\n",
       "      <td>let build simple view tutorial io tutorial run...</td>\n",
       "      <td># Let's Build a Simple Database\\n\\n[View rende...</td>\n",
       "      <td>let build simple view tutorial io tutorial run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rui314/chibicc</td>\n",
       "      <td>C</td>\n",
       "      <td># chibicc: A Small C Compiler\\n\\n(The old mast...</td>\n",
       "      <td>chibicc  a small c compiler\\n\\n the old mast...</td>\n",
       "      <td>small compiler old master old tree old branch ...</td>\n",
       "      <td># chibicc: A Small C Compiler\\n\\n(The old mast...</td>\n",
       "      <td>small compiler old master old tree old branch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nelhage/reptyr</td>\n",
       "      <td>C</td>\n",
       "      <td>reptyr - A tool for \"re-ptying\" programs.\\n===...</td>\n",
       "      <td>reptyr   a tool for  re ptying  programs \\n   ...</td>\n",
       "      <td>tool utility taking running program new termin...</td>\n",
       "      <td>reptyr - A tool for \"re-ptying\" programs.\\n===...</td>\n",
       "      <td>tool utility taking running program new termin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EyalAr/lwip</td>\n",
       "      <td>C</td>\n",
       "      <td>[![Version](http://img.shields.io/npm/v/lwip.s...</td>\n",
       "      <td>version  http   img shields io npm v lwip s...</td>\n",
       "      <td>version io package build status travis branch ...</td>\n",
       "      <td>[![Version](http://img.shields.io/npm/v/lwip.s...</td>\n",
       "      <td>version io package build status travis branch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ibireme/yyjson</td>\n",
       "      <td>C</td>\n",
       "      <td>\\n# Introduction\\n\\n[![Build](https://flat.bad...</td>\n",
       "      <td>\\n  introduction\\n\\n   build  https   flat bad...</td>\n",
       "      <td>build flat net status master flat net io licen...</td>\n",
       "      <td>\\n# Introduction\\n\\n[![Build](https://flat.bad...</td>\n",
       "      <td>build flat net status master flat net io licen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 repo language  \\\n",
       "0  cstack/db_tutorial        C   \n",
       "1      rui314/chibicc        C   \n",
       "2      nelhage/reptyr        C   \n",
       "3         EyalAr/lwip        C   \n",
       "4      ibireme/yyjson        C   \n",
       "\n",
       "                                     readme_contents  \\\n",
       "0  # Let's Build a Simple Database\\n\\n[View rende...   \n",
       "1  # chibicc: A Small C Compiler\\n\\n(The old mast...   \n",
       "2  reptyr - A tool for \"re-ptying\" programs.\\n===...   \n",
       "3  [![Version](http://img.shields.io/npm/v/lwip.s...   \n",
       "4  \\n# Introduction\\n\\n[![Build](https://flat.bad...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0    let's build a simple database\\n\\n view rende...   \n",
       "1    chibicc  a small c compiler\\n\\n the old mast...   \n",
       "2  reptyr   a tool for  re ptying  programs \\n   ...   \n",
       "3     version  http   img shields io npm v lwip s...   \n",
       "4  \\n  introduction\\n\\n   build  https   flat bad...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  let build simple view tutorial io tutorial run...   \n",
       "1  small compiler old master old tree old branch ...   \n",
       "2  tool utility taking running program new termin...   \n",
       "3  version io package build status travis branch ...   \n",
       "4  build flat net status master flat net io licen...   \n",
       "\n",
       "                                                 org  \\\n",
       "0  # Let's Build a Simple Database\\n\\n[View rende...   \n",
       "1  # chibicc: A Small C Compiler\\n\\n(The old mast...   \n",
       "2  reptyr - A tool for \"re-ptying\" programs.\\n===...   \n",
       "3  [![Version](http://img.shields.io/npm/v/lwip.s...   \n",
       "4  \\n# Introduction\\n\\n[![Build](https://flat.bad...   \n",
       "\n",
       "                                             stemmed  \n",
       "0  let build simple view tutorial io tutorial run...  \n",
       "1  small compiler old master old tree old branch ...  \n",
       "2  tool utility taking running program new termin...  \n",
       "3  version io package build status travis branch ...  \n",
       "4  build flat net status master flat net io licen...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cstack/db_tutorial</td>\n",
       "      <td>C</td>\n",
       "      <td># Let's Build a Simple Database\\n\\n[View rende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rui314/chibicc</td>\n",
       "      <td>C</td>\n",
       "      <td># chibicc: A Small C Compiler\\n\\n(The old mast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nelhage/reptyr</td>\n",
       "      <td>C</td>\n",
       "      <td>reptyr - A tool for \"re-ptying\" programs.\\n===...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EyalAr/lwip</td>\n",
       "      <td>C</td>\n",
       "      <td>[![Version](http://img.shields.io/npm/v/lwip.s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ibireme/yyjson</td>\n",
       "      <td>C</td>\n",
       "      <td>\\n# Introduction\\n\\n[![Build](https://flat.bad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 repo language  \\\n",
       "0  cstack/db_tutorial        C   \n",
       "1      rui314/chibicc        C   \n",
       "2      nelhage/reptyr        C   \n",
       "3         EyalAr/lwip        C   \n",
       "4      ibireme/yyjson        C   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # Let's Build a Simple Database\\n\\n[View rende...  \n",
       "1  # chibicc: A Small C Compiler\\n\\n(The old mast...  \n",
       "2  reptyr - A tool for \"re-ptying\" programs.\\n===...  \n",
       "3  [![Version](http://img.shields.io/npm/v/lwip.s...  \n",
       "4  \\n# Introduction\\n\\n[![Build](https://flat.bad...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.merge(datadf,a,how='left',right_index=True,left_index=True)\n",
    "display(df.head(),datadf.head())## Verify merge\n",
    "datadf=df #finish prepped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reorder=['language',\n",
    "'repo',\n",
    " 'readme_contents',\n",
    " 'cleaned',\n",
    " 'stemmed',\n",
    " 'lemmatized']\n",
    "  \n",
    "datadf=datadf[reorder]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datadf\n",
    "pd.to_pickle(datadf,'cleanpickle.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
